#!/usr/bin/env python3
"""Custom UMAP visualizations for retain / forget / OOD splits.

This script reproduces the Mahalanobis evaluation UMAP plots with custom
markers and sampling options tailored for the forgetful learning scenario.
It generates 2 × N figures (ID picking either 20- or 90-class subsets) for
each requested OOD dataset.

Example usage:

    python scripts/umap_custom_visualization.py \
        --in-dataset CIFAR-100 \
        --backbone resnet34 \
        --method-tag "top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid" \
        --pretrain-ckpt checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt \
        --adapter-path checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter \
        --forget-classes "0,8,11,40,51,57,66,67,88,94" \
        --ood-datasets "SVHN places365 LSUN iSUN dtd"

The script expects feature caches generated by the evaluation pipeline under
``cache/{backbone}-{method-tag}/{in_dataset}``.
"""

from __future__ import annotations

import argparse
import hashlib
import os
import subprocess
import sys
from dataclasses import dataclass
from pathlib import Path
from typing import List, Optional, Sequence, Tuple

import numpy as np


PROJECT_ROOT = Path(__file__).resolve().parent.parent
FEATURE_EXTRACT = PROJECT_ROOT / "feature_extract.py"
ELLIPSE_DIAMETER = 2.5


def _lazy_import_umap():
    try:
        import umap.umap_ as umap  # type: ignore
    except ImportError as exc:  # pragma: no cover - dependency availability
        raise RuntimeError(
            "Failed to import umap.umap_. Install umap-learn first:"
            " pip install umap-learn"
        ) from exc
    return umap


def _lazy_import_matplotlib():
    try:
        import matplotlib.pyplot as plt  # type: ignore
        from matplotlib.patches import Ellipse  # type: ignore
    except ImportError as exc:  # pragma: no cover - dependency availability
        raise RuntimeError(
            "Failed to import matplotlib. Install it first:"
            " pip install matplotlib"
        ) from exc
    return plt, Ellipse


def parse_int_list(raw: str) -> List[int]:
    if not raw:
        return []
    items = []
    for token in raw.replace(";", ",").split(","):
        tok = token.strip()
        if not tok:
            continue
        try:
            items.append(int(tok))
        except ValueError:
            raise ValueError(f"Cannot parse integer from '{tok}'")
    return items


def parse_str_list(raw: str) -> List[str]:
    if not raw:
        return []
    tokens = raw.split()
    if len(tokens) == 1 and "," in raw:
        tokens = [tok.strip() for tok in raw.split(",") if tok.strip()]
    return tokens


def normalize_features(features: np.ndarray) -> np.ndarray:
    norms = np.linalg.norm(features, ord=2, axis=-1, keepdims=True)
    return features / (norms + 1e-10)


def load_feature_split(
    cache_root: str,
    backbone: str,
    method_tag: str,
    in_dataset: str,
    split: str,
) -> np.ndarray:
    feat_path = os.path.join(
        cache_root,
        f"{backbone}-{method_tag}",
        in_dataset,
        f"{split}_{backbone}-{method_tag}_features.npy",
    )
    if not os.path.exists(feat_path):
        raise FileNotFoundError(f"Missing feature cache: {feat_path}")
    feats = np.load(feat_path, allow_pickle=True).astype(np.float32)
    return normalize_features(feats)


def load_label_split(
    cache_root: str,
    backbone: str,
    method_tag: str,
    in_dataset: str,
    split: str,
) -> np.ndarray:
    label_path = os.path.join(
        cache_root,
        f"{backbone}-{method_tag}",
        in_dataset,
        f"{split}_{backbone}-{method_tag}_labels.npy",
    )
    if not os.path.exists(label_path):
        raise FileNotFoundError(f"Missing label cache: {label_path}")
    labels = np.load(label_path, allow_pickle=True)
    return labels.astype(np.int64)


def load_forget_features(
    cache_root: str,
    backbone: str,
    method_tag: str,
    in_dataset: str,
) -> Optional[np.ndarray]:
    feat_path = os.path.join(
        cache_root,
        f"{backbone}-{method_tag}",
        in_dataset,
        "forget",
        f"{backbone}-{method_tag}_features.npy",
    )
    if not os.path.exists(feat_path):
        return None
    feats = np.load(feat_path, allow_pickle=True).astype(np.float32)
    return normalize_features(feats)


def load_ood_features(
    cache_root: str,
    backbone: str,
    method_tag: str,
    in_dataset: str,
    dataset: str,
) -> np.ndarray:
    feat_path = os.path.join(
        cache_root,
        f"{backbone}-{method_tag}",
        in_dataset,
        dataset,
        f"{backbone}-{method_tag}_features.npy",
    )
    if not os.path.exists(feat_path):
        raise FileNotFoundError(f"Missing OOD feature cache: {feat_path}")
    feats = np.load(feat_path, allow_pickle=True).astype(np.float32)
    return normalize_features(feats)


def select_id_classes(
    retain_classes: Sequence[int],
    target_count: int,
    rng: np.random.Generator,
) -> List[int]:
    retain = list(sorted(set(int(c) for c in retain_classes)))
    if target_count >= len(retain):
        return retain
    idx = rng.choice(len(retain), size=target_count, replace=False)
    selected = [retain[i] for i in np.sort(idx)]
    return selected


def sample_by_class(
    features: np.ndarray,
    labels: np.ndarray,
    classes: Sequence[int],
    per_class_limit: int,
    rng: np.random.Generator,
) -> np.ndarray:
    selected_vectors: List[np.ndarray] = []
    for cls in classes:
        cls_mask = labels == cls
        cls_feats = features[cls_mask]
        if cls_feats.size == 0:
            continue
        limit = cls_feats.shape[0] if per_class_limit <= 0 else min(per_class_limit, cls_feats.shape[0])
        if cls_feats.shape[0] > limit:
            idx = rng.choice(cls_feats.shape[0], size=limit, replace=False)
            cls_feats = cls_feats[idx]
        selected_vectors.append(cls_feats)
    if not selected_vectors:
        return np.zeros((0, features.shape[1]), dtype=features.dtype)
    return np.concatenate(selected_vectors, axis=0)


def sample_limit(arr: Optional[np.ndarray], limit: int, rng: np.random.Generator) -> Optional[np.ndarray]:
    if arr is None or arr.size == 0:
        return None
    if limit > 0 and arr.shape[0] > limit:
        idx = rng.choice(arr.shape[0], size=limit, replace=False)
        return arr[idx]
    return arr


@dataclass
class UmapInputs:
    id_embeddings: np.ndarray
    ood_embeddings: np.ndarray
    forget_embeddings: Optional[np.ndarray]


def build_embeddings(
    cache_root: str,
    backbone: str,
    method_tag: str,
    in_dataset: str,
    forget_classes: Sequence[int],
    id_class_count: int,
    ood_dataset: str,
    seed: int,
    id_per_class: int,
    ood_limit: int,
    forget_limit: int,
) -> UmapInputs:
    rng = np.random.default_rng(seed)

    val_feats = load_feature_split(cache_root, backbone, method_tag, in_dataset, "val")
    val_labels = load_label_split(cache_root, backbone, method_tag, in_dataset, "val")

    all_classes = sorted(int(c) for c in np.unique(val_labels))
    forget_set = set(int(c) for c in forget_classes)
    retain_classes = [c for c in all_classes if c not in forget_set]
    if not retain_classes:
        raise RuntimeError("No retain classes available after excluding forget set.")

    id_classes = select_id_classes(retain_classes, id_class_count, rng)
    id_embeddings = sample_by_class(val_feats, val_labels, id_classes, id_per_class, rng)

    ood_embeddings_full = load_ood_features(cache_root, backbone, method_tag, in_dataset, ood_dataset)
    ood_embeddings = sample_limit(ood_embeddings_full, ood_limit, rng)
    if ood_embeddings is None:
        raise RuntimeError(f"No OOD embeddings found for dataset '{ood_dataset}'.")

    forget_embeddings_full = load_forget_features(cache_root, backbone, method_tag, in_dataset)
    forget_embeddings = sample_limit(forget_embeddings_full, forget_limit, rng)

    return UmapInputs(id_embeddings=id_embeddings, ood_embeddings=ood_embeddings, forget_embeddings=forget_embeddings)


def run_umap(embeddings: Sequence[np.ndarray], metric: str, n_neighbors: int, min_dist: float, seed: int) -> np.ndarray:
    umap = _lazy_import_umap()
    reducer = umap.UMAP(
        n_neighbors=n_neighbors,
        min_dist=min_dist,
        metric=metric,
        random_state=seed,
        transform_seed=seed,
    )
    stacked = np.concatenate(embeddings, axis=0)
    return reducer.fit_transform(stacked)


def add_ellipse(ax, points: np.ndarray, color: str, alpha: float, zorder: int, lw: float = 0.0, ls: str = "solid") -> None:
    if points.shape[0] < 3:
        return
    center = points.mean(axis=0)
    cov = np.cov(points, rowvar=False)
    eigvals, eigvecs = np.linalg.eigh(cov)
    order = np.argsort(eigvals)[::-1]
    eigvals = eigvals[order]
    eigvecs = eigvecs[:, order]
    # 2 * sqrt(eigenvalue) approximates 1-sigma spread; scale slightly for aesthetics
    width, height = 2.5 * np.sqrt(np.maximum(eigvals, 1e-6))
    angle = np.degrees(np.arctan2(eigvecs[1, 0], eigvecs[0, 0]))
    plt, Ellipse = _lazy_import_matplotlib()
    ellipse = Ellipse(
        xy=center,
        width=width,
        height=height,
        angle=angle,
        facecolor=color,
        edgecolor=color if lw > 0 else color,
        alpha=alpha,
        linewidth=lw,
        linestyle=ls,
        zorder=zorder,
    )
    ax.add_patch(ellipse)


def _ellipse_stats(points: np.ndarray) -> Optional[Tuple[np.ndarray, np.ndarray, np.ndarray]]:
    if points.shape[0] < 3:
        return None
    center = points.mean(axis=0)
    cov = np.cov(points, rowvar=False)
    eigvals, eigvecs = np.linalg.eigh(cov)
    order = np.argsort(eigvals)[::-1]
    eigvals = eigvals[order]
    eigvecs = eigvecs[:, order]
    inv_cov = np.linalg.pinv(cov)
    return center, eigvals, inv_cov


def _mahalanobis_distance(points: np.ndarray, center: np.ndarray, inv_cov: np.ndarray) -> np.ndarray:
    delta = points - center
    md2 = np.einsum("nd,dd,nd->n", delta, inv_cov, delta)
    return np.sqrt(np.maximum(md2, 0.0))


def _mahalanobis_dropout(
    points: np.ndarray,
    center: np.ndarray,
    inv_cov: np.ndarray,
    scale: float,
    label: str,
    context: str,
    rng: np.random.Generator,
    ellipse_radius: float,
) -> Tuple[np.ndarray, np.ndarray]:
    if points.shape[0] == 0 or scale <= 0:
        return points, np.empty((0,), dtype=float)
    md = _mahalanobis_distance(points, center, inv_cov)
    outside = md > ellipse_radius
    if not np.any(outside):
        return points, md
    probs = np.zeros_like(md)
    probs[outside] = np.clip((md[outside] - ellipse_radius) / scale, 0.0, 1.0)
    keep_mask = rng.random(size=points.shape[0]) >= probs
    kept = int(keep_mask.sum())
    removed = keep_mask.size - kept
    if kept == 0:
        idx = int(np.argmin(md))
        keep_mask[idx] = True
        kept = 1
        removed = keep_mask.size - 1
    if removed > 0:
        pct = removed / keep_mask.size
        print(f"[umap] stochastic drop {removed} ({pct:.1%}) {label} points for {context}")
    return points[keep_mask], md[keep_mask]


def _filter_outliers_with_mahalanobis(
    points: np.ndarray,
    md: np.ndarray,
    quantile: float,
    label: str,
    context: str,
) -> Tuple[np.ndarray, np.ndarray]:
    if points.shape[0] == 0 or md.size == 0 or quantile >= 1.0 or quantile <= 0.0:
        return points, md
    thresh = np.quantile(md, quantile)
    mask = md <= thresh
    kept = int(mask.sum())
    removed = int(mask.size - kept)
    if kept == 0:
        idx = int(np.argmin(md))
        mask[idx] = True
        kept = 1
        removed = mask.size - 1
    if removed > 0:
        pct = removed / mask.size
        print(f"[umap] trimmed {removed} ({pct:.1%}) {label} points for {context}")
    return points[mask], md[mask]


def plot_embeddings(
    umap_coords: np.ndarray,
    counts: Tuple[int, int, Optional[int]],
    id_label: str,
    ood_label: str,
    forget_label: str,
    title: str,
    output_path: str,
    ellipse: bool = True,
    outlier_quantile: float = 1.0,
    stochastic_dropout_scale: float = 0.0,
    rng: Optional[np.random.Generator] = None,
) -> None:
    plt, _ = _lazy_import_matplotlib()
    id_count, ood_count, forget_count = counts

    markers = {
        id_label: ("o", "#1f77b4"),
        ood_label: ("^", "#ff7f0e"),
        forget_label: ("s", "#2ca02c"),
    }
    zorders = {
        id_label: 6,
        ood_label: 4,
        forget_label: 5,
    }

    id_slice = umap_coords[:id_count]
    ood_slice = umap_coords[id_count:id_count + ood_count]
    forget_slice = None
    if forget_count:
        forget_slice = umap_coords[id_count + ood_count: id_count + ood_count + forget_count]

    id_raw = id_slice.copy()
    ood_raw = ood_slice.copy()
    forget_raw = forget_slice.copy() if forget_slice is not None else None

    stats_id_raw = _ellipse_stats(id_raw)
    stats_ood_raw = _ellipse_stats(ood_raw)
    stats_forget_raw = _ellipse_stats(forget_raw) if forget_raw is not None else None

    ellipse_radius = ELLIPSE_DIAMETER / 2.0
    context = os.path.basename(output_path)

    md_id = _mahalanobis_distance(id_raw, stats_id_raw[0], stats_id_raw[2]) if stats_id_raw else np.empty((0,), dtype=float)
    md_ood = _mahalanobis_distance(ood_raw, stats_ood_raw[0], stats_ood_raw[2]) if stats_ood_raw else np.empty((0,), dtype=float)
    md_forget = _mahalanobis_distance(forget_raw, stats_forget_raw[0], stats_forget_raw[2]) if stats_forget_raw else np.empty((0,), dtype=float)

    if stats_id_raw:
        id_slice, md_id = _filter_outliers_with_mahalanobis(id_raw, md_id, outlier_quantile, "ID", context)
        stats_id = _ellipse_stats(id_slice)
    else:
        stats_id = None

    if stats_ood_raw:
        ood_slice, md_ood = _filter_outliers_with_mahalanobis(ood_raw, md_ood, outlier_quantile, "OOD", context)
        stats_ood = _ellipse_stats(ood_slice)
    else:
        stats_ood = None

    if stats_forget_raw:
        forget_slice, md_forget = _filter_outliers_with_mahalanobis(forget_raw, md_forget, outlier_quantile, "Forget", context)
        stats_forget = _ellipse_stats(forget_slice)
    else:
        stats_forget = None

    if stats_id:
        md_id = _mahalanobis_distance(id_slice, stats_id[0], stats_id[2])
    if stats_ood:
        md_ood = _mahalanobis_distance(ood_slice, stats_ood[0], stats_ood[2])
    if stats_forget and forget_slice is not None:
        md_forget = _mahalanobis_distance(forget_slice, stats_forget[0], stats_forget[2])

    fig, ax = plt.subplots(figsize=(9.0, 6.5), dpi=140)

    if ellipse:
        if stats_id and id_slice.shape[0] >= 3:
            add_ellipse(ax, id_slice, markers[id_label][1], alpha=0.15, zorder=1)
        if stats_ood and ood_slice.shape[0] >= 3:
            add_ellipse(ax, ood_slice, markers[ood_label][1], alpha=0.12, zorder=1)
        if stats_forget and forget_slice is not None and forget_slice.shape[0] >= 3:
            add_ellipse(ax, forget_slice, markers[forget_label][1], alpha=0.12, zorder=1, lw=1.0, ls="--")

    if stochastic_dropout_scale > 0:
        rng = rng or np.random.default_rng()
        if stats_id and md_id.size:
            id_slice, md_id = _mahalanobis_dropout(id_slice, stats_id[0], stats_id[2], stochastic_dropout_scale, "ID", context, rng, ellipse_radius)
        if stats_ood and md_ood.size:
            ood_slice, md_ood = _mahalanobis_dropout(ood_slice, stats_ood[0], stats_ood[2], stochastic_dropout_scale, "OOD", context, rng, ellipse_radius)
        if stats_forget and forget_slice is not None and md_forget.size:
            forget_slice, md_forget = _mahalanobis_dropout(forget_slice, stats_forget[0], stats_forget[2], stochastic_dropout_scale, "Forget", context, rng, ellipse_radius)

    sc_ood = ax.scatter(ood_slice[:, 0], ood_slice[:, 1], s=26, marker=markers[ood_label][0], c=markers[ood_label][1], label=ood_label, alpha=0.8, linewidths=0.2, edgecolors="white", zorder=zorders.get(ood_label, 4))
    if forget_slice is not None and forget_slice.shape[0] > 0:
        sc_forget = ax.scatter(forget_slice[:, 0], forget_slice[:, 1], s=24, marker=markers[forget_label][0], c=markers[forget_label][1], label=forget_label, alpha=0.75, linewidths=0.2, edgecolors="white", zorder=zorders.get(forget_label, 5))
    else:
        sc_forget = None
    sc_id = ax.scatter(id_slice[:, 0], id_slice[:, 1], s=18, marker=markers[id_label][0], c=markers[id_label][1], label=id_label, alpha=0.75, linewidths=0.2, edgecolors="white", zorder=zorders.get(id_label, 6))

    ax.set_title(title, fontsize=13)
    ax.set_xlabel("UMAP-1")
    ax.set_ylabel("UMAP-2")
    legend_handles = [sc_id, sc_ood]
    legend_labels = [id_label, ood_label]
    if sc_forget is not None:
        legend_handles.append(sc_forget)
        legend_labels.append(forget_label)
    ax.legend(legend_handles, legend_labels, loc="upper right", frameon=False)
    ax.grid(False)
    ax.set_xticks([])
    ax.set_yticks([])
    fig.tight_layout()
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    fig.savefig(output_path)
    plt.close(fig)


def sanitize_filename(text: str) -> str:
    safe = []
    for ch in text:
        if ch.isalnum():
            safe.append(ch)
        elif ch in {"-", "_"}:
            safe.append(ch)
        else:
            safe.append("-")
    return "".join(safe)


def stable_int(text: str) -> int:
    digest = hashlib.sha1(text.encode("utf-8")).hexdigest()
    return int(digest[:8], 16)


def cache_file_paths(
    cache_root: str,
    backbone: str,
    method_tag: str,
    in_dataset: str,
    ood_names: Sequence[str],
    include_forget: bool,
) -> List[str]:
    base_dir = Path(cache_root) / f"{backbone}-{method_tag}" / in_dataset
    paths = [
        base_dir / f"val_{backbone}-{method_tag}_features.npy",
        base_dir / f"val_{backbone}-{method_tag}_labels.npy",
    ]
    for name in ood_names:
        paths.append(base_dir / name / f"{backbone}-{method_tag}_features.npy")
    if include_forget:
        paths.append(base_dir / "forget" / f"{backbone}-{method_tag}_features.npy")
    return [str(p) for p in paths]


def ensure_feature_caches(
    *,
    cache_root: str,
    backbone: str,
    method_tag: str,
    in_dataset: str,
    ood_names: Sequence[str],
    forget_csv: Optional[str],
    pretrain_ckpt: str,
    adapter_path: Optional[str],
    batch_size: int,
    cache_size: int,
    seed: int,
    gpu: str,
    lora_target: str,
    lora_r: int,
    lora_alpha: int,
    lora_dropout: float,
    lora_impl: str,
) -> None:
    required = cache_file_paths(
        cache_root=cache_root,
        backbone=backbone,
        method_tag=method_tag,
        in_dataset=in_dataset,
        ood_names=ood_names,
        include_forget=bool(forget_csv),
    )
    missing = [path for path in required if not os.path.exists(path)]
    if not missing:
        return

    if not FEATURE_EXTRACT.exists():
        raise FileNotFoundError(f"feature_extract.py not found at {FEATURE_EXTRACT}")

    forget_csv_str = (forget_csv or "").strip()
    feature_cmd: List[str] = [
        sys.executable,
        str(FEATURE_EXTRACT),
        "--in-dataset",
        in_dataset,
        "--backbone",
        backbone,
        "--method",
        method_tag,
        "--save-path",
        pretrain_ckpt,
        "--cache-size",
        str(cache_size),
        "--batch-size",
        str(batch_size),
        "--seed",
        str(seed),
        "--gpu",
        gpu,
        "--out-datasets",
    ]
    feature_cmd.extend(list(ood_names))
    if forget_csv_str:
        feature_cmd.extend(["--forget_csv", forget_csv_str])
    if adapter_path:
        feature_cmd.extend([
            "--use_lora",
            "--lora_impl",
            lora_impl,
            "--adapter_load_path",
            adapter_path,
            "--lora_target",
            lora_target,
            "--lora_r",
            str(lora_r),
            "--lora_alpha",
            str(lora_alpha),
            "--lora_dropout",
            str(lora_dropout),
        ])

    print("[cache] Missing feature caches detected – running feature_extract.py")
    env = os.environ.copy()
    threading_layer = env.get("MKL_THREADING_LAYER")
    if threading_layer and threading_layer.upper() == "INTEL":
        env["MKL_THREADING_LAYER"] = "GNU"
    env.setdefault("MKL_THREADING_LAYER", "GNU")
    env.setdefault("MKL_SERVICE_FORCE_INTEL", "1")
    try:
        subprocess.run(feature_cmd, check=True, env=env)
    except subprocess.CalledProcessError as exc:
        raise RuntimeError("feature_extract.py failed; see above logs for details") from exc

    remaining = [path for path in required if not os.path.exists(path)]
    if remaining:
        critical_missing = [p for p in remaining if "/forget/" not in p]
        if critical_missing:
            raise RuntimeError(
                "Feature extraction completed but some caches are still missing: "
                + ", ".join(critical_missing)
            )
        print(
            "[cache] Warning: forget cache files missing after extraction; "
            "forget embeddings will be skipped."
        )


def main():
    parser = argparse.ArgumentParser(description="Generate custom UMAP forget/retain/OOD plots.")
    parser.add_argument("--in-dataset", required=True, help="ID dataset name, e.g. CIFAR-100")
    parser.add_argument("--backbone", required=True, help="Backbone name, e.g. resnet34")
    parser.add_argument("--method-tag", required=True, help="Full method tag used in cache directory name")
    parser.add_argument("--pretrain-ckpt", required=True, help="Path to the base checkpoint for feature extraction")
    parser.add_argument("--adapter-path", default=None, help="Optional LoRA adapter directory to load during feature extraction")
    parser.add_argument("--forget-classes", required=True, help="Comma separated class indices that were forgotten")
    parser.add_argument("--ood-datasets", default="SVHN places365 LSUN iSUN dtd", help="Space or comma separated OOD dataset names")
    parser.add_argument("--cache-root", default="cache", help="Root directory of cached features")
    parser.add_argument("--output-dir", default="figs/umap_custom", help="Directory to save generated figures")
    parser.add_argument("--id-class-counts", default="20,90", help="Comma separated ID class counts to sample (e.g. '20,90')")
    parser.add_argument("--id-per-class", type=int, default=60, help="Max samples per ID class (0 = keep all)")
    parser.add_argument("--ood-limit", type=int, default=1200, help="Max OOD samples to plot per dataset")
    parser.add_argument("--forget-limit", type=int, default=800, help="Max forget samples to plot (0 = keep all)")
    parser.add_argument("--seed", type=int, default=0, help="Random seed for sampling and UMAP")
    parser.add_argument("--gpu", default="0", help="CUDA visible device for feature extraction")
    parser.add_argument("--batch-size", type=int, default=128, help="Batch size for feature extraction")
    parser.add_argument("--cache-size", type=int, default=6, help="Cache size argument forwarded to feature extraction")
    parser.add_argument("--n-neighbors", type=int, default=15, help="UMAP n_neighbors parameter")
    parser.add_argument("--min-dist", type=float, default=0.05, help="UMAP min_dist parameter")
    parser.add_argument("--metric", default="cosine", help="UMAP metric")
    parser.add_argument("--no-ellipses", action="store_true", help="Disable covariance ellipses")
    parser.add_argument("--lora-target", default="both", help="LoRA target modules when loading adapters")
    parser.add_argument("--lora-r", type=int, default=8, help="LoRA rank when loading adapters")
    parser.add_argument("--lora-alpha", type=int, default=32, help="LoRA alpha when loading adapters")
    parser.add_argument("--lora-dropout", type=float, default=0.05, help="LoRA dropout when loading adapters")
    parser.add_argument("--lora-impl", default="peft", choices=["native", "peft"], help="LoRA implementation backend for feature extraction")
    parser.add_argument("--outlier-quantile", type=float, default=1.0, help="Retain only points within this distance quantile (0-1] for each domain")
    parser.add_argument("--stochastic-dropout-scale", type=float, default=0.0, help="Scale factor controlling stochastic removal probability based on Mahalanobis distance from the covariance ellipse")

    args = parser.parse_args()

    forget_classes = parse_int_list(args.forget_classes)
    ood_names = parse_str_list(args.ood_datasets)
    if not ood_names:
        raise ValueError("At least one OOD dataset must be provided.")
    id_counts = parse_int_list(args.id_class_counts)
    if not id_counts:
        raise ValueError("At least one ID class count must be provided.")

    ensure_feature_caches(
        cache_root=args.cache_root,
        backbone=args.backbone,
        method_tag=args.method_tag,
        in_dataset=args.in_dataset,
        ood_names=ood_names,
        forget_csv=args.forget_classes,
        pretrain_ckpt=args.pretrain_ckpt,
        adapter_path=args.adapter_path,
        batch_size=args.batch_size,
        cache_size=args.cache_size,
        seed=args.seed,
        gpu=args.gpu,
        lora_target=args.lora_target,
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        lora_impl=args.lora_impl,
    )

    base_filename = sanitize_filename(
        f"{args.in_dataset}_{args.backbone}_{args.method_tag}"
    )

    for id_count in id_counts:
        for ood in ood_names:
            compound_seed = args.seed + id_count * 31 + stable_int(ood) % 10007
            embeddings = build_embeddings(
                cache_root=args.cache_root,
                backbone=args.backbone,
                method_tag=args.method_tag,
                in_dataset=args.in_dataset,
                forget_classes=forget_classes,
                id_class_count=id_count,
                ood_dataset=ood,
                seed=compound_seed,
                id_per_class=args.id_per_class,
                ood_limit=args.ood_limit,
                forget_limit=args.forget_limit,
            )

            stack_inputs = [embeddings.id_embeddings, embeddings.ood_embeddings]
            counts = [embeddings.id_embeddings.shape[0], embeddings.ood_embeddings.shape[0]]
            if embeddings.forget_embeddings is not None and embeddings.forget_embeddings.size > 0:
                stack_inputs.append(embeddings.forget_embeddings)
                counts.append(embeddings.forget_embeddings.shape[0])
            else:
                counts.append(None)

            umap_coords = run_umap(
                stack_inputs,
                metric=args.metric,
                n_neighbors=args.n_neighbors,
                min_dist=args.min_dist,
                seed=compound_seed,
            )

            id_label = f"ID ({id_count} classes)"
            ood_label = f"OOD ({ood})"
            forget_label = "Forget"

            filename = f"umap_custom_{base_filename}_id{id_count}_{sanitize_filename(ood)}.png"
            output_path = os.path.join(args.output_dir, filename)
            title = f"UMAP {args.in_dataset} · {args.backbone}\n{args.method_tag}\nID {id_count} cls vs OOD {ood}"

            rng = np.random.default_rng(compound_seed + 7919)

            plot_embeddings(
                umap_coords,
                counts=(counts[0], counts[1], counts[2]),
                id_label=id_label,
                ood_label=ood_label,
                forget_label=forget_label,
                title=title,
                output_path=output_path,
                ellipse=(not args.no_ellipses),
                outlier_quantile=float(args.outlier_quantile),
                stochastic_dropout_scale=float(args.stochastic_dropout_scale),
                rng=rng,
            )

            print(f"[saved] {output_path}")


if __name__ == "__main__":
    main()


