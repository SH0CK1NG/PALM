nohup: ignoring input
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.1, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_center_set='all', forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:107: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:23<07:18, 23.09s/it] 10%|█         | 2/20 [00:42<06:12, 20.71s/it] 15%|█▌        | 3/20 [01:01<05:44, 20.29s/it] 20%|██        | 4/20 [01:21<05:19, 19.97s/it] 25%|██▌       | 5/20 [01:41<04:59, 19.93s/it] 30%|███       | 6/20 [02:01<04:39, 20.00s/it] 35%|███▌      | 7/20 [02:21<04:19, 19.93s/it] 40%|████      | 8/20 [02:40<03:57, 19.77s/it][loss] ep 0 it 0 total=13.4199 mle=1.4969 pcon=5.2950 forget=6.6280 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 50 total=13.4477 mle=1.4710 pcon=5.2879 forget=6.6889 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 100 total=13.5022 mle=1.5970 pcon=5.2809 forget=6.6243 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 150 total=13.6315 mle=1.7522 pcon=5.2738 forget=6.6055 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 200 total=13.5351 mle=1.6198 pcon=5.2670 forget=6.6483 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 250 total=13.3388 mle=1.4374 pcon=5.2603 forget=6.6410 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 300 total=13.3810 mle=1.4844 pcon=5.2540 forget=6.6426 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 350 total=13.4970 mle=1.5890 pcon=5.2476 forget=6.6604 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 1 it 10 total=13.5411 mle=1.5756 pcon=5.2409 forget=6.7247 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 60 total=13.4008 mle=1.5656 pcon=5.2346 forget=6.6006 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 110 total=13.3308 mle=1.4495 pcon=5.2284 forget=6.6528 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 160 total=13.5478 mle=1.6637 pcon=5.2224 forget=6.6617 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 210 total=13.5128 mle=1.6528 pcon=5.2167 forget=6.6433 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 260 total=13.3437 mle=1.5306 pcon=5.2112 forget=6.6019 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 310 total=13.4531 mle=1.6294 pcon=5.2056 forget=6.6181 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 360 total=13.5240 mle=1.6850 pcon=5.2003 forget=6.6388 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 2 it 20 total=13.3064 mle=1.4936 pcon=5.1950 forget=6.6177 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 70 total=13.5029 mle=1.7523 pcon=5.1899 forget=6.5607 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 120 total=13.4412 mle=1.6371 pcon=5.1847 forget=6.6194 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 170 total=13.2327 mle=1.4588 pcon=5.1796 forget=6.5943 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 220 total=13.3318 mle=1.5209 pcon=5.1745 forget=6.6363 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 270 total=13.5350 mle=1.7609 pcon=5.1699 forget=6.6042 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 320 total=13.2532 mle=1.5193 pcon=5.1651 forget=6.5688 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 370 total=13.4533 mle=1.7167 pcon=5.1604 forget=6.5761 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 3 it 30 total=13.3770 mle=1.6248 pcon=5.1559 forget=6.5963 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 80 total=13.3576 mle=1.5783 pcon=5.1518 forget=6.6275 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 130 total=13.3901 mle=1.6599 pcon=5.1474 forget=6.5828 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 180 total=13.5043 mle=1.7920 pcon=5.1434 forget=6.5688 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 230 total=13.2084 mle=1.5001 pcon=5.1394 forget=6.5689 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 280 total=13.4172 mle=1.7224 pcon=5.1352 forget=6.5596 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 330 total=13.3540 mle=1.6110 pcon=5.1309 forget=6.6121 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 380 total=13.3349 mle=1.5924 pcon=5.1273 forget=6.6152 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 4 it 40 total=13.2078 mle=1.5630 pcon=5.1235 forget=6.5214 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 90 total=13.2996 mle=1.6330 pcon=5.1194 forget=6.5471 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 140 total=13.2677 mle=1.5738 pcon=5.1157 forget=6.5782 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 190 total=13.0959 mle=1.4068 pcon=5.1122 forget=6.5769 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 240 total=13.3256 mle=1.6708 pcon=5.1082 forget=6.5466 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 290 total=13.0900 mle=1.4383 pcon=5.1046 forget=6.5471 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 340 total=13.4347 mle=1.7741 pcon=5.1011 forget=6.5595 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 5 it 0 total=13.4045 mle=1.7697 pcon=5.0976 forget=6.5372 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 50 total=13.2988 mle=1.6954 pcon=5.0939 forget=6.5095 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 100 total=13.1512 mle=1.5229 pcon=5.0905 forget=6.5377 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 150 total=13.1341 mle=1.5207 pcon=5.0869 forget=6.5265 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 200 total=13.1811 mle=1.6068 pcon=5.0837 forget=6.4906 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 250 total=13.2247 mle=1.6769 pcon=5.0805 forget=6.4674 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 300 total=13.0772 mle=1.5238 pcon=5.0771 forget=6.4764 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 350 total=13.1951 mle=1.6447 pcon=5.0739 forget=6.4765 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 6 it 10 total=12.9951 mle=1.3785 pcon=5.0706 forget=6.5460 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 60 total=12.9692 mle=1.4823 pcon=5.0674 forget=6.4195 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 110 total=13.0748 mle=1.5338 pcon=5.0642 forget=6.4767 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 160 total=13.1192 mle=1.6077 pcon=5.0610 forget=6.4505 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 210 total=13.0530 mle=1.5460 pcon=5.0582 forget=6.4487 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 260 total=13.2226 mle=1.6992 pcon=5.0549 forget=6.4685 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 310 total=13.0564 mle=1.4943 pcon=5.0520 forget=6.5101 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 360 total=13.1091 mle=1.5899 pcon=5.0490 forget=6.4701 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 7 it 20 total=13.0748 mle=1.5739 pcon=5.0460 forget=6.4550 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 70 total=13.1592 mle=1.6616 pcon=5.0433 forget=6.4544 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 120 total=13.0681 mle=1.5556 pcon=5.0404 forget=6.4722 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 170 total=13.0273 mle=1.5077 pcon=5.0377 forget=6.4819 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 220 total=13.1369 mle=1.5712 pcon=5.0355 forget=6.5302 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 270 total=13.2896 mle=1.6938 pcon=5.0332 forget=6.5626 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 320 total=13.2401 mle=1.6435 pcon=5.0306 forget=6.5660 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 370 total=13.1639 mle=1.5763 pcon=5.0285 forget=6.5591 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 30 total=13.1392 mle=1.5000 pcon=5.0266 forget=6.6126 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 80 total=13.1556 mle=1.5274 pcon=5.0247 forget=6.6036 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 130 total=13.0950 mle=1.4635 pcon=5.0226 forget=6.6089 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 180 total=13.2833 mle=1.6449 pcon=5.0207 forget=6.6177 nr=64 nf=64 protos=540 dmin_norm=NA
 45%|████▌     | 9/20 [02:59<03:35, 19.59s/it] 50%|█████     | 10/20 [03:19<03:15, 19.55s/it] 55%|█████▌    | 11/20 [03:38<02:56, 19.56s/it] 60%|██████    | 12/20 [03:58<02:35, 19.49s/it] 65%|██████▌   | 13/20 [04:17<02:15, 19.42s/it] 70%|███████   | 14/20 [04:36<01:56, 19.44s/it] 75%|███████▌  | 15/20 [04:56<01:37, 19.42s/it] 80%|████████  | 16/20 [05:15<01:17, 19.43s/it] 85%|████████▌ | 17/20 [05:35<00:58, 19.38s/it][loss] ep 8 it 230 total=13.1151 mle=1.4783 pcon=5.0193 forget=6.6174 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 280 total=13.2615 mle=1.6218 pcon=5.0175 forget=6.6222 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 330 total=13.3777 mle=1.6646 pcon=5.0155 forget=6.6977 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 380 total=13.1303 mle=1.4496 pcon=5.0137 forget=6.6671 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 40 total=13.2274 mle=1.5679 pcon=5.0119 forget=6.6476 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 90 total=13.3123 mle=1.6198 pcon=5.0099 forget=6.6827 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 140 total=13.4435 mle=1.7068 pcon=5.0085 forget=6.7282 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 190 total=13.2219 mle=1.5367 pcon=5.0069 forget=6.6783 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 240 total=13.2209 mle=1.5056 pcon=5.0054 forget=6.7099 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 290 total=13.2250 mle=1.5098 pcon=5.0034 forget=6.7118 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 340 total=13.2037 mle=1.5143 pcon=5.0018 forget=6.6876 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 0 total=13.2608 mle=1.5597 pcon=4.9999 forget=6.7012 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 50 total=13.2410 mle=1.5281 pcon=4.9979 forget=6.7150 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 100 total=13.3536 mle=1.6780 pcon=4.9961 forget=6.6794 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 150 total=13.3778 mle=1.7274 pcon=4.9944 forget=6.6560 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 200 total=13.4013 mle=1.7547 pcon=4.9925 forget=6.6541 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 250 total=13.1841 mle=1.5364 pcon=4.9908 forget=6.6569 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 300 total=13.1449 mle=1.5170 pcon=4.9892 forget=6.6387 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 350 total=13.2735 mle=1.5839 pcon=4.9874 forget=6.7022 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 10 total=13.1540 mle=1.5088 pcon=4.9855 forget=6.6597 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 60 total=13.1726 mle=1.5699 pcon=4.9836 forget=6.6191 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 110 total=13.1032 mle=1.5278 pcon=4.9819 forget=6.5935 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 160 total=13.1944 mle=1.5945 pcon=4.9797 forget=6.6202 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 210 total=13.2847 mle=1.6910 pcon=4.9780 forget=6.6157 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 260 total=13.1387 mle=1.5439 pcon=4.9766 forget=6.6183 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 310 total=13.4432 mle=1.8560 pcon=4.9746 forget=6.6126 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 360 total=13.1600 mle=1.6149 pcon=4.9728 forget=6.5724 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 20 total=13.1316 mle=1.5526 pcon=4.9708 forget=6.6082 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 70 total=13.1789 mle=1.6134 pcon=4.9690 forget=6.5965 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 120 total=13.1055 mle=1.5154 pcon=4.9672 forget=6.6229 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 170 total=13.2117 mle=1.6807 pcon=4.9655 forget=6.5655 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 220 total=13.0814 mle=1.5348 pcon=4.9637 forget=6.5829 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 270 total=13.0666 mle=1.4960 pcon=4.9623 forget=6.6083 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 320 total=13.0162 mle=1.4737 pcon=4.9606 forget=6.5819 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 370 total=13.3037 mle=1.7523 pcon=4.9590 forget=6.5924 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 30 total=12.9457 mle=1.3999 pcon=4.9574 forget=6.5884 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 80 total=13.0563 mle=1.4393 pcon=4.9558 forget=6.6612 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 130 total=13.0361 mle=1.5042 pcon=4.9540 forget=6.5779 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 180 total=13.0351 mle=1.5368 pcon=4.9526 forget=6.5457 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 230 total=13.1368 mle=1.6152 pcon=4.9511 forget=6.5704 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 280 total=13.0216 mle=1.5252 pcon=4.9496 forget=6.5468 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 330 total=13.0622 mle=1.5518 pcon=4.9481 forget=6.5623 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 380 total=13.0455 mle=1.5620 pcon=4.9466 forget=6.5369 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 14 it 40 total=13.1764 mle=1.7057 pcon=4.9449 forget=6.5258 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 90 total=13.1972 mle=1.7055 pcon=4.9438 forget=6.5479 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 140 total=13.0215 mle=1.5348 pcon=4.9428 forget=6.5439 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 190 total=13.2668 mle=1.7979 pcon=4.9413 forget=6.5276 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 240 total=13.0400 mle=1.5795 pcon=4.9402 forget=6.5203 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 290 total=13.0440 mle=1.6094 pcon=4.9388 forget=6.4958 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 340 total=13.0798 mle=1.6357 pcon=4.9374 forget=6.5067 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 15 it 0 total=13.1157 mle=1.6465 pcon=4.9362 forget=6.5330 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 50 total=12.8943 mle=1.4595 pcon=4.9348 forget=6.4999 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 100 total=12.9818 mle=1.5383 pcon=4.9336 forget=6.5099 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 150 total=13.1007 mle=1.6373 pcon=4.9323 forget=6.5311 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 200 total=13.0051 mle=1.5938 pcon=4.9311 forget=6.4802 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 250 total=13.2600 mle=1.8346 pcon=4.9298 forget=6.4957 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 300 total=12.9645 mle=1.5318 pcon=4.9288 forget=6.5039 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 350 total=12.9711 mle=1.5080 pcon=4.9276 forget=6.5355 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 16 it 10 total=12.9669 mle=1.5488 pcon=4.9263 forget=6.4918 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 60 total=12.8855 mle=1.4879 pcon=4.9252 forget=6.4724 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 110 total=12.9081 mle=1.5133 pcon=4.9243 forget=6.4704 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 160 total=13.0131 mle=1.6109 pcon=4.9229 forget=6.4793 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 210 total=12.9082 mle=1.5376 pcon=4.9215 forget=6.4491 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 260 total=13.2408 mle=1.8462 pcon=4.9202 forget=6.4743 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 310 total=13.1048 mle=1.7110 pcon=4.9191 forget=6.4747 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 360 total=12.9372 mle=1.5534 pcon=4.9179 forget=6.4659 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 17 it 20 total=13.0930 mle=1.6850 pcon=4.9165 forget=6.4914 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 70 total=13.1321 mle=1.7629 pcon=4.9157 forget=6.4535 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 120 total=12.9460 mle=1.5485 pcon=4.9144 forget=6.4830 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 170 total=12.9560 mle=1.5721 pcon=4.9129 forget=6.4710 nr=64 nf=64 protos=540 dmin_norm=NA
 90%|█████████ | 18/20 [05:54<00:39, 19.53s/it] 95%|█████████▌| 19/20 [06:15<00:19, 19.75s/it]100%|██████████| 20/20 [06:35<00:00, 19.80s/it]100%|██████████| 20/20 [06:35<00:00, 19.75s/it]
[loss] ep 17 it 220 total=13.0375 mle=1.6149 pcon=4.9118 forget=6.5109 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 270 total=12.8238 mle=1.4474 pcon=4.9111 forget=6.4653 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 320 total=13.1243 mle=1.7558 pcon=4.9101 forget=6.4585 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 370 total=13.0691 mle=1.7036 pcon=4.9088 forget=6.4567 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 18 it 30 total=12.9160 mle=1.5646 pcon=4.9082 forget=6.4432 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 80 total=13.0233 mle=1.6238 pcon=4.9072 forget=6.4922 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 130 total=13.0115 mle=1.6120 pcon=4.9062 forget=6.4933 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 180 total=12.8769 mle=1.5141 pcon=4.9053 forget=6.4575 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 230 total=12.9327 mle=1.5587 pcon=4.9044 forget=6.4696 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 280 total=12.9853 mle=1.6081 pcon=4.9036 forget=6.4736 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 330 total=13.0960 mle=1.7461 pcon=4.9027 forget=6.4472 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 380 total=13.0606 mle=1.6626 pcon=4.9022 forget=6.4959 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 19 it 40 total=12.8899 mle=1.4761 pcon=4.9016 forget=6.5121 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 90 total=13.0825 mle=1.6434 pcon=4.9012 forget=6.5379 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 140 total=12.9357 mle=1.5411 pcon=4.9004 forget=6.4942 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 190 total=12.9901 mle=1.6127 pcon=4.8995 forget=6.4779 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 240 total=13.1691 mle=1.8023 pcon=4.8988 forget=6.4680 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 290 total=12.9507 mle=1.5447 pcon=4.8984 forget=6.5077 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 340 total=13.0024 mle=1.5726 pcon=4.8980 forget=6.5318 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:12,  2.94it/s]  3%|▎         | 11/391 [00:00<00:12, 30.84it/s]  5%|▌         | 21/391 [00:00<00:07, 50.05it/s]  7%|▋         | 29/391 [00:00<00:06, 57.24it/s]  9%|▉         | 37/391 [00:00<00:06, 53.51it/s] 11%|█▏        | 44/391 [00:00<00:06, 51.85it/s] 13%|█▎        | 51/391 [00:01<00:06, 55.56it/s] 15%|█▍        | 58/391 [00:01<00:06, 52.94it/s] 16%|█▋        | 64/391 [00:01<00:06, 52.47it/s] 18%|█▊        | 70/391 [00:01<00:06, 53.16it/s] 19%|█▉        | 76/391 [00:01<00:05, 52.70it/s] 21%|██        | 82/391 [00:01<00:06, 50.59it/s] 23%|██▎       | 88/391 [00:01<00:05, 50.89it/s] 24%|██▍       | 94/391 [00:01<00:05, 52.31it/s] 26%|██▌       | 100/391 [00:02<00:05, 50.77it/s] 27%|██▋       | 106/391 [00:02<00:05, 50.96it/s] 29%|██▊       | 112/391 [00:02<00:05, 52.22it/s] 30%|███       | 118/391 [00:02<00:05, 51.12it/s] 32%|███▏      | 124/391 [00:02<00:05, 50.24it/s] 33%|███▎      | 130/391 [00:02<00:05, 50.38it/s] 35%|███▍      | 136/391 [00:02<00:04, 52.52it/s] 36%|███▋      | 142/391 [00:02<00:04, 50.85it/s] 38%|███▊      | 148/391 [00:02<00:04, 50.95it/s] 39%|███▉      | 154/391 [00:03<00:04, 51.47it/s] 41%|████      | 160/391 [00:03<00:04, 51.29it/s] 42%|████▏     | 166/391 [00:03<00:04, 50.32it/s] 44%|████▍     | 172/391 [00:03<00:04, 50.28it/s] 46%|████▌     | 178/391 [00:03<00:04, 51.37it/s] 47%|████▋     | 184/391 [00:03<00:04, 50.10it/s] 49%|████▊     | 190/391 [00:03<00:03, 50.33it/s] 50%|█████     | 196/391 [00:03<00:03, 50.81it/s] 52%|█████▏    | 202/391 [00:04<00:03, 51.12it/s] 53%|█████▎    | 208/391 [00:04<00:03, 51.33it/s] 55%|█████▍    | 214/391 [00:04<00:03, 50.43it/s] 56%|█████▋    | 220/391 [00:04<00:03, 50.96it/s] 58%|█████▊    | 226/391 [00:04<00:03, 50.79it/s] 59%|█████▉    | 232/391 [00:04<00:03, 50.22it/s] 61%|██████    | 238/391 [00:04<00:02, 52.24it/s] 62%|██████▏   | 244/391 [00:04<00:02, 51.51it/s] 64%|██████▍   | 250/391 [00:04<00:02, 51.41it/s] 65%|██████▌   | 256/391 [00:05<00:02, 51.00it/s] 67%|██████▋   | 262/391 [00:05<00:02, 51.45it/s] 69%|██████▊   | 268/391 [00:05<00:02, 50.17it/s] 70%|███████   | 274/391 [00:05<00:02, 50.31it/s] 72%|███████▏  | 280/391 [00:05<00:02, 51.27it/s] 73%|███████▎  | 286/391 [00:05<00:02, 50.58it/s] 75%|███████▍  | 292/391 [00:05<00:01, 51.63it/s] 76%|███████▌  | 298/391 [00:05<00:01, 51.53it/s] 78%|███████▊  | 305/391 [00:06<00:01, 53.97it/s] 80%|███████▉  | 311/391 [00:06<00:01, 52.00it/s] 81%|████████  | 317/391 [00:06<00:01, 51.29it/s] 83%|████████▎ | 323/391 [00:06<00:01, 51.69it/s] 84%|████████▍ | 329/391 [00:06<00:01, 51.10it/s] 86%|████████▌ | 335/391 [00:06<00:01, 51.87it/s] 87%|████████▋ | 341/391 [00:06<00:01, 49.75it/s] 88%|████████▊ | 346/391 [00:06<00:00, 49.73it/s] 90%|████████▉ | 351/391 [00:06<00:00, 49.47it/s] 91%|█████████▏| 357/391 [00:07<00:00, 49.77it/s] 93%|█████████▎| 363/391 [00:07<00:00, 50.93it/s] 94%|█████████▍| 369/391 [00:07<00:00, 50.64it/s] 96%|█████████▌| 375/391 [00:07<00:00, 50.81it/s] 97%|█████████▋| 381/391 [00:07<00:00, 50.97it/s] 99%|█████████▉| 387/391 [00:07<00:00, 51.45it/s]100%|██████████| 391/391 [00:07<00:00, 50.39it/s]
50000 images processed, 7.845911741256714 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:26,  2.98it/s] 14%|█▍        | 11/79 [00:00<00:02, 31.13it/s] 22%|██▏       | 17/79 [00:00<00:01, 37.92it/s] 29%|██▉       | 23/79 [00:00<00:01, 42.41it/s] 37%|███▋      | 29/79 [00:00<00:01, 45.06it/s] 44%|████▍     | 35/79 [00:00<00:00, 47.22it/s] 52%|█████▏    | 41/79 [00:01<00:00, 48.65it/s] 59%|█████▉    | 47/79 [00:01<00:00, 49.97it/s] 67%|██████▋   | 53/79 [00:01<00:00, 49.11it/s] 75%|███████▍  | 59/79 [00:01<00:00, 50.41it/s] 82%|████████▏ | 65/79 [00:01<00:00, 49.72it/s] 90%|████████▉ | 71/79 [00:01<00:00, 49.78it/s] 97%|█████████▋| 77/79 [00:01<00:00, 50.67it/s]100%|██████████| 79/79 [00:01<00:00, 44.59it/s]
10000 images processed, 1.7955965995788574 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:09,  2.93it/s]  4%|▍         | 9/204 [00:00<00:07, 25.35it/s]  8%|▊         | 16/204 [00:00<00:04, 38.39it/s] 11%|█         | 22/204 [00:00<00:04, 40.85it/s] 14%|█▎        | 28/204 [00:00<00:04, 41.88it/s] 18%|█▊        | 36/204 [00:00<00:03, 51.04it/s] 21%|██        | 42/204 [00:01<00:03, 51.04it/s] 24%|██▎       | 48/204 [00:01<00:03, 50.66it/s] 26%|██▋       | 54/204 [00:01<00:03, 48.96it/s] 29%|██▉       | 60/204 [00:01<00:02, 49.89it/s] 32%|███▏      | 66/204 [00:01<00:02, 49.36it/s] 35%|███▌      | 72/204 [00:01<00:02, 48.18it/s] 38%|███▊      | 78/204 [00:01<00:02, 49.31it/s] 41%|████      | 83/204 [00:01<00:02, 47.51it/s] 44%|████▎     | 89/204 [00:02<00:02, 47.84it/s] 47%|████▋     | 95/204 [00:02<00:02, 49.79it/s] 50%|████▉     | 101/204 [00:02<00:02, 49.16it/s] 52%|█████▏    | 107/204 [00:02<00:01, 49.30it/s] 55%|█████▍    | 112/204 [00:02<00:01, 47.20it/s] 58%|█████▊    | 119/204 [00:02<00:01, 51.32it/s] 61%|██████▏   | 125/204 [00:02<00:01, 49.47it/s] 64%|██████▎   | 130/204 [00:02<00:01, 48.91it/s] 67%|██████▋   | 137/204 [00:02<00:01, 52.46it/s] 70%|███████   | 143/204 [00:03<00:01, 49.94it/s] 73%|███████▎  | 149/204 [00:03<00:01, 50.18it/s] 76%|███████▌  | 155/204 [00:03<00:00, 50.44it/s] 79%|███████▉  | 162/204 [00:03<00:00, 53.61it/s] 82%|████████▏ | 168/204 [00:03<00:00, 50.80it/s] 85%|████████▌ | 174/204 [00:03<00:00, 49.62it/s] 88%|████████▊ | 180/204 [00:03<00:00, 52.14it/s] 91%|█████████ | 186/204 [00:03<00:00, 49.66it/s] 94%|█████████▍| 192/204 [00:04<00:00, 49.31it/s] 98%|█████████▊| 199/204 [00:04<00:00, 52.95it/s]100%|██████████| 204/204 [00:04<00:00, 47.73it/s]
26032 images processed, 4.318084478378296 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:36,  2.12it/s] 11%|█▏        | 9/79 [00:00<00:03, 17.62it/s] 20%|██        | 16/79 [00:00<00:02, 27.97it/s] 27%|██▋       | 21/79 [00:00<00:02, 28.59it/s] 33%|███▎      | 26/79 [00:01<00:01, 32.80it/s] 39%|███▉      | 31/79 [00:01<00:01, 34.27it/s] 44%|████▍     | 35/79 [00:01<00:01, 34.29it/s] 51%|█████     | 40/79 [00:01<00:01, 36.70it/s] 58%|█████▊    | 46/79 [00:01<00:00, 41.60it/s] 65%|██████▍   | 51/79 [00:01<00:00, 36.04it/s] 71%|███████   | 56/79 [00:01<00:00, 38.68it/s] 77%|███████▋  | 61/79 [00:01<00:00, 40.26it/s] 84%|████████▎ | 66/79 [00:02<00:00, 42.12it/s] 90%|████████▉ | 71/79 [00:02<00:00, 37.48it/s] 95%|█████████▍| 75/79 [00:02<00:00, 37.67it/s]100%|██████████| 79/79 [00:02<00:00, 33.79it/s]
10000 images processed, 2.375368356704712 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:30,  2.57it/s] 14%|█▍        | 11/79 [00:00<00:02, 28.22it/s] 25%|██▌       | 20/79 [00:00<00:01, 43.91it/s] 34%|███▍      | 27/79 [00:00<00:01, 44.79it/s] 42%|████▏     | 33/79 [00:00<00:00, 46.59it/s] 49%|████▉     | 39/79 [00:00<00:00, 49.20it/s] 57%|█████▋    | 45/79 [00:01<00:00, 48.50it/s] 65%|██████▍   | 51/79 [00:01<00:00, 48.89it/s] 72%|███████▏  | 57/79 [00:01<00:00, 48.35it/s] 81%|████████  | 64/79 [00:01<00:00, 51.45it/s] 89%|████████▊ | 70/79 [00:01<00:00, 50.25it/s] 96%|█████████▌| 76/79 [00:01<00:00, 49.04it/s]100%|██████████| 79/79 [00:01<00:00, 44.65it/s]
10000 images processed, 1.792917013168335 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:23,  2.94it/s]  9%|▊         | 6/70 [00:00<00:03, 16.11it/s] 16%|█▌        | 11/70 [00:00<00:02, 25.49it/s] 29%|██▊       | 20/70 [00:00<00:01, 43.77it/s] 43%|████▎     | 30/70 [00:00<00:00, 58.89it/s] 57%|█████▋    | 40/70 [00:00<00:00, 69.48it/s] 70%|███████   | 49/70 [00:00<00:00, 75.00it/s] 84%|████████▍ | 59/70 [00:01<00:00, 80.73it/s] 99%|█████████▊| 69/70 [00:01<00:00, 85.38it/s]100%|██████████| 70/70 [00:01<00:00, 57.78it/s]
8925 images processed, 1.2491812705993652 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:39,  1.13it/s]  4%|▍         | 2/45 [00:00<00:18,  2.35it/s] 20%|██        | 9/45 [00:01<00:03,  9.79it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.96it/s] 31%|███       | 14/45 [00:01<00:02, 12.43it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.83it/s] 47%|████▋     | 21/45 [00:01<00:01, 17.19it/s] 53%|█████▎    | 24/45 [00:02<00:01, 15.14it/s] 58%|█████▊    | 26/45 [00:02<00:01, 14.00it/s] 67%|██████▋   | 30/45 [00:02<00:01, 13.48it/s] 73%|███████▎  | 33/45 [00:02<00:00, 12.94it/s] 78%|███████▊  | 35/45 [00:03<00:00, 13.42it/s] 89%|████████▉ | 40/45 [00:03<00:00, 19.68it/s] 96%|█████████▌| 43/45 [00:03<00:00, 11.34it/s]100%|██████████| 45/45 [00:03<00:00, 11.85it/s]
5640 images processed, 3.8297042846679688 seconds used

24.89326500892639
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.68  99.34
places365     68.20  81.01
LSUN          18.08  96.06
iSUN          71.93  81.60
dtd           38.63  91.15
forget        75.60  87.27
AVG           45.85  89.40
Forget-Acc: 0.7860 | Retain-Acc: 0.7394
Forget-as-OOD (retain known vs forget novel):
  FPR: 75.60 AUROC: 87.27 AUIN: 98.41
10.9977548122406
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1-lora_r8a32d0.05_rf.png
