nohup: ignoring input
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
ckpt loaded from /home/shaokun/PALM/checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21605312
Saved centers -> cache/resnet34-top5-palm-cache6-ema0.999/CIFAR-100/class_centers.pt
Saved precision -> cache/resnet34-top5-palm-cache6-ema0.999/CIFAR-100/precision.pt
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:52: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=1, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=1, fine_tune=False, temp=0.1, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='/home/shaokun/PALM/checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path=None, adapter_load_path=None, forget_classes='0,1,2,3,4,5,6,7,8,9', forget_list_path=None, forget_center_set='retain', forget_lambda=0.1, forget_margin=100.0, centers_path='cache/resnet34-top5-palm-cache6-ema0.999/CIFAR-100/class_centers.pt', precision_path='cache/resnet34-top5-palm-cache6-ema0.999/CIFAR-100/precision.pt', batch_forget_mode='balanced', umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, debug_fixed_batch=True, debug_fixed_batch_steps=200, argument=True)
Files already downloaded and verified
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/1 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:129: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/shaokun/PALM/trainer.py:205: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(False):
[loss] ep 0 it 0 total=21.5418 mle=6.4464 pcon=6.3970 forget=8.6984 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.01609992980957
[loss] ep 0 it 1 total=21.5368 mle=6.4423 pcon=6.3970 forget=8.6975 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.024906158447266
[loss] ep 0 it 2 total=21.5306 mle=6.4363 pcon=6.3970 forget=8.6973 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.026744842529297
[loss] ep 0 it 3 total=21.5241 mle=6.4298 pcon=6.3970 forget=8.6973 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.026522636413574
[loss] ep 0 it 4 total=21.5197 mle=6.4254 pcon=6.3970 forget=8.6973 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.02672290802002
[loss] ep 0 it 5 total=21.5144 mle=6.4199 pcon=6.3970 forget=8.6975 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.024951934814453
[loss] ep 0 it 6 total=21.5089 mle=6.4146 pcon=6.3970 forget=8.6973 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.026657104492188
[loss] ep 0 it 7 total=21.5035 mle=6.4091 pcon=6.3969 forget=8.6974 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.025538444519043
[loss] ep 0 it 8 total=21.4981 mle=6.4037 pcon=6.3969 forget=8.6974 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.025721549987793
[loss] ep 0 it 9 total=21.4928 mle=6.3984 pcon=6.3969 forget=8.6974 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.025711059570312
[loss] ep 0 it 10 total=21.4872 mle=6.3931 pcon=6.3969 forget=8.6973 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.027469635009766
[loss] ep 0 it 11 total=21.4818 mle=6.3877 pcon=6.3969 forget=8.6972 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.027690887451172
[loss] ep 0 it 12 total=21.4766 mle=6.3826 pcon=6.3969 forget=8.6972 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.02814769744873
[loss] ep 0 it 13 total=21.4713 mle=6.3772 pcon=6.3969 forget=8.6973 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.027305603027344
[loss] ep 0 it 14 total=21.4661 mle=6.3720 pcon=6.3969 forget=8.6972 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.027603149414062
[loss] ep 0 it 15 total=21.4608 mle=6.3667 pcon=6.3969 forget=8.6972 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.028081893920898
[loss] ep 0 it 16 total=21.4557 mle=6.3616 pcon=6.3968 forget=8.6973 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.026975631713867
[loss] ep 0 it 17 total=21.4505 mle=6.3564 pcon=6.3968 forget=8.6973 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.027093887329102
[loss] ep 0 it 18 total=21.4451 mle=6.3512 pcon=6.3968 forget=8.6970 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.029955863952637
[loss] ep 0 it 19 total=21.4401 mle=6.3461 pcon=6.3968 forget=8.6972 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.027820587158203
[loss] ep 0 it 20 total=21.4347 mle=6.3409 pcon=6.3968 forget=8.6970 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.030471801757812
[loss] ep 0 it 21 total=21.4296 mle=6.3358 pcon=6.3968 forget=8.6971 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.029277801513672
[loss] ep 0 it 22 total=21.4246 mle=6.3307 pcon=6.3967 forget=8.6971 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.028833389282227
[loss] ep 0 it 23 total=21.4198 mle=6.3261 pcon=6.3967 forget=8.6970 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.029630661010742
[loss] ep 0 it 24 total=21.4151 mle=6.3212 pcon=6.3967 forget=8.6971 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.028621673583984
[loss] ep 0 it 25 total=21.4091 mle=6.3156 pcon=6.3967 forget=8.6968 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.031911849975586
[loss] ep 0 it 26 total=21.4041 mle=6.3106 pcon=6.3966 forget=8.6969 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.031201362609863
[loss] ep 0 it 27 total=21.3991 mle=6.3056 pcon=6.3966 forget=8.6969 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.0311918258667
[loss] ep 0 it 28 total=21.3940 mle=6.3007 pcon=6.3966 forget=8.6967 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.032685279846191
[loss] ep 0 it 29 total=21.3895 mle=6.2957 pcon=6.3966 forget=8.6972 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.028241157531738
[loss] ep 0 it 30 total=21.3840 mle=6.2908 pcon=6.3965 forget=8.6966 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.033586502075195
[loss] ep 0 it 31 total=21.3796 mle=6.2864 pcon=6.3965 forget=8.6967 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.033106803894043
[loss] ep 0 it 32 total=21.3746 mle=6.2816 pcon=6.3965 forget=8.6965 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.034812927246094
[loss] ep 0 it 33 total=21.3696 mle=6.2766 pcon=6.3965 forget=8.6966 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.03421688079834
[loss] ep 0 it 34 total=21.3641 mle=6.2713 pcon=6.3964 forget=8.6963 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.036552429199219
[loss] ep 0 it 35 total=21.3597 mle=6.2670 pcon=6.3964 forget=8.6963 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.037059783935547
[loss] ep 0 it 36 total=21.3550 mle=6.2623 pcon=6.3964 forget=8.6963 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.036689758300781
[loss] ep 0 it 37 total=21.3496 mle=6.2569 pcon=6.3963 forget=8.6963 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.03680419921875
[loss] ep 0 it 38 total=21.3453 mle=6.2527 pcon=6.3963 forget=8.6964 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.036041259765625
[loss] ep 0 it 39 total=21.3402 mle=6.2480 pcon=6.3962 forget=8.6960 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.039834976196289
[loss] ep 0 it 40 total=21.3351 mle=6.2426 pcon=6.3962 forget=8.6963 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.037368774414062
[loss] ep 0 it 41 total=21.3309 mle=6.2385 pcon=6.3962 forget=8.6961 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.038518905639648
[loss] ep 0 it 42 total=21.3257 mle=6.2338 pcon=6.3961 forget=8.6958 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.04222297668457
[loss] ep 0 it 43 total=21.3213 mle=6.2293 pcon=6.3961 forget=8.6959 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.04062271118164
[loss] ep 0 it 44 total=21.3165 mle=6.2246 pcon=6.3961 forget=8.6958 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.0420560836792
[loss] ep 0 it 45 total=21.3118 mle=6.2200 pcon=6.3960 forget=8.6958 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.041793823242188
[loss] ep 0 it 46 total=21.3069 mle=6.2154 pcon=6.3960 forget=8.6956 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.044306755065918
[loss] ep 0 it 47 total=21.3023 mle=6.2109 pcon=6.3959 forget=8.6954 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.045515060424805
[loss] ep 0 it 48 total=21.2976 mle=6.2062 pcon=6.3959 forget=8.6955 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.045449256896973
[loss] ep 0 it 49 total=21.2927 mle=6.2018 pcon=6.3958 forget=8.6950 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.049747467041016
[loss] ep 0 it 50 total=21.2883 mle=6.1972 pcon=6.3958 forget=8.6953 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.046947479248047
[loss] ep 0 it 51 total=21.2835 mle=6.1927 pcon=6.3957 forget=8.6951 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.049285888671875
[loss] ep 0 it 52 total=21.2789 mle=6.1883 pcon=6.3957 forget=8.6949 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.050721168518066
[loss] ep 0 it 53 total=21.2743 mle=6.1840 pcon=6.3956 forget=8.6947 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.052827835083008
[loss] ep 0 it 54 total=21.2701 mle=6.1794 pcon=6.3956 forget=8.6951 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.049131393432617
[loss] ep 0 it 55 total=21.2654 mle=6.1750 pcon=6.3955 forget=8.6948 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.052032470703125
[loss] ep 0 it 56 total=21.2606 mle=6.1705 pcon=6.3955 forget=8.6945 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.054546356201172
[loss] ep 0 it 57 total=21.2563 mle=6.1662 pcon=6.3954 forget=8.6947 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.053414344787598
[loss] ep 0 it 58 total=21.2516 mle=6.1618 pcon=6.3954 forget=8.6944 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.055978775024414
[loss] ep 0 it 59 total=21.2471 mle=6.1576 pcon=6.3953 forget=8.6942 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.058398246765137
[loss] ep 0 it 60 total=21.2426 mle=6.1532 pcon=6.3953 forget=8.6941 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.058856010437012
[loss] ep 0 it 61 total=21.2382 mle=6.1489 pcon=6.3952 forget=8.6941 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.059449195861816
[loss] ep 0 it 62 total=21.2337 mle=6.1447 pcon=6.3952 forget=8.6939 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.061452865600586
[loss] ep 0 it 63 total=21.2294 mle=6.1405 pcon=6.3951 forget=8.6938 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.061988830566406
[loss] ep 0 it 64 total=21.2251 mle=6.1362 pcon=6.3951 forget=8.6938 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.062034606933594
[loss] ep 0 it 65 total=21.2205 mle=6.1321 pcon=6.3950 forget=8.6935 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.065262794494629
[loss] ep 0 it 66 total=21.2162 mle=6.1279 pcon=6.3949 forget=8.6934 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.0662260055542
[loss] ep 0 it 67 total=21.2120 mle=6.1236 pcon=6.3949 forget=8.6935 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.065069198608398
[loss] ep 0 it 68 total=21.2076 mle=6.1196 pcon=6.3948 forget=8.6932 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.068096160888672
[loss] ep 0 it 69 total=21.2033 mle=6.1154 pcon=6.3948 forget=8.6932 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.068485260009766
[loss] ep 0 it 70 total=21.1988 mle=6.1112 pcon=6.3947 forget=8.6929 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.070783615112305
[loss] ep 0 it 71 total=21.1944 mle=6.1072 pcon=6.3946 forget=8.6926 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.073949813842773
[loss] ep 0 it 72 total=21.1903 mle=6.1031 pcon=6.3945 forget=8.6926 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.073522567749023
[loss] ep 0 it 73 total=21.1857 mle=6.0992 pcon=6.3945 forget=8.6921 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.079416275024414
[loss] ep 0 it 74 total=21.1815 mle=6.0951 pcon=6.3944 forget=8.6920 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.079538345336914
[loss] ep 0 it 75 total=21.1776 mle=6.0910 pcon=6.3943 forget=8.6922 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.077770233154297
[loss] ep 0 it 76 total=21.1732 mle=6.0871 pcon=6.3943 forget=8.6918 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.081770896911621
[loss] ep 0 it 77 total=21.1692 mle=6.0831 pcon=6.3942 forget=8.6919 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.08123779296875
[loss] ep 0 it 78 total=21.1646 mle=6.0792 pcon=6.3941 forget=8.6913 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.086650848388672
[loss] ep 0 it 79 total=21.1611 mle=6.0752 pcon=6.3941 forget=8.6919 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.081253051757812
[loss] ep 0 it 80 total=21.1564 mle=6.0713 pcon=6.3940 forget=8.6911 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.089463233947754
[loss] ep 0 it 81 total=21.1527 mle=6.0675 pcon=6.3939 forget=8.6913 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.087406158447266
[loss] ep 0 it 82 total=21.1486 mle=6.0636 pcon=6.3938 forget=8.6912 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.08820915222168
[loss] ep 0 it 83 total=21.1442 mle=6.0598 pcon=6.3938 forget=8.6907 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.093310356140137
[loss] ep 0 it 84 total=21.1403 mle=6.0558 pcon=6.3937 forget=8.6907 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.092676162719727
[loss] ep 0 it 85 total=21.1359 mle=6.0520 pcon=6.3936 forget=8.6902 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.097652435302734
[loss] ep 0 it 86 total=21.1321 mle=6.0483 pcon=6.3935 forget=8.6903 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.097066879272461
[loss] ep 0 it 87 total=21.1280 mle=6.0445 pcon=6.3935 forget=8.6900 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.099523544311523
[loss] ep 0 it 88 total=21.1242 mle=6.0408 pcon=6.3934 forget=8.6901 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.099431991577148
[loss] ep 0 it 89 total=21.1201 mle=6.0371 pcon=6.3933 forget=8.6897 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.102834701538086
[loss] ep 0 it 90 total=21.1160 mle=6.0334 pcon=6.3932 forget=8.6894 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.105737686157227
[loss] ep 0 it 91 total=21.1122 mle=6.0296 pcon=6.3931 forget=8.6894 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.105579376220703
[loss] ep 0 it 92 total=21.1082 mle=6.0259 pcon=6.3930 forget=8.6893 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.107230186462402
[loss] ep 0 it 93 total=21.1041 mle=6.0223 pcon=6.3930 forget=8.6889 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.1112699508667
[loss] ep 0 it 94 total=21.1004 mle=6.0186 pcon=6.3929 forget=8.6889 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.110937118530273
[loss] ep 0 it 95 total=21.0965 mle=6.0151 pcon=6.3928 forget=8.6887 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.113433837890625
[loss] ep 0 it 96 total=21.0925 mle=6.0114 pcon=6.3927 forget=8.6884 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.115848541259766
[loss] ep 0 it 97 total=21.0888 mle=6.0079 pcon=6.3926 forget=8.6883 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.11674976348877
[loss] ep 0 it 98 total=21.0849 mle=6.0044 pcon=6.3925 forget=8.6880 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.120264053344727
[loss] ep 0 it 99 total=21.0809 mle=6.0007 pcon=6.3924 forget=8.6877 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.122701644897461
[loss] ep 0 it 100 total=21.0770 mle=5.9972 pcon=6.3924 forget=8.6875 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.12539005279541
[loss] ep 0 it 101 total=21.0733 mle=5.9938 pcon=6.3923 forget=8.6873 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.1270112991333
[loss] ep 0 it 102 total=21.0697 mle=5.9903 pcon=6.3922 forget=8.6873 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.127235412597656
[loss] ep 0 it 103 total=21.0657 mle=5.9867 pcon=6.3921 forget=8.6869 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.130561828613281
[loss] ep 0 it 104 total=21.0620 mle=5.9833 pcon=6.3920 forget=8.6867 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.133293151855469
[loss] ep 0 it 105 total=21.0582 mle=5.9799 pcon=6.3919 forget=8.6865 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.135307312011719
[loss] ep 0 it 106 total=21.0548 mle=5.9765 pcon=6.3918 forget=8.6865 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.135344505310059
[loss] ep 0 it 107 total=21.0507 mle=5.9731 pcon=6.3917 forget=8.6860 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.140318870544434
[loss] ep 0 it 108 total=21.0472 mle=5.9696 pcon=6.3916 forget=8.6860 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.139949798583984
[loss] ep 0 it 109 total=21.0434 mle=5.9663 pcon=6.3915 forget=8.6856 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.144050598144531
[loss] ep 0 it 110 total=21.0397 mle=5.9629 pcon=6.3914 forget=8.6854 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.145709991455078
[loss] ep 0 it 111 total=21.0362 mle=5.9597 pcon=6.3913 forget=8.6852 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.147903442382812
[loss] ep 0 it 112 total=21.0324 mle=5.9562 pcon=6.3912 forget=8.6850 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.150476455688477
[loss] ep 0 it 113 total=21.0284 mle=5.9531 pcon=6.3911 forget=8.6842 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.157888412475586
[loss] ep 0 it 114 total=21.0252 mle=5.9498 pcon=6.3910 forget=8.6845 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.15542221069336
[loss] ep 0 it 115 total=21.0215 mle=5.9466 pcon=6.3909 forget=8.6840 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.159889221191406
[loss] ep 0 it 116 total=21.0181 mle=5.9433 pcon=6.3908 forget=8.6840 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.159857749938965
[loss] ep 0 it 117 total=21.0146 mle=5.9401 pcon=6.3907 forget=8.6838 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.161553382873535
[loss] ep 0 it 118 total=21.0109 mle=5.9368 pcon=6.3906 forget=8.6835 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.16497802734375
[loss] ep 0 it 119 total=21.0075 mle=5.9337 pcon=6.3905 forget=8.6833 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.166982650756836
[loss] ep 0 it 120 total=21.0040 mle=5.9304 pcon=6.3904 forget=8.6832 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.168103218078613
[loss] ep 0 it 121 total=21.0003 mle=5.9273 pcon=6.3903 forget=8.6827 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.172889709472656
[loss] ep 0 it 122 total=20.9966 mle=5.9243 pcon=6.3901 forget=8.6822 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.178497314453125
[loss] ep 0 it 123 total=20.9933 mle=5.9212 pcon=6.3900 forget=8.6820 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.179741859436035
[loss] ep 0 it 124 total=20.9898 mle=5.9179 pcon=6.3899 forget=8.6820 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.180181503295898
[loss] ep 0 it 125 total=20.9859 mle=5.9150 pcon=6.3898 forget=8.6811 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.188621520996094
[loss] ep 0 it 126 total=20.9832 mle=5.9118 pcon=6.3897 forget=8.6816 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.18378734588623
[loss] ep 0 it 127 total=20.9792 mle=5.9088 pcon=6.3896 forget=8.6808 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.191718101501465
[loss] ep 0 it 128 total=20.9756 mle=5.9057 pcon=6.3895 forget=8.6804 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.196416854858398
[loss] ep 0 it 129 total=20.9727 mle=5.9028 pcon=6.3894 forget=8.6806 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.194268226623535
[loss] ep 0 it 130 total=20.9692 mle=5.8998 pcon=6.3892 forget=8.6802 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.197724342346191
[loss] ep 0 it 131 total=20.9656 mle=5.8968 pcon=6.3891 forget=8.6797 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.202552795410156
[loss] ep 0 it 132 total=20.9624 mle=5.8939 pcon=6.3890 forget=8.6795 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.204755783081055
[loss] ep 0 it 133 total=20.9593 mle=5.8909 pcon=6.3889 forget=8.6795 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.205095291137695
[loss] ep 0 it 134 total=20.9555 mle=5.8880 pcon=6.3888 forget=8.6788 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.212435722351074
[loss] ep 0 it 135 total=20.9519 mle=5.8851 pcon=6.3886 forget=8.6782 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.21805191040039
[loss] ep 0 it 136 total=20.9494 mle=5.8821 pcon=6.3885 forget=8.6787 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.212540626525879
[loss] ep 0 it 137 total=20.9455 mle=5.8792 pcon=6.3884 forget=8.6778 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.221773147583008
[loss] ep 0 it 138 total=20.9421 mle=5.8764 pcon=6.3883 forget=8.6775 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.225106239318848
[loss] ep 0 it 139 total=20.9388 mle=5.8735 pcon=6.3881 forget=8.6771 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.229117393493652
[loss] ep 0 it 140 total=20.9353 mle=5.8706 pcon=6.3880 forget=8.6766 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.233604431152344
[loss] ep 0 it 141 total=20.9323 mle=5.8679 pcon=6.3879 forget=8.6765 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.234781265258789
[loss] ep 0 it 142 total=20.9294 mle=5.8651 pcon=6.3878 forget=8.6765 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.234686851501465
[loss] ep 0 it 143 total=20.9259 mle=5.8623 pcon=6.3877 forget=8.6759 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.240704536437988
[loss] ep 0 it 144 total=20.9229 mle=5.8595 pcon=6.3875 forget=8.6759 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.241406440734863
[loss] ep 0 it 145 total=20.9195 mle=5.8568 pcon=6.3874 forget=8.6754 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.24641227722168
[loss] ep 0 it 146 total=20.9165 mle=5.8539 pcon=6.3873 forget=8.6753 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.24724006652832
[loss] ep 0 it 147 total=20.9130 mle=5.8514 pcon=6.3871 forget=8.6745 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.255331039428711
[loss] ep 0 it 148 total=20.9096 mle=5.8486 pcon=6.3870 forget=8.6740 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.259933471679688
[loss] ep 0 it 149 total=20.9065 mle=5.8459 pcon=6.3869 forget=8.6737 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.263218879699707
[loss] ep 0 it 150 total=20.9034 mle=5.8432 pcon=6.3867 forget=8.6734 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.265850067138672
[loss] ep 0 it 151 total=20.9003 mle=5.8406 pcon=6.3866 forget=8.6732 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.26850414276123
[loss] ep 0 it 152 total=20.8972 mle=5.8378 pcon=6.3865 forget=8.6729 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.270683288574219
[loss] ep 0 it 153 total=20.8943 mle=5.8353 pcon=6.3863 forget=8.6727 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.272990226745605
[loss] ep 0 it 154 total=20.8906 mle=5.8326 pcon=6.3862 forget=8.6718 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.282496452331543
[loss] ep 0 it 155 total=20.8876 mle=5.8300 pcon=6.3861 forget=8.6715 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.285228729248047
[loss] ep 0 it 156 total=20.8846 mle=5.8275 pcon=6.3859 forget=8.6712 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.287821769714355
[loss] ep 0 it 157 total=20.8811 mle=5.8248 pcon=6.3858 forget=8.6705 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.295083045959473
[loss] ep 0 it 158 total=20.8785 mle=5.8223 pcon=6.3856 forget=8.6705 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.294759750366211
[loss] ep 0 it 159 total=20.8753 mle=5.8197 pcon=6.3855 forget=8.6702 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.29836368560791
[loss] ep 0 it 160 total=20.8719 mle=5.8172 pcon=6.3854 forget=8.6693 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.306724548339844
[loss] ep 0 it 161 total=20.8692 mle=5.8147 pcon=6.3852 forget=8.6693 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.306955337524414
[loss] ep 0 it 162 total=20.8662 mle=5.8122 pcon=6.3851 forget=8.6690 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.310327529907227
[loss] ep 0 it 163 total=20.8631 mle=5.8097 pcon=6.3849 forget=8.6685 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.314759254455566
[loss] ep 0 it 164 total=20.8596 mle=5.8072 pcon=6.3848 forget=8.6676 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.324037551879883
[loss] ep 0 it 165 total=20.8568 mle=5.8047 pcon=6.3846 forget=8.6674 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.325977325439453
[loss] ep 0 it 166 total=20.8536 mle=5.8023 pcon=6.3845 forget=8.6668 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.332353591918945
[loss] ep 0 it 167 total=20.8504 mle=5.7999 pcon=6.3843 forget=8.6662 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.338409423828125
[loss] ep 0 it 168 total=20.8479 mle=5.7975 pcon=6.3842 forget=8.6662 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.337672233581543
[loss] ep 0 it 169 total=20.8448 mle=5.7951 pcon=6.3840 forget=8.6656 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.34363842010498
[loss] ep 0 it 170 total=20.8415 mle=5.7927 pcon=6.3839 forget=8.6649 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.351024627685547
[loss] ep 0 it 171 total=20.8388 mle=5.7903 pcon=6.3837 forget=8.6648 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.35212516784668
[loss] ep 0 it 172 total=20.8351 mle=5.7879 pcon=6.3836 forget=8.6636 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.363680839538574
[loss] ep 0 it 173 total=20.8321 mle=5.7856 pcon=6.3835 forget=8.6631 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.369400978088379
[loss] ep 0 it 174 total=20.8295 mle=5.7832 pcon=6.3833 forget=8.6630 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.370147705078125
[loss] ep 0 it 175 total=20.8273 mle=5.7809 pcon=6.3831 forget=8.6633 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.367116928100586
[loss] ep 0 it 176 total=20.8237 mle=5.7786 pcon=6.3830 forget=8.6621 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.378608703613281
[loss] ep 0 it 177 total=20.8209 mle=5.7764 pcon=6.3828 forget=8.6617 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.382721900939941
[loss] ep 0 it 178 total=20.8180 mle=5.7740 pcon=6.3827 forget=8.6613 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.387049674987793
[loss] ep 0 it 179 total=20.8144 mle=5.7717 pcon=6.3825 forget=8.6602 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.39773941040039
100%|██████████| 1/1 [00:12<00:00, 13.00s/it]100%|██████████| 1/1 [00:12<00:00, 13.00s/it]
[loss] ep 0 it 180 total=20.8117 mle=5.7696 pcon=6.3824 forget=8.6597 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.402586936950684
[loss] ep 0 it 181 total=20.8090 mle=5.7673 pcon=6.3822 forget=8.6595 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.40519905090332
[loss] ep 0 it 182 total=20.8060 mle=5.7651 pcon=6.3820 forget=8.6588 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.411855697631836
[loss] ep 0 it 183 total=20.8028 mle=5.7627 pcon=6.3819 forget=8.6583 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.417108535766602
[loss] ep 0 it 184 total=20.8002 mle=5.7606 pcon=6.3817 forget=8.6578 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.421647071838379
[loss] ep 0 it 185 total=20.7968 mle=5.7583 pcon=6.3816 forget=8.6569 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.43082046508789
[loss] ep 0 it 186 total=20.7937 mle=5.7562 pcon=6.3814 forget=8.6561 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.438758850097656
[loss] ep 0 it 187 total=20.7911 mle=5.7540 pcon=6.3812 forget=8.6559 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.44061279296875
[loss] ep 0 it 188 total=20.7875 mle=5.7518 pcon=6.3811 forget=8.6546 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.454444885253906
[loss] ep 0 it 189 total=20.7850 mle=5.7498 pcon=6.3809 forget=8.6543 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.456693649291992
[loss] ep 0 it 190 total=20.7822 mle=5.7476 pcon=6.3807 forget=8.6538 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.462291717529297
[loss] ep 0 it 191 total=20.7795 mle=5.7455 pcon=6.3806 forget=8.6534 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.465717315673828
[loss] ep 0 it 192 total=20.7762 mle=5.7434 pcon=6.3804 forget=8.6524 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.475813865661621
[loss] ep 0 it 193 total=20.7742 mle=5.7412 pcon=6.3802 forget=8.6527 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.472979545593262
[loss] ep 0 it 194 total=20.7704 mle=5.7393 pcon=6.3801 forget=8.6511 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.488998413085938
[loss] ep 0 it 195 total=20.7685 mle=5.7371 pcon=6.3799 forget=8.6515 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.485315322875977
[loss] ep 0 it 196 total=20.7653 mle=5.7350 pcon=6.3797 forget=8.6505 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.494844436645508
[loss] ep 0 it 197 total=20.7612 mle=5.7330 pcon=6.3795 forget=8.6486 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.514307022094727
[loss] ep 0 it 198 total=20.7583 mle=5.7309 pcon=6.3794 forget=8.6480 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.52004623413086
[loss] ep 0 it 199 total=20.7557 mle=5.7288 pcon=6.3792 forget=8.6476 nr=64 nf=64 centers=90x512 P=512x512 dmin_norm=13.523630142211914
Done fixed-batch LoRA debug run.
