nohup: ignoring input unrecorded
==== Stage 1: inc={0,8,11,40,51}; seen={}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='0,8,11,40,51', forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt

  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():

  2%|▏         | 1/50 [00:07<06:03,  7.42s/it]
  4%|▍         | 2/50 [00:08<02:46,  3.46s/it]
  6%|▌         | 3/50 [00:08<01:42,  2.19s/it]
  8%|▊         | 4/50 [00:09<01:10,  1.53s/it]
 10%|█         | 5/50 [00:09<00:53,  1.19s/it]
 12%|█▏        | 6/50 [00:10<00:45,  1.03s/it]
 14%|█▍        | 7/50 [00:11<00:38,  1.11it/s]
 16%|█▌        | 8/50 [00:11<00:34,  1.23it/s]
 18%|█▊        | 9/50 [00:12<00:30,  1.33it/s]
 20%|██        | 10/50 [00:13<00:27,  1.45it/s]
 22%|██▏       | 11/50 [00:13<00:27,  1.39it/s]
 24%|██▍       | 12/50 [00:14<00:25,  1.49it/s]
 26%|██▌       | 13/50 [00:15<00:26,  1.40it/s]
 28%|██▊       | 14/50 [00:15<00:25,  1.41it/s]
 30%|███       | 15/50 [00:16<00:24,  1.45it/s]
 32%|███▏      | 16/50 [00:17<00:23,  1.47it/s]
 34%|███▍      | 17/50 [00:17<00:22,  1.50it/s]
 36%|███▌      | 18/50 [00:18<00:21,  1.52it/s]
 38%|███▊      | 19/50 [00:19<00:20,  1.51it/s]
 40%|████      | 20/50 [00:19<00:19,  1.55it/s]
 42%|████▏     | 21/50 [00:20<00:18,  1.57it/s]
 44%|████▍     | 22/50 [00:20<00:17,  1.64it/s]
 46%|████▌     | 23/50 [00:21<00:16,  1.68it/s]
 48%|████▊     | 24/50 [00:22<00:15,  1.66it/s]
 50%|█████     | 25/50 [00:23<00:17,  1.42it/s]
 52%|█████▏    | 26/50 [00:23<00:15,  1.51it/s]
 54%|█████▍    | 27/50 [00:24<00:14,  1.57it/s]
 56%|█████▌    | 28/50 [00:24<00:14,  1.48it/s]
 58%|█████▊    | 29/50 [00:25<00:13,  1.52it/s][loss] ep 0 it 0 total=17.3683 mle=11.7426 pcon=5.6257 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 2 it 10 total=17.0159 mle=11.3822 pcon=5.6338 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 5 it 0 total=13.1338 mle=7.4960 pcon=5.6378 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 7 it 10 total=10.8175 mle=5.1958 pcon=5.6217 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 10 it 0 total=10.3561 mle=4.7561 pcon=5.6000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 12 it 10 total=10.1757 mle=4.5980 pcon=5.5777 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 15 it 0 total=9.8509 mle=4.2952 pcon=5.5557 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 17 it 10 total=9.6582 mle=4.1243 pcon=5.5339 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 20 it 0 total=9.5007 mle=3.9884 pcon=5.5124 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 22 it 10 total=9.2995 mle=3.8087 pcon=5.4908 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 25 it 0 total=9.1662 mle=3.6970 pcon=5.4692 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 27 it 10 total=9.0699 mle=3.6224 pcon=5.4475 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1

 60%|██████    | 30/50 [00:26<00:13,  1.46it/s]
 62%|██████▏   | 31/50 [00:26<00:12,  1.49it/s]
 64%|██████▍   | 32/50 [00:27<00:11,  1.50it/s]
 66%|██████▌   | 33/50 [00:28<00:11,  1.51it/s]
 68%|██████▊   | 34/50 [00:28<00:10,  1.50it/s]
 70%|███████   | 35/50 [00:29<00:09,  1.52it/s]
 72%|███████▏  | 36/50 [00:30<00:10,  1.38it/s]
 74%|███████▍  | 37/50 [00:31<00:10,  1.29it/s]
 76%|███████▌  | 38/50 [00:32<00:09,  1.31it/s]
 78%|███████▊  | 39/50 [00:32<00:07,  1.42it/s]
 80%|████████  | 40/50 [00:33<00:07,  1.39it/s]
 82%|████████▏ | 41/50 [00:34<00:07,  1.25it/s]
 84%|████████▍ | 42/50 [00:35<00:06,  1.33it/s]
 86%|████████▌ | 43/50 [00:35<00:05,  1.32it/s]
 88%|████████▊ | 44/50 [00:36<00:04,  1.31it/s]
 90%|█████████ | 45/50 [00:37<00:03,  1.37it/s]
 92%|█████████▏| 46/50 [00:37<00:02,  1.40it/s]
 94%|█████████▍| 47/50 [00:38<00:02,  1.45it/s]
 96%|█████████▌| 48/50 [00:39<00:01,  1.48it/s]
 98%|█████████▊| 49/50 [00:39<00:00,  1.45it/s]
100%|██████████| 50/50 [00:40<00:00,  1.52it/s]
100%|██████████| 50/50 [00:40<00:00,  1.24it/s]
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 30 it 0 total=8.9881 mle=3.5621 pcon=5.4260 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 32 it 10 total=8.9616 mle=3.5572 pcon=5.4045 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 35 it 0 total=8.8749 mle=3.4918 pcon=5.3832 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 37 it 10 total=8.8193 mle=3.4571 pcon=5.3623 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 40 it 0 total=8.7942 mle=3.4526 pcon=5.3417 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 42 it 10 total=8.7465 mle=3.4248 pcon=5.3217 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 45 it 0 total=8.7256 mle=3.4233 pcon=5.3023 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 47 it 10 total=8.6749 mle=3.3914 pcon=5.2835 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] active adapters set to: default
resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stage1-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images

  0%|          | 0/430 [00:00<?, ?it/s]
  0%|          | 1/430 [00:00<03:14,  2.20it/s]
  3%|▎         | 11/430 [00:00<00:16, 25.31it/s]
  5%|▍         | 21/430 [00:00<00:09, 43.79it/s]
  7%|▋         | 31/430 [00:00<00:06, 57.94it/s]
 10%|▉         | 41/430 [00:00<00:05, 68.34it/s]
 12%|█▏        | 51/430 [00:00<00:05, 74.02it/s]
 14%|█▍        | 60/430 [00:01<00:04, 77.82it/s]
 16%|█▋        | 70/430 [00:01<00:04, 83.23it/s]
 19%|█▊        | 80/430 [00:01<00:04, 86.65it/s]
 21%|██        | 90/430 [00:01<00:03, 88.56it/s]
 23%|██▎       | 100/430 [00:01<00:03, 90.86it/s]
 26%|██▌       | 110/430 [00:01<00:03, 91.54it/s]
 28%|██▊       | 120/430 [00:01<00:03, 91.85it/s]
 30%|███       | 130/430 [00:01<00:03, 93.19it/s]
 33%|███▎      | 140/430 [00:01<00:03, 93.64it/s]
 35%|███▍      | 150/430 [00:02<00:02, 94.28it/s]
 37%|███▋      | 160/430 [00:02<00:02, 94.79it/s]
 40%|███▉      | 170/430 [00:02<00:02, 95.24it/s]
 42%|████▏     | 180/430 [00:02<00:02, 95.17it/s]
 44%|████▍     | 190/430 [00:02<00:02, 94.79it/s]
 47%|████▋     | 200/430 [00:02<00:02, 94.96it/s]
 49%|████▉     | 210/430 [00:02<00:02, 95.29it/s]
 51%|█████     | 220/430 [00:02<00:02, 95.71it/s]
 53%|█████▎    | 230/430 [00:02<00:02, 95.92it/s]
 56%|█████▌    | 240/430 [00:02<00:01, 96.20it/s]
 58%|█████▊    | 250/430 [00:03<00:01, 96.24it/s]
 60%|██████    | 260/430 [00:03<00:01, 96.28it/s]
 63%|██████▎   | 270/430 [00:03<00:01, 96.39it/s]
 65%|██████▌   | 280/430 [00:03<00:01, 96.48it/s]
 67%|██████▋   | 290/430 [00:03<00:01, 96.65it/s]
 70%|██████▉   | 300/430 [00:03<00:01, 96.55it/s]
 72%|███████▏  | 310/430 [00:03<00:01, 96.70it/s]
 74%|███████▍  | 320/430 [00:03<00:01, 96.63it/s]
 77%|███████▋  | 330/430 [00:03<00:01, 96.57it/s]
 79%|███████▉  | 340/430 [00:04<00:00, 96.68it/s]
 81%|████████▏ | 350/430 [00:04<00:00, 96.65it/s]
 84%|████████▎ | 360/430 [00:04<00:00, 96.60it/s]
 86%|████████▌ | 370/430 [00:04<00:00, 96.38it/s]
 88%|████████▊ | 380/430 [00:04<00:00, 96.14it/s]
 91%|█████████ | 390/430 [00:04<00:00, 95.94it/s]
 93%|█████████▎| 400/430 [00:04<00:00, 95.21it/s]
 95%|█████████▌| 410/430 [00:04<00:00, 95.72it/s]
 98%|█████████▊| 420/430 [00:04<00:00, 95.97it/s]
100%|██████████| 430/430 [00:04<00:00, 93.96it/s]
100%|██████████| 430/430 [00:04<00:00, 86.70it/s]
55000 images processed, 5.011584997177124 seconds used

Processing in-distribution CIFAR-110 images

  0%|          | 0/86 [00:00<?, ?it/s]
  1%|          | 1/86 [00:00<00:36,  2.34it/s]
 12%|█▏        | 10/86 [00:00<00:03, 24.00it/s]
 23%|██▎       | 20/86 [00:00<00:01, 43.02it/s]
 35%|███▍      | 30/86 [00:00<00:00, 57.37it/s]
 47%|████▋     | 40/86 [00:00<00:00, 68.24it/s]
 58%|█████▊    | 50/86 [00:00<00:00, 75.75it/s]
 70%|██████▉   | 60/86 [00:01<00:00, 81.53it/s]
 81%|████████▏ | 70/86 [00:01<00:00, 86.14it/s]
 93%|█████████▎| 80/86 [00:01<00:00, 89.53it/s]
100%|██████████| 86/86 [00:01<00:00, 64.59it/s]
11000 images processed, 1.3441097736358643 seconds used

Processing out-of-distribution SVHN images

  0%|          | 0/204 [00:00<?, ?it/s]
  0%|          | 1/204 [00:00<01:21,  2.49it/s]
  4%|▍         | 9/204 [00:00<00:08, 22.65it/s]
  9%|▉         | 19/204 [00:00<00:04, 42.59it/s]
 14%|█▍        | 29/204 [00:00<00:03, 57.55it/s]
 19%|█▉        | 39/204 [00:00<00:02, 68.42it/s]
 24%|██▍       | 49/204 [00:00<00:02, 76.26it/s]
 29%|██▉       | 59/204 [00:01<00:01, 82.13it/s]
 34%|███▍      | 69/204 [00:01<00:01, 85.50it/s]
 39%|███▊      | 79/204 [00:01<00:01, 87.33it/s]
 44%|████▎     | 89/204 [00:01<00:01, 88.45it/s]
 49%|████▊     | 99/204 [00:01<00:01, 90.08it/s]
 53%|█████▎    | 109/204 [00:01<00:01, 91.46it/s]
 58%|█████▊    | 119/204 [00:01<00:00, 92.63it/s]
 63%|██████▎   | 129/204 [00:01<00:00, 89.40it/s]
 68%|██████▊   | 139/204 [00:01<00:00, 90.77it/s]
 73%|███████▎  | 149/204 [00:02<00:00, 91.10it/s]
 78%|███████▊  | 159/204 [00:02<00:00, 91.91it/s]
 83%|████████▎ | 169/204 [00:02<00:00, 92.25it/s]
 88%|████████▊ | 179/204 [00:02<00:00, 91.97it/s]
 93%|█████████▎| 189/204 [00:02<00:00, 92.86it/s]
 98%|█████████▊| 199/204 [00:02<00:00, 93.25it/s]
100%|██████████| 204/204 [00:02<00:00, 78.28it/s]
26032 images processed, 2.6438217163085938 seconds used

Processing out-of-distribution places365 images

  0%|          | 0/79 [00:00<?, ?it/s]
  1%|▏         | 1/79 [00:00<00:51,  1.51it/s]
 13%|█▎        | 10/79 [00:00<00:04, 16.98it/s]
 25%|██▌       | 20/79 [00:00<00:01, 32.19it/s]
 38%|███▊      | 30/79 [00:00<00:01, 46.25it/s]
 51%|█████     | 40/79 [00:01<00:00, 58.01it/s]
 63%|██████▎   | 50/79 [00:01<00:00, 67.57it/s]
 76%|███████▌  | 60/79 [00:01<00:00, 75.10it/s]
 89%|████████▊ | 70/79 [00:01<00:00, 80.68it/s]
100%|██████████| 79/79 [00:01<00:00, 51.65it/s]
10000 images processed, 1.554316520690918 seconds used

Processing out-of-distribution LSUN images

  0%|          | 0/79 [00:00<?, ?it/s]
  1%|▏         | 1/79 [00:00<00:35,  2.23it/s]
 13%|█▎        | 10/79 [00:00<00:02, 23.30it/s]
 25%|██▌       | 20/79 [00:00<00:01, 42.19it/s]
 38%|███▊      | 30/79 [00:00<00:00, 56.85it/s]
 51%|█████     | 40/79 [00:00<00:00, 67.49it/s]
 63%|██████▎   | 50/79 [00:00<00:00, 75.63it/s]
 76%|███████▌  | 60/79 [00:01<00:00, 81.79it/s]
 89%|████████▊ | 70/79 [00:01<00:00, 86.37it/s]
100%|██████████| 79/79 [00:01<00:00, 62.57it/s]
10000 images processed, 1.288262128829956 seconds used

Processing out-of-distribution iSUN images

  0%|          | 0/70 [00:00<?, ?it/s]
  1%|▏         | 1/70 [00:00<00:33,  2.05it/s]
 16%|█▌        | 11/70 [00:00<00:02, 23.86it/s]
 30%|███       | 21/70 [00:00<00:01, 41.49it/s]
 44%|████▍     | 31/70 [00:00<00:00, 55.59it/s]
 59%|█████▊    | 41/70 [00:00<00:00, 66.05it/s]
 73%|███████▎  | 51/70 [00:01<00:00, 74.55it/s]
 87%|████████▋ | 61/70 [00:01<00:00, 80.84it/s]
100%|██████████| 70/70 [00:01<00:00, 56.53it/s]
8925 images processed, 1.3105950355529785 seconds used

Processing out-of-distribution dtd images

  0%|          | 0/45 [00:00<?, ?it/s]
  2%|▏         | 1/45 [00:01<00:47,  1.09s/it]
  4%|▍         | 2/45 [00:01<00:22,  1.88it/s]
 27%|██▋       | 12/45 [00:01<00:02, 15.12it/s]
 38%|███▊      | 17/45 [00:01<00:01, 16.93it/s]
 47%|████▋     | 21/45 [00:01<00:01, 20.01it/s]
 56%|█████▌    | 25/45 [00:02<00:01, 16.18it/s]
 73%|███████▎  | 33/45 [00:02<00:00, 24.82it/s]
 82%|████████▏ | 37/45 [00:02<00:00, 27.05it/s]
 91%|█████████ | 41/45 [00:02<00:00, 20.26it/s]
100%|██████████| 45/45 [00:02<00:00, 16.88it/s]
5640 images processed, 2.6892566680908203 seconds used

17.65595316886902
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.54  99.04  73.65
places365     49.44  89.46  51.36
LSUN          25.65  95.16  76.03
iSUN          25.79  95.00  73.66
dtd           24.50  94.41  71.71
AVG           25.78  94.61  69.28
[incremental] Overall: 0.8500 New: 0.8500 Old: nan
[incremental] Final(Top-1): 0.8500  Average: 0.8500
1.7247505187988281
==== Stage 2: inc={66,67,88,94,57}; seen={0,8,11,40,51}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2', adapter_load_path='checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1', lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='66,67,88,94,57', forget_classes_seen='0,8,11,40,51', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt

  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():

  2%|▏         | 1/50 [00:07<06:27,  7.90s/it]
  4%|▍         | 2/50 [00:08<02:56,  3.67s/it]
  6%|▌         | 3/50 [00:09<01:45,  2.25s/it]
  8%|▊         | 4/50 [00:09<01:14,  1.61s/it]
 10%|█         | 5/50 [00:10<00:56,  1.25s/it]
 12%|█▏        | 6/50 [00:10<00:44,  1.02s/it]
 14%|█▍        | 7/50 [00:11<00:41,  1.04it/s]
 16%|█▌        | 8/50 [00:12<00:37,  1.12it/s]
 18%|█▊        | 9/50 [00:13<00:34,  1.18it/s]
 20%|██        | 10/50 [00:13<00:30,  1.31it/s]
 22%|██▏       | 11/50 [00:14<00:29,  1.31it/s]
 24%|██▍       | 12/50 [00:15<00:27,  1.37it/s]
 26%|██▌       | 13/50 [00:15<00:25,  1.45it/s]
 28%|██▊       | 14/50 [00:16<00:24,  1.46it/s]
 30%|███       | 15/50 [00:17<00:25,  1.40it/s]
 32%|███▏      | 16/50 [00:17<00:23,  1.47it/s]
 34%|███▍      | 17/50 [00:18<00:22,  1.48it/s]
 36%|███▌      | 18/50 [00:19<00:21,  1.51it/s]
 38%|███▊      | 19/50 [00:19<00:20,  1.52it/s]
 40%|████      | 20/50 [00:20<00:18,  1.59it/s]
 42%|████▏     | 21/50 [00:21<00:18,  1.60it/s]
 44%|████▍     | 22/50 [00:21<00:17,  1.62it/s]
 46%|████▌     | 23/50 [00:22<00:16,  1.61it/s]
 48%|████▊     | 24/50 [00:22<00:15,  1.67it/s]
 50%|█████     | 25/50 [00:23<00:15,  1.61it/s]
 52%|█████▏    | 26/50 [00:24<00:14,  1.61it/s]
 54%|█████▍    | 27/50 [00:24<00:15,  1.48it/s]
 56%|█████▌    | 28/50 [00:25<00:14,  1.56it/s]
 58%|█████▊    | 29/50 [00:26<00:14,  1.42it/s][loss] ep 0 it 0 total=14.5608 mle=5.5834 pcon=8.9774 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 2 it 10 total=14.1338 mle=5.1691 pcon=8.9647 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 5 it 0 total=13.7627 mle=4.8132 pcon=8.9494 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 7 it 10 total=13.4843 mle=4.5523 pcon=8.9320 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 10 it 0 total=13.1533 mle=4.2403 pcon=8.9130 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 12 it 10 total=12.9276 mle=4.0348 pcon=8.8928 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 15 it 0 total=12.7366 mle=3.8648 pcon=8.8718 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 17 it 10 total=12.6259 mle=3.7757 pcon=8.8502 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 20 it 0 total=12.4812 mle=3.6530 pcon=8.8282 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 22 it 10 total=12.3760 mle=3.5702 pcon=8.8057 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 25 it 0 total=12.3287 mle=3.5454 pcon=8.7833 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 27 it 10 total=12.2692 mle=3.5085 pcon=8.7607 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2

 60%|██████    | 30/50 [00:27<00:15,  1.31it/s]
 62%|██████▏   | 31/50 [00:27<00:13,  1.39it/s]
 64%|██████▍   | 32/50 [00:28<00:12,  1.46it/s]
 66%|██████▌   | 33/50 [00:29<00:11,  1.45it/s]
 68%|██████▊   | 34/50 [00:29<00:11,  1.44it/s]
 70%|███████   | 35/50 [00:30<00:10,  1.47it/s]
 72%|███████▏  | 36/50 [00:31<00:09,  1.49it/s]
 74%|███████▍  | 37/50 [00:31<00:08,  1.58it/s]
 76%|███████▌  | 38/50 [00:32<00:07,  1.53it/s]
 78%|███████▊  | 39/50 [00:33<00:07,  1.56it/s]
 80%|████████  | 40/50 [00:33<00:06,  1.60it/s]
 82%|████████▏ | 41/50 [00:34<00:05,  1.56it/s]
 84%|████████▍ | 42/50 [00:34<00:05,  1.60it/s]
 86%|████████▌ | 43/50 [00:35<00:04,  1.54it/s]
 88%|████████▊ | 44/50 [00:36<00:03,  1.53it/s]
 90%|█████████ | 45/50 [00:36<00:03,  1.56it/s]
 92%|█████████▏| 46/50 [00:37<00:02,  1.59it/s]
 94%|█████████▍| 47/50 [00:38<00:01,  1.64it/s]
 96%|█████████▌| 48/50 [00:38<00:01,  1.54it/s]
 98%|█████████▊| 49/50 [00:39<00:00,  1.53it/s]
100%|██████████| 50/50 [00:40<00:00,  1.52it/s]
100%|██████████| 50/50 [00:40<00:00,  1.25it/s]
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 30 it 0 total=12.2087 mle=3.4704 pcon=8.7383 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 32 it 10 total=12.1607 mle=3.4448 pcon=8.7160 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 35 it 0 total=12.1199 mle=3.4257 pcon=8.6942 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 37 it 10 total=12.0978 mle=3.4251 pcon=8.6728 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 40 it 0 total=12.0757 mle=3.4238 pcon=8.6519 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 42 it 10 total=12.0390 mle=3.4073 pcon=8.6317 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 45 it 0 total=12.0094 mle=3.3972 pcon=8.6122 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 47 it 10 total=11.9887 mle=3.3953 pcon=8.5935 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] active adapters set to: default
resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stage2-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images

  0%|          | 0/430 [00:00<?, ?it/s]
  0%|          | 1/430 [00:00<03:43,  1.92it/s]
  3%|▎         | 11/430 [00:00<00:18, 22.61it/s]
  5%|▍         | 21/430 [00:00<00:10, 39.96it/s]
  7%|▋         | 31/430 [00:00<00:07, 54.28it/s]
 10%|▉         | 41/430 [00:00<00:05, 65.42it/s]
 12%|█▏        | 51/430 [00:01<00:05, 73.82it/s]
 14%|█▍        | 61/430 [00:01<00:04, 80.16it/s]
 17%|█▋        | 71/430 [00:01<00:04, 84.78it/s]
 19%|█▉        | 81/430 [00:01<00:03, 88.09it/s]
 21%|██        | 91/430 [00:01<00:03, 90.49it/s]
 23%|██▎       | 101/430 [00:01<00:03, 84.32it/s]
 26%|██▌       | 110/430 [00:01<00:04, 79.61it/s]
 28%|██▊       | 119/430 [00:01<00:04, 74.62it/s]
 30%|██▉       | 127/430 [00:01<00:04, 74.63it/s]
 31%|███▏      | 135/430 [00:02<00:04, 71.11it/s]
 33%|███▎      | 143/430 [00:02<00:04, 71.19it/s]
 36%|███▌      | 153/430 [00:02<00:03, 77.77it/s]
 38%|███▊      | 163/430 [00:02<00:03, 83.06it/s]
 40%|████      | 173/430 [00:02<00:02, 86.55it/s]
 43%|████▎     | 183/430 [00:02<00:02, 88.36it/s]
 45%|████▍     | 193/430 [00:02<00:02, 89.30it/s]
 47%|████▋     | 202/430 [00:02<00:02, 82.54it/s]
 49%|████▉     | 211/430 [00:02<00:02, 80.42it/s]
 51%|█████     | 220/430 [00:03<00:02, 73.15it/s]
 53%|█████▎    | 230/430 [00:03<00:02, 78.31it/s]
 56%|█████▌    | 240/430 [00:03<00:02, 83.10it/s]
 58%|█████▊    | 250/430 [00:03<00:02, 86.09it/s]
 60%|██████    | 260/430 [00:03<00:01, 87.64it/s]
 63%|██████▎   | 270/430 [00:03<00:01, 90.29it/s]
 65%|██████▌   | 280/430 [00:03<00:01, 91.69it/s]
 67%|██████▋   | 290/430 [00:03<00:01, 92.18it/s]
 70%|██████▉   | 300/430 [00:03<00:01, 91.43it/s]
 72%|███████▏  | 310/430 [00:04<00:01, 86.86it/s]
 74%|███████▍  | 320/430 [00:04<00:01, 88.66it/s]
 77%|███████▋  | 329/430 [00:04<00:01, 84.31it/s]
 79%|███████▉  | 339/430 [00:04<00:01, 87.39it/s]
 81%|████████  | 349/430 [00:04<00:00, 89.92it/s]
 83%|████████▎ | 359/430 [00:04<00:00, 89.21it/s]
 86%|████████▌ | 368/430 [00:04<00:00, 88.27it/s]
 88%|████████▊ | 378/430 [00:04<00:00, 89.98it/s]
 90%|█████████ | 388/430 [00:04<00:00, 90.20it/s]
 93%|█████████▎| 398/430 [00:05<00:00, 87.48it/s]
 95%|█████████▍| 408/430 [00:05<00:00, 90.39it/s]
 97%|█████████▋| 418/430 [00:05<00:00, 92.65it/s]
100%|█████████▉| 428/430 [00:05<00:00, 89.75it/s]
100%|██████████| 430/430 [00:05<00:00, 78.55it/s]
55000 images processed, 5.526772737503052 seconds used

Processing in-distribution CIFAR-110 images

  0%|          | 0/86 [00:00<?, ?it/s]
  1%|          | 1/86 [00:00<00:53,  1.60it/s]
 10%|█         | 9/86 [00:00<00:04, 16.10it/s]
 22%|██▏       | 19/86 [00:00<00:02, 33.37it/s]
 34%|███▎      | 29/86 [00:00<00:01, 48.16it/s]
 45%|████▌     | 39/86 [00:01<00:00, 60.34it/s]
 57%|█████▋    | 49/86 [00:01<00:00, 69.71it/s]
 69%|██████▊   | 59/86 [00:01<00:00, 77.15it/s]
 80%|████████  | 69/86 [00:01<00:00, 82.85it/s]
 92%|█████████▏| 79/86 [00:01<00:00, 86.79it/s]
100%|██████████| 86/86 [00:01<00:00, 56.25it/s]
11000 images processed, 1.5422708988189697 seconds used

Processing out-of-distribution SVHN images

  0%|          | 0/204 [00:00<?, ?it/s]
  0%|          | 1/204 [00:00<01:26,  2.34it/s]
  4%|▍         | 8/204 [00:00<00:10, 19.04it/s]
  9%|▉         | 18/204 [00:00<00:04, 39.22it/s]
 13%|█▎        | 27/204 [00:00<00:03, 52.64it/s]
 18%|█▊        | 37/204 [00:00<00:02, 64.68it/s]
 23%|██▎       | 47/204 [00:00<00:02, 73.34it/s]
 28%|██▊       | 57/204 [00:01<00:01, 79.95it/s]
 33%|███▎      | 67/204 [00:01<00:01, 84.39it/s]
 38%|███▊      | 77/204 [00:01<00:01, 87.15it/s]
 43%|████▎     | 87/204 [00:01<00:01, 89.54it/s]
 48%|████▊     | 97/204 [00:01<00:01, 91.52it/s]
 52%|█████▏    | 107/204 [00:01<00:01, 92.94it/s]
 57%|█████▋    | 117/204 [00:01<00:00, 94.06it/s]
 62%|██████▏   | 127/204 [00:01<00:00, 94.57it/s]
 67%|██████▋   | 137/204 [00:01<00:00, 94.47it/s]
 72%|███████▏  | 147/204 [00:02<00:00, 94.56it/s]
 77%|███████▋  | 157/204 [00:02<00:00, 94.97it/s]
 82%|████████▏ | 167/204 [00:02<00:00, 94.85it/s]
 87%|████████▋ | 177/204 [00:02<00:00, 94.91it/s]
 92%|█████████▏| 187/204 [00:02<00:00, 95.60it/s]
 97%|█████████▋| 197/204 [00:02<00:00, 95.99it/s]
100%|██████████| 204/204 [00:02<00:00, 78.48it/s]
26032 images processed, 2.6459429264068604 seconds used

Processing out-of-distribution places365 images

  0%|          | 0/79 [00:00<?, ?it/s]
  1%|▏         | 1/79 [00:00<00:44,  1.74it/s]
 13%|█▎        | 10/79 [00:00<00:03, 19.01it/s]
 22%|██▏       | 17/79 [00:00<00:02, 26.86it/s]
 33%|███▎      | 26/79 [00:00<00:01, 40.04it/s]
 43%|████▎     | 34/79 [00:01<00:00, 48.57it/s]
 54%|█████▍    | 43/79 [00:01<00:00, 57.83it/s]
 67%|██████▋   | 53/79 [00:01<00:00, 67.79it/s]
 80%|███████▉  | 63/79 [00:01<00:00, 75.61it/s]
 92%|█████████▏| 73/79 [00:01<00:00, 81.62it/s]
100%|██████████| 79/79 [00:01<00:00, 50.98it/s]
10000 images processed, 1.5715196132659912 seconds used

Processing out-of-distribution LSUN images

  0%|          | 0/79 [00:00<?, ?it/s]
  1%|▏         | 1/79 [00:00<00:34,  2.27it/s]
  8%|▊         | 6/79 [00:00<00:05, 13.76it/s]
 20%|██        | 16/79 [00:00<00:01, 35.08it/s]
 33%|███▎      | 26/79 [00:00<00:01, 50.94it/s]
 46%|████▌     | 36/79 [00:00<00:00, 62.63it/s]
 58%|█████▊    | 46/79 [00:00<00:00, 71.59it/s]
 71%|███████   | 56/79 [00:01<00:00, 78.48it/s]
 84%|████████▎ | 66/79 [00:01<00:00, 83.67it/s]
 96%|█████████▌| 76/79 [00:01<00:00, 87.64it/s]
100%|██████████| 79/79 [00:01<00:00, 60.09it/s]
10000 images processed, 1.3359367847442627 seconds used

Processing out-of-distribution iSUN images

  0%|          | 0/70 [00:00<?, ?it/s]
  1%|▏         | 1/70 [00:00<00:31,  2.21it/s]
  7%|▋         | 5/70 [00:00<00:05, 11.01it/s]
 21%|██▏       | 15/70 [00:00<00:01, 32.65it/s]
 36%|███▌      | 25/70 [00:00<00:00, 49.48it/s]
 50%|█████     | 35/70 [00:00<00:00, 61.88it/s]
 63%|██████▎   | 44/70 [00:00<00:00, 68.61it/s]
 77%|███████▋  | 54/70 [00:01<00:00, 76.64it/s]
 91%|█████████▏| 64/70 [00:01<00:00, 82.59it/s]
100%|██████████| 70/70 [00:01<00:00, 55.53it/s]
8925 images processed, 1.289381980895996 seconds used

Processing out-of-distribution dtd images

  0%|          | 0/45 [00:00<?, ?it/s]
  2%|▏         | 1/45 [00:00<00:38,  1.14it/s]
  4%|▍         | 2/45 [00:01<00:19,  2.16it/s]
 27%|██▋       | 12/45 [00:01<00:01, 17.01it/s]
 38%|███▊      | 17/45 [00:01<00:01, 18.39it/s]
 47%|████▋     | 21/45 [00:01<00:01, 21.96it/s]
 56%|█████▌    | 25/45 [00:01<00:00, 24.75it/s]
 73%|███████▎  | 33/45 [00:02<00:00, 21.44it/s]
 93%|█████████▎| 42/45 [00:02<00:00, 31.77it/s]
100%|██████████| 45/45 [00:02<00:00, 19.60it/s]
5640 images processed, 2.318812847137451 seconds used

17.794271230697632
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.78  98.86  78.80
places365     52.90  85.09  47.12
LSUN          44.43  89.39  62.24
iSUN          35.80  91.31  65.41
dtd           32.96  92.09  73.38
AVG           33.97  91.35  65.39
[incremental] Overall: 0.7380 New: 0.7180 Old: 0.7580
[incremental] Final(Top-1): 0.7380  Average: 0.7940
2.391935348510742
==== Stage 3: inc={59,58,44,93,10}; seen={0,8,11,40,51,66,67,88,94,57}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3', adapter_load_path='checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2', lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='59,58,44,93,10', forget_classes_seen='0,8,11,40,51,66,67,88,94,57', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt

  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():

  2%|▏         | 1/50 [00:06<05:02,  6.18s/it]
  4%|▍         | 2/50 [00:06<02:24,  3.02s/it]
  6%|▌         | 3/50 [00:07<01:32,  1.96s/it]
  8%|▊         | 4/50 [00:08<01:05,  1.43s/it]
 10%|█         | 5/50 [00:08<00:50,  1.13s/it]
 12%|█▏        | 6/50 [00:09<00:41,  1.05it/s]
 14%|█▍        | 7/50 [00:10<00:39,  1.08it/s]
 16%|█▌        | 8/50 [00:10<00:34,  1.21it/s]
 18%|█▊        | 9/50 [00:11<00:30,  1.34it/s]
 20%|██        | 10/50 [00:12<00:28,  1.39it/s]
 22%|██▏       | 11/50 [00:12<00:25,  1.53it/s]
 24%|██▍       | 12/50 [00:13<00:24,  1.56it/s]
 26%|██▌       | 13/50 [00:13<00:22,  1.67it/s]
 28%|██▊       | 14/50 [00:14<00:21,  1.68it/s]
 30%|███       | 15/50 [00:14<00:20,  1.71it/s]
 32%|███▏      | 16/50 [00:15<00:19,  1.77it/s]
 34%|███▍      | 17/50 [00:16<00:19,  1.70it/s]
 36%|███▌      | 18/50 [00:16<00:19,  1.63it/s]
 38%|███▊      | 19/50 [00:17<00:19,  1.59it/s]
 40%|████      | 20/50 [00:18<00:18,  1.64it/s]
 42%|████▏     | 21/50 [00:18<00:17,  1.66it/s]
 44%|████▍     | 22/50 [00:19<00:17,  1.64it/s]
 46%|████▌     | 23/50 [00:19<00:17,  1.59it/s]
 48%|████▊     | 24/50 [00:20<00:15,  1.68it/s]
 50%|█████     | 25/50 [00:21<00:14,  1.67it/s]
 52%|█████▏    | 26/50 [00:21<00:14,  1.66it/s]
 54%|█████▍    | 27/50 [00:22<00:13,  1.71it/s]
 56%|█████▌    | 28/50 [00:22<00:12,  1.75it/s]
 58%|█████▊    | 29/50 [00:23<00:11,  1.76it/s][loss] ep 0 it 0 total=15.1286 mle=5.4398 pcon=9.6888 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 2 it 10 total=14.7188 mle=5.0422 pcon=9.6766 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 5 it 0 total=14.3105 mle=4.6490 pcon=9.6615 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 7 it 10 total=13.9806 mle=4.3368 pcon=9.6438 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 10 it 0 total=13.6886 mle=4.0646 pcon=9.6240 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 12 it 10 total=13.4911 mle=3.8883 pcon=9.6028 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 15 it 0 total=13.3258 mle=3.7454 pcon=9.5804 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 17 it 10 total=13.2235 mle=3.6663 pcon=9.5572 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 20 it 0 total=13.1178 mle=3.5842 pcon=9.5336 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 22 it 10 total=13.0140 mle=3.5044 pcon=9.5096 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 25 it 0 total=12.9762 mle=3.4908 pcon=9.4854 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 27 it 10 total=12.8714 mle=3.4101 pcon=9.4613 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3

 60%|██████    | 30/50 [00:23<00:11,  1.73it/s]
 62%|██████▏   | 31/50 [00:24<00:11,  1.66it/s]
 64%|██████▍   | 32/50 [00:25<00:10,  1.64it/s]
 66%|██████▌   | 33/50 [00:25<00:10,  1.68it/s]
 68%|██████▊   | 34/50 [00:26<00:09,  1.62it/s]
 70%|███████   | 35/50 [00:26<00:08,  1.72it/s]
 72%|███████▏  | 36/50 [00:27<00:08,  1.65it/s]
 74%|███████▍  | 37/50 [00:28<00:07,  1.64it/s]
 76%|███████▌  | 38/50 [00:28<00:07,  1.59it/s]
 78%|███████▊  | 39/50 [00:29<00:07,  1.56it/s]
 80%|████████  | 40/50 [00:30<00:06,  1.51it/s]
 82%|████████▏ | 41/50 [00:30<00:05,  1.54it/s]
 84%|████████▍ | 42/50 [00:31<00:05,  1.52it/s]
 86%|████████▌ | 43/50 [00:32<00:04,  1.61it/s]
 88%|████████▊ | 44/50 [00:32<00:03,  1.69it/s]
 90%|█████████ | 45/50 [00:33<00:03,  1.66it/s]
 92%|█████████▏| 46/50 [00:33<00:02,  1.65it/s]
 94%|█████████▍| 47/50 [00:34<00:01,  1.61it/s]
 96%|█████████▌| 48/50 [00:35<00:01,  1.60it/s]
 98%|█████████▊| 49/50 [00:35<00:00,  1.52it/s]
100%|██████████| 50/50 [00:36<00:00,  1.46it/s]
100%|██████████| 50/50 [00:36<00:00,  1.36it/s]
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 30 it 0 total=12.8651 mle=3.4277 pcon=9.4374 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 32 it 10 total=12.8291 mle=3.4151 pcon=9.4140 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 35 it 0 total=12.7841 mle=3.3933 pcon=9.3908 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 37 it 10 total=12.7478 mle=3.3794 pcon=9.3684 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 40 it 0 total=12.7261 mle=3.3794 pcon=9.3467 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 42 it 10 total=12.7153 mle=3.3896 pcon=9.3257 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 45 it 0 total=12.6733 mle=3.3679 pcon=9.3055 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 47 it 10 total=12.6566 mle=3.3705 pcon=9.2861 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] active adapters set to: default
resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stage3-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images

  0%|          | 0/430 [00:00<?, ?it/s]
  0%|          | 1/430 [00:00<03:52,  1.85it/s]
  2%|▏         | 8/430 [00:00<00:26, 16.02it/s]
  4%|▍         | 18/430 [00:00<00:11, 34.85it/s]
  7%|▋         | 28/430 [00:00<00:07, 50.37it/s]
  9%|▉         | 38/430 [00:00<00:06, 61.77it/s]
 11%|█         | 48/430 [00:01<00:05, 71.05it/s]
 13%|█▎        | 58/430 [00:01<00:04, 77.46it/s]
 16%|█▌        | 68/430 [00:01<00:04, 82.45it/s]
 18%|█▊        | 78/430 [00:01<00:04, 86.46it/s]
 20%|██        | 88/430 [00:01<00:03, 89.19it/s]
 23%|██▎       | 98/430 [00:01<00:03, 90.97it/s]
 25%|██▌       | 108/430 [00:01<00:03, 91.04it/s]
 27%|██▋       | 118/430 [00:01<00:03, 92.68it/s]
 30%|██▉       | 128/430 [00:01<00:03, 93.00it/s]
 32%|███▏      | 138/430 [00:02<00:03, 93.69it/s]
 34%|███▍      | 148/430 [00:02<00:02, 94.23it/s]
 37%|███▋      | 158/430 [00:02<00:02, 94.37it/s]
 39%|███▉      | 168/430 [00:02<00:02, 94.62it/s]
 41%|████▏     | 178/430 [00:02<00:02, 94.44it/s]
 44%|████▎     | 188/430 [00:02<00:02, 94.96it/s]
 46%|████▌     | 198/430 [00:02<00:02, 94.63it/s]
 48%|████▊     | 208/430 [00:02<00:02, 94.82it/s]
 51%|█████     | 218/430 [00:02<00:02, 94.47it/s]
 53%|█████▎    | 228/430 [00:02<00:02, 94.57it/s]
 55%|█████▌    | 238/430 [00:03<00:02, 94.91it/s]
 58%|█████▊    | 248/430 [00:03<00:01, 95.09it/s]
 60%|██████    | 258/430 [00:03<00:01, 95.15it/s]
 62%|██████▏   | 268/430 [00:03<00:01, 95.18it/s]
 65%|██████▍   | 278/430 [00:03<00:01, 95.38it/s]
 67%|██████▋   | 288/430 [00:03<00:01, 95.41it/s]
 69%|██████▉   | 298/430 [00:03<00:01, 94.75it/s]
 72%|███████▏  | 308/430 [00:03<00:01, 95.18it/s]
 74%|███████▍  | 318/430 [00:03<00:01, 95.47it/s]
 76%|███████▋  | 328/430 [00:04<00:01, 95.25it/s]
 79%|███████▊  | 338/430 [00:04<00:00, 95.09it/s]
 81%|████████  | 348/430 [00:04<00:00, 94.77it/s]
 83%|████████▎ | 358/430 [00:04<00:00, 94.28it/s]
 86%|████████▌ | 368/430 [00:04<00:00, 93.37it/s]
 88%|████████▊ | 378/430 [00:04<00:00, 93.34it/s]
 90%|█████████ | 388/430 [00:04<00:00, 93.75it/s]
 93%|█████████▎| 398/430 [00:04<00:00, 93.74it/s]
 95%|█████████▍| 408/430 [00:04<00:00, 94.88it/s]
 97%|█████████▋| 418/430 [00:04<00:00, 95.66it/s]
100%|█████████▉| 428/430 [00:05<00:00, 96.18it/s]
100%|██████████| 430/430 [00:05<00:00, 84.33it/s]
55000 images processed, 5.169246435165405 seconds used

Processing in-distribution CIFAR-110 images

  0%|          | 0/86 [00:00<?, ?it/s]
  1%|          | 1/86 [00:00<00:42,  2.01it/s]
 13%|█▎        | 11/86 [00:00<00:03, 23.46it/s]
 24%|██▍       | 21/86 [00:00<00:01, 40.98it/s]
 36%|███▌      | 31/86 [00:00<00:01, 54.78it/s]
 48%|████▊     | 41/86 [00:00<00:00, 65.17it/s]
 59%|█████▉    | 51/86 [00:01<00:00, 73.00it/s]
 71%|███████   | 61/86 [00:01<00:00, 79.40it/s]
 83%|████████▎ | 71/86 [00:01<00:00, 84.23it/s]
 94%|█████████▍| 81/86 [00:01<00:00, 87.87it/s]
100%|██████████| 86/86 [00:01<00:00, 61.28it/s]
11000 images processed, 1.4228308200836182 seconds used

Processing out-of-distribution SVHN images

  0%|          | 0/204 [00:00<?, ?it/s]
  0%|          | 1/204 [00:00<01:53,  1.79it/s]
  5%|▌         | 11/204 [00:00<00:09, 21.38it/s]
 10%|█         | 21/204 [00:00<00:04, 38.32it/s]
 15%|█▌        | 31/204 [00:00<00:03, 52.04it/s]
 20%|█▉        | 40/204 [00:00<00:02, 61.49it/s]
 25%|██▍       | 50/204 [00:01<00:02, 69.73it/s]
 29%|██▉       | 60/204 [00:01<00:01, 76.39it/s]
 34%|███▍      | 70/204 [00:01<00:01, 80.73it/s]
 39%|███▉      | 80/204 [00:01<00:01, 84.19it/s]
 44%|████▍     | 90/204 [00:01<00:01, 86.63it/s]
 49%|████▉     | 100/204 [00:01<00:01, 88.69it/s]
 54%|█████▍    | 110/204 [00:01<00:01, 90.13it/s]
 59%|█████▉    | 120/204 [00:01<00:00, 89.08it/s]
 64%|██████▎   | 130/204 [00:01<00:00, 89.14it/s]
 69%|██████▊   | 140/204 [00:02<00:00, 89.88it/s]
 74%|███████▎  | 150/204 [00:02<00:00, 88.41it/s]
 78%|███████▊  | 159/204 [00:02<00:00, 88.06it/s]
 83%|████████▎ | 169/204 [00:02<00:00, 89.40it/s]
 87%|████████▋ | 178/204 [00:02<00:00, 87.65it/s]
 92%|█████████▏| 188/204 [00:02<00:00, 90.37it/s]
 97%|█████████▋| 198/204 [00:02<00:00, 92.24it/s]
100%|██████████| 204/204 [00:02<00:00, 73.28it/s]
26032 images processed, 2.832292318344116 seconds used

Processing out-of-distribution places365 images

  0%|          | 0/79 [00:00<?, ?it/s]
  1%|▏         | 1/79 [00:00<00:59,  1.32it/s]
 14%|█▍        | 11/79 [00:00<00:04, 16.73it/s]
 25%|██▌       | 20/79 [00:00<00:01, 30.04it/s]
 37%|███▋      | 29/79 [00:01<00:01, 42.14it/s]
 48%|████▊     | 38/79 [00:01<00:00, 52.85it/s]
 59%|█████▉    | 47/79 [00:01<00:00, 61.75it/s]
 71%|███████   | 56/79 [00:01<00:00, 68.88it/s]
 84%|████████▎ | 66/79 [00:01<00:00, 75.43it/s]
 96%|█████████▌| 76/79 [00:01<00:00, 81.23it/s]
100%|██████████| 79/79 [00:02<00:00, 38.51it/s]
10000 images processed, 2.082057476043701 seconds used

Processing out-of-distribution LSUN images

  0%|          | 0/79 [00:00<?, ?it/s]
  1%|▏         | 1/79 [00:00<00:47,  1.63it/s]
 14%|█▍        | 11/79 [00:00<00:03, 19.91it/s]
 25%|██▌       | 20/79 [00:00<00:01, 34.50it/s]
 38%|███▊      | 30/79 [00:00<00:01, 48.73it/s]
 51%|█████     | 40/79 [00:01<00:00, 60.24it/s]
 63%|██████▎   | 50/79 [00:01<00:00, 69.05it/s]
 76%|███████▌  | 60/79 [00:01<00:00, 76.35it/s]
 89%|████████▊ | 70/79 [00:01<00:00, 82.02it/s]
100%|██████████| 79/79 [00:01<00:00, 54.90it/s]
10000 images processed, 1.4647700786590576 seconds used

Processing out-of-distribution iSUN images

  0%|          | 0/70 [00:00<?, ?it/s]
  1%|▏         | 1/70 [00:00<00:40,  1.68it/s]
 11%|█▏        | 8/70 [00:00<00:04, 14.59it/s]
 23%|██▎       | 16/70 [00:00<00:01, 28.51it/s]
 36%|███▌      | 25/70 [00:00<00:01, 42.85it/s]
 49%|████▊     | 34/70 [00:01<00:00, 54.14it/s]
 63%|██████▎   | 44/70 [00:01<00:00, 64.45it/s]
 77%|███████▋  | 54/70 [00:01<00:00, 72.72it/s]
 91%|█████████▏| 64/70 [00:01<00:00, 79.18it/s]
100%|██████████| 70/70 [00:01<00:00, 50.08it/s]
8925 images processed, 1.4346778392791748 seconds used

Processing out-of-distribution dtd images

  0%|          | 0/45 [00:00<?, ?it/s]
  2%|▏         | 1/45 [00:01<00:48,  1.10s/it]
  4%|▍         | 2/45 [00:01<00:22,  1.90it/s]
 27%|██▋       | 12/45 [00:01<00:02, 15.19it/s]
 38%|███▊      | 17/45 [00:01<00:01, 16.95it/s]
 47%|████▋     | 21/45 [00:01<00:01, 20.12it/s]
 56%|█████▌    | 25/45 [00:01<00:00, 20.27it/s]
 62%|██████▏   | 28/45 [00:02<00:00, 19.18it/s]
 73%|███████▎  | 33/45 [00:02<00:00, 24.24it/s]
 82%|████████▏ | 37/45 [00:02<00:00, 26.39it/s]
 93%|█████████▎| 42/45 [00:02<00:00, 13.96it/s]
100%|██████████| 45/45 [00:03<00:00, 14.85it/s]
5640 images processed, 3.066373825073242 seconds used

19.19396734237671
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.87  98.76  81.70
places365     61.15  83.07  47.88
LSUN          45.26  89.33  68.46
iSUN          40.54  90.58  71.27
dtd           35.43  90.87  75.73
AVG           37.25  90.52  69.01
[incremental] Overall: 0.6973 New: 0.7000 Old: 0.6960
[incremental] Final(Top-1): 0.6973  Average: 0.7618
2.4131476879119873
==== Stage 4: inc={64,22,42,9,90}; seen={0,8,11,40,51,66,67,88,94,57,59,58,44,93,10}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4', adapter_load_path='checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3', lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='64,22,42,9,90', forget_classes_seen='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt

  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():

  2%|▏         | 1/50 [00:06<05:15,  6.43s/it]
  4%|▍         | 2/50 [00:07<02:24,  3.00s/it]
  6%|▌         | 3/50 [00:07<01:30,  1.92s/it]
  8%|▊         | 4/50 [00:08<01:04,  1.40s/it]
 10%|█         | 5/50 [00:08<00:50,  1.13s/it]
 12%|█▏        | 6/50 [00:09<00:47,  1.09s/it]
 14%|█▍        | 7/50 [00:10<00:42,  1.02it/s]
 16%|█▌        | 8/50 [00:11<00:36,  1.15it/s]
 18%|█▊        | 9/50 [00:11<00:32,  1.26it/s]
 20%|██        | 10/50 [00:12<00:29,  1.35it/s]
 22%|██▏       | 11/50 [00:13<00:26,  1.47it/s]
 24%|██▍       | 12/50 [00:13<00:26,  1.43it/s]
 26%|██▌       | 13/50 [00:14<00:25,  1.46it/s]
 28%|██▊       | 14/50 [00:15<00:23,  1.52it/s]
 30%|███       | 15/50 [00:15<00:22,  1.58it/s]
 32%|███▏      | 16/50 [00:16<00:21,  1.56it/s]
 34%|███▍      | 17/50 [00:16<00:20,  1.63it/s]
 36%|███▌      | 18/50 [00:17<00:19,  1.61it/s]
 38%|███▊      | 19/50 [00:18<00:19,  1.62it/s]
 40%|████      | 20/50 [00:18<00:20,  1.50it/s]
 42%|████▏     | 21/50 [00:19<00:20,  1.45it/s]
 44%|████▍     | 22/50 [00:20<00:19,  1.44it/s]
 46%|████▌     | 23/50 [00:21<00:19,  1.42it/s]
 48%|████▊     | 24/50 [00:21<00:18,  1.43it/s]
 50%|█████     | 25/50 [00:22<00:17,  1.46it/s]
 52%|█████▏    | 26/50 [00:22<00:15,  1.55it/s]
 54%|█████▍    | 27/50 [00:23<00:14,  1.62it/s]
 56%|█████▌    | 28/50 [00:24<00:13,  1.58it/s]
 58%|█████▊    | 29/50 [00:24<00:12,  1.67it/s][loss] ep 0 it 0 total=15.7635 mle=5.6740 pcon=10.0895 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 2 it 10 total=15.2931 mle=5.2130 pcon=10.0801 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 5 it 0 total=14.8636 mle=4.7963 pcon=10.0673 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 7 it 10 total=14.4836 mle=4.4319 pcon=10.0517 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 10 it 0 total=14.2375 mle=4.2039 pcon=10.0336 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 12 it 10 total=13.9724 mle=3.9587 pcon=10.0136 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 15 it 0 total=13.7856 mle=3.7933 pcon=9.9923 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 17 it 10 total=13.6490 mle=3.6789 pcon=9.9700 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 20 it 0 total=13.5186 mle=3.5716 pcon=9.9470 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 22 it 10 total=13.4587 mle=3.5353 pcon=9.9234 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 25 it 0 total=13.3970 mle=3.4974 pcon=9.8996 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 27 it 10 total=13.3264 mle=3.4507 pcon=9.8758 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4

 60%|██████    | 30/50 [00:25<00:12,  1.56it/s]
 62%|██████▏   | 31/50 [00:26<00:11,  1.62it/s]
 64%|██████▍   | 32/50 [00:26<00:11,  1.54it/s]
 66%|██████▌   | 33/50 [00:27<00:10,  1.60it/s]
 68%|██████▊   | 34/50 [00:27<00:09,  1.65it/s]
 70%|███████   | 35/50 [00:28<00:09,  1.61it/s]
 72%|███████▏  | 36/50 [00:29<00:08,  1.61it/s]
 74%|███████▍  | 37/50 [00:29<00:07,  1.66it/s]
 76%|███████▌  | 38/50 [00:30<00:07,  1.67it/s]
 78%|███████▊  | 39/50 [00:30<00:06,  1.67it/s]
 80%|████████  | 40/50 [00:31<00:05,  1.73it/s]
 82%|████████▏ | 41/50 [00:31<00:05,  1.75it/s]
 84%|████████▍ | 42/50 [00:32<00:04,  1.66it/s]
 86%|████████▌ | 43/50 [00:33<00:04,  1.63it/s]
 88%|████████▊ | 44/50 [00:33<00:03,  1.63it/s]
 90%|█████████ | 45/50 [00:34<00:03,  1.67it/s]
 92%|█████████▏| 46/50 [00:35<00:02,  1.62it/s]
 94%|█████████▍| 47/50 [00:35<00:01,  1.61it/s]
 96%|█████████▌| 48/50 [00:36<00:01,  1.68it/s]
 98%|█████████▊| 49/50 [00:36<00:00,  1.64it/s]
100%|██████████| 50/50 [00:37<00:00,  1.63it/s]
100%|██████████| 50/50 [00:37<00:00,  1.33it/s]
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 30 it 0 total=13.2967 mle=3.4449 pcon=9.8519 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 32 it 10 total=13.2580 mle=3.4297 pcon=9.8283 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 35 it 0 total=13.2227 mle=3.4176 pcon=9.8051 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 37 it 10 total=13.1707 mle=3.3882 pcon=9.7826 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 40 it 0 total=13.1623 mle=3.4019 pcon=9.7605 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 42 it 10 total=13.1390 mle=3.3999 pcon=9.7391 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 45 it 0 total=13.1082 mle=3.3895 pcon=9.7186 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 47 it 10 total=13.0874 mle=3.3886 pcon=9.6988 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] active adapters set to: default
resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stage4-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images

  0%|          | 0/430 [00:00<?, ?it/s]
  0%|          | 1/430 [00:00<04:24,  1.62it/s]
  3%|▎         | 11/430 [00:00<00:21, 19.76it/s]
  5%|▍         | 20/430 [00:00<00:11, 34.38it/s]
  7%|▋         | 30/430 [00:00<00:08, 48.51it/s]
  9%|▉         | 40/430 [00:01<00:06, 59.94it/s]
 12%|█▏        | 50/430 [00:01<00:05, 68.72it/s]
 14%|█▍        | 60/430 [00:01<00:04, 75.99it/s]
 16%|█▋        | 70/430 [00:01<00:04, 81.73it/s]
 19%|█▊        | 80/430 [00:01<00:04, 85.80it/s]
 21%|██        | 90/430 [00:01<00:03, 88.83it/s]
 23%|██▎       | 100/430 [00:01<00:03, 91.11it/s]
 26%|██▌       | 110/430 [00:01<00:03, 92.83it/s]
 28%|██▊       | 120/430 [00:01<00:03, 93.74it/s]
 30%|███       | 130/430 [00:01<00:03, 94.47it/s]
 33%|███▎      | 140/430 [00:02<00:03, 94.74it/s]
 35%|███▍      | 150/430 [00:02<00:02, 95.16it/s]
 37%|███▋      | 160/430 [00:02<00:02, 95.46it/s]
 40%|███▉      | 170/430 [00:02<00:02, 96.00it/s]
 42%|████▏     | 180/430 [00:02<00:02, 95.78it/s]
 44%|████▍     | 190/430 [00:02<00:02, 96.00it/s]
 47%|████▋     | 200/430 [00:02<00:02, 96.28it/s]
 49%|████▉     | 210/430 [00:02<00:02, 96.55it/s]
 51%|█████     | 220/430 [00:02<00:02, 96.58it/s]
 53%|█████▎    | 230/430 [00:03<00:02, 96.54it/s]
 56%|█████▌    | 240/430 [00:03<00:01, 96.49it/s]
 58%|█████▊    | 250/430 [00:03<00:01, 96.60it/s]
 60%|██████    | 260/430 [00:03<00:01, 96.78it/s]
 63%|██████▎   | 270/430 [00:03<00:01, 96.87it/s]
 65%|██████▌   | 280/430 [00:03<00:01, 96.85it/s]
 67%|██████▋   | 290/430 [00:03<00:01, 96.88it/s]
 70%|██████▉   | 300/430 [00:03<00:01, 96.92it/s]
 72%|███████▏  | 310/430 [00:03<00:01, 96.72it/s]
 74%|███████▍  | 320/430 [00:03<00:01, 96.74it/s]
 77%|███████▋  | 330/430 [00:04<00:01, 96.65it/s]
 79%|███████▉  | 340/430 [00:04<00:00, 96.66it/s]
 81%|████████▏ | 350/430 [00:04<00:00, 96.80it/s]
 84%|████████▎ | 360/430 [00:04<00:00, 96.81it/s]
 86%|████████▌ | 370/430 [00:04<00:00, 96.78it/s]
 88%|████████▊ | 380/430 [00:04<00:00, 96.92it/s]
 91%|█████████ | 390/430 [00:04<00:00, 96.87it/s]
 93%|█████████▎| 400/430 [00:04<00:00, 96.54it/s]
 95%|█████████▌| 410/430 [00:04<00:00, 96.89it/s]
 98%|█████████▊| 420/430 [00:04<00:00, 97.21it/s]
100%|██████████| 430/430 [00:05<00:00, 95.13it/s]
100%|██████████| 430/430 [00:05<00:00, 84.46it/s]
55000 images processed, 5.152912139892578 seconds used

Processing in-distribution CIFAR-110 images

  0%|          | 0/86 [00:00<?, ?it/s]
  1%|          | 1/86 [00:00<00:41,  2.07it/s]
 12%|█▏        | 10/86 [00:00<00:03, 21.99it/s]
 22%|██▏       | 19/86 [00:00<00:01, 37.76it/s]
 34%|███▎      | 29/86 [00:00<00:01, 52.27it/s]
 44%|████▍     | 38/86 [00:00<00:00, 61.94it/s]
 56%|█████▌    | 48/86 [00:01<00:00, 70.81it/s]
 67%|██████▋   | 58/86 [00:01<00:00, 77.33it/s]
 79%|███████▉  | 68/86 [00:01<00:00, 82.90it/s]
 91%|█████████ | 78/86 [00:01<00:00, 86.98it/s]
100%|██████████| 86/86 [00:01<00:00, 60.77it/s]
11000 images processed, 1.4341542720794678 seconds used

Processing out-of-distribution SVHN images

  0%|          | 0/204 [00:00<?, ?it/s]
  0%|          | 1/204 [00:00<01:58,  1.71it/s]
  5%|▌         | 11/204 [00:00<00:09, 20.66it/s]
 10%|█         | 21/204 [00:00<00:04, 37.31it/s]
 15%|█▌        | 31/204 [00:00<00:03, 51.26it/s]
 20%|██        | 41/204 [00:01<00:02, 62.29it/s]
 25%|██▌       | 51/204 [00:01<00:02, 70.91it/s]
 30%|██▉       | 61/204 [00:01<00:01, 77.68it/s]
 35%|███▍      | 71/204 [00:01<00:01, 82.57it/s]
 40%|███▉      | 81/204 [00:01<00:01, 86.15it/s]
 45%|████▍     | 91/204 [00:01<00:01, 88.60it/s]
 50%|████▉     | 101/204 [00:01<00:01, 90.03it/s]
 54%|█████▍    | 111/204 [00:01<00:01, 90.57it/s]
 59%|█████▉    | 121/204 [00:01<00:00, 91.83it/s]
 64%|██████▍   | 131/204 [00:01<00:00, 92.72it/s]
 69%|██████▉   | 141/204 [00:02<00:00, 93.70it/s]
 74%|███████▍  | 151/204 [00:02<00:00, 94.28it/s]
 79%|███████▉  | 161/204 [00:02<00:00, 94.38it/s]
 84%|████████▍ | 171/204 [00:02<00:00, 94.67it/s]
 89%|████████▊ | 181/204 [00:02<00:00, 94.77it/s]
 94%|█████████▎| 191/204 [00:02<00:00, 95.14it/s]
 99%|█████████▊| 201/204 [00:02<00:00, 95.66it/s]
100%|██████████| 204/204 [00:02<00:00, 74.64it/s]
26032 images processed, 2.773533344268799 seconds used

Processing out-of-distribution places365 images

  0%|          | 0/79 [00:00<?, ?it/s]
  1%|▏         | 1/79 [00:00<01:01,  1.27it/s]
  9%|▉         | 7/79 [00:00<00:07, 10.21it/s]
 20%|██        | 16/79 [00:00<00:02, 24.41it/s]
 33%|███▎      | 26/79 [00:01<00:01, 39.25it/s]
 46%|████▌     | 36/79 [00:01<00:00, 52.11it/s]
 58%|█████▊    | 46/79 [00:01<00:00, 62.59it/s]
 71%|███████   | 56/79 [00:01<00:00, 70.82it/s]
 82%|████████▏ | 65/79 [00:01<00:00, 69.68it/s]
 95%|█████████▍| 75/79 [00:01<00:00, 76.66it/s]
100%|██████████| 79/79 [00:02<00:00, 29.38it/s]
10000 images processed, 2.7506682872772217 seconds used

Processing out-of-distribution LSUN images

  0%|          | 0/79 [00:00<?, ?it/s]
  1%|▏         | 1/79 [00:00<00:45,  1.73it/s]
 11%|█▏        | 9/79 [00:00<00:04, 17.20it/s]
 24%|██▍       | 19/79 [00:00<00:01, 33.67it/s]
 37%|███▋      | 29/79 [00:00<00:01, 47.97it/s]
 49%|████▉     | 39/79 [00:01<00:00, 59.62it/s]
 62%|██████▏   | 49/79 [00:01<00:00, 69.07it/s]
 75%|███████▍  | 59/79 [00:01<00:00, 76.61it/s]
 87%|████████▋ | 69/79 [00:01<00:00, 82.27it/s]
100%|██████████| 79/79 [00:01<00:00, 55.39it/s]
10000 images processed, 1.4500305652618408 seconds used

Processing out-of-distribution iSUN images

  0%|          | 0/70 [00:00<?, ?it/s]
  1%|▏         | 1/70 [00:00<00:40,  1.70it/s]
 16%|█▌        | 11/70 [00:00<00:02, 20.43it/s]
 30%|███       | 21/70 [00:00<00:01, 36.96it/s]
 44%|████▍     | 31/70 [00:00<00:00, 50.87it/s]
 59%|█████▊    | 41/70 [00:01<00:00, 61.36it/s]
 73%|███████▎  | 51/70 [00:01<00:00, 70.64it/s]
 87%|████████▋ | 61/70 [00:01<00:00, 77.79it/s]
100%|██████████| 70/70 [00:01<00:00, 52.75it/s]
8925 images processed, 1.3579425811767578 seconds used

Processing out-of-distribution dtd images

  0%|          | 0/45 [00:00<?, ?it/s]
  2%|▏         | 1/45 [00:00<00:42,  1.02it/s]
  4%|▍         | 2/45 [00:01<00:20,  2.09it/s]
 27%|██▋       | 12/45 [00:01<00:01, 16.57it/s]
 38%|███▊      | 17/45 [00:01<00:01, 18.49it/s]
 47%|████▋     | 21/45 [00:01<00:01, 21.83it/s]
 56%|█████▌    | 25/45 [00:01<00:00, 22.70it/s]
 73%|███████▎  | 33/45 [00:02<00:00, 24.20it/s]
 82%|████████▏ | 37/45 [00:02<00:00, 25.61it/s]
100%|██████████| 45/45 [00:02<00:00, 20.23it/s]
5640 images processed, 2.2478930950164795 seconds used

18.80194401741028
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.14  98.72  84.70
places365     73.32  79.76  49.30
LSUN          48.39  88.79  71.90
iSUN          50.48  88.43  72.08
dtd           40.94  90.07  78.16
AVG           43.45  89.15  71.23
[incremental] Overall: 0.6205 New: 0.6200 Old: 0.6207
[incremental] Final(Top-1): 0.6205  Average: 0.7265
2.6850972175598145
==== Stage 5: inc={100,101,102,103,104}; seen={0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5', adapter_load_path='checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4', lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='100,101,102,103,104', forget_classes_seen='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt

  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():

  2%|▏         | 1/50 [00:13<10:59, 13.45s/it]
  4%|▍         | 2/50 [00:16<06:03,  7.57s/it]
  6%|▌         | 3/50 [00:17<03:29,  4.45s/it]
  8%|▊         | 4/50 [00:18<02:20,  3.05s/it]
 10%|█         | 5/50 [00:19<01:50,  2.46s/it]
 12%|█▏        | 6/50 [00:21<01:33,  2.13s/it]
 14%|█▍        | 7/50 [00:22<01:19,  1.85s/it]
 16%|█▌        | 8/50 [00:23<01:01,  1.46s/it]
 18%|█▊        | 9/50 [00:24<00:53,  1.31s/it]
 20%|██        | 10/50 [00:25<00:48,  1.21s/it]
 22%|██▏       | 11/50 [00:26<00:45,  1.17s/it]
 24%|██▍       | 12/50 [00:27<00:42,  1.11s/it]
 26%|██▌       | 13/50 [00:28<00:43,  1.17s/it]
 28%|██▊       | 14/50 [00:29<00:39,  1.09s/it]
 30%|███       | 15/50 [00:30<00:36,  1.04s/it]
 32%|███▏      | 16/50 [00:31<00:37,  1.12s/it]
 34%|███▍      | 17/50 [00:32<00:33,  1.02s/it]
 36%|███▌      | 18/50 [00:33<00:36,  1.13s/it]
 38%|███▊      | 19/50 [00:35<00:37,  1.20s/it]
 40%|████      | 20/50 [00:36<00:36,  1.22s/it]
 42%|████▏     | 21/50 [00:38<00:36,  1.27s/it]
 44%|████▍     | 22/50 [00:39<00:35,  1.27s/it]
 46%|████▌     | 23/50 [00:40<00:31,  1.18s/it]
 48%|████▊     | 24/50 [00:41<00:32,  1.25s/it]
 50%|█████     | 25/50 [00:43<00:32,  1.29s/it]
 52%|█████▏    | 26/50 [00:44<00:31,  1.32s/it]
 54%|█████▍    | 27/50 [00:46<00:32,  1.43s/it]
 56%|█████▌    | 28/50 [00:47<00:30,  1.37s/it]
 58%|█████▊    | 29/50 [00:48<00:28,  1.35s/it][loss] ep 0 it 0 total=15.9795 mle=5.5962 pcon=10.3833 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 2 it 10 total=15.5029 mle=5.1297 pcon=10.3732 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 5 it 0 total=15.0869 mle=4.7269 pcon=10.3600 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 7 it 10 total=14.7592 mle=4.4156 pcon=10.3437 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 10 it 0 total=14.4124 mle=4.0874 pcon=10.3249 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 12 it 10 total=14.2182 mle=3.9138 pcon=10.3044 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 15 it 0 total=14.0543 mle=3.7719 pcon=10.2824 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 17 it 10 total=13.9254 mle=3.6660 pcon=10.2594 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 20 it 0 total=13.8084 mle=3.5726 pcon=10.2358 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 22 it 10 total=13.7313 mle=3.5198 pcon=10.2115 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 25 it 0 total=13.6742 mle=3.4872 pcon=10.1870 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 27 it 10 total=13.6233 mle=3.4608 pcon=10.1625 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5

 60%|██████    | 30/50 [00:50<00:27,  1.35s/it]
 62%|██████▏   | 31/50 [00:51<00:26,  1.42s/it]
 64%|██████▍   | 32/50 [00:53<00:26,  1.48s/it]
 66%|██████▌   | 33/50 [00:54<00:26,  1.54s/it]
 68%|██████▊   | 34/50 [00:56<00:25,  1.58s/it]
 70%|███████   | 35/50 [00:58<00:23,  1.55s/it]
 72%|███████▏  | 36/50 [00:59<00:21,  1.55s/it]
 74%|███████▍  | 37/50 [01:01<00:19,  1.53s/it]
 76%|███████▌  | 38/50 [01:02<00:17,  1.48s/it]
 78%|███████▊  | 39/50 [01:03<00:16,  1.47s/it]
 80%|████████  | 40/50 [01:05<00:16,  1.60s/it]
 82%|████████▏ | 41/50 [01:07<00:14,  1.59s/it]
 84%|████████▍ | 42/50 [01:08<00:12,  1.50s/it]
 86%|████████▌ | 43/50 [01:09<00:10,  1.44s/it]
 88%|████████▊ | 44/50 [01:11<00:08,  1.41s/it]
 90%|█████████ | 45/50 [01:12<00:07,  1.45s/it]
 92%|█████████▏| 46/50 [01:14<00:05,  1.42s/it]
 94%|█████████▍| 47/50 [01:15<00:04,  1.47s/it]
 96%|█████████▌| 48/50 [01:17<00:03,  1.52s/it]
 98%|█████████▊| 49/50 [01:18<00:01,  1.49s/it]
100%|██████████| 50/50 [01:20<00:00,  1.48s/it]
100%|██████████| 50/50 [01:20<00:00,  1.61s/it]
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 30 it 0 total=13.5865 mle=3.4485 pcon=10.1380 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 32 it 10 total=13.5451 mle=3.4310 pcon=10.1140 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 35 it 0 total=13.5069 mle=3.4164 pcon=10.0904 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 37 it 10 total=13.4804 mle=3.4130 pcon=10.0674 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 40 it 0 total=13.4437 mle=3.3986 pcon=10.0451 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 42 it 10 total=13.4292 mle=3.4056 pcon=10.0236 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 45 it 0 total=13.4018 mle=3.3990 pcon=10.0028 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 47 it 10 total=13.3785 mle=3.3957 pcon=9.9828 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] active adapters set to: default
resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stage5-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images

  0%|          | 0/430 [00:00<?, ?it/s]
  0%|          | 1/430 [00:00<04:29,  1.59it/s]
  1%|▏         | 6/430 [00:00<00:40, 10.46it/s]
  3%|▎         | 12/430 [00:00<00:20, 20.01it/s]
  4%|▍         | 17/430 [00:00<00:15, 26.40it/s]
  5%|▌         | 22/430 [00:01<00:12, 31.62it/s]
  7%|▋         | 29/430 [00:01<00:09, 40.26it/s]
  8%|▊         | 35/430 [00:01<00:08, 43.94it/s]
 10%|▉         | 41/430 [00:01<00:08, 45.06it/s]
 11%|█         | 47/430 [00:01<00:08, 45.66it/s]
 13%|█▎        | 54/430 [00:01<00:07, 51.48it/s]
 14%|█▍        | 60/430 [00:01<00:07, 50.22it/s]
 15%|█▌        | 66/430 [00:01<00:07, 49.20it/s]
 17%|█▋        | 72/430 [00:02<00:07, 50.22it/s]
 18%|█▊        | 79/430 [00:02<00:06, 54.56it/s]
 20%|█▉        | 85/430 [00:02<00:06, 55.22it/s]
 21%|██        | 91/430 [00:02<00:06, 54.11it/s]
 23%|██▎       | 97/430 [00:02<00:06, 52.64it/s]
 24%|██▍       | 103/430 [00:02<00:06, 50.89it/s]
 25%|██▌       | 109/430 [00:02<00:06, 49.54it/s]
 27%|██▋       | 115/430 [00:02<00:06, 51.41it/s]
 28%|██▊       | 121/430 [00:02<00:05, 52.46it/s]
 30%|██▉       | 127/430 [00:03<00:05, 50.87it/s]
 31%|███       | 133/430 [00:03<00:05, 49.55it/s]
 33%|███▎      | 140/430 [00:03<00:05, 54.24it/s]
 34%|███▍      | 146/430 [00:03<00:05, 53.04it/s]
 35%|███▌      | 152/430 [00:03<00:05, 51.28it/s]
 37%|███▋      | 158/430 [00:03<00:05, 49.98it/s]
 39%|███▊      | 166/430 [00:03<00:04, 55.08it/s]
 40%|████      | 172/430 [00:03<00:04, 53.60it/s]
 41%|████▏     | 178/430 [00:04<00:04, 55.20it/s]
 43%|████▎     | 184/430 [00:04<00:04, 52.42it/s]
 44%|████▍     | 190/430 [00:04<00:04, 51.74it/s]
 46%|████▌     | 196/430 [00:04<00:04, 50.70it/s]
 47%|████▋     | 203/430 [00:04<00:04, 55.57it/s]
 49%|████▊     | 209/430 [00:04<00:04, 53.23it/s]
 50%|█████     | 215/430 [00:04<00:04, 50.49it/s]
 51%|█████▏    | 221/430 [00:04<00:04, 50.49it/s]
 53%|█████▎    | 228/430 [00:04<00:03, 53.30it/s]
 54%|█████▍    | 234/430 [00:05<00:03, 51.20it/s]
 56%|█████▌    | 240/430 [00:05<00:03, 50.00it/s]
 57%|█████▋    | 247/430 [00:05<00:03, 54.14it/s]
 59%|█████▉    | 253/430 [00:05<00:03, 51.92it/s]
 60%|██████    | 259/430 [00:05<00:03, 50.14it/s]
 62%|██████▏   | 265/430 [00:05<00:03, 51.08it/s]
 63%|██████▎   | 272/430 [00:05<00:02, 53.65it/s]
 65%|██████▍   | 278/430 [00:05<00:02, 52.37it/s]
 66%|██████▌   | 284/430 [00:06<00:02, 53.74it/s]
 67%|██████▋   | 290/430 [00:06<00:02, 53.03it/s]
 69%|██████▉   | 296/430 [00:06<00:02, 51.02it/s]
 70%|███████   | 302/430 [00:06<00:02, 49.67it/s]
 72%|███████▏  | 309/430 [00:06<00:02, 53.45it/s]
 73%|███████▎  | 315/430 [00:06<00:02, 51.50it/s]
 75%|███████▍  | 321/430 [00:06<00:02, 50.27it/s]
 76%|███████▌  | 327/430 [00:06<00:02, 50.19it/s]
 78%|███████▊  | 334/430 [00:07<00:01, 53.06it/s]
 79%|███████▉  | 340/430 [00:07<00:01, 51.29it/s]
 80%|████████  | 346/430 [00:07<00:01, 49.79it/s]
 82%|████████▏ | 353/430 [00:07<00:01, 54.32it/s]
 83%|████████▎ | 359/430 [00:07<00:01, 52.01it/s]
 85%|████████▍ | 365/430 [00:07<00:01, 50.43it/s]
 86%|████████▋ | 371/430 [00:07<00:01, 51.30it/s]
 88%|████████▊ | 377/430 [00:07<00:01, 51.55it/s]
 89%|████████▉ | 383/430 [00:07<00:00, 50.02it/s]
 90%|█████████ | 389/430 [00:08<00:00, 49.28it/s]
 92%|█████████▏| 396/430 [00:08<00:00, 53.29it/s]
 93%|█████████▎| 402/430 [00:08<00:00, 51.07it/s]
 95%|█████████▍| 408/430 [00:08<00:00, 50.04it/s]
 96%|█████████▋| 414/430 [00:08<00:00, 52.49it/s]
 98%|█████████▊| 420/430 [00:08<00:00, 54.32it/s]
 99%|█████████▉| 426/430 [00:08<00:00, 51.96it/s]
100%|██████████| 430/430 [00:08<00:00, 48.35it/s]
55000 images processed, 8.955541849136353 seconds used

Processing in-distribution CIFAR-110 images

  0%|          | 0/86 [00:00<?, ?it/s]
  1%|          | 1/86 [00:00<00:37,  2.25it/s]
  6%|▌         | 5/86 [00:00<00:07, 11.24it/s]
 12%|█▏        | 10/86 [00:00<00:03, 20.80it/s]
 17%|█▋        | 15/86 [00:00<00:02, 27.98it/s]
 24%|██▍       | 21/86 [00:00<00:01, 36.57it/s]
 30%|███       | 26/86 [00:00<00:01, 39.57it/s]
 36%|███▌      | 31/86 [00:01<00:01, 41.73it/s]
 42%|████▏     | 36/86 [00:01<00:01, 43.38it/s]
 49%|████▉     | 42/86 [00:01<00:00, 47.81it/s]
 56%|█████▌    | 48/86 [00:01<00:00, 47.29it/s]
 62%|██████▏   | 53/86 [00:01<00:00, 47.04it/s]
 69%|██████▊   | 59/86 [00:01<00:00, 50.60it/s]
 76%|███████▌  | 65/86 [00:01<00:00, 52.18it/s]
 83%|████████▎ | 71/86 [00:01<00:00, 51.98it/s]
 91%|█████████ | 78/86 [00:01<00:00, 54.18it/s]
 98%|█████████▊| 84/86 [00:02<00:00, 53.61it/s]
100%|██████████| 86/86 [00:02<00:00, 40.48it/s]
11000 images processed, 2.142367124557495 seconds used

Processing out-of-distribution SVHN images

  0%|          | 0/204 [00:00<?, ?it/s]
  0%|          | 1/204 [00:00<02:48,  1.21it/s]
  3%|▎         | 6/204 [00:00<00:24,  8.17it/s]
  5%|▌         | 11/204 [00:01<00:12, 15.24it/s]
  8%|▊         | 16/204 [00:01<00:08, 21.50it/s]
 10%|█         | 21/204 [00:01<00:06, 26.85it/s]
 13%|█▎        | 27/204 [00:01<00:05, 34.11it/s]
 16%|█▌        | 32/204 [00:01<00:04, 37.42it/s]
 18%|█▊        | 37/204 [00:01<00:04, 39.01it/s]
 21%|██        | 42/204 [00:01<00:04, 40.11it/s]
 24%|██▍       | 49/204 [00:01<00:03, 45.97it/s]
 26%|██▋       | 54/204 [00:01<00:03, 45.31it/s]
 29%|██▉       | 59/204 [00:02<00:03, 44.27it/s]
 32%|███▏      | 66/204 [00:02<00:02, 48.37it/s]
 35%|███▍      | 71/204 [00:02<00:02, 47.37it/s]
 37%|███▋      | 76/204 [00:02<00:02, 46.10it/s]
 40%|████      | 82/204 [00:02<00:02, 49.00it/s]
 43%|████▎     | 87/204 [00:02<00:02, 48.93it/s]
 45%|████▌     | 92/204 [00:02<00:02, 46.98it/s]
 48%|████▊     | 97/204 [00:02<00:02, 45.78it/s]
 51%|█████     | 104/204 [00:02<00:02, 49.58it/s]
 53%|█████▎    | 109/204 [00:03<00:01, 47.75it/s]
 56%|█████▌    | 114/204 [00:03<00:01, 47.13it/s]
 59%|█████▉    | 120/204 [00:03<00:01, 50.21it/s]
 62%|██████▏   | 126/204 [00:03<00:01, 49.22it/s]
 64%|██████▍   | 131/204 [00:03<00:01, 47.67it/s]
 67%|██████▋   | 136/204 [00:03<00:01, 46.69it/s]
 70%|███████   | 143/204 [00:03<00:01, 50.29it/s]
 73%|███████▎  | 149/204 [00:03<00:01, 48.13it/s]
 75%|███████▌  | 154/204 [00:04<00:01, 46.92it/s]
 78%|███████▊  | 160/204 [00:04<00:00, 49.64it/s]
 81%|████████  | 165/204 [00:04<00:00, 48.04it/s]
 83%|████████▎ | 170/204 [00:04<00:00, 46.62it/s]
 86%|████████▋ | 176/204 [00:04<00:00, 49.57it/s]
 89%|████████▉ | 182/204 [00:04<00:00, 49.69it/s]
 92%|█████████▏| 187/204 [00:04<00:00, 47.55it/s]
 94%|█████████▍| 192/204 [00:04<00:00, 46.10it/s]
 98%|█████████▊| 199/204 [00:04<00:00, 49.75it/s]
100%|██████████| 204/204 [00:05<00:00, 49.16it/s]
100%|██████████| 204/204 [00:05<00:00, 40.39it/s]
26032 images processed, 5.100059747695923 seconds used

Processing out-of-distribution places365 images

  0%|          | 0/79 [00:00<?, ?it/s]
  1%|▏         | 1/79 [00:00<00:54,  1.44it/s]
  4%|▍         | 3/79 [00:00<00:17,  4.40it/s]
 10%|█         | 8/79 [00:00<00:05, 12.63it/s]
 16%|█▋        | 13/79 [00:01<00:03, 19.76it/s]
 24%|██▍       | 19/79 [00:01<00:02, 28.41it/s]
 32%|███▏      | 25/79 [00:01<00:01, 34.44it/s]
 38%|███▊      | 30/79 [00:01<00:01, 38.11it/s]
 46%|████▌     | 36/79 [00:01<00:01, 41.30it/s]
 52%|█████▏    | 41/79 [00:01<00:00, 43.25it/s]
 58%|█████▊    | 46/79 [00:01<00:00, 43.12it/s]
 65%|██████▍   | 51/79 [00:01<00:00, 43.01it/s]
 73%|███████▎  | 58/79 [00:01<00:00, 48.96it/s]
 81%|████████  | 64/79 [00:02<00:00, 48.91it/s]
 89%|████████▊ | 70/79 [00:02<00:00, 49.88it/s]
 96%|█████████▌| 76/79 [00:02<00:00, 51.15it/s]
100%|██████████| 79/79 [00:02<00:00, 33.43it/s]
10000 images processed, 2.3951621055603027 seconds used

Processing out-of-distribution LSUN images

  0%|          | 0/79 [00:00<?, ?it/s]
  1%|▏         | 1/79 [00:00<00:39,  1.96it/s]
  4%|▍         | 3/79 [00:00<00:13,  5.85it/s]
 10%|█         | 8/79 [00:00<00:04, 16.18it/s]
 18%|█▊        | 14/79 [00:00<00:02, 25.91it/s]
 24%|██▍       | 19/79 [00:00<00:01, 30.89it/s]
 30%|███       | 24/79 [00:01<00:01, 34.42it/s]
 38%|███▊      | 30/79 [00:01<00:01, 41.12it/s]
 44%|████▍     | 35/79 [00:01<00:01, 41.65it/s]
 51%|█████     | 40/79 [00:01<00:00, 42.35it/s]
 57%|█████▋    | 45/79 [00:01<00:00, 44.35it/s]
 65%|██████▍   | 51/79 [00:01<00:00, 48.30it/s]
 72%|███████▏  | 57/79 [00:01<00:00, 46.14it/s]
 78%|███████▊  | 62/79 [00:01<00:00, 45.18it/s]
 85%|████████▍ | 67/79 [00:01<00:00, 45.36it/s]
 91%|█████████ | 72/79 [00:02<00:00, 43.20it/s]
 97%|█████████▋| 77/79 [00:02<00:00, 42.46it/s]
100%|██████████| 79/79 [00:02<00:00, 34.96it/s]
10000 images processed, 2.302649736404419 seconds used

Processing out-of-distribution iSUN images

  0%|          | 0/70 [00:00<?, ?it/s]
  1%|▏         | 1/70 [00:00<00:56,  1.21it/s]
  7%|▋         | 5/70 [00:00<00:09,  6.73it/s]
 14%|█▍        | 10/70 [00:01<00:04, 14.04it/s]
 21%|██▏       | 15/70 [00:01<00:02, 20.51it/s]
 30%|███       | 21/70 [00:01<00:01, 27.99it/s]
 37%|███▋      | 26/70 [00:01<00:01, 31.76it/s]
 44%|████▍     | 31/70 [00:01<00:01, 35.64it/s]
 51%|█████▏    | 36/70 [00:01<00:00, 37.91it/s]
 59%|█████▊    | 41/70 [00:01<00:00, 39.31it/s]
 69%|██████▊   | 48/70 [00:01<00:00, 45.96it/s]
 76%|███████▌  | 53/70 [00:01<00:00, 45.25it/s]
 83%|████████▎ | 58/70 [00:02<00:00, 44.73it/s]
 90%|█████████ | 63/70 [00:02<00:00, 45.33it/s]
100%|██████████| 70/70 [00:02<00:00, 49.73it/s]
100%|██████████| 70/70 [00:02<00:00, 30.49it/s]
8925 images processed, 2.3258984088897705 seconds used

Processing out-of-distribution dtd images

  0%|          | 0/45 [00:00<?, ?it/s]
  2%|▏         | 1/45 [00:01<00:55,  1.26s/it]
 11%|█         | 5/45 [00:01<00:08,  4.79it/s]
 20%|██        | 9/45 [00:01<00:03,  9.21it/s]
 31%|███       | 14/45 [00:01<00:02, 15.28it/s]
 40%|████      | 18/45 [00:01<00:01, 16.94it/s]
 51%|█████     | 23/45 [00:02<00:01, 16.51it/s]
 62%|██████▏   | 28/45 [00:02<00:00, 21.31it/s]
 73%|███████▎  | 33/45 [00:02<00:00, 18.24it/s]
 84%|████████▍ | 38/45 [00:02<00:00, 22.59it/s]
 96%|█████████▌| 43/45 [00:02<00:00, 26.56it/s]
100%|██████████| 45/45 [00:02<00:00, 16.01it/s]
5640 images processed, 2.8310182094573975 seconds used

27.8835506439209
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.28  98.72  87.13
places365     77.94  79.12  53.49
LSUN          49.35  89.16  75.82
iSUN          53.24  88.35  75.44
dtd           41.76  90.30  81.75
AVG           45.31  89.13  74.73
[incremental] Overall: 0.5548 New: 0.4600 Old: 0.5785
[incremental] Final(Top-1): 0.5548  Average: 0.6921
9.714916467666626
==== Stage 6: inc={105,106,107,108,109}; seen={0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90,100,101,102,103,104}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6', adapter_load_path='checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5', lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='105,106,107,108,109', forget_classes_seen='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90,100,101,102,103,104', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt

  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():

  2%|▏         | 1/50 [00:07<05:48,  7.11s/it]
  4%|▍         | 2/50 [00:08<03:01,  3.78s/it]
  6%|▌         | 3/50 [00:10<02:12,  2.81s/it]
  8%|▊         | 4/50 [00:12<01:55,  2.52s/it]
 10%|█         | 5/50 [00:13<01:31,  2.04s/it]
 12%|█▏        | 6/50 [00:14<01:11,  1.63s/it]
 14%|█▍        | 7/50 [00:15<00:57,  1.35s/it]
 16%|█▌        | 8/50 [00:15<00:49,  1.18s/it]
 18%|█▊        | 9/50 [00:16<00:43,  1.07s/it]
 20%|██        | 10/50 [00:17<00:39,  1.02it/s]
 22%|██▏       | 11/50 [00:18<00:36,  1.07it/s]
 24%|██▍       | 12/50 [00:19<00:33,  1.14it/s]
 26%|██▌       | 13/50 [00:19<00:31,  1.18it/s]
 28%|██▊       | 14/50 [00:20<00:30,  1.20it/s]
 30%|███       | 15/50 [00:22<00:38,  1.10s/it]
 32%|███▏      | 16/50 [00:24<00:43,  1.27s/it]
 34%|███▍      | 17/50 [00:25<00:46,  1.40s/it]
 36%|███▌      | 18/50 [00:27<00:47,  1.50s/it]
 38%|███▊      | 19/50 [00:29<00:48,  1.55s/it]
 40%|████      | 20/50 [00:29<00:39,  1.33s/it]
 42%|████▏     | 21/50 [00:30<00:31,  1.09s/it]
 44%|████▍     | 22/50 [00:31<00:26,  1.06it/s]
 46%|████▌     | 23/50 [00:32<00:30,  1.12s/it]
 48%|████▊     | 24/50 [00:34<00:31,  1.23s/it]
 50%|█████     | 25/50 [00:35<00:29,  1.17s/it]
 52%|█████▏    | 26/50 [00:36<00:29,  1.23s/it]
 54%|█████▍    | 27/50 [00:37<00:29,  1.30s/it]
 56%|█████▌    | 28/50 [00:39<00:31,  1.42s/it]
 58%|█████▊    | 29/50 [00:41<00:30,  1.45s/it][loss] ep 0 it 0 total=16.1360 mle=5.5321 pcon=10.6038 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 2 it 10 total=15.6688 mle=5.0750 pcon=10.5938 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 5 it 0 total=15.2480 mle=4.6674 pcon=10.5806 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 7 it 10 total=14.9088 mle=4.3444 pcon=10.5644 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 10 it 0 total=14.6119 mle=4.0660 pcon=10.5459 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 12 it 10 total=14.4039 mle=3.8784 pcon=10.5256 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 15 it 0 total=14.2275 mle=3.7238 pcon=10.5037 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 17 it 10 total=14.1076 mle=3.6266 pcon=10.4810 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 20 it 0 total=14.0126 mle=3.5551 pcon=10.4575 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 22 it 10 total=13.9445 mle=3.5112 pcon=10.4334 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 25 it 0 total=13.8789 mle=3.4698 pcon=10.4091 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 27 it 10 total=13.8330 mle=3.4483 pcon=10.3847 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6

 60%|██████    | 30/50 [00:42<00:29,  1.47s/it]
 62%|██████▏   | 31/50 [00:44<00:28,  1.50s/it]
 64%|██████▍   | 32/50 [00:45<00:26,  1.46s/it]
 66%|██████▌   | 33/50 [00:46<00:23,  1.40s/it]
 68%|██████▊   | 34/50 [00:48<00:23,  1.47s/it]
 70%|███████   | 35/50 [00:49<00:21,  1.44s/it]
 72%|███████▏  | 36/50 [00:51<00:19,  1.42s/it]
 74%|███████▍  | 37/50 [00:52<00:18,  1.41s/it]
 76%|███████▌  | 38/50 [00:54<00:17,  1.47s/it]
 78%|███████▊  | 39/50 [00:55<00:16,  1.48s/it]
 80%|████████  | 40/50 [00:56<00:13,  1.35s/it]
 82%|████████▏ | 41/50 [00:58<00:12,  1.37s/it]
 84%|████████▍ | 42/50 [00:59<00:11,  1.39s/it]
 86%|████████▌ | 43/50 [01:00<00:09,  1.34s/it]
 88%|████████▊ | 44/50 [01:02<00:07,  1.31s/it]
 90%|█████████ | 45/50 [01:03<00:06,  1.24s/it]
 92%|█████████▏| 46/50 [01:04<00:05,  1.28s/it]
 94%|█████████▍| 47/50 [01:05<00:03,  1.28s/it]
 96%|█████████▌| 48/50 [01:07<00:02,  1.31s/it]
 98%|█████████▊| 49/50 [01:08<00:01,  1.42s/it]
100%|██████████| 50/50 [01:10<00:00,  1.45s/it]
100%|██████████| 50/50 [01:10<00:00,  1.41s/it]
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 30 it 0 total=13.7937 mle=3.4333 pcon=10.3604 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 32 it 10 total=13.7598 mle=3.4234 pcon=10.3364 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 35 it 0 total=13.7144 mle=3.4017 pcon=10.3128 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 37 it 10 total=13.6903 mle=3.4006 pcon=10.2897 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 40 it 0 total=13.6670 mle=3.3997 pcon=10.2674 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 42 it 10 total=13.6390 mle=3.3932 pcon=10.2458 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 45 it 0 total=13.6113 mle=3.3864 pcon=10.2248 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 47 it 10 total=13.5883 mle=3.3835 pcon=10.2048 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] active adapters set to: default
resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stage6-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images

  0%|          | 0/430 [00:00<?, ?it/s]
  0%|          | 1/430 [00:00<05:39,  1.27it/s]
  1%|▏         | 6/430 [00:00<00:49,  8.49it/s]
  3%|▎         | 12/430 [00:01<00:24, 17.39it/s]
  4%|▍         | 17/430 [00:01<00:24, 16.77it/s]
  5%|▌         | 23/430 [00:01<00:17, 23.37it/s]
  7%|▋         | 29/430 [00:01<00:13, 29.62it/s]
  8%|▊         | 35/430 [00:01<00:11, 35.53it/s]
  9%|▉         | 40/430 [00:01<00:10, 38.43it/s]
 10%|█         | 45/430 [00:01<00:09, 40.70it/s]
 12%|█▏        | 50/430 [00:01<00:08, 42.55it/s]
 13%|█▎        | 57/430 [00:02<00:07, 48.66it/s]
 15%|█▍        | 63/430 [00:02<00:07, 49.16it/s]
 16%|█▋        | 70/430 [00:02<00:06, 52.17it/s]
 18%|█▊        | 76/430 [00:02<00:06, 52.23it/s]
 19%|█▉        | 82/430 [00:02<00:06, 50.69it/s]
 20%|██        | 88/430 [00:02<00:06, 49.46it/s]
 22%|██▏       | 95/430 [00:02<00:06, 53.52it/s]
 23%|██▎       | 101/430 [00:02<00:06, 51.60it/s]
 25%|██▍       | 107/430 [00:03<00:06, 50.06it/s]
 26%|██▋       | 113/430 [00:03<00:06, 51.23it/s]
 28%|██▊       | 119/430 [00:03<00:05, 53.25it/s]
 29%|██▉       | 125/430 [00:03<00:05, 51.29it/s]
 30%|███       | 131/430 [00:03<00:05, 49.93it/s]
 32%|███▏      | 138/430 [00:03<00:05, 53.84it/s]
 33%|███▎      | 144/430 [00:03<00:05, 51.76it/s]
 35%|███▍      | 150/430 [00:03<00:05, 50.12it/s]
 36%|███▋      | 156/430 [00:03<00:05, 51.60it/s]
 38%|███▊      | 162/430 [00:04<00:04, 53.76it/s]
 39%|███▉      | 168/430 [00:04<00:05, 52.21it/s]
 41%|████      | 175/430 [00:04<00:04, 55.00it/s]
 42%|████▏     | 181/430 [00:04<00:04, 52.66it/s]
 43%|████▎     | 187/430 [00:04<00:04, 50.87it/s]
 45%|████▍     | 193/430 [00:04<00:04, 50.66it/s]
 47%|████▋     | 200/430 [00:04<00:04, 53.47it/s]
 48%|████▊     | 206/430 [00:04<00:04, 51.68it/s]
 49%|████▉     | 212/430 [00:05<00:04, 50.10it/s]
 51%|█████     | 219/430 [00:05<00:03, 54.91it/s]
 52%|█████▏    | 225/430 [00:05<00:03, 52.29it/s]
 54%|█████▎    | 231/430 [00:05<00:03, 50.71it/s]
 55%|█████▌    | 237/430 [00:05<00:03, 50.58it/s]
 57%|█████▋    | 244/430 [00:05<00:03, 54.03it/s]
 58%|█████▊    | 250/430 [00:05<00:03, 51.92it/s]
 60%|█████▉    | 256/430 [00:05<00:03, 50.33it/s]
 61%|██████    | 263/430 [00:06<00:03, 53.99it/s]
 63%|██████▎   | 269/430 [00:06<00:03, 51.82it/s]
 64%|██████▍   | 275/430 [00:06<00:03, 50.36it/s]
 65%|██████▌   | 281/430 [00:06<00:02, 50.49it/s]
 67%|██████▋   | 288/430 [00:06<00:02, 53.17it/s]
 68%|██████▊   | 294/430 [00:06<00:02, 51.43it/s]
 70%|██████▉   | 300/430 [00:06<00:02, 49.75it/s]
 71%|███████▏  | 307/430 [00:06<00:02, 53.19it/s]
 73%|███████▎  | 313/430 [00:07<00:02, 52.31it/s]
 74%|███████▍  | 319/430 [00:07<00:02, 50.83it/s]
 76%|███████▌  | 325/430 [00:07<00:02, 49.53it/s]
 77%|███████▋  | 332/430 [00:07<00:01, 54.12it/s]
 79%|███████▊  | 338/430 [00:07<00:01, 51.84it/s]
 80%|████████  | 344/430 [00:07<00:01, 50.52it/s]
 82%|████████▏ | 351/430 [00:07<00:01, 53.78it/s]
 83%|████████▎ | 357/430 [00:07<00:01, 53.10it/s]
 84%|████████▍ | 363/430 [00:07<00:01, 51.01it/s]
 86%|████████▌ | 369/430 [00:08<00:01, 49.79it/s]
 87%|████████▋ | 376/430 [00:08<00:00, 54.55it/s]
 89%|████████▉ | 382/430 [00:08<00:00, 51.98it/s]
 90%|█████████ | 388/430 [00:08<00:00, 50.52it/s]
 92%|█████████▏| 394/430 [00:08<00:00, 52.98it/s]
 93%|█████████▎| 400/430 [00:08<00:00, 53.35it/s]
 94%|█████████▍| 406/430 [00:08<00:00, 51.41it/s]
 96%|█████████▌| 412/430 [00:08<00:00, 50.25it/s]
 97%|█████████▋| 419/430 [00:09<00:00, 53.63it/s]
 99%|█████████▉| 425/430 [00:09<00:00, 53.37it/s]
100%|██████████| 430/430 [00:09<00:00, 46.41it/s]
55000 images processed, 9.83640432357788 seconds used

Processing in-distribution CIFAR-110 images

  0%|          | 0/86 [00:00<?, ?it/s]
  1%|          | 1/86 [00:00<00:43,  1.96it/s]
  8%|▊         | 7/86 [00:00<00:05, 14.14it/s]
 14%|█▍        | 12/86 [00:00<00:03, 22.17it/s]
 20%|█▉        | 17/86 [00:00<00:02, 28.60it/s]
 28%|██▊       | 24/86 [00:00<00:01, 38.63it/s]
 35%|███▍      | 30/86 [00:01<00:01, 41.19it/s]
 41%|████      | 35/86 [00:01<00:01, 42.82it/s]
 47%|████▋     | 40/86 [00:01<00:01, 44.06it/s]
 55%|█████▍    | 47/86 [00:01<00:00, 50.72it/s]
 62%|██████▏   | 53/86 [00:01<00:00, 49.64it/s]
 69%|██████▊   | 59/86 [00:01<00:00, 48.70it/s]
 76%|███████▌  | 65/86 [00:01<00:00, 50.07it/s]
 83%|████████▎ | 71/86 [00:01<00:00, 52.19it/s]
 90%|████████▉ | 77/86 [00:01<00:00, 51.98it/s]
 97%|█████████▋| 83/86 [00:02<00:00, 53.57it/s]
100%|██████████| 86/86 [00:02<00:00, 40.35it/s]
11000 images processed, 2.2123618125915527 seconds used

Processing out-of-distribution SVHN images

  0%|          | 0/204 [00:00<?, ?it/s]
  0%|          | 1/204 [00:00<01:48,  1.88it/s]
  3%|▎         | 7/204 [00:00<00:14, 13.49it/s]
  6%|▌         | 12/204 [00:00<00:09, 20.91it/s]
  8%|▊         | 17/204 [00:00<00:06, 26.80it/s]
 12%|█▏        | 24/204 [00:01<00:04, 36.26it/s]
 14%|█▍        | 29/204 [00:01<00:04, 38.57it/s]
 17%|█▋        | 34/204 [00:01<00:04, 40.20it/s]
 19%|█▉        | 39/204 [00:01<00:03, 41.78it/s]
 22%|██▏       | 45/204 [00:01<00:03, 45.15it/s]
 25%|██▍       | 50/204 [00:01<00:03, 44.42it/s]
 27%|██▋       | 55/204 [00:01<00:03, 43.89it/s]
 30%|██▉       | 61/204 [00:01<00:02, 47.99it/s]
 32%|███▏      | 66/204 [00:01<00:02, 48.22it/s]
 35%|███▍      | 71/204 [00:02<00:02, 46.50it/s]
 37%|███▋      | 76/204 [00:02<00:02, 45.91it/s]
 40%|███▉      | 81/204 [00:02<00:02, 45.59it/s]
 43%|████▎     | 88/204 [00:02<00:02, 49.73it/s]
 46%|████▌     | 93/204 [00:02<00:02, 48.33it/s]
 49%|████▊     | 99/204 [00:02<00:02, 49.26it/s]
 51%|█████     | 104/204 [00:02<00:02, 47.24it/s]
 53%|█████▎    | 109/204 [00:02<00:02, 45.86it/s]
 56%|█████▌    | 114/204 [00:02<00:01, 46.08it/s]
 58%|█████▊    | 119/204 [00:03<00:01, 45.08it/s]
 61%|██████    | 124/204 [00:03<00:01, 42.94it/s]
 63%|██████▎   | 129/204 [00:03<00:01, 39.79it/s]
 66%|██████▌   | 134/204 [00:03<00:01, 38.44it/s]
 68%|██████▊   | 139/204 [00:03<00:01, 39.47it/s]
 71%|███████   | 145/204 [00:03<00:01, 44.26it/s]
 74%|███████▎  | 150/204 [00:03<00:01, 43.79it/s]
 76%|███████▌  | 155/204 [00:03<00:01, 44.61it/s]
 78%|███████▊  | 160/204 [00:04<00:01, 43.90it/s]
 81%|████████  | 165/204 [00:04<00:00, 44.14it/s]
 84%|████████▍ | 171/204 [00:04<00:00, 48.21it/s]
 86%|████████▋ | 176/204 [00:04<00:00, 47.41it/s]
 89%|████████▊ | 181/204 [00:04<00:00, 46.55it/s]
 91%|█████████ | 186/204 [00:04<00:00, 45.56it/s]
 94%|█████████▎| 191/204 [00:04<00:00, 44.95it/s]
 97%|█████████▋| 197/204 [00:04<00:00, 48.96it/s]
 99%|█████████▉| 202/204 [00:04<00:00, 48.89it/s]
100%|██████████| 204/204 [00:04<00:00, 41.44it/s]
26032 images processed, 5.088602542877197 seconds used

Processing out-of-distribution places365 images

  0%|          | 0/79 [00:00<?, ?it/s]
  1%|▏         | 1/79 [00:00<00:48,  1.59it/s]
  8%|▊         | 6/79 [00:00<00:07, 10.39it/s]
 15%|█▌        | 12/79 [00:00<00:03, 20.49it/s]
 22%|██▏       | 17/79 [00:00<00:02, 26.15it/s]
 28%|██▊       | 22/79 [00:01<00:01, 30.88it/s]
 35%|███▌      | 28/79 [00:01<00:01, 37.00it/s]
 44%|████▍     | 35/79 [00:01<00:01, 43.28it/s]
 52%|█████▏    | 41/79 [00:01<00:00, 43.08it/s]
 58%|█████▊    | 46/79 [00:01<00:00, 44.62it/s]
 65%|██████▍   | 51/79 [00:01<00:00, 43.98it/s]
 71%|███████   | 56/79 [00:01<00:00, 43.63it/s]
 77%|███████▋  | 61/79 [00:01<00:00, 44.22it/s]
 85%|████████▍ | 67/79 [00:02<00:00, 46.89it/s]
 92%|█████████▏| 73/79 [00:02<00:00, 47.52it/s]
 99%|█████████▊| 78/79 [00:02<00:00, 46.98it/s]
100%|██████████| 79/79 [00:02<00:00, 35.04it/s]
10000 images processed, 2.2856333255767822 seconds used

Processing out-of-distribution LSUN images

  0%|          | 0/79 [00:00<?, ?it/s]
  1%|▏         | 1/79 [00:00<00:45,  1.70it/s]
  8%|▊         | 6/79 [00:00<00:06, 10.79it/s]
 15%|█▌        | 12/79 [00:00<00:03, 21.11it/s]
 22%|██▏       | 17/79 [00:00<00:02, 26.75it/s]
 28%|██▊       | 22/79 [00:01<00:01, 31.78it/s]
 34%|███▍      | 27/79 [00:01<00:01, 35.06it/s]
 41%|████      | 32/79 [00:01<00:01, 37.27it/s]
 48%|████▊     | 38/79 [00:01<00:00, 42.63it/s]
 54%|█████▍    | 43/79 [00:01<00:00, 42.82it/s]
 62%|██████▏   | 49/79 [00:01<00:00, 45.00it/s]
 70%|██████▉   | 55/79 [00:01<00:00, 48.11it/s]
 77%|███████▋  | 61/79 [00:01<00:00, 46.41it/s]
 84%|████████▎ | 66/79 [00:01<00:00, 45.31it/s]
 90%|████████▉ | 71/79 [00:02<00:00, 46.44it/s]
 96%|█████████▌| 76/79 [00:02<00:00, 45.53it/s]
100%|██████████| 79/79 [00:02<00:00, 35.31it/s]
10000 images processed, 2.3196988105773926 seconds used

Processing out-of-distribution iSUN images

  0%|          | 0/70 [00:00<?, ?it/s]
  1%|▏         | 1/70 [00:00<00:40,  1.72it/s]
 10%|█         | 7/70 [00:00<00:04, 12.94it/s]
 17%|█▋        | 12/70 [00:00<00:02, 20.88it/s]
 24%|██▍       | 17/70 [00:00<00:01, 27.65it/s]
 33%|███▎      | 23/70 [00:01<00:01, 34.31it/s]
 41%|████▏     | 29/70 [00:01<00:01, 39.56it/s]
 49%|████▊     | 34/70 [00:01<00:00, 40.88it/s]
 56%|█████▌    | 39/70 [00:01<00:00, 41.85it/s]
 63%|██████▎   | 44/70 [00:01<00:00, 42.11it/s]
 70%|███████   | 49/70 [00:01<00:00, 42.73it/s]
 80%|████████  | 56/70 [00:01<00:00, 49.01it/s]
 89%|████████▊ | 62/70 [00:01<00:00, 48.64it/s]
 97%|█████████▋| 68/70 [00:01<00:00, 49.99it/s]
100%|██████████| 70/70 [00:01<00:00, 35.47it/s]
8925 images processed, 2.001648187637329 seconds used

Processing out-of-distribution dtd images

  0%|          | 0/45 [00:00<?, ?it/s]
  2%|▏         | 1/45 [00:01<00:50,  1.16s/it]
  4%|▍         | 2/45 [00:01<00:23,  1.82it/s]
 16%|█▌        | 7/45 [00:01<00:04,  8.16it/s]
 27%|██▋       | 12/45 [00:01<00:02, 14.45it/s]
 38%|███▊      | 17/45 [00:01<00:01, 16.29it/s]
 49%|████▉     | 22/45 [00:01<00:01, 21.70it/s]
 58%|█████▊    | 26/45 [00:02<00:01, 14.38it/s]
 69%|██████▉   | 31/45 [00:02<00:00, 18.76it/s]
 80%|████████  | 36/45 [00:02<00:00, 23.13it/s]
 89%|████████▉ | 40/45 [00:02<00:00, 22.21it/s]
100%|██████████| 45/45 [00:02<00:00, 26.96it/s]
100%|██████████| 45/45 [00:02<00:00, 15.55it/s]
5640 images processed, 2.916747808456421 seconds used

28.291161060333252
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.17  98.80  89.51
places365     79.69  78.52  56.58
LSUN          47.63  89.90  79.41
iSUN          53.96  88.24  77.82
dtd           41.15  90.81  84.71
AVG           45.32  89.25  77.61
[incremental] Overall: 0.5070 New: 0.4860 Old: 0.5112
[incremental] Final(Top-1): 0.5070  Average: 0.6613
8.933373212814331
[done] continual incremental run finished. Adapters at: checkpoints/CIFAR-110-resnet34-MLEFixed-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack
