nohup: ignoring input
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.1, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_center_set='all', forget_lambda=0.5, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:107: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:26<08:21, 26.39s/it] 10%|█         | 2/20 [00:45<06:41, 22.33s/it] 15%|█▌        | 3/20 [01:05<05:56, 20.95s/it] 20%|██        | 4/20 [01:24<05:25, 20.33s/it] 25%|██▌       | 5/20 [01:44<05:00, 20.02s/it] 30%|███       | 6/20 [02:03<04:36, 19.73s/it] 35%|███▌      | 7/20 [02:22<04:15, 19.62s/it] 40%|████      | 8/20 [02:41<03:53, 19.48s/it][loss] ep 0 it 0 total=10.1059 mle=1.4969 pcon=5.2950 forget=3.3140 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 50 total=10.1034 mle=1.4710 pcon=5.2879 forget=3.3444 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 100 total=10.1900 mle=1.5970 pcon=5.2809 forget=3.3122 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 150 total=10.3288 mle=1.7523 pcon=5.2738 forget=3.3027 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 200 total=10.2110 mle=1.6199 pcon=5.2670 forget=3.3241 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 250 total=10.0183 mle=1.4374 pcon=5.2603 forget=3.3205 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 300 total=10.0599 mle=1.4845 pcon=5.2541 forget=3.3213 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 350 total=10.1670 mle=1.5891 pcon=5.2476 forget=3.3302 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-planB_adapter
[loss] ep 1 it 10 total=10.1788 mle=1.5755 pcon=5.2409 forget=3.3624 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 60 total=10.1005 mle=1.5655 pcon=5.2346 forget=3.3003 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 110 total=10.0046 mle=1.4496 pcon=5.2285 forget=3.3265 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 160 total=10.2170 mle=1.6636 pcon=5.2224 forget=3.3310 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 210 total=10.1912 mle=1.6527 pcon=5.2167 forget=3.3218 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 260 total=10.0429 mle=1.5306 pcon=5.2112 forget=3.3011 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 310 total=10.1447 mle=1.6298 pcon=5.2056 forget=3.3093 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 360 total=10.2048 mle=1.6850 pcon=5.2003 forget=3.3196 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-planB_adapter
[loss] ep 2 it 20 total=9.9976 mle=1.4934 pcon=5.1950 forget=3.3091 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 70 total=10.2231 mle=1.7525 pcon=5.1899 forget=3.2807 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 120 total=10.1316 mle=1.6370 pcon=5.1847 forget=3.3100 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 170 total=9.9361 mle=1.4590 pcon=5.1796 forget=3.2975 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 220 total=10.0144 mle=1.5213 pcon=5.1745 forget=3.3185 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 270 total=10.2337 mle=1.7611 pcon=5.1699 forget=3.3027 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 320 total=9.9695 mle=1.5195 pcon=5.1652 forget=3.2849 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 370 total=10.1661 mle=1.7167 pcon=5.1605 forget=3.2889 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-planB_adapter
[loss] ep 3 it 30 total=10.0804 mle=1.6255 pcon=5.1559 forget=3.2990 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 80 total=10.0448 mle=1.5783 pcon=5.1519 forget=3.3146 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 130 total=10.1001 mle=1.6603 pcon=5.1475 forget=3.2923 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 180 total=10.2175 mle=1.7885 pcon=5.1435 forget=3.2854 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 230 total=9.9251 mle=1.5001 pcon=5.1395 forget=3.2855 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 280 total=10.1387 mle=1.7226 pcon=5.1353 forget=3.2809 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 330 total=10.0498 mle=1.6109 pcon=5.1311 forget=3.3078 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 380 total=10.0301 mle=1.5924 pcon=5.1275 forget=3.3103 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-planB_adapter
[loss] ep 4 it 40 total=9.9498 mle=1.5635 pcon=5.1236 forget=3.2627 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 90 total=10.0291 mle=1.6336 pcon=5.1196 forget=3.2760 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 140 total=9.9819 mle=1.5738 pcon=5.1159 forget=3.2923 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 190 total=9.8114 mle=1.4069 pcon=5.1124 forget=3.2921 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 240 total=10.0592 mle=1.6712 pcon=5.1086 forget=3.2794 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 290 total=9.8238 mle=1.4388 pcon=5.1050 forget=3.2800 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 340 total=10.1646 mle=1.7743 pcon=5.1015 forget=3.2888 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-planB_adapter
[loss] ep 5 it 0 total=10.1449 mle=1.7706 pcon=5.0982 forget=3.2761 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 50 total=10.0558 mle=1.6942 pcon=5.0945 forget=3.2671 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 100 total=9.8927 mle=1.5209 pcon=5.0913 forget=3.2805 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 150 total=9.8865 mle=1.5213 pcon=5.0878 forget=3.2775 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 200 total=9.9546 mle=1.6070 pcon=5.0849 forget=3.2628 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 250 total=10.0141 mle=1.6769 pcon=5.0819 forget=3.2554 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 300 total=9.8699 mle=1.5206 pcon=5.0787 forget=3.2706 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 350 total=9.9838 mle=1.6478 pcon=5.0758 forget=3.2602 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-planB_adapter
[loss] ep 6 it 10 total=9.7470 mle=1.3750 pcon=5.0729 forget=3.2991 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 60 total=9.7938 mle=1.4878 pcon=5.0700 forget=3.2360 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 110 total=9.8718 mle=1.5325 pcon=5.0673 forget=3.2720 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 160 total=9.9259 mle=1.6025 pcon=5.0644 forget=3.2590 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 210 total=9.8621 mle=1.5482 pcon=5.0621 forget=3.2518 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 260 total=10.0364 mle=1.7104 pcon=5.0591 forget=3.2669 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 310 total=9.8130 mle=1.4810 pcon=5.0565 forget=3.2755 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 360 total=9.8935 mle=1.5896 pcon=5.0539 forget=3.2500 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-planB_adapter
[loss] ep 7 it 20 total=9.8576 mle=1.5680 pcon=5.0510 forget=3.2386 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 70 total=9.9346 mle=1.6552 pcon=5.0484 forget=3.2309 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 120 total=9.8383 mle=1.5593 pcon=5.0456 forget=3.2334 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 170 total=9.7695 mle=1.5055 pcon=5.0430 forget=3.2210 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 220 total=9.8571 mle=1.5693 pcon=5.0407 forget=3.2471 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 270 total=9.9899 mle=1.6977 pcon=5.0382 forget=3.2540 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 320 total=9.9127 mle=1.6315 pcon=5.0353 forget=3.2459 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 370 total=9.8389 mle=1.5750 pcon=5.0329 forget=3.2310 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-planB_adapter
[loss] ep 8 it 30 total=9.7645 mle=1.4875 pcon=5.0305 forget=3.2465 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 80 total=9.7858 mle=1.5260 pcon=5.0280 forget=3.2318 nr=64 nf=64 protos=540 dmin_norm=NA
 45%|████▌     | 9/20 [03:01<03:34, 19.54s/it] 50%|█████     | 10/20 [03:20<03:14, 19.45s/it] 55%|█████▌    | 11/20 [03:39<02:52, 19.20s/it] 60%|██████    | 12/20 [03:58<02:34, 19.28s/it] 65%|██████▌   | 13/20 [04:17<02:14, 19.20s/it] 70%|███████   | 14/20 [04:36<01:54, 19.14s/it] 75%|███████▌  | 15/20 [04:55<01:35, 19.01s/it] 80%|████████  | 16/20 [05:14<01:15, 18.97s/it] 85%|████████▌ | 17/20 [05:33<00:57, 19.02s/it][loss] ep 8 it 130 total=9.7132 mle=1.4566 pcon=5.0254 forget=3.2312 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 180 total=9.8704 mle=1.6298 pcon=5.0229 forget=3.2177 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 230 total=9.7107 mle=1.4742 pcon=5.0208 forget=3.2156 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 280 total=9.8326 mle=1.6063 pcon=5.0184 forget=3.2080 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 330 total=9.9239 mle=1.6598 pcon=5.0157 forget=3.2484 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 380 total=9.6768 mle=1.4424 pcon=5.0132 forget=3.2212 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-planB_adapter
[loss] ep 9 it 40 total=9.7814 mle=1.5645 pcon=5.0108 forget=3.2061 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 90 total=9.8570 mle=1.6148 pcon=5.0082 forget=3.2340 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 140 total=9.9706 mle=1.7114 pcon=5.0062 forget=3.2529 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 190 total=9.7613 mle=1.5339 pcon=5.0042 forget=3.2232 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 240 total=9.7548 mle=1.5053 pcon=5.0023 forget=3.2472 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 290 total=9.7625 mle=1.5083 pcon=5.0001 forget=3.2541 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 340 total=9.7588 mle=1.5147 pcon=4.9982 forget=3.2459 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-planB_adapter
[loss] ep 10 it 0 total=9.8182 mle=1.5589 pcon=4.9961 forget=3.2631 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 50 total=9.8023 mle=1.5286 pcon=4.9941 forget=3.2795 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 100 total=9.9315 mle=1.6740 pcon=4.9924 forget=3.2651 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 150 total=9.9822 mle=1.7307 pcon=4.9907 forget=3.2608 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 200 total=10.0188 mle=1.7551 pcon=4.9891 forget=3.2747 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 250 total=9.8070 mle=1.5335 pcon=4.9876 forget=3.2860 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 300 total=9.7873 mle=1.5217 pcon=4.9863 forget=3.2793 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 350 total=9.8887 mle=1.5811 pcon=4.9848 forget=3.3228 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 10 total=9.8280 mle=1.5309 pcon=4.9832 forget=3.3138 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 60 total=9.8456 mle=1.5739 pcon=4.9818 forget=3.2900 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 110 total=9.8000 mle=1.5275 pcon=4.9805 forget=3.2920 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 160 total=9.8892 mle=1.5971 pcon=4.9788 forget=3.3133 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 210 total=9.9808 mle=1.6930 pcon=4.9775 forget=3.3102 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 260 total=9.8586 mle=1.5547 pcon=4.9767 forget=3.3273 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 310 total=10.1646 mle=1.8608 pcon=4.9752 forget=3.3285 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 360 total=9.8907 mle=1.6066 pcon=4.9739 forget=3.3101 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 20 total=9.8600 mle=1.5574 pcon=4.9724 forget=3.3301 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 70 total=9.9361 mle=1.6328 pcon=4.9711 forget=3.3321 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 120 total=9.8405 mle=1.5160 pcon=4.9699 forget=3.3546 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 170 total=9.9691 mle=1.6749 pcon=4.9686 forget=3.3256 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 220 total=9.8402 mle=1.5383 pcon=4.9673 forget=3.3346 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 270 total=9.8091 mle=1.4910 pcon=4.9664 forget=3.3517 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 320 total=9.7774 mle=1.4759 pcon=4.9652 forget=3.3363 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 370 total=10.0757 mle=1.7533 pcon=4.9639 forget=3.3584 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 30 total=9.7062 mle=1.3992 pcon=4.9628 forget=3.3442 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 80 total=9.7995 mle=1.4454 pcon=4.9615 forget=3.3926 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 130 total=9.8176 mle=1.5096 pcon=4.9601 forget=3.3479 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 180 total=9.8061 mle=1.5209 pcon=4.9591 forget=3.3261 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 230 total=9.8950 mle=1.5983 pcon=4.9579 forget=3.3388 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 280 total=9.8077 mle=1.5270 pcon=4.9567 forget=3.3240 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 330 total=9.8440 mle=1.5437 pcon=4.9554 forget=3.3449 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 380 total=9.8341 mle=1.5497 pcon=4.9542 forget=3.3302 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 40 total=9.9758 mle=1.7009 pcon=4.9528 forget=3.3221 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 90 total=9.9880 mle=1.6998 pcon=4.9519 forget=3.3364 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 140 total=9.8172 mle=1.5354 pcon=4.9512 forget=3.3306 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 190 total=10.0671 mle=1.7912 pcon=4.9498 forget=3.3260 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 240 total=9.8499 mle=1.5744 pcon=4.9489 forget=3.3265 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 290 total=9.8587 mle=1.6027 pcon=4.9478 forget=3.3083 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 340 total=9.8985 mle=1.6332 pcon=4.9466 forget=3.3187 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 0 total=9.9195 mle=1.6379 pcon=4.9456 forget=3.3360 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 50 total=9.7153 mle=1.4537 pcon=4.9444 forget=3.3172 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 100 total=9.7912 mle=1.5293 pcon=4.9433 forget=3.3186 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 150 total=9.9117 mle=1.6327 pcon=4.9423 forget=3.3367 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 200 total=9.8270 mle=1.5775 pcon=4.9412 forget=3.3082 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 250 total=10.0738 mle=1.8035 pcon=4.9401 forget=3.3301 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 300 total=9.7970 mle=1.5312 pcon=4.9393 forget=3.3265 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 350 total=9.7719 mle=1.5079 pcon=4.9383 forget=3.3257 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 10 total=9.7880 mle=1.5317 pcon=4.9372 forget=3.3190 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 60 total=9.7237 mle=1.4793 pcon=4.9362 forget=3.3082 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 110 total=9.7485 mle=1.5091 pcon=4.9355 forget=3.3039 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 160 total=9.8606 mle=1.6173 pcon=4.9343 forget=3.3089 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 210 total=9.7600 mle=1.5347 pcon=4.9332 forget=3.2921 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 260 total=10.0720 mle=1.8457 pcon=4.9320 forget=3.2943 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 310 total=9.9398 mle=1.7024 pcon=4.9311 forget=3.3063 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 360 total=9.7923 mle=1.5549 pcon=4.9301 forget=3.3073 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 20 total=9.9230 mle=1.6787 pcon=4.9288 forget=3.3154 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 70 total=9.9774 mle=1.7576 pcon=4.9282 forget=3.2915 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 120 total=9.7855 mle=1.5503 pcon=4.9271 forget=3.3081 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 170 total=9.7852 mle=1.5617 pcon=4.9258 forget=3.2976 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 220 total=9.8571 mle=1.6147 pcon=4.9248 forget=3.3176 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 270 total=9.6638 mle=1.4388 pcon=4.9244 forget=3.3006 nr=64 nf=64 protos=540 dmin_norm=NA
 90%|█████████ | 18/20 [05:52<00:38, 19.01s/it] 95%|█████████▌| 19/20 [06:10<00:18, 18.79s/it]100%|██████████| 20/20 [06:30<00:00, 19.03s/it]100%|██████████| 20/20 [06:30<00:00, 19.52s/it]
[loss] ep 17 it 320 total=9.9557 mle=1.7385 pcon=4.9235 forget=3.2937 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 370 total=9.8977 mle=1.6852 pcon=4.9224 forget=3.2901 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 30 total=9.7751 mle=1.5720 pcon=4.9220 forget=3.2811 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 80 total=9.8641 mle=1.6320 pcon=4.9212 forget=3.3110 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 130 total=9.8449 mle=1.6174 pcon=4.9203 forget=3.3071 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 180 total=9.7299 mle=1.5117 pcon=4.9197 forget=3.2986 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 230 total=9.7696 mle=1.5557 pcon=4.9189 forget=3.2951 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 280 total=9.8418 mle=1.6244 pcon=4.9182 forget=3.2992 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 330 total=9.9417 mle=1.7377 pcon=4.9176 forget=3.2864 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 380 total=9.8614 mle=1.6397 pcon=4.9172 forget=3.3045 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-planB_adapter
[loss] ep 19 it 40 total=9.6806 mle=1.4562 pcon=4.9168 forget=3.3075 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 90 total=9.8742 mle=1.6369 pcon=4.9166 forget=3.3207 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 140 total=9.7564 mle=1.5445 pcon=4.9159 forget=3.2960 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 190 total=9.7986 mle=1.5943 pcon=4.9151 forget=3.2891 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 240 total=9.9962 mle=1.7942 pcon=4.9146 forget=3.2874 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 290 total=9.7384 mle=1.5215 pcon=4.9143 forget=3.3026 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 340 total=9.8159 mle=1.5749 pcon=4.9140 forget=3.3270 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:48,  3.61it/s]  3%|▎         | 11/391 [00:00<00:10, 34.56it/s]  4%|▍         | 17/391 [00:00<00:09, 40.59it/s]  6%|▌         | 23/391 [00:00<00:08, 43.39it/s]  7%|▋         | 29/391 [00:00<00:07, 45.78it/s] 10%|▉         | 38/391 [00:00<00:06, 57.36it/s] 12%|█▏        | 45/391 [00:00<00:06, 54.17it/s] 13%|█▎        | 51/391 [00:01<00:06, 53.70it/s] 15%|█▍        | 57/391 [00:01<00:06, 51.74it/s] 17%|█▋        | 67/391 [00:01<00:05, 63.78it/s] 19%|█▉        | 74/391 [00:01<00:05, 59.30it/s] 21%|██        | 81/391 [00:01<00:05, 57.09it/s] 22%|██▏       | 87/391 [00:01<00:05, 54.17it/s] 25%|██▍       | 96/391 [00:01<00:04, 60.45it/s] 26%|██▋       | 103/391 [00:02<00:05, 56.73it/s] 28%|██▊       | 109/391 [00:02<00:05, 55.57it/s] 29%|██▉       | 115/391 [00:02<00:04, 55.95it/s] 32%|███▏      | 124/391 [00:02<00:04, 63.09it/s] 34%|███▎      | 131/391 [00:02<00:04, 58.68it/s] 35%|███▌      | 137/391 [00:02<00:04, 57.43it/s] 37%|███▋      | 143/391 [00:02<00:04, 55.33it/s] 39%|███▊      | 151/391 [00:02<00:04, 59.79it/s] 40%|████      | 158/391 [00:02<00:04, 56.82it/s] 42%|████▏     | 164/391 [00:03<00:04, 55.49it/s] 43%|████▎     | 170/391 [00:03<00:04, 54.55it/s] 45%|████▌     | 176/391 [00:03<00:04, 53.61it/s] 47%|████▋     | 182/391 [00:03<00:04, 52.01it/s] 48%|████▊     | 188/391 [00:03<00:03, 51.63it/s] 50%|████▉     | 194/391 [00:03<00:03, 52.16it/s] 51%|█████▏    | 201/391 [00:03<00:03, 56.81it/s] 54%|█████▍    | 211/391 [00:03<00:02, 67.69it/s] 56%|█████▋    | 220/391 [00:03<00:02, 72.50it/s] 59%|█████▉    | 230/391 [00:04<00:02, 79.33it/s] 61%|██████    | 239/391 [00:04<00:01, 80.97it/s] 64%|██████▎   | 249/391 [00:04<00:01, 85.55it/s] 66%|██████▌   | 259/391 [00:04<00:01, 88.81it/s] 69%|██████▊   | 268/391 [00:04<00:01, 88.98it/s] 71%|███████   | 277/391 [00:04<00:01, 72.20it/s] 73%|███████▎  | 285/391 [00:04<00:01, 64.30it/s] 75%|███████▍  | 292/391 [00:04<00:01, 61.63it/s] 76%|███████▋  | 299/391 [00:05<00:01, 57.18it/s] 78%|███████▊  | 305/391 [00:05<00:01, 55.69it/s] 80%|███████▉  | 311/391 [00:05<00:01, 55.02it/s] 81%|████████  | 317/391 [00:05<00:01, 53.77it/s] 83%|████████▎ | 323/391 [00:05<00:01, 53.42it/s] 84%|████████▍ | 329/391 [00:05<00:01, 51.55it/s] 86%|████████▌ | 335/391 [00:05<00:01, 52.15it/s] 87%|████████▋ | 341/391 [00:05<00:00, 51.30it/s] 89%|████████▊ | 347/391 [00:06<00:00, 50.62it/s] 91%|█████████ | 355/391 [00:06<00:00, 56.81it/s] 92%|█████████▏| 361/391 [00:06<00:00, 56.91it/s] 94%|█████████▍| 367/391 [00:06<00:00, 54.54it/s] 95%|█████████▌| 373/391 [00:06<00:00, 53.06it/s] 97%|█████████▋| 379/391 [00:06<00:00, 52.55it/s] 98%|█████████▊| 385/391 [00:06<00:00, 52.59it/s]100%|██████████| 391/391 [00:06<00:00, 51.45it/s]100%|██████████| 391/391 [00:06<00:00, 57.02it/s]
50000 images processed, 6.958848476409912 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:20,  3.90it/s] 14%|█▍        | 11/79 [00:00<00:01, 36.98it/s] 23%|██▎       | 18/79 [00:00<00:01, 47.33it/s] 32%|███▏      | 25/79 [00:00<00:01, 48.42it/s] 39%|███▉      | 31/79 [00:00<00:00, 48.59it/s] 47%|████▋     | 37/79 [00:00<00:00, 50.63it/s] 54%|█████▍    | 43/79 [00:00<00:00, 50.07it/s] 62%|██████▏   | 49/79 [00:01<00:00, 50.83it/s] 70%|██████▉   | 55/79 [00:01<00:00, 49.81it/s] 77%|███████▋  | 61/79 [00:01<00:00, 51.92it/s] 85%|████████▍ | 67/79 [00:01<00:00, 51.09it/s] 92%|█████████▏| 73/79 [00:01<00:00, 50.66it/s]100%|██████████| 79/79 [00:01<00:00, 51.04it/s]100%|██████████| 79/79 [00:01<00:00, 47.43it/s]
10000 images processed, 1.6860737800598145 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<00:59,  3.43it/s]  3%|▎         | 6/204 [00:00<00:11, 17.52it/s]  5%|▌         | 11/204 [00:00<00:07, 26.60it/s]  9%|▉         | 19/204 [00:00<00:04, 42.32it/s] 12%|█▏        | 25/204 [00:00<00:03, 45.24it/s] 15%|█▌        | 31/204 [00:00<00:03, 45.09it/s] 18%|█▊        | 36/204 [00:00<00:03, 45.53it/s] 22%|██▏       | 44/204 [00:01<00:02, 53.61it/s] 25%|██▍       | 50/204 [00:01<00:03, 50.53it/s] 27%|██▋       | 56/204 [00:01<00:02, 49.53it/s] 30%|███       | 62/204 [00:01<00:02, 51.67it/s] 33%|███▎      | 68/204 [00:01<00:02, 49.80it/s] 36%|███▋      | 74/204 [00:01<00:02, 49.63it/s] 39%|███▉      | 80/204 [00:01<00:02, 49.01it/s] 43%|████▎     | 87/204 [00:01<00:02, 52.00it/s] 46%|████▌     | 93/204 [00:02<00:02, 49.38it/s] 48%|████▊     | 98/204 [00:02<00:02, 48.29it/s] 51%|█████     | 104/204 [00:02<00:01, 51.30it/s] 54%|█████▍    | 110/204 [00:02<00:01, 49.12it/s] 57%|█████▋    | 116/204 [00:02<00:01, 49.08it/s] 60%|█████▉    | 122/204 [00:02<00:01, 51.92it/s] 63%|██████▎   | 128/204 [00:02<00:01, 53.50it/s] 66%|██████▌   | 134/204 [00:02<00:01, 50.02it/s] 69%|██████▊   | 140/204 [00:03<00:01, 48.62it/s] 73%|███████▎  | 148/204 [00:03<00:01, 54.14it/s] 75%|███████▌  | 154/204 [00:03<00:00, 51.46it/s] 78%|███████▊  | 160/204 [00:03<00:00, 50.06it/s] 82%|████████▏ | 167/204 [00:03<00:00, 54.99it/s] 85%|████████▍ | 173/204 [00:03<00:00, 55.80it/s] 88%|████████▊ | 179/204 [00:03<00:00, 51.56it/s] 91%|█████████ | 185/204 [00:03<00:00, 49.35it/s] 94%|█████████▍| 192/204 [00:03<00:00, 53.42it/s] 97%|█████████▋| 198/204 [00:04<00:00, 50.92it/s]100%|██████████| 204/204 [00:04<00:00, 50.93it/s]100%|██████████| 204/204 [00:04<00:00, 48.05it/s]
26032 images processed, 4.316647052764893 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:38,  2.04it/s]  8%|▊         | 6/79 [00:00<00:05, 12.45it/s] 14%|█▍        | 11/79 [00:00<00:03, 21.03it/s] 24%|██▍       | 19/79 [00:00<00:01, 35.05it/s] 32%|███▏      | 25/79 [00:00<00:01, 41.17it/s] 39%|███▉      | 31/79 [00:01<00:01, 41.72it/s] 46%|████▌     | 36/79 [00:01<00:01, 42.72it/s] 53%|█████▎    | 42/79 [00:01<00:00, 46.77it/s] 61%|██████    | 48/79 [00:01<00:00, 46.66it/s] 67%|██████▋   | 53/79 [00:01<00:00, 46.94it/s] 75%|███████▍  | 59/79 [00:01<00:00, 49.48it/s] 82%|████████▏ | 65/79 [00:01<00:00, 49.22it/s] 90%|████████▉ | 71/79 [00:01<00:00, 48.46it/s] 96%|█████████▌| 76/79 [00:01<00:00, 46.95it/s]100%|██████████| 79/79 [00:02<00:00, 39.27it/s]
10000 images processed, 2.051790237426758 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:22,  3.42it/s]  8%|▊         | 6/79 [00:00<00:03, 18.37it/s] 15%|█▌        | 12/79 [00:00<00:02, 29.40it/s] 22%|██▏       | 17/79 [00:00<00:01, 35.23it/s] 30%|███       | 24/79 [00:00<00:01, 42.68it/s] 37%|███▋      | 29/79 [00:00<00:01, 42.66it/s] 43%|████▎     | 34/79 [00:00<00:01, 44.12it/s] 52%|█████▏    | 41/79 [00:01<00:00, 48.61it/s] 59%|█████▉    | 47/79 [00:01<00:00, 47.07it/s] 67%|██████▋   | 53/79 [00:01<00:00, 48.20it/s] 75%|███████▍  | 59/79 [00:01<00:00, 50.66it/s] 82%|████████▏ | 65/79 [00:01<00:00, 49.19it/s] 89%|████████▊ | 70/79 [00:01<00:00, 48.48it/s] 95%|█████████▍| 75/79 [00:01<00:00, 48.19it/s]100%|██████████| 79/79 [00:01<00:00, 42.95it/s]
10000 images processed, 1.86372709274292 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:22,  3.02it/s]  9%|▊         | 6/70 [00:00<00:03, 16.58it/s] 16%|█▌        | 11/70 [00:00<00:02, 25.61it/s] 26%|██▌       | 18/70 [00:00<00:01, 38.29it/s] 39%|███▊      | 27/70 [00:00<00:00, 50.89it/s] 47%|████▋     | 33/70 [00:00<00:00, 49.36it/s] 56%|█████▌    | 39/70 [00:01<00:00, 48.98it/s] 64%|██████▍   | 45/70 [00:01<00:00, 51.22it/s] 73%|███████▎  | 51/70 [00:01<00:00, 51.14it/s] 81%|████████▏ | 57/70 [00:01<00:00, 48.91it/s] 90%|█████████ | 63/70 [00:01<00:00, 48.37it/s]100%|██████████| 70/70 [00:01<00:00, 44.02it/s]
8925 images processed, 1.6163249015808105 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:38,  1.14it/s] 16%|█▌        | 7/45 [00:01<00:04,  8.67it/s] 22%|██▏       | 10/45 [00:01<00:04,  7.61it/s] 31%|███       | 14/45 [00:01<00:02, 11.31it/s] 38%|███▊      | 17/45 [00:01<00:02, 10.38it/s] 44%|████▍     | 20/45 [00:02<00:01, 12.79it/s] 51%|█████     | 23/45 [00:02<00:01, 14.35it/s] 58%|█████▊    | 26/45 [00:02<00:01, 11.69it/s] 62%|██████▏   | 28/45 [00:02<00:01, 11.26it/s] 71%|███████   | 32/45 [00:02<00:00, 15.57it/s] 78%|███████▊  | 35/45 [00:03<00:01,  9.46it/s] 89%|████████▉ | 40/45 [00:03<00:00, 14.15it/s] 96%|█████████▌| 43/45 [00:04<00:00,  8.93it/s]100%|██████████| 45/45 [00:04<00:00, 10.37it/s]
5640 images processed, 4.365688323974609 seconds used

24.483909845352173
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.58  99.35
places365     68.04  81.13
LSUN          17.60  96.09
iSUN          72.27  81.55
dtd           38.46  91.26
forget        78.50  86.91
AVG           46.24  89.38
Forget-Acc: 0.7790 | Retain-Acc: 0.7411
Forget-as-OOD (retain known vs forget novel):
  FPR: 78.50 AUROC: 86.91 AUIN: 98.36
10.370041608810425
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05_rf.png
