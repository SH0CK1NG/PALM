nohup: ignoring input
==== Stage 1: forget_inc={0,8,11,40,51}; forget_seen={}; all={0,8,11,40,51,66,67,88,94,57} ====
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1', adapter_load_path=None, adapter_load_paths=None, lora_new_adapter_name=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc='0,8,11,40,51', forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [01:20<1:05:52, 80.66s/it]  4%|▍         | 2/50 [01:37<34:38, 43.30s/it]    6%|▌         | 3/50 [01:52<23:35, 30.12s/it]  8%|▊         | 4/50 [02:10<19:27, 25.39s/it] 10%|█         | 5/50 [02:43<21:01, 28.04s/it] 12%|█▏        | 6/50 [03:15<21:41, 29.59s/it] 14%|█▍        | 7/50 [03:46<21:22, 29.83s/it][loss] ep 0 it 0 total=8.4097 mle=1.6267 pcon=5.2951 forget=1.4879 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 0 it 50 total=8.7569 mle=1.9947 pcon=5.2910 forget=1.4712 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 100 total=8.8802 mle=2.1356 pcon=5.2868 forget=1.4578 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 150 total=9.2349 mle=2.4875 pcon=5.2829 forget=1.4645 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 0 it 200 total=9.2723 mle=2.5439 pcon=5.2787 forget=1.4496 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 250 total=8.6726 mle=1.9316 pcon=5.2747 forget=1.4662 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 0 it 300 total=8.4971 mle=1.7803 pcon=5.2707 forget=1.4462 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 350 total=9.0523 mle=2.3364 pcon=5.2668 forget=1.4491 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 1 it 10 total=8.3919 mle=1.6598 pcon=5.2628 forget=1.4693 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 1 it 60 total=8.3570 mle=1.6492 pcon=5.2590 forget=1.4488 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 1 it 110 total=8.4057 mle=1.6988 pcon=5.2552 forget=1.4517 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 1 it 160 total=8.5315 mle=1.7837 pcon=5.2515 forget=1.4963 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 1 it 210 total=8.2689 mle=1.5548 pcon=5.2477 forget=1.4664 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 1 it 260 total=8.7201 mle=2.0037 pcon=5.2439 forget=1.4725 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 1 it 310 total=8.9390 mle=2.2288 pcon=5.2404 forget=1.4699 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 1 it 360 total=8.5475 mle=1.8686 pcon=5.2366 forget=1.4423 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 2 it 20 total=8.0426 mle=1.3554 pcon=5.2331 forget=1.4542 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 2 it 70 total=8.4409 mle=1.8003 pcon=5.2298 forget=1.4108 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 2 it 120 total=8.3366 mle=1.6570 pcon=5.2262 forget=1.4534 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 2 it 170 total=8.7021 mle=2.0440 pcon=5.2228 forget=1.4353 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 2 it 220 total=9.0546 mle=2.4081 pcon=5.2195 forget=1.4270 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 2 it 270 total=8.4799 mle=1.8172 pcon=5.2162 forget=1.4466 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 2 it 320 total=8.2501 mle=1.5928 pcon=5.2130 forget=1.4443 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 2 it 370 total=8.6813 mle=2.0348 pcon=5.2098 forget=1.4367 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 3 it 30 total=8.2823 mle=1.6330 pcon=5.2065 forget=1.4427 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 3 it 80 total=8.7729 mle=2.0477 pcon=5.2032 forget=1.5220 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 3 it 130 total=8.5106 mle=1.9329 pcon=5.2003 forget=1.3774 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 3 it 180 total=8.7390 mle=2.1011 pcon=5.1969 forget=1.4410 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 3 it 230 total=8.4280 mle=1.8244 pcon=5.1937 forget=1.4099 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 3 it 280 total=8.4483 mle=1.8525 pcon=5.1909 forget=1.4049 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 3 it 330 total=8.7579 mle=2.0952 pcon=5.1880 forget=1.4747 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 3 it 380 total=8.3132 mle=1.6387 pcon=5.1849 forget=1.4895 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 4 it 40 total=8.5240 mle=1.8600 pcon=5.1821 forget=1.4819 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 4 it 90 total=8.2765 mle=1.7144 pcon=5.1790 forget=1.3830 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 4 it 140 total=9.0108 mle=2.3583 pcon=5.1762 forget=1.4763 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 4 it 190 total=8.4975 mle=1.8556 pcon=5.1731 forget=1.4688 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 4 it 240 total=8.4979 mle=1.8216 pcon=5.1701 forget=1.5062 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 4 it 290 total=8.2818 mle=1.7157 pcon=5.1672 forget=1.3989 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 4 it 340 total=8.6384 mle=2.0387 pcon=5.1643 forget=1.4354 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 5 it 0 total=8.5788 mle=2.0075 pcon=5.1619 forget=1.4095 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 5 it 50 total=8.8869 mle=2.3014 pcon=5.1590 forget=1.4264 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 5 it 100 total=8.8501 mle=2.2188 pcon=5.1564 forget=1.4750 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 5 it 150 total=8.4289 mle=1.8598 pcon=5.1539 forget=1.4152 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 5 it 200 total=9.1524 mle=2.5830 pcon=5.1511 forget=1.4183 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 5 it 250 total=8.3931 mle=1.7598 pcon=5.1485 forget=1.4848 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 5 it 300 total=8.7815 mle=2.2386 pcon=5.1460 forget=1.3969 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 5 it 350 total=8.8126 mle=2.2539 pcon=5.1434 forget=1.4154 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 6 it 10 total=8.1658 mle=1.6210 pcon=5.1409 forget=1.4039 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 6 it 60 total=8.4751 mle=1.9200 pcon=5.1383 forget=1.4168 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 6 it 110 total=8.3707 mle=1.8516 pcon=5.1357 forget=1.3834 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 6 it 160 total=8.5823 mle=2.0544 pcon=5.1330 forget=1.3949 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 6 it 210 total=8.4007 mle=1.8512 pcon=5.1299 forget=1.4195 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 6 it 260 total=8.4752 mle=1.9270 pcon=5.1272 forget=1.4209 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 6 it 310 total=8.5552 mle=1.9831 pcon=5.1246 forget=1.4475 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 6 it 360 total=8.5022 mle=1.9072 pcon=5.1225 forget=1.4726 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 7 it 20 total=8.2130 mle=1.6838 pcon=5.1198 forget=1.4093 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 7 it 70 total=8.1564 mle=1.6640 pcon=5.1172 forget=1.3752 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 7 it 120 total=8.6474 mle=2.1497 pcon=5.1149 forget=1.3829 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 7 it 170 total=8.2574 mle=1.6817 pcon=5.1126 forget=1.4631 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
 16%|█▌        | 8/50 [04:00<17:24, 24.87s/it] 18%|█▊        | 9/50 [04:14<14:45, 21.60s/it] 20%|██        | 10/50 [04:28<12:51, 19.30s/it] 22%|██▏       | 11/50 [04:42<11:26, 17.59s/it] 24%|██▍       | 12/50 [04:59<11:06, 17.54s/it] 26%|██▌       | 13/50 [05:14<10:16, 16.66s/it] 28%|██▊       | 14/50 [05:29<09:35, 15.98s/it] 30%|███       | 15/50 [05:43<09:03, 15.52s/it][loss] ep 7 it 220 total=8.0992 mle=1.5848 pcon=5.1101 forget=1.4043 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 7 it 270 total=9.2850 mle=2.8264 pcon=5.1075 forget=1.3510 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 7 it 320 total=8.4351 mle=1.9334 pcon=5.1049 forget=1.3968 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 7 it 370 total=8.3673 mle=1.8711 pcon=5.1024 forget=1.3938 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 8 it 30 total=8.5728 mle=2.0813 pcon=5.1002 forget=1.3913 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 8 it 80 total=8.1367 mle=1.6261 pcon=5.0975 forget=1.4130 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 8 it 130 total=8.5035 mle=1.9970 pcon=5.0950 forget=1.4115 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 8 it 180 total=8.4667 mle=1.9921 pcon=5.0927 forget=1.3820 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 8 it 230 total=8.1783 mle=1.6775 pcon=5.0902 forget=1.4106 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 8 it 280 total=7.8593 mle=1.4216 pcon=5.0876 forget=1.3501 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 8 it 330 total=8.8788 mle=2.3858 pcon=5.0848 forget=1.4082 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 8 it 380 total=8.2307 mle=1.7397 pcon=5.0823 forget=1.4087 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 9 it 40 total=8.3519 mle=1.9139 pcon=5.0797 forget=1.3583 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 9 it 90 total=8.3204 mle=1.8318 pcon=5.0773 forget=1.4113 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 9 it 140 total=8.1275 mle=1.6915 pcon=5.0751 forget=1.3610 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 9 it 190 total=8.0826 mle=1.6183 pcon=5.0730 forget=1.3913 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 9 it 240 total=8.5544 mle=2.1159 pcon=5.0707 forget=1.3679 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 9 it 290 total=8.1486 mle=1.6794 pcon=5.0685 forget=1.4007 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 9 it 340 total=8.0062 mle=1.5738 pcon=5.0663 forget=1.3661 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 10 it 0 total=8.0603 mle=1.5900 pcon=5.0638 forget=1.4065 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 10 it 50 total=8.1400 mle=1.7010 pcon=5.0618 forget=1.3772 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 10 it 100 total=7.9373 mle=1.4903 pcon=5.0593 forget=1.3876 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 10 it 150 total=8.2567 mle=1.8153 pcon=5.0570 forget=1.3844 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 10 it 200 total=8.4017 mle=1.9568 pcon=5.0551 forget=1.3897 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 10 it 250 total=8.9019 mle=2.4600 pcon=5.0529 forget=1.3890 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 10 it 300 total=8.2755 mle=1.8252 pcon=5.0507 forget=1.3996 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 10 it 350 total=8.2059 mle=1.7584 pcon=5.0488 forget=1.3987 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 11 it 10 total=8.1484 mle=1.7124 pcon=5.0468 forget=1.3892 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 11 it 60 total=8.0430 mle=1.6029 pcon=5.0448 forget=1.3954 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 11 it 110 total=7.8338 mle=1.3935 pcon=5.0428 forget=1.3975 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 11 it 160 total=8.2047 mle=1.7616 pcon=5.0409 forget=1.4023 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 11 it 210 total=8.1242 mle=1.6802 pcon=5.0388 forget=1.4052 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 11 it 260 total=7.9285 mle=1.4742 pcon=5.0365 forget=1.4178 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 11 it 310 total=8.4018 mle=1.9598 pcon=5.0343 forget=1.4078 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 11 it 360 total=8.2258 mle=1.7761 pcon=5.0321 forget=1.4176 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 12 it 20 total=7.9589 mle=1.5067 pcon=5.0299 forget=1.4223 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 70 total=8.2087 mle=1.7419 pcon=5.0276 forget=1.4392 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 12 it 120 total=8.1216 mle=1.6434 pcon=5.0254 forget=1.4528 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 12 it 170 total=8.3886 mle=1.9213 pcon=5.0230 forget=1.4443 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 220 total=8.1087 mle=1.6330 pcon=5.0205 forget=1.4552 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 12 it 270 total=8.0880 mle=1.6128 pcon=5.0180 forget=1.4572 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 320 total=8.2829 mle=1.8142 pcon=5.0158 forget=1.4529 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 12 it 370 total=8.4803 mle=2.0042 pcon=5.0133 forget=1.4629 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 13 it 30 total=8.4353 mle=1.9821 pcon=5.0110 forget=1.4422 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 13 it 80 total=8.2349 mle=1.7446 pcon=5.0087 forget=1.4816 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 13 it 130 total=8.2974 mle=1.8093 pcon=5.0064 forget=1.4816 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 13 it 180 total=8.0977 mle=1.6236 pcon=5.0039 forget=1.4702 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 13 it 230 total=8.0364 mle=1.5616 pcon=5.0014 forget=1.4733 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 13 it 280 total=8.0172 mle=1.5698 pcon=4.9992 forget=1.4482 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 13 it 330 total=8.1585 mle=1.6762 pcon=4.9968 forget=1.4855 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 13 it 380 total=8.1336 mle=1.6765 pcon=4.9945 forget=1.4625 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 14 it 40 total=7.9689 mle=1.5054 pcon=4.9923 forget=1.4712 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 90 total=8.2732 mle=1.7832 pcon=4.9902 forget=1.4998 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 140 total=8.0912 mle=1.6238 pcon=4.9880 forget=1.4794 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 14 it 190 total=7.9722 mle=1.5289 pcon=4.9858 forget=1.4575 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 240 total=8.0072 mle=1.5841 pcon=4.9835 forget=1.4396 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 14 it 290 total=7.9035 mle=1.4809 pcon=4.9812 forget=1.4414 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 14 it 340 total=7.9943 mle=1.5971 pcon=4.9792 forget=1.4180 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 15 it 0 total=7.9164 mle=1.5307 pcon=4.9769 forget=1.4088 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
 32%|███▏      | 16/50 [06:00<08:58, 15.82s/it] 34%|███▍      | 17/50 [06:14<08:27, 15.37s/it] 36%|███▌      | 18/50 [06:28<08:01, 15.06s/it] 38%|███▊      | 19/50 [06:43<07:40, 14.84s/it] 40%|████      | 20/50 [06:57<07:21, 14.73s/it] 42%|████▏     | 21/50 [07:11<07:02, 14.58s/it] 44%|████▍     | 22/50 [07:27<07:00, 15.01s/it][loss] ep 15 it 50 total=7.9805 mle=1.5786 pcon=4.9747 forget=1.4272 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 15 it 100 total=8.0135 mle=1.6053 pcon=4.9728 forget=1.4354 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 150 total=8.2900 mle=1.9112 pcon=4.9707 forget=1.4081 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 200 total=7.9316 mle=1.6030 pcon=4.9687 forget=1.3600 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 15 it 250 total=8.0310 mle=1.7057 pcon=4.9668 forget=1.3585 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 15 it 300 total=7.9783 mle=1.7222 pcon=4.9646 forget=1.2915 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 350 total=7.6942 mle=1.5099 pcon=4.9626 forget=1.2216 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 16 it 10 total=7.7672 mle=1.4591 pcon=4.9606 forget=1.3476 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 16 it 60 total=7.7679 mle=1.5525 pcon=4.9586 forget=1.2568 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 16 it 110 total=7.8885 mle=1.6973 pcon=4.9566 forget=1.2346 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 16 it 160 total=7.7835 mle=1.5688 pcon=4.9547 forget=1.2599 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 16 it 210 total=7.9964 mle=1.8025 pcon=4.9527 forget=1.2412 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 16 it 260 total=7.8323 mle=1.6984 pcon=4.9506 forget=1.1833 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 16 it 310 total=7.8266 mle=1.6615 pcon=4.9486 forget=1.2166 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 16 it 360 total=7.8339 mle=1.7051 pcon=4.9466 forget=1.1822 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 17 it 20 total=8.0539 mle=1.9518 pcon=4.9447 forget=1.1574 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 70 total=7.8157 mle=1.6441 pcon=4.9429 forget=1.2287 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 120 total=7.8666 mle=1.8124 pcon=4.9409 forget=1.1133 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 170 total=7.8531 mle=1.8048 pcon=4.9389 forget=1.1094 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 17 it 220 total=7.8695 mle=1.8354 pcon=4.9366 forget=1.0974 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 17 it 270 total=7.6124 mle=1.6102 pcon=4.9344 forget=1.0678 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 17 it 320 total=7.5928 mle=1.6579 pcon=4.9318 forget=1.0031 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 17 it 370 total=7.5901 mle=1.6697 pcon=4.9292 forget=0.9913 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 18 it 30 total=7.4783 mle=1.5510 pcon=4.9266 forget=1.0007 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 18 it 80 total=7.6342 mle=1.7094 pcon=4.9240 forget=1.0009 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 18 it 130 total=7.4347 mle=1.5260 pcon=4.9213 forget=0.9874 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 18 it 180 total=7.5883 mle=1.7019 pcon=4.9185 forget=0.9679 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 18 it 230 total=7.5435 mle=1.6873 pcon=4.9160 forget=0.9403 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 18 it 280 total=7.5401 mle=1.6617 pcon=4.9132 forget=0.9651 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 18 it 330 total=7.8069 mle=1.9090 pcon=4.9104 forget=0.9875 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 18 it 380 total=7.3197 mle=1.4340 pcon=4.9078 forget=0.9779 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 19 it 40 total=7.5950 mle=1.7154 pcon=4.9053 forget=0.9743 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 19 it 90 total=7.4482 mle=1.5591 pcon=4.9026 forget=0.9865 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 19 it 140 total=7.5274 mle=1.6585 pcon=4.8999 forget=0.9689 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 19 it 190 total=7.4413 mle=1.5467 pcon=4.8972 forget=0.9973 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 19 it 240 total=7.5051 mle=1.6206 pcon=4.8946 forget=0.9899 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 19 it 290 total=7.5088 mle=1.6637 pcon=4.8922 forget=0.9529 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 19 it 340 total=7.3047 mle=1.4291 pcon=4.8896 forget=0.9859 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 20 it 0 total=7.3094 mle=1.4365 pcon=4.8872 forget=0.9856 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 20 it 50 total=7.5043 mle=1.6389 pcon=4.8847 forget=0.9807 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 20 it 100 total=7.4672 mle=1.5840 pcon=4.8822 forget=1.0009 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 20 it 150 total=7.7341 mle=1.8664 pcon=4.8797 forget=0.9880 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 20 it 200 total=7.5605 mle=1.6818 pcon=4.8773 forget=1.0015 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 20 it 250 total=7.6073 mle=1.7465 pcon=4.8749 forget=0.9860 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 20 it 300 total=7.4178 mle=1.5475 pcon=4.8725 forget=0.9978 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 20 it 350 total=7.4693 mle=1.5934 pcon=4.8703 forget=1.0056 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 21 it 10 total=7.3549 mle=1.4996 pcon=4.8679 forget=0.9874 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 21 it 60 total=7.4455 mle=1.5707 pcon=4.8654 forget=1.0094 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 21 it 110 total=7.3402 mle=1.4712 pcon=4.8632 forget=1.0058 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 21 it 160 total=7.4053 mle=1.5419 pcon=4.8609 forget=1.0025 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 21 it 210 total=7.3492 mle=1.4667 pcon=4.8585 forget=1.0239 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 21 it 260 total=7.6608 mle=1.7750 pcon=4.8565 forget=1.0293 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 21 it 310 total=7.4915 mle=1.6161 pcon=4.8543 forget=1.0211 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 21 it 360 total=7.5601 mle=1.6718 pcon=4.8524 forget=1.0360 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 22 it 20 total=7.5039 mle=1.6163 pcon=4.8503 forget=1.0373 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 22 it 70 total=7.7837 mle=1.9002 pcon=4.8481 forget=1.0354 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 22 it 120 total=7.6921 mle=1.7876 pcon=4.8460 forget=1.0585 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 22 it 170 total=7.4461 mle=1.5628 pcon=4.8440 forget=1.0393 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 22 it 220 total=7.5922 mle=1.6996 pcon=4.8420 forget=1.0506 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 22 it 270 total=7.5012 mle=1.5935 pcon=4.8401 forget=1.0676 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
 46%|████▌     | 23/50 [07:42<06:40, 14.84s/it] 48%|████▊     | 24/50 [07:56<06:20, 14.64s/it] 50%|█████     | 25/50 [08:10<06:03, 14.56s/it] 52%|█████▏    | 26/50 [08:27<06:02, 15.09s/it] 54%|█████▍    | 27/50 [08:41<05:40, 14.80s/it] 56%|█████▌    | 28/50 [08:55<05:21, 14.61s/it] 58%|█████▊    | 29/50 [09:09<05:01, 14.34s/it] 60%|██████    | 30/50 [09:22<04:44, 14.21s/it] 62%|██████▏   | 31/50 [09:36<04:27, 14.09s/it][loss] ep 22 it 320 total=7.5141 mle=1.6127 pcon=4.8382 forget=1.0632 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 22 it 370 total=7.6677 mle=1.7559 pcon=4.8363 forget=1.0754 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 23 it 30 total=7.6253 mle=1.7343 pcon=4.8344 forget=1.0566 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 23 it 80 total=7.5368 mle=1.6324 pcon=4.8325 forget=1.0719 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 23 it 130 total=7.5925 mle=1.6894 pcon=4.8304 forget=1.0727 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 23 it 180 total=7.5340 mle=1.6268 pcon=4.8285 forget=1.0787 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 23 it 230 total=7.3629 mle=1.4527 pcon=4.8265 forget=1.0837 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 23 it 280 total=7.6244 mle=1.7042 pcon=4.8249 forget=1.0953 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 23 it 330 total=7.6254 mle=1.7106 pcon=4.8231 forget=1.0917 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 23 it 380 total=7.6027 mle=1.6833 pcon=4.8212 forget=1.0982 favg=0.0000 nr=21 nf=21 protos=570 fproto_sim=NA
[loss] ep 24 it 40 total=7.4477 mle=1.5319 pcon=4.8194 forget=1.0964 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 24 it 90 total=7.5820 mle=1.6110 pcon=4.8177 forget=1.1534 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 24 it 140 total=7.3745 mle=1.4403 pcon=4.8160 forget=1.1182 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 24 it 190 total=7.5766 mle=1.6440 pcon=4.8143 forget=1.1183 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 24 it 240 total=7.5139 mle=1.5727 pcon=4.8127 forget=1.1285 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 24 it 290 total=7.8360 mle=1.8967 pcon=4.8108 forget=1.1285 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 24 it 340 total=7.5920 mle=1.6378 pcon=4.8093 forget=1.1448 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 0 total=7.4831 mle=1.5195 pcon=4.8078 forget=1.1558 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 50 total=7.5440 mle=1.5888 pcon=4.8064 forget=1.1488 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 25 it 100 total=7.7003 mle=1.7574 pcon=4.8048 forget=1.1381 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 25 it 150 total=7.5825 mle=1.6231 pcon=4.8033 forget=1.1561 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 25 it 200 total=7.7151 mle=1.7392 pcon=4.8017 forget=1.1741 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 250 total=7.4744 mle=1.5180 pcon=4.8003 forget=1.1562 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 25 it 300 total=7.6722 mle=1.6904 pcon=4.7988 forget=1.1830 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 350 total=7.6591 mle=1.6924 pcon=4.7974 forget=1.1693 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 10 total=7.7069 mle=1.7401 pcon=4.7959 forget=1.1709 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 26 it 60 total=7.6844 mle=1.7127 pcon=4.7946 forget=1.1771 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 110 total=7.5899 mle=1.6090 pcon=4.7933 forget=1.1876 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 26 it 160 total=7.5006 mle=1.5284 pcon=4.7919 forget=1.1803 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 26 it 210 total=7.6867 mle=1.7030 pcon=4.7906 forget=1.1930 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 26 it 260 total=7.5887 mle=1.6158 pcon=4.7893 forget=1.1837 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 26 it 310 total=7.6133 mle=1.6173 pcon=4.7880 forget=1.2081 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 360 total=7.6361 mle=1.6378 pcon=4.7866 forget=1.2117 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 27 it 20 total=7.5727 mle=1.5840 pcon=4.7852 forget=1.2035 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 27 it 70 total=7.4589 mle=1.4591 pcon=4.7838 forget=1.2160 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 27 it 120 total=7.8396 mle=1.8529 pcon=4.7825 forget=1.2043 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 27 it 170 total=7.6232 mle=1.6252 pcon=4.7812 forget=1.2169 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 27 it 220 total=7.4789 mle=1.4778 pcon=4.7800 forget=1.2211 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 27 it 270 total=7.4865 mle=1.4766 pcon=4.7786 forget=1.2313 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 27 it 320 total=7.5823 mle=1.5771 pcon=4.7773 forget=1.2279 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 27 it 370 total=7.5093 mle=1.4888 pcon=4.7761 forget=1.2444 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 28 it 30 total=7.8771 mle=1.8769 pcon=4.7749 forget=1.2253 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 28 it 80 total=7.7333 mle=1.7244 pcon=4.7737 forget=1.2352 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 28 it 130 total=7.5589 mle=1.5412 pcon=4.7726 forget=1.2451 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 28 it 180 total=7.5260 mle=1.5140 pcon=4.7714 forget=1.2405 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 28 it 230 total=7.6848 mle=1.6669 pcon=4.7704 forget=1.2474 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 28 it 280 total=7.5871 mle=1.5757 pcon=4.7693 forget=1.2420 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 28 it 330 total=7.5631 mle=1.5361 pcon=4.7682 forget=1.2588 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 28 it 380 total=7.6378 mle=1.6170 pcon=4.7670 forget=1.2538 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 29 it 40 total=7.6630 mle=1.6224 pcon=4.7658 forget=1.2748 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 29 it 90 total=7.7300 mle=1.7029 pcon=4.7649 forget=1.2622 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 29 it 140 total=7.6603 mle=1.6312 pcon=4.7638 forget=1.2653 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 29 it 190 total=7.7335 mle=1.6848 pcon=4.7627 forget=1.2860 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 29 it 240 total=7.8091 mle=1.7644 pcon=4.7618 forget=1.2829 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 29 it 290 total=7.6554 mle=1.6128 pcon=4.7607 forget=1.2820 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 29 it 340 total=7.7072 mle=1.6644 pcon=4.7598 forget=1.2830 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 30 it 0 total=7.7428 mle=1.6962 pcon=4.7588 forget=1.2879 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 30 it 50 total=7.7217 mle=1.6818 pcon=4.7577 forget=1.2822 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 30 it 100 total=7.6384 mle=1.5931 pcon=4.7570 forget=1.2883 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 30 it 150 total=7.7777 mle=1.7290 pcon=4.7561 forget=1.2926 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 30 it 200 total=7.7445 mle=1.6919 pcon=4.7553 forget=1.2974 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 30 it 250 total=7.5319 mle=1.4709 pcon=4.7544 forget=1.3066 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 30 it 300 total=7.7316 mle=1.6671 pcon=4.7535 forget=1.3110 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 30 it 350 total=7.6505 mle=1.6011 pcon=4.7526 forget=1.2969 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 31 it 10 total=7.6101 mle=1.5589 pcon=4.7518 forget=1.2994 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 31 it 60 total=7.7533 mle=1.6853 pcon=4.7509 forget=1.3171 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 31 it 110 total=7.4762 mle=1.4096 pcon=4.7501 forget=1.3165 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 31 it 160 total=7.5636 mle=1.4829 pcon=4.7493 forget=1.3313 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 31 it 210 total=7.7769 mle=1.7223 pcon=4.7485 forget=1.3062 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
 64%|██████▍   | 32/50 [09:50<04:14, 14.12s/it] 66%|██████▌   | 33/50 [10:04<03:58, 14.03s/it] 68%|██████▊   | 34/50 [10:18<03:44, 14.05s/it] 70%|███████   | 35/50 [10:32<03:30, 14.03s/it] 72%|███████▏  | 36/50 [10:47<03:17, 14.08s/it] 74%|███████▍  | 37/50 [11:00<03:02, 14.03s/it] 76%|███████▌  | 38/50 [11:15<02:49, 14.10s/it] 78%|███████▊  | 39/50 [11:29<02:34, 14.01s/it] 80%|████████  | 40/50 [11:43<02:21, 14.11s/it][loss] ep 31 it 260 total=7.6753 mle=1.6031 pcon=4.7478 forget=1.3244 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 31 it 310 total=7.5900 mle=1.5232 pcon=4.7469 forget=1.3198 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 31 it 360 total=7.6447 mle=1.5717 pcon=4.7463 forget=1.3267 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 32 it 20 total=7.5594 mle=1.4861 pcon=4.7456 forget=1.3277 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 32 it 70 total=7.5824 mle=1.5070 pcon=4.7447 forget=1.3307 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 32 it 120 total=7.6276 mle=1.5358 pcon=4.7439 forget=1.3478 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 32 it 170 total=7.7515 mle=1.6619 pcon=4.7432 forget=1.3464 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 32 it 220 total=7.7666 mle=1.6733 pcon=4.7424 forget=1.3508 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 32 it 270 total=7.6914 mle=1.6055 pcon=4.7416 forget=1.3443 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 32 it 320 total=7.7679 mle=1.6844 pcon=4.7407 forget=1.3428 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 32 it 370 total=7.5644 mle=1.4748 pcon=4.7401 forget=1.3496 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 33 it 30 total=7.9206 mle=1.8312 pcon=4.7394 forget=1.3499 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 33 it 80 total=7.6025 mle=1.5155 pcon=4.7387 forget=1.3483 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 33 it 130 total=7.6375 mle=1.5374 pcon=4.7380 forget=1.3621 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 33 it 180 total=7.7338 mle=1.6398 pcon=4.7373 forget=1.3566 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 33 it 230 total=7.8390 mle=1.7400 pcon=4.7367 forget=1.3623 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 33 it 280 total=8.1914 mle=2.0752 pcon=4.7360 forget=1.3802 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 33 it 330 total=7.5892 mle=1.4914 pcon=4.7352 forget=1.3626 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 33 it 380 total=7.6519 mle=1.5426 pcon=4.7347 forget=1.3747 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 34 it 40 total=7.7610 mle=1.6514 pcon=4.7341 forget=1.3755 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 34 it 90 total=7.7540 mle=1.6408 pcon=4.7335 forget=1.3798 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 34 it 140 total=7.8582 mle=1.7452 pcon=4.7329 forget=1.3801 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 34 it 190 total=7.7044 mle=1.5800 pcon=4.7323 forget=1.3922 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 34 it 240 total=7.7114 mle=1.5972 pcon=4.7317 forget=1.3825 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 34 it 290 total=7.6003 mle=1.4768 pcon=4.7312 forget=1.3924 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 34 it 340 total=7.6395 mle=1.5154 pcon=4.7306 forget=1.3935 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 35 it 0 total=7.6815 mle=1.5541 pcon=4.7300 forget=1.3975 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 35 it 50 total=7.9827 mle=1.8518 pcon=4.7294 forget=1.4015 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 35 it 100 total=7.7215 mle=1.5890 pcon=4.7289 forget=1.4035 favg=0.0000 nr=43 nf=43 protos=570 fproto_sim=NA
[loss] ep 35 it 150 total=7.7165 mle=1.5859 pcon=4.7285 forget=1.4022 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 35 it 200 total=7.6174 mle=1.4793 pcon=4.7280 forget=1.4102 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 35 it 250 total=7.8424 mle=1.7025 pcon=4.7275 forget=1.4124 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 35 it 300 total=7.7601 mle=1.6168 pcon=4.7270 forget=1.4163 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 35 it 350 total=7.6695 mle=1.5264 pcon=4.7265 forget=1.4167 favg=0.0000 nr=21 nf=21 protos=570 fproto_sim=NA
[loss] ep 36 it 10 total=7.7218 mle=1.5806 pcon=4.7259 forget=1.4153 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 36 it 60 total=7.6362 mle=1.4897 pcon=4.7255 forget=1.4211 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 36 it 110 total=7.8420 mle=1.6814 pcon=4.7250 forget=1.4356 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 36 it 160 total=7.5493 mle=1.3867 pcon=4.7245 forget=1.4381 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 36 it 210 total=7.8743 mle=1.6886 pcon=4.7241 forget=1.4615 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 36 it 260 total=7.8407 mle=1.6652 pcon=4.7238 forget=1.4516 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 36 it 310 total=7.7042 mle=1.5392 pcon=4.7234 forget=1.4416 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 36 it 360 total=7.7122 mle=1.5388 pcon=4.7231 forget=1.4503 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 37 it 20 total=7.6593 mle=1.4819 pcon=4.7228 forget=1.4546 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 37 it 70 total=8.0230 mle=1.8499 pcon=4.7223 forget=1.4508 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 37 it 120 total=7.6629 mle=1.4778 pcon=4.7219 forget=1.4632 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 37 it 170 total=7.6199 mle=1.4308 pcon=4.7215 forget=1.4676 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 37 it 220 total=7.9380 mle=1.7494 pcon=4.7211 forget=1.4676 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 37 it 270 total=7.7967 mle=1.6144 pcon=4.7207 forget=1.4616 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 37 it 320 total=7.6953 mle=1.5084 pcon=4.7203 forget=1.4665 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 37 it 370 total=7.7898 mle=1.5899 pcon=4.7200 forget=1.4799 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 38 it 30 total=7.9296 mle=1.7268 pcon=4.7197 forget=1.4832 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 38 it 80 total=7.7952 mle=1.5890 pcon=4.7193 forget=1.4869 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 38 it 130 total=7.8048 mle=1.5916 pcon=4.7190 forget=1.4942 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 38 it 180 total=7.6500 mle=1.4295 pcon=4.7187 forget=1.5018 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 38 it 230 total=7.6667 mle=1.4379 pcon=4.7185 forget=1.5103 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 38 it 280 total=8.2322 mle=2.0107 pcon=4.7181 forget=1.5035 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 38 it 330 total=7.7989 mle=1.5701 pcon=4.7178 forget=1.5109 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 38 it 380 total=7.8304 mle=1.5990 pcon=4.7175 forget=1.5139 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 39 it 40 total=7.8330 mle=1.5927 pcon=4.7172 forget=1.5231 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 39 it 90 total=7.8180 mle=1.5587 pcon=4.7168 forget=1.5425 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 39 it 140 total=8.0727 mle=1.8397 pcon=4.7166 forget=1.5164 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 39 it 190 total=7.9622 mle=1.6959 pcon=4.7164 forget=1.5499 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 39 it 240 total=7.9908 mle=1.7477 pcon=4.7162 forget=1.5268 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 39 it 290 total=8.0893 mle=1.8207 pcon=4.7160 forget=1.5526 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 39 it 340 total=7.8937 mle=1.6415 pcon=4.7158 forget=1.5365 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 40 it 0 total=7.6898 mle=1.4205 pcon=4.7154 forget=1.5539 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 40 it 50 total=7.7301 mle=1.4675 pcon=4.7151 forget=1.5474 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 40 it 100 total=8.0984 mle=1.8277 pcon=4.7148 forget=1.5559 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 40 it 150 total=7.8385 mle=1.5611 pcon=4.7146 forget=1.5628 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
 82%|████████▏ | 41/50 [11:57<02:06, 14.06s/it] 84%|████████▍ | 42/50 [12:11<01:53, 14.16s/it] 86%|████████▌ | 43/50 [12:25<01:38, 14.07s/it] 88%|████████▊ | 44/50 [12:39<01:24, 14.11s/it] 90%|█████████ | 45/50 [12:54<01:11, 14.20s/it] 92%|█████████▏| 46/50 [13:08<00:57, 14.29s/it] 94%|█████████▍| 47/50 [13:22<00:42, 14.26s/it] 96%|█████████▌| 48/50 [13:37<00:28, 14.48s/it] 98%|█████████▊| 49/50 [13:52<00:14, 14.45s/it][loss] ep 40 it 200 total=7.9558 mle=1.6686 pcon=4.7143 forget=1.5729 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 40 it 250 total=7.9660 mle=1.6911 pcon=4.7140 forget=1.5608 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 40 it 300 total=7.9753 mle=1.6695 pcon=4.7138 forget=1.5920 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 40 it 350 total=7.7570 mle=1.4727 pcon=4.7136 forget=1.5708 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 41 it 10 total=7.8640 mle=1.5647 pcon=4.7134 forget=1.5858 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 41 it 60 total=7.9505 mle=1.6595 pcon=4.7133 forget=1.5777 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 41 it 110 total=8.0295 mle=1.7128 pcon=4.7131 forget=1.6036 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 41 it 160 total=7.7129 mle=1.4015 pcon=4.7129 forget=1.5986 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 41 it 210 total=8.1605 mle=1.8471 pcon=4.7126 forget=1.6008 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 41 it 260 total=8.0452 mle=1.7319 pcon=4.7124 forget=1.6010 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 41 it 310 total=7.8119 mle=1.4960 pcon=4.7122 forget=1.6037 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 41 it 360 total=7.9560 mle=1.6371 pcon=4.7121 forget=1.6068 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 42 it 20 total=7.9004 mle=1.5729 pcon=4.7120 forget=1.6155 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 70 total=7.9841 mle=1.6466 pcon=4.7119 forget=1.6256 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 120 total=8.1846 mle=1.8519 pcon=4.7117 forget=1.6210 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 42 it 170 total=8.1401 mle=1.7969 pcon=4.7116 forget=1.6317 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 220 total=7.8803 mle=1.5390 pcon=4.7114 forget=1.6299 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 42 it 270 total=8.0202 mle=1.6677 pcon=4.7112 forget=1.6413 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 42 it 320 total=8.1243 mle=1.7772 pcon=4.7111 forget=1.6361 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 42 it 370 total=7.8852 mle=1.5396 pcon=4.7109 forget=1.6347 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 43 it 30 total=8.3206 mle=1.9794 pcon=4.7108 forget=1.6305 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 43 it 80 total=8.2316 mle=1.8676 pcon=4.7105 forget=1.6534 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 43 it 130 total=8.3585 mle=1.9857 pcon=4.7104 forget=1.6624 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 43 it 180 total=7.9412 mle=1.5647 pcon=4.7103 forget=1.6663 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 43 it 230 total=7.8542 mle=1.4777 pcon=4.7103 forget=1.6663 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 43 it 280 total=7.8012 mle=1.4321 pcon=4.7103 forget=1.6588 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 43 it 330 total=8.0299 mle=1.6477 pcon=4.7102 forget=1.6720 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 43 it 380 total=8.1692 mle=1.7910 pcon=4.7102 forget=1.6679 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 44 it 40 total=8.5209 mle=2.1306 pcon=4.7101 forget=1.6803 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 44 it 90 total=7.8512 mle=1.4443 pcon=4.7099 forget=1.6970 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 44 it 140 total=8.0517 mle=1.6619 pcon=4.7097 forget=1.6800 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 44 it 190 total=8.0829 mle=1.6797 pcon=4.7097 forget=1.6935 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 44 it 240 total=8.1388 mle=1.7246 pcon=4.7096 forget=1.7045 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 44 it 290 total=8.3289 mle=1.9323 pcon=4.7096 forget=1.6870 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 44 it 340 total=7.9443 mle=1.5472 pcon=4.7096 forget=1.6875 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 45 it 0 total=7.7929 mle=1.3911 pcon=4.7095 forget=1.6924 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 50 total=8.0053 mle=1.5903 pcon=4.7094 forget=1.7056 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 100 total=8.1047 mle=1.6797 pcon=4.7093 forget=1.7157 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 45 it 150 total=8.0500 mle=1.6109 pcon=4.7093 forget=1.7298 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 45 it 200 total=8.0263 mle=1.5982 pcon=4.7091 forget=1.7190 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 45 it 250 total=8.1023 mle=1.6676 pcon=4.7090 forget=1.7257 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 300 total=7.8465 mle=1.4230 pcon=4.7090 forget=1.7145 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 45 it 350 total=8.3124 mle=1.8666 pcon=4.7089 forget=1.7368 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 46 it 10 total=8.0369 mle=1.5907 pcon=4.7089 forget=1.7373 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 46 it 60 total=7.9810 mle=1.5282 pcon=4.7089 forget=1.7439 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 46 it 110 total=7.9287 mle=1.4864 pcon=4.7088 forget=1.7335 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 46 it 160 total=8.2823 mle=1.8388 pcon=4.7089 forget=1.7347 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 46 it 210 total=8.2240 mle=1.7787 pcon=4.7087 forget=1.7367 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 46 it 260 total=8.2614 mle=1.7850 pcon=4.7086 forget=1.7678 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 46 it 310 total=8.1551 mle=1.6775 pcon=4.7085 forget=1.7691 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 46 it 360 total=8.1988 mle=1.7525 pcon=4.7086 forget=1.7377 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 47 it 20 total=8.4644 mle=1.9752 pcon=4.7086 forget=1.7807 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 47 it 70 total=7.9402 mle=1.4618 pcon=4.7085 forget=1.7700 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 47 it 120 total=8.0552 mle=1.5814 pcon=4.7085 forget=1.7653 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 47 it 170 total=8.1201 mle=1.6516 pcon=4.7084 forget=1.7601 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 47 it 220 total=8.0752 mle=1.6168 pcon=4.7084 forget=1.7501 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 47 it 270 total=8.1697 mle=1.6865 pcon=4.7084 forget=1.7748 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 47 it 320 total=7.9941 mle=1.5187 pcon=4.7084 forget=1.7670 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 47 it 370 total=8.0426 mle=1.5474 pcon=4.7083 forget=1.7869 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 30 total=8.2199 mle=1.7415 pcon=4.7082 forget=1.7702 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 48 it 80 total=8.1841 mle=1.6789 pcon=4.7082 forget=1.7970 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 48 it 130 total=8.0772 mle=1.5778 pcon=4.7083 forget=1.7911 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 180 total=8.2688 mle=1.7716 pcon=4.7083 forget=1.7889 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 48 it 230 total=8.3337 mle=1.8277 pcon=4.7082 forget=1.7978 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 48 it 280 total=8.1753 mle=1.6694 pcon=4.7084 forget=1.7976 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 48 it 330 total=7.9760 mle=1.4532 pcon=4.7084 forget=1.8143 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 380 total=8.1586 mle=1.6417 pcon=4.7085 forget=1.8085 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 49 it 40 total=7.9667 mle=1.4595 pcon=4.7084 forget=1.7988 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 49 it 90 total=8.1761 mle=1.6570 pcon=4.7085 forget=1.8107 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
100%|██████████| 50/50 [14:09<00:00, 15.19s/it]100%|██████████| 50/50 [14:09<00:00, 16.98s/it]
[loss] ep 49 it 140 total=7.9452 mle=1.4311 pcon=4.7084 forget=1.8056 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 49 it 190 total=8.1227 mle=1.5897 pcon=4.7085 forget=1.8246 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 49 it 240 total=8.3032 mle=1.7617 pcon=4.7084 forget=1.8331 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 49 it 290 total=8.0634 mle=1.5544 pcon=4.7085 forget=1.8005 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 49 it 340 total=8.1609 mle=1.6322 pcon=4.7086 forget=1.8201 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:42,  1.75it/s]  3%|▎         | 11/391 [00:00<00:18, 21.03it/s]  5%|▌         | 21/391 [00:00<00:09, 37.52it/s]  8%|▊         | 31/391 [00:00<00:07, 51.12it/s] 10%|█         | 40/391 [00:00<00:05, 60.60it/s] 13%|█▎        | 50/391 [00:01<00:04, 69.07it/s] 15%|█▌        | 60/391 [00:01<00:04, 75.98it/s] 18%|█▊        | 70/391 [00:01<00:03, 80.63it/s] 20%|██        | 79/391 [00:01<00:03, 82.30it/s] 23%|██▎       | 89/391 [00:01<00:03, 86.10it/s] 25%|██▌       | 99/391 [00:01<00:03, 88.66it/s] 28%|██▊       | 109/391 [00:01<00:03, 88.00it/s] 30%|███       | 119/391 [00:01<00:03, 90.21it/s] 33%|███▎      | 129/391 [00:01<00:02, 89.49it/s] 36%|███▌      | 139/391 [00:02<00:02, 90.41it/s] 38%|███▊      | 149/391 [00:02<00:02, 91.49it/s] 41%|████      | 159/391 [00:02<00:02, 92.23it/s] 43%|████▎     | 169/391 [00:02<00:02, 92.98it/s] 46%|████▌     | 179/391 [00:02<00:02, 93.59it/s] 48%|████▊     | 189/391 [00:02<00:02, 93.06it/s] 51%|█████     | 199/391 [00:02<00:02, 93.74it/s] 53%|█████▎    | 209/391 [00:02<00:01, 93.98it/s] 56%|█████▌    | 219/391 [00:02<00:01, 94.26it/s] 59%|█████▊    | 229/391 [00:03<00:01, 94.48it/s] 61%|██████    | 239/391 [00:03<00:01, 94.28it/s] 64%|██████▎   | 249/391 [00:03<00:01, 94.42it/s] 66%|██████▌   | 259/391 [00:03<00:01, 94.92it/s] 69%|██████▉   | 269/391 [00:03<00:01, 92.52it/s] 71%|███████▏  | 279/391 [00:03<00:01, 93.35it/s] 74%|███████▍  | 289/391 [00:03<00:01, 93.84it/s] 76%|███████▋  | 299/391 [00:03<00:00, 94.41it/s] 79%|███████▉  | 309/391 [00:03<00:00, 91.63it/s] 82%|████████▏ | 319/391 [00:04<00:00, 92.58it/s] 84%|████████▍ | 329/391 [00:04<00:00, 90.83it/s] 87%|████████▋ | 339/391 [00:04<00:00, 92.33it/s] 89%|████████▉ | 349/391 [00:04<00:00, 91.42it/s] 92%|█████████▏| 359/391 [00:04<00:00, 91.98it/s] 94%|█████████▍| 369/391 [00:04<00:00, 93.62it/s] 97%|█████████▋| 379/391 [00:04<00:00, 94.80it/s] 99%|█████████▉| 389/391 [00:04<00:00, 95.58it/s]100%|██████████| 391/391 [00:04<00:00, 81.82it/s]
50000 images processed, 5.145941734313965 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:46,  1.66it/s] 13%|█▎        | 10/79 [00:00<00:03, 18.62it/s] 24%|██▍       | 19/79 [00:00<00:01, 33.78it/s] 37%|███▋      | 29/79 [00:00<00:01, 48.36it/s] 48%|████▊     | 38/79 [00:01<00:00, 57.88it/s] 61%|██████    | 48/79 [00:01<00:00, 67.14it/s] 73%|███████▎  | 58/79 [00:01<00:00, 74.66it/s] 86%|████████▌ | 68/79 [00:01<00:00, 80.42it/s] 99%|█████████▊| 78/79 [00:01<00:00, 84.33it/s]100%|██████████| 79/79 [00:02<00:00, 35.50it/s]
10000 images processed, 2.316065549850464 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:37,  1.29it/s]  4%|▍         | 9/204 [00:00<00:14, 13.35it/s]  9%|▉         | 19/204 [00:00<00:06, 28.57it/s] 14%|█▍        | 29/204 [00:01<00:04, 42.25it/s] 19%|█▉        | 39/204 [00:01<00:03, 53.87it/s] 24%|██▎       | 48/204 [00:01<00:02, 61.69it/s] 28%|██▊       | 58/204 [00:01<00:02, 70.30it/s] 33%|███▎      | 67/204 [00:01<00:01, 74.91it/s] 38%|███▊      | 77/204 [00:01<00:01, 80.60it/s] 43%|████▎     | 87/204 [00:01<00:01, 84.56it/s] 48%|████▊     | 97/204 [00:01<00:01, 86.52it/s] 52%|█████▏    | 107/204 [00:01<00:01, 87.31it/s] 57%|█████▋    | 117/204 [00:02<00:00, 89.63it/s] 62%|██████▏   | 127/204 [00:02<00:00, 91.13it/s] 67%|██████▋   | 137/204 [00:02<00:00, 92.46it/s] 72%|███████▏  | 147/204 [00:02<00:00, 93.19it/s] 77%|███████▋  | 157/204 [00:02<00:00, 93.50it/s] 82%|████████▏ | 167/204 [00:02<00:00, 93.17it/s] 87%|████████▋ | 177/204 [00:02<00:00, 91.70it/s] 92%|█████████▏| 187/204 [00:02<00:00, 92.17it/s] 97%|█████████▋| 197/204 [00:02<00:00, 92.57it/s]100%|██████████| 204/204 [00:02<00:00, 68.05it/s]
26032 images processed, 3.1956706047058105 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:57,  1.36it/s] 14%|█▍        | 11/79 [00:00<00:03, 17.18it/s] 25%|██▌       | 20/79 [00:00<00:01, 30.63it/s] 38%|███▊      | 30/79 [00:01<00:01, 44.65it/s] 51%|█████     | 40/79 [00:01<00:00, 56.42it/s] 62%|██████▏   | 49/79 [00:01<00:00, 64.54it/s] 75%|███████▍  | 59/79 [00:01<00:00, 72.80it/s] 87%|████████▋ | 69/79 [00:01<00:00, 78.96it/s]100%|██████████| 79/79 [00:01<00:00, 50.41it/s]
10000 images processed, 1.6649997234344482 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:49,  1.56it/s] 13%|█▎        | 10/79 [00:00<00:03, 17.64it/s] 24%|██▍       | 19/79 [00:00<00:01, 32.56it/s] 37%|███▋      | 29/79 [00:00<00:01, 47.22it/s] 49%|████▉     | 39/79 [00:01<00:00, 59.18it/s] 61%|██████    | 48/79 [00:01<00:00, 66.81it/s] 73%|███████▎  | 58/79 [00:01<00:00, 74.81it/s] 86%|████████▌ | 68/79 [00:01<00:00, 80.84it/s] 99%|█████████▊| 78/79 [00:01<00:00, 85.31it/s]100%|██████████| 79/79 [00:01<00:00, 53.71it/s]
10000 images processed, 1.548245906829834 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:37,  1.85it/s] 13%|█▎        | 9/70 [00:00<00:03, 18.11it/s] 27%|██▋       | 19/70 [00:00<00:01, 36.25it/s] 40%|████      | 28/70 [00:00<00:00, 49.12it/s] 53%|█████▎    | 37/70 [00:00<00:00, 58.66it/s] 67%|██████▋   | 47/70 [00:01<00:00, 68.59it/s] 81%|████████▏ | 57/70 [00:01<00:00, 76.12it/s] 96%|█████████▌| 67/70 [00:01<00:00, 81.71it/s]100%|██████████| 70/70 [00:01<00:00, 53.29it/s]
8925 images processed, 1.4123458862304688 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:53,  1.21s/it]  4%|▍         | 2/45 [00:01<00:24,  1.74it/s] 27%|██▋       | 12/45 [00:01<00:02, 14.20it/s] 38%|███▊      | 17/45 [00:01<00:02, 13.82it/s] 49%|████▉     | 22/45 [00:01<00:01, 18.55it/s] 58%|█████▊    | 26/45 [00:02<00:00, 19.02it/s] 73%|███████▎  | 33/45 [00:02<00:00, 17.68it/s] 80%|████████  | 36/45 [00:02<00:00, 19.09it/s]100%|██████████| 45/45 [00:02<00:00, 29.79it/s]100%|██████████| 45/45 [00:02<00:00, 16.22it/s]
5640 images processed, 2.838604211807251 seconds used

20.068865060806274
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.55  99.36
places365     68.45  80.52
LSUN          22.01  94.99
iSUN          72.53  81.45
dtd           38.40  91.28
AVG           40.79  89.52
Retain-Acc: 0.7485
Forget-as-OOD (retain known vs forget novel):
  FPR: 55.20 AUROC: 88.46 AUIN: 99.25
11.276674270629883
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1_rf.png
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-seen: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:59,  1.63it/s]  3%|▎         | 10/391 [00:00<00:20, 18.17it/s]  5%|▌         | 20/391 [00:00<00:10, 34.83it/s]  8%|▊         | 30/391 [00:00<00:07, 49.12it/s] 10%|█         | 40/391 [00:01<00:05, 60.09it/s] 13%|█▎        | 50/391 [00:01<00:04, 69.53it/s] 15%|█▌        | 60/391 [00:01<00:04, 75.71it/s] 18%|█▊        | 70/391 [00:01<00:03, 81.43it/s] 20%|██        | 80/391 [00:01<00:03, 83.92it/s] 23%|██▎       | 90/391 [00:01<00:03, 87.17it/s] 26%|██▌       | 100/391 [00:01<00:03, 88.29it/s] 28%|██▊       | 110/391 [00:01<00:03, 90.03it/s] 31%|███       | 120/391 [00:01<00:02, 91.53it/s] 33%|███▎      | 130/391 [00:02<00:02, 91.33it/s] 36%|███▌      | 140/391 [00:02<00:02, 91.81it/s] 38%|███▊      | 150/391 [00:02<00:02, 92.66it/s] 41%|████      | 160/391 [00:02<00:02, 93.15it/s] 43%|████▎     | 170/391 [00:02<00:02, 92.36it/s] 46%|████▌     | 180/391 [00:02<00:02, 92.88it/s] 49%|████▊     | 190/391 [00:02<00:02, 91.86it/s] 51%|█████     | 200/391 [00:02<00:02, 92.31it/s] 54%|█████▎    | 210/391 [00:02<00:01, 93.31it/s] 56%|█████▋    | 220/391 [00:02<00:01, 90.44it/s] 59%|█████▉    | 230/391 [00:03<00:01, 90.46it/s] 61%|██████▏   | 240/391 [00:03<00:01, 90.50it/s] 64%|██████▍   | 250/391 [00:03<00:01, 90.38it/s] 66%|██████▋   | 260/391 [00:03<00:01, 89.56it/s] 69%|██████▉   | 270/391 [00:03<00:01, 91.26it/s] 72%|███████▏  | 280/391 [00:03<00:01, 91.41it/s] 74%|███████▍  | 290/391 [00:03<00:01, 92.50it/s] 77%|███████▋  | 300/391 [00:03<00:01, 90.06it/s] 79%|███████▉  | 310/391 [00:03<00:00, 91.82it/s] 82%|████████▏ | 320/391 [00:04<00:00, 93.03it/s] 84%|████████▍ | 330/391 [00:04<00:00, 93.09it/s] 87%|████████▋ | 340/391 [00:04<00:00, 94.10it/s] 90%|████████▉ | 350/391 [00:04<00:00, 92.53it/s] 92%|█████████▏| 360/391 [00:04<00:00, 90.84it/s] 95%|█████████▍| 370/391 [00:04<00:00, 92.73it/s] 97%|█████████▋| 380/391 [00:04<00:00, 94.19it/s]100%|█████████▉| 390/391 [00:04<00:00, 95.26it/s]100%|██████████| 391/391 [00:04<00:00, 80.78it/s]
50000 images processed, 4.924402236938477 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:41,  1.87it/s] 14%|█▍        | 11/79 [00:00<00:03, 22.23it/s] 27%|██▋       | 21/79 [00:00<00:01, 39.31it/s] 38%|███▊      | 30/79 [00:00<00:00, 51.29it/s] 51%|█████     | 40/79 [00:00<00:00, 62.39it/s] 63%|██████▎   | 50/79 [00:01<00:00, 71.23it/s] 76%|███████▌  | 60/79 [00:01<00:00, 78.08it/s] 89%|████████▊ | 70/79 [00:01<00:00, 83.28it/s]100%|██████████| 79/79 [00:01<00:00, 43.83it/s]
10000 images processed, 1.8275113105773926 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-seen/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:53,  1.80it/s]  5%|▍         | 10/204 [00:00<00:09, 19.52it/s] 10%|▉         | 20/204 [00:00<00:04, 36.92it/s] 14%|█▍        | 29/204 [00:00<00:03, 48.97it/s] 19%|█▉        | 39/204 [00:00<00:02, 60.57it/s] 24%|██▍       | 49/204 [00:01<00:02, 69.29it/s] 28%|██▊       | 58/204 [00:01<00:01, 74.24it/s] 33%|███▎      | 68/204 [00:01<00:01, 79.59it/s] 38%|███▊      | 77/204 [00:01<00:01, 81.65it/s] 43%|████▎     | 87/204 [00:01<00:01, 85.38it/s] 47%|████▋     | 96/204 [00:01<00:01, 86.15it/s] 52%|█████▏    | 106/204 [00:01<00:01, 88.38it/s] 57%|█████▋    | 116/204 [00:01<00:00, 89.38it/s] 62%|██████▏   | 126/204 [00:01<00:00, 90.82it/s] 67%|██████▋   | 136/204 [00:02<00:00, 89.67it/s] 72%|███████▏  | 146/204 [00:02<00:00, 91.47it/s] 76%|███████▋  | 156/204 [00:02<00:00, 90.38it/s] 81%|████████▏ | 166/204 [00:02<00:00, 91.84it/s] 86%|████████▋ | 176/204 [00:02<00:00, 92.75it/s] 91%|█████████ | 186/204 [00:02<00:00, 93.88it/s] 96%|█████████▌| 196/204 [00:02<00:00, 94.75it/s]100%|██████████| 204/204 [00:02<00:00, 73.44it/s]
26032 images processed, 2.8183562755584717 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:57,  1.35it/s] 14%|█▍        | 11/79 [00:00<00:03, 17.11it/s] 25%|██▌       | 20/79 [00:00<00:01, 30.54it/s] 37%|███▋      | 29/79 [00:01<00:01, 42.66it/s] 48%|████▊     | 38/79 [00:01<00:00, 53.38it/s] 61%|██████    | 48/79 [00:01<00:00, 63.34it/s] 73%|███████▎  | 58/79 [00:01<00:00, 69.11it/s] 86%|████████▌ | 68/79 [00:01<00:00, 76.29it/s] 99%|█████████▊| 78/79 [00:01<00:00, 81.95it/s]100%|██████████| 79/79 [00:01<00:00, 49.52it/s]
10000 images processed, 1.6296544075012207 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:42,  1.86it/s] 14%|█▍        | 11/79 [00:00<00:03, 22.15it/s] 27%|██▋       | 21/79 [00:00<00:01, 39.34it/s] 39%|███▉      | 31/79 [00:00<00:00, 53.21it/s] 52%|█████▏    | 41/79 [00:00<00:00, 64.15it/s] 65%|██████▍   | 51/79 [00:01<00:00, 72.80it/s] 77%|███████▋  | 61/79 [00:01<00:00, 79.52it/s] 90%|████████▉ | 71/79 [00:01<00:00, 84.50it/s]100%|██████████| 79/79 [00:01<00:00, 58.51it/s]
10000 images processed, 1.3704283237457275 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:38,  1.79it/s] 14%|█▍        | 10/70 [00:00<00:03, 19.43it/s] 27%|██▋       | 19/70 [00:00<00:01, 34.83it/s] 40%|████      | 28/70 [00:00<00:00, 47.61it/s] 54%|█████▍    | 38/70 [00:00<00:00, 59.72it/s] 69%|██████▊   | 48/70 [00:01<00:00, 69.12it/s] 83%|████████▎ | 58/70 [00:01<00:00, 75.95it/s] 97%|█████████▋| 68/70 [00:01<00:00, 81.42it/s]100%|██████████| 70/70 [00:01<00:00, 52.73it/s]
8925 images processed, 1.3587596416473389 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:49,  1.12s/it]  4%|▍         | 2/45 [00:01<00:24,  1.76it/s] 27%|██▋       | 12/45 [00:01<00:02, 14.33it/s] 38%|███▊      | 17/45 [00:01<00:01, 15.54it/s] 47%|████▋     | 21/45 [00:01<00:01, 17.69it/s] 56%|█████▌    | 25/45 [00:01<00:00, 20.27it/s] 73%|███████▎  | 33/45 [00:02<00:00, 19.53it/s] 80%|████████  | 36/45 [00:02<00:00, 20.80it/s] 98%|█████████▊| 44/45 [00:02<00:00, 30.35it/s]100%|██████████| 45/45 [00:02<00:00, 17.11it/s]
5640 images processed, 2.6544277667999268 seconds used

18.23073148727417
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.55  99.36
places365     68.45  80.52
LSUN          22.01  94.99
iSUN          72.53  81.45
dtd           38.40  91.28
AVG           40.79  89.52
Retain-Acc: 0.7485
Forget-as-OOD (retain known vs forget novel):
  FPR: 55.20 AUROC: 88.46 AUIN: 99.25
10.618467807769775
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-seen_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-seen_rf.png
==== Stage 2: forget_inc={66,67,88,94,57}; forget_seen={0,8,11,40,51}; all={0,8,11,40,51,66,67,88,94,57} ====
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2', adapter_load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1', adapter_load_paths=None, lora_new_adapter_name=None, lora_stack=False, lora_orth_enable=True, lora_orth_lambda=0.1, lora_orth_ref_paths='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1', forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc='66,67,88,94,57', forget_classes_seen='0,8,11,40,51', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [02:17<1:52:13, 137.42s/it]  4%|▍         | 2/50 [03:13<1:11:42, 89.63s/it]   6%|▌         | 3/50 [04:06<57:10, 73.00s/it]    8%|▊         | 4/50 [05:04<51:26, 67.10s/it] 10%|█         | 5/50 [05:57<46:27, 61.94s/it] 12%|█▏        | 6/50 [06:51<43:17, 59.03s/it] 14%|█▍        | 7/50 [07:44<40:58, 57.16s/it] 16%|█▌        | 8/50 [08:37<39:03, 55.80s/it] 18%|█▊        | 9/50 [09:32<37:56, 55.53s/it][loss] ep 0 it 0 total=17317116.0000 mle=2.1291 pcon=5.2950 forget=1.3417 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 1 it 10 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=23 nf=23 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 2 it 20 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=24 nf=24 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=40 nf=40 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 3 it 30 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=23 nf=23 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 4 it 40 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=23 nf=23 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 5 it 0 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 6 it 10 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=24 nf=24 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 7 it 20 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 8 it 30 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=41 nf=41 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 9 it 40 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
 20%|██        | 10/50 [10:24<36:25, 54.65s/it] 22%|██▏       | 11/50 [11:35<38:40, 59.50s/it] 24%|██▍       | 12/50 [12:32<37:09, 58.67s/it] 26%|██▌       | 13/50 [13:26<35:18, 57.27s/it] 28%|██▊       | 14/50 [14:20<33:51, 56.43s/it] 30%|███       | 15/50 [15:15<32:40, 56.01s/it] 32%|███▏      | 16/50 [16:14<32:08, 56.73s/it] 34%|███▍      | 17/50 [17:08<30:49, 56.05s/it] 36%|███▌      | 18/50 [18:03<29:40, 55.63s/it] 38%|███▊      | 19/50 [18:57<28:31, 55.22s/it][loss] ep 9 it 340 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 10 it 0 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 11 it 10 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 12 it 20 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 13 it 30 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 14 it 40 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=23 nf=23 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 15 it 0 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 16 it 10 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 17 it 20 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=41 nf=41 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 19 it 40 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=24 nf=24 protos=540 fproto_sim=NA
 40%|████      | 20/50 [19:51<27:30, 55.00s/it] 42%|████▏     | 21/50 [20:46<26:34, 54.98s/it] 44%|████▍     | 22/50 [21:47<26:30, 56.79s/it] 46%|████▌     | 23/50 [22:43<25:21, 56.37s/it] 48%|████▊     | 24/50 [23:39<24:26, 56.40s/it] 50%|█████     | 25/50 [24:36<23:31, 56.47s/it] 52%|█████▏    | 26/50 [25:35<22:52, 57.19s/it] 54%|█████▍    | 27/50 [26:32<21:54, 57.16s/it] 56%|█████▌    | 28/50 [27:28<20:50, 56.86s/it] 58%|█████▊    | 29/50 [28:24<19:51, 56.74s/it][loss] ep 19 it 240 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 20 it 0 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 21 it 10 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=40 nf=40 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 22 it 20 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 23 it 30 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=43 nf=43 protos=540 fproto_sim=NA
[loss] ep 24 it 40 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 24 it 240 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 25 it 0 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 25 it 150 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 25 it 200 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 26 it 10 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=23 nf=23 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 26 it 310 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 27 it 20 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 28 it 30 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 29 it 40 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=40 nf=40 protos=540 fproto_sim=NA
 60%|██████    | 30/50 [29:21<18:50, 56.53s/it] 62%|██████▏   | 31/50 [30:17<17:53, 56.51s/it] 64%|██████▍   | 32/50 [31:14<16:57, 56.51s/it] 66%|██████▌   | 33/50 [32:10<15:59, 56.45s/it] 68%|██████▊   | 34/50 [33:07<15:04, 56.52s/it] 70%|███████   | 35/50 [34:03<14:09, 56.61s/it] 72%|███████▏  | 36/50 [35:00<13:11, 56.52s/it] 74%|███████▍  | 37/50 [35:56<12:13, 56.42s/it] 76%|███████▌  | 38/50 [36:53<11:18, 56.51s/it] 78%|███████▊  | 39/50 [37:49<10:22, 56.59s/it][loss] ep 29 it 140 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 30 it 0 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 31 it 10 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 31 it 260 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 32 it 20 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 32 it 170 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=41 nf=41 protos=540 fproto_sim=NA
[loss] ep 33 it 30 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 33 it 380 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 34 it 40 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 34 it 140 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 34 it 240 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 35 it 0 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 35 it 50 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 35 it 100 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=21 nf=21 protos=540 fproto_sim=NA
[loss] ep 35 it 150 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 35 it 200 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 35 it 250 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 35 it 300 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 35 it 350 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=43 nf=43 protos=540 fproto_sim=NA
[loss] ep 36 it 10 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 36 it 60 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 36 it 110 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 36 it 160 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 36 it 210 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 36 it 260 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 36 it 310 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 36 it 360 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 37 it 20 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 37 it 70 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=24 nf=24 protos=540 fproto_sim=NA
[loss] ep 37 it 120 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 37 it 170 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 37 it 220 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 37 it 270 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 37 it 320 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 37 it 370 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 38 it 30 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 38 it 80 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 38 it 130 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 38 it 180 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 38 it 230 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 38 it 280 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 38 it 330 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 38 it 380 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
 80%|████████  | 40/50 [38:46<09:25, 56.56s/it] 82%|████████▏ | 41/50 [39:42<08:28, 56.50s/it] 84%|████████▍ | 42/50 [40:39<07:32, 56.59s/it] 86%|████████▌ | 43/50 [41:36<06:36, 56.66s/it] 88%|████████▊ | 44/50 [42:32<05:39, 56.56s/it] 90%|█████████ | 45/50 [43:29<04:42, 56.56s/it] 92%|█████████▏| 46/50 [44:25<03:46, 56.55s/it] 94%|█████████▍| 47/50 [45:22<02:49, 56.51s/it] 96%|█████████▌| 48/50 [46:18<01:53, 56.53s/it][loss] ep 39 it 40 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 39 it 90 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 39 it 140 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 39 it 190 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 39 it 240 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 39 it 290 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 39 it 340 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 40 it 0 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 40 it 50 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 40 it 100 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 40 it 150 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 40 it 200 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 40 it 250 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 40 it 300 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 40 it 350 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 41 it 10 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 41 it 60 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 41 it 110 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 41 it 160 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 41 it 210 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 41 it 260 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 41 it 310 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 41 it 360 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 42 it 20 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 42 it 70 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 42 it 120 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 42 it 170 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 42 it 220 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 42 it 270 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 42 it 320 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 42 it 370 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 43 it 30 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 43 it 80 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 43 it 130 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 43 it 180 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 43 it 230 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 43 it 280 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 43 it 330 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 43 it 380 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 44 it 40 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 44 it 90 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 44 it 140 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 44 it 190 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=24 nf=24 protos=540 fproto_sim=NA
[loss] ep 44 it 240 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 44 it 290 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 44 it 340 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 45 it 0 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 45 it 50 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 45 it 100 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 45 it 150 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 45 it 200 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 45 it 250 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 45 it 300 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=40 nf=40 protos=540 fproto_sim=NA
[loss] ep 45 it 350 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 46 it 10 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 46 it 60 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 46 it 110 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 46 it 160 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 46 it 210 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 46 it 260 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 46 it 310 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 46 it 360 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 47 it 20 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 47 it 70 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 47 it 120 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 47 it 170 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 47 it 220 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 47 it 270 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 47 it 320 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 47 it 370 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 48 it 30 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 48 it 80 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 48 it 130 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 48 it 180 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 48 it 230 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 48 it 280 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
 98%|█████████▊| 49/50 [47:15<00:56, 56.69s/it]100%|██████████| 50/50 [48:16<00:00, 57.89s/it]100%|██████████| 50/50 [48:16<00:00, 57.93s/it]
[loss] ep 48 it 330 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 48 it 380 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 49 it 40 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 49 it 90 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 49 it 140 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 49 it 190 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 49 it 240 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 49 it 290 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 49 it 340 total=nan mle=nan pcon=nan forget=nan favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc1: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<04:29,  1.45it/s]  3%|▎         | 10/391 [00:00<00:23, 16.40it/s]  5%|▌         | 20/391 [00:00<00:11, 32.41it/s]  8%|▊         | 30/391 [00:01<00:07, 46.26it/s] 10%|▉         | 39/391 [00:01<00:06, 55.96it/s] 13%|█▎        | 49/391 [00:01<00:05, 65.83it/s] 15%|█▌        | 59/391 [00:01<00:04, 73.83it/s] 18%|█▊        | 69/391 [00:01<00:04, 79.21it/s] 20%|██        | 79/391 [00:01<00:03, 83.07it/s] 23%|██▎       | 89/391 [00:01<00:03, 84.91it/s] 25%|██▌       | 99/391 [00:01<00:03, 87.95it/s] 28%|██▊       | 109/391 [00:01<00:03, 88.09it/s] 30%|███       | 119/391 [00:01<00:03, 89.81it/s] 33%|███▎      | 129/391 [00:02<00:02, 90.91it/s] 36%|███▌      | 139/391 [00:02<00:02, 90.77it/s] 38%|███▊      | 149/391 [00:02<00:02, 92.40it/s] 41%|████      | 159/391 [00:02<00:02, 89.68it/s] 43%|████▎     | 169/391 [00:02<00:02, 91.46it/s] 46%|████▌     | 179/391 [00:02<00:02, 89.93it/s] 48%|████▊     | 189/391 [00:02<00:02, 89.19it/s] 51%|█████     | 199/391 [00:02<00:02, 90.29it/s] 53%|█████▎    | 209/391 [00:02<00:02, 90.42it/s] 56%|█████▌    | 219/391 [00:03<00:01, 91.95it/s] 59%|█████▊    | 229/391 [00:03<00:01, 92.71it/s] 61%|██████    | 239/391 [00:03<00:01, 92.65it/s] 64%|██████▎   | 249/391 [00:03<00:01, 91.67it/s] 66%|██████▌   | 259/391 [00:03<00:01, 90.97it/s] 69%|██████▉   | 269/391 [00:03<00:01, 91.04it/s] 71%|███████▏  | 279/391 [00:03<00:01, 92.02it/s] 74%|███████▍  | 289/391 [00:03<00:01, 92.97it/s] 76%|███████▋  | 299/391 [00:03<00:00, 92.08it/s] 79%|███████▉  | 309/391 [00:04<00:00, 91.70it/s] 82%|████████▏ | 319/391 [00:04<00:00, 88.65it/s] 84%|████████▍ | 328/391 [00:04<00:00, 87.73it/s] 86%|████████▋ | 338/391 [00:04<00:00, 89.85it/s] 89%|████████▉ | 348/391 [00:04<00:00, 86.80it/s] 92%|█████████▏| 358/391 [00:04<00:00, 89.05it/s] 94%|█████████▍| 367/391 [00:04<00:00, 88.93it/s] 96%|█████████▋| 377/391 [00:04<00:00, 91.51it/s] 99%|█████████▉| 387/391 [00:04<00:00, 93.39it/s]100%|██████████| 391/391 [00:04<00:00, 78.68it/s]
50000 images processed, 5.0716235637664795 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:41,  1.89it/s] 14%|█▍        | 11/79 [00:00<00:03, 22.40it/s] 27%|██▋       | 21/79 [00:00<00:01, 39.68it/s] 39%|███▉      | 31/79 [00:00<00:00, 53.12it/s] 51%|█████     | 40/79 [00:00<00:00, 62.28it/s] 63%|██████▎   | 50/79 [00:01<00:00, 70.86it/s] 76%|███████▌  | 60/79 [00:01<00:00, 77.89it/s] 89%|████████▊ | 70/79 [00:01<00:00, 83.02it/s]100%|██████████| 79/79 [00:01<00:00, 42.74it/s]
10000 images processed, 1.8686838150024414 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc1/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:58,  1.72it/s]  5%|▌         | 11/204 [00:00<00:09, 20.84it/s] 10%|█         | 21/204 [00:00<00:04, 37.34it/s] 15%|█▌        | 31/204 [00:00<00:03, 51.39it/s] 20%|██        | 41/204 [00:01<00:02, 62.72it/s] 25%|██▌       | 51/204 [00:01<00:02, 70.86it/s] 30%|██▉       | 61/204 [00:01<00:01, 76.87it/s] 35%|███▍      | 71/204 [00:01<00:01, 81.33it/s] 40%|███▉      | 81/204 [00:01<00:01, 83.32it/s] 44%|████▍     | 90/204 [00:01<00:01, 84.03it/s] 49%|████▊     | 99/204 [00:01<00:01, 85.39it/s] 53%|█████▎    | 109/204 [00:01<00:01, 88.03it/s] 58%|█████▊    | 119/204 [00:01<00:00, 88.20it/s] 63%|██████▎   | 129/204 [00:01<00:00, 89.83it/s] 68%|██████▊   | 139/204 [00:02<00:00, 90.28it/s] 73%|███████▎  | 149/204 [00:02<00:00, 90.01it/s] 78%|███████▊  | 159/204 [00:02<00:00, 90.58it/s] 83%|████████▎ | 169/204 [00:02<00:00, 91.35it/s] 88%|████████▊ | 179/204 [00:02<00:00, 92.65it/s] 93%|█████████▎| 189/204 [00:02<00:00, 93.76it/s] 98%|█████████▊| 199/204 [00:02<00:00, 94.58it/s]100%|██████████| 204/204 [00:02<00:00, 73.26it/s]
26032 images processed, 2.8252782821655273 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:58,  1.33it/s] 13%|█▎        | 10/79 [00:00<00:04, 15.42it/s] 24%|██▍       | 19/79 [00:00<00:02, 28.73it/s] 37%|███▋      | 29/79 [00:01<00:01, 42.69it/s] 48%|████▊     | 38/79 [00:01<00:00, 52.59it/s] 59%|█████▉    | 47/79 [00:01<00:00, 60.12it/s] 71%|███████   | 56/79 [00:01<00:00, 67.14it/s] 82%|████████▏ | 65/79 [00:01<00:00, 67.67it/s] 95%|█████████▍| 75/79 [00:01<00:00, 74.84it/s]100%|██████████| 79/79 [00:01<00:00, 47.50it/s]
10000 images processed, 1.6971769332885742 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:51,  1.50it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.30it/s] 23%|██▎       | 18/79 [00:00<00:02, 29.93it/s] 35%|███▌      | 28/79 [00:00<00:01, 44.70it/s] 47%|████▋     | 37/79 [00:01<00:00, 54.62it/s] 58%|█████▊    | 46/79 [00:01<00:00, 62.19it/s] 71%|███████   | 56/79 [00:01<00:00, 71.29it/s] 84%|████████▎ | 66/79 [00:01<00:00, 77.97it/s] 96%|█████████▌| 76/79 [00:01<00:00, 83.00it/s]100%|██████████| 79/79 [00:01<00:00, 51.50it/s]
10000 images processed, 1.5531597137451172 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:44,  1.54it/s] 14%|█▍        | 10/70 [00:00<00:03, 17.21it/s] 26%|██▌       | 18/70 [00:00<00:01, 29.87it/s] 40%|████      | 28/70 [00:00<00:00, 45.05it/s] 53%|█████▎    | 37/70 [00:01<00:00, 55.34it/s] 67%|██████▋   | 47/70 [00:01<00:00, 65.71it/s] 81%|████████▏ | 57/70 [00:01<00:00, 73.98it/s] 96%|█████████▌| 67/70 [00:01<00:00, 80.31it/s]100%|██████████| 70/70 [00:01<00:00, 49.41it/s]
8925 images processed, 1.4536361694335938 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:48,  1.11s/it]  4%|▍         | 2/45 [00:01<00:22,  1.88it/s] 20%|██        | 9/45 [00:01<00:03, 11.16it/s] 38%|███▊      | 17/45 [00:01<00:01, 14.24it/s] 51%|█████     | 23/45 [00:01<00:01, 17.84it/s] 73%|███████▎  | 33/45 [00:02<00:00, 18.69it/s] 96%|█████████▌| 43/45 [00:02<00:00, 27.76it/s]100%|██████████| 45/45 [00:02<00:00, 17.15it/s]
5640 images processed, 2.6476595401763916 seconds used

18.757686614990234
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.74  99.29
places365     74.73  79.11
LSUN          20.51  95.38
iSUN          78.34  79.48
dtd           43.60  90.14
AVG           43.98  88.68
Retain-Acc: 0.7087
Forget-as-OOD (retain known vs forget novel):
  FPR: 77.80 AUROC: 84.40 AUIN: 99.01
23.365134239196777
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc1_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc1_rf.png
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc2: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<04:00,  1.62it/s]  3%|▎         | 10/391 [00:00<00:20, 18.21it/s]  5%|▍         | 19/391 [00:00<00:11, 33.21it/s]  7%|▋         | 29/391 [00:00<00:07, 47.51it/s] 10%|▉         | 39/391 [00:01<00:05, 59.01it/s] 13%|█▎        | 49/391 [00:01<00:05, 67.87it/s] 15%|█▌        | 59/391 [00:01<00:04, 74.88it/s] 17%|█▋        | 68/391 [00:01<00:04, 75.63it/s] 20%|█▉        | 77/391 [00:01<00:03, 79.24it/s] 22%|██▏       | 86/391 [00:01<00:03, 81.54it/s] 25%|██▍       | 96/391 [00:01<00:03, 84.54it/s] 27%|██▋       | 106/391 [00:01<00:03, 86.28it/s] 29%|██▉       | 115/391 [00:01<00:03, 87.14it/s] 32%|███▏      | 125/391 [00:02<00:02, 88.76it/s] 35%|███▍      | 135/391 [00:02<00:02, 90.02it/s] 37%|███▋      | 145/391 [00:02<00:02, 89.71it/s] 40%|███▉      | 155/391 [00:02<00:02, 89.44it/s] 42%|████▏     | 165/391 [00:02<00:02, 91.12it/s] 45%|████▍     | 175/391 [00:02<00:02, 91.33it/s] 47%|████▋     | 185/391 [00:02<00:02, 91.88it/s] 50%|████▉     | 195/391 [00:02<00:02, 89.98it/s] 52%|█████▏    | 205/391 [00:02<00:02, 89.98it/s] 55%|█████▍    | 215/391 [00:02<00:01, 90.47it/s] 58%|█████▊    | 225/391 [00:03<00:01, 89.02it/s] 60%|██████    | 235/391 [00:03<00:01, 90.71it/s] 63%|██████▎   | 245/391 [00:03<00:01, 89.78it/s] 65%|██████▌   | 255/391 [00:03<00:01, 91.49it/s] 68%|██████▊   | 265/391 [00:03<00:01, 92.57it/s] 70%|███████   | 275/391 [00:03<00:01, 86.37it/s] 73%|███████▎  | 285/391 [00:03<00:01, 88.13it/s] 75%|███████▌  | 295/391 [00:03<00:01, 90.33it/s] 78%|███████▊  | 305/391 [00:03<00:00, 91.64it/s] 81%|████████  | 315/391 [00:04<00:00, 92.60it/s] 83%|████████▎ | 325/391 [00:04<00:00, 91.49it/s] 86%|████████▌ | 335/391 [00:04<00:00, 92.63it/s] 88%|████████▊ | 345/391 [00:04<00:00, 93.23it/s] 91%|█████████ | 355/391 [00:04<00:00, 91.11it/s] 93%|█████████▎| 365/391 [00:04<00:00, 91.48it/s] 96%|█████████▌| 375/391 [00:04<00:00, 92.69it/s] 98%|█████████▊| 385/391 [00:04<00:00, 93.82it/s]100%|██████████| 391/391 [00:04<00:00, 79.36it/s]
50000 images processed, 5.02417516708374 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:54,  1.43it/s] 14%|█▍        | 11/79 [00:00<00:03, 17.87it/s] 27%|██▋       | 21/79 [00:00<00:01, 33.10it/s] 38%|███▊      | 30/79 [00:01<00:01, 44.67it/s] 51%|█████     | 40/79 [00:01<00:00, 56.12it/s] 62%|██████▏   | 49/79 [00:01<00:00, 63.76it/s] 75%|███████▍  | 59/79 [00:01<00:00, 71.98it/s] 87%|████████▋ | 69/79 [00:01<00:00, 78.06it/s]100%|██████████| 79/79 [00:01<00:00, 79.81it/s]100%|██████████| 79/79 [00:01<00:00, 50.31it/s]
10000 images processed, 1.591176986694336 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc2/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:15,  1.50it/s]  4%|▍         | 9/204 [00:00<00:12, 15.12it/s]  9%|▉         | 18/204 [00:00<00:06, 29.65it/s] 14%|█▎        | 28/204 [00:00<00:03, 44.22it/s] 18%|█▊        | 37/204 [00:01<00:03, 54.51it/s] 23%|██▎       | 46/204 [00:01<00:02, 62.28it/s] 27%|██▋       | 56/204 [00:01<00:02, 70.36it/s] 32%|███▏      | 65/204 [00:01<00:01, 72.45it/s] 37%|███▋      | 75/204 [00:01<00:01, 78.10it/s] 41%|████      | 84/204 [00:01<00:01, 81.24it/s] 46%|████▌     | 93/204 [00:01<00:01, 83.50it/s] 50%|█████     | 103/204 [00:01<00:01, 85.75it/s] 55%|█████▍    | 112/204 [00:01<00:01, 85.86it/s] 59%|█████▉    | 121/204 [00:02<00:00, 85.60it/s] 64%|██████▍   | 131/204 [00:02<00:00, 87.86it/s] 69%|██████▊   | 140/204 [00:02<00:00, 88.28it/s] 74%|███████▎  | 150/204 [00:02<00:00, 89.32it/s] 78%|███████▊  | 159/204 [00:02<00:00, 89.22it/s] 82%|████████▏ | 168/204 [00:02<00:00, 89.40it/s] 87%|████████▋ | 178/204 [00:02<00:00, 90.04it/s] 92%|█████████▏| 188/204 [00:02<00:00, 91.44it/s] 97%|█████████▋| 198/204 [00:02<00:00, 92.42it/s]100%|██████████| 204/204 [00:02<00:00, 68.64it/s]
26032 images processed, 3.0259933471679688 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:56,  1.37it/s] 13%|█▎        | 10/79 [00:00<00:04, 15.81it/s] 24%|██▍       | 19/79 [00:00<00:02, 29.39it/s] 35%|███▌      | 28/79 [00:01<00:01, 41.66it/s] 48%|████▊     | 38/79 [00:01<00:00, 53.82it/s] 59%|█████▉    | 47/79 [00:01<00:00, 61.91it/s] 72%|███████▏  | 57/79 [00:01<00:00, 70.61it/s] 85%|████████▍ | 67/79 [00:01<00:00, 77.46it/s] 97%|█████████▋| 77/79 [00:01<00:00, 82.71it/s]100%|██████████| 79/79 [00:01<00:00, 49.76it/s]
10000 images processed, 1.6193974018096924 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:56,  1.38it/s] 10%|█         | 8/79 [00:00<00:05, 12.60it/s] 23%|██▎       | 18/79 [00:00<00:02, 28.77it/s] 35%|███▌      | 28/79 [00:01<00:01, 42.67it/s] 47%|████▋     | 37/79 [00:01<00:00, 52.87it/s] 59%|█████▉    | 47/79 [00:01<00:00, 63.30it/s] 72%|███████▏  | 57/79 [00:01<00:00, 71.92it/s] 85%|████████▍ | 67/79 [00:01<00:00, 78.40it/s] 97%|█████████▋| 77/79 [00:01<00:00, 83.47it/s]100%|██████████| 79/79 [00:01<00:00, 49.86it/s]
10000 images processed, 1.6046903133392334 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:43,  1.59it/s] 14%|█▍        | 10/70 [00:00<00:03, 17.94it/s] 29%|██▊       | 20/70 [00:00<00:01, 34.41it/s] 43%|████▎     | 30/70 [00:00<00:00, 48.32it/s] 57%|█████▋    | 40/70 [00:01<00:00, 59.48it/s] 71%|███████▏  | 50/70 [00:01<00:00, 68.76it/s] 86%|████████▌ | 60/70 [00:01<00:00, 76.05it/s]100%|██████████| 70/70 [00:01<00:00, 80.23it/s]100%|██████████| 70/70 [00:01<00:00, 50.84it/s]
8925 images processed, 1.4104382991790771 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:51,  1.17s/it]  7%|▋         | 3/45 [00:01<00:14,  2.90it/s] 29%|██▉       | 13/45 [00:01<00:02, 15.72it/s] 40%|████      | 18/45 [00:01<00:01, 15.03it/s] 51%|█████     | 23/45 [00:01<00:01, 18.12it/s] 67%|██████▋   | 30/45 [00:02<00:00, 21.30it/s] 76%|███████▌  | 34/45 [00:02<00:00, 18.85it/s] 98%|█████████▊| 44/45 [00:02<00:00, 30.41it/s]100%|██████████| 45/45 [00:02<00:00, 17.54it/s]
5640 images processed, 2.5845565795898438 seconds used

18.844183444976807
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.64  99.31
places365     73.15  79.36
LSUN          19.62  95.60
iSUN          77.04  79.75
dtd           42.39  90.36
AVG           42.97  88.88
Retain-Acc: 0.7087
Forget-as-OOD (retain known vs forget novel):
  FPR: 69.20 AUROC: 87.58 AUIN: 99.28
30.652185916900635
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc2_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc2_rf.png
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-seen: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<04:28,  1.45it/s]  3%|▎         | 10/391 [00:00<00:23, 16.48it/s]  5%|▍         | 19/391 [00:00<00:12, 30.61it/s]  7%|▋         | 29/391 [00:01<00:08, 45.04it/s] 10%|▉         | 38/391 [00:01<00:06, 55.00it/s] 12%|█▏        | 48/391 [00:01<00:05, 64.83it/s] 15%|█▍        | 57/391 [00:01<00:04, 70.32it/s] 17%|█▋        | 66/391 [00:01<00:04, 74.34it/s] 19%|█▉        | 75/391 [00:01<00:04, 75.83it/s] 22%|██▏       | 85/391 [00:01<00:03, 79.56it/s] 24%|██▍       | 95/391 [00:01<00:03, 82.98it/s] 27%|██▋       | 104/391 [00:01<00:03, 81.16it/s] 29%|██▉       | 113/391 [00:01<00:03, 82.96it/s] 31%|███       | 122/391 [00:02<00:03, 82.53it/s] 34%|███▎      | 131/391 [00:02<00:03, 82.03it/s] 36%|███▌      | 140/391 [00:02<00:03, 82.49it/s] 38%|███▊      | 149/391 [00:02<00:02, 81.84it/s] 40%|████      | 158/391 [00:02<00:02, 82.13it/s] 43%|████▎     | 167/391 [00:02<00:02, 83.25it/s] 45%|████▌     | 176/391 [00:02<00:02, 80.51it/s] 47%|████▋     | 185/391 [00:02<00:02, 81.34it/s] 50%|████▉     | 194/391 [00:02<00:02, 82.83it/s] 52%|█████▏    | 203/391 [00:03<00:02, 83.00it/s] 54%|█████▍    | 212/391 [00:03<00:02, 84.64it/s] 57%|█████▋    | 221/391 [00:03<00:02, 83.78it/s] 59%|█████▉    | 230/391 [00:03<00:01, 85.28it/s] 61%|██████    | 239/391 [00:03<00:01, 85.58it/s] 63%|██████▎   | 248/391 [00:03<00:01, 82.38it/s] 66%|██████▌   | 257/391 [00:03<00:01, 82.23it/s] 68%|██████▊   | 266/391 [00:03<00:01, 81.48it/s] 71%|███████   | 276/391 [00:03<00:01, 84.28it/s] 73%|███████▎  | 285/391 [00:04<00:01, 85.81it/s] 75%|███████▌  | 294/391 [00:04<00:01, 86.11it/s] 77%|███████▋  | 303/391 [00:04<00:01, 85.42it/s] 80%|████████  | 313/391 [00:04<00:00, 86.89it/s] 83%|████████▎ | 323/391 [00:04<00:00, 88.58it/s] 85%|████████▍ | 332/391 [00:04<00:00, 85.25it/s] 87%|████████▋ | 342/391 [00:04<00:00, 87.16it/s] 90%|████████▉ | 351/391 [00:04<00:00, 86.83it/s] 92%|█████████▏| 360/391 [00:04<00:00, 84.84it/s] 95%|█████████▍| 370/391 [00:05<00:00, 88.02it/s] 97%|█████████▋| 380/391 [00:05<00:00, 90.76it/s]100%|█████████▉| 390/391 [00:05<00:00, 92.80it/s]100%|██████████| 391/391 [00:05<00:00, 74.46it/s]
50000 images processed, 5.3455400466918945 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:51,  1.52it/s]  9%|▉         | 7/79 [00:00<00:06, 11.90it/s] 20%|██        | 16/79 [00:00<00:02, 27.04it/s] 32%|███▏      | 25/79 [00:00<00:01, 40.04it/s] 42%|████▏     | 33/79 [00:01<00:00, 49.36it/s] 53%|█████▎    | 42/79 [00:01<00:00, 58.06it/s] 66%|██████▌   | 52/79 [00:01<00:00, 67.35it/s] 78%|███████▊  | 62/79 [00:01<00:00, 75.02it/s] 91%|█████████ | 72/79 [00:01<00:00, 80.01it/s]100%|██████████| 79/79 [00:01<00:00, 49.16it/s]
10000 images processed, 1.642432451248169 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-seen/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:21,  1.43it/s]  4%|▍         | 8/204 [00:00<00:15, 13.05it/s]  8%|▊         | 17/204 [00:00<00:06, 27.92it/s] 13%|█▎        | 26/204 [00:01<00:04, 40.46it/s] 17%|█▋        | 35/204 [00:01<00:03, 50.30it/s] 22%|██▏       | 44/204 [00:01<00:02, 58.69it/s] 26%|██▌       | 53/204 [00:01<00:02, 65.70it/s] 31%|███       | 63/204 [00:01<00:01, 72.60it/s] 36%|███▌      | 73/204 [00:01<00:01, 77.28it/s] 40%|████      | 82/204 [00:01<00:01, 79.60it/s] 45%|████▍     | 91/204 [00:01<00:01, 78.11it/s] 50%|████▉     | 101/204 [00:01<00:01, 82.13it/s] 54%|█████▍    | 110/204 [00:02<00:01, 81.08it/s] 58%|█████▊    | 119/204 [00:02<00:01, 80.97it/s] 63%|██████▎   | 128/204 [00:02<00:00, 80.69it/s] 68%|██████▊   | 138/204 [00:02<00:00, 83.94it/s] 72%|███████▏  | 147/204 [00:02<00:00, 81.61it/s] 76%|███████▋  | 156/204 [00:02<00:00, 81.60it/s] 81%|████████  | 165/204 [00:02<00:00, 81.74it/s] 85%|████████▌ | 174/204 [00:02<00:00, 82.00it/s] 90%|█████████ | 184/204 [00:02<00:00, 86.27it/s] 95%|█████████▌| 194/204 [00:02<00:00, 89.28it/s]100%|██████████| 204/204 [00:03<00:00, 89.85it/s]100%|██████████| 204/204 [00:03<00:00, 65.60it/s]
26032 images processed, 3.153235673904419 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:14,  1.04it/s] 10%|█         | 8/79 [00:01<00:07,  9.82it/s] 22%|██▏       | 17/79 [00:01<00:02, 22.14it/s] 30%|███       | 24/79 [00:01<00:01, 30.23it/s] 43%|████▎     | 34/79 [00:01<00:01, 43.40it/s] 54%|█████▍    | 43/79 [00:01<00:00, 53.53it/s] 66%|██████▌   | 52/79 [00:01<00:00, 61.75it/s] 78%|███████▊  | 62/79 [00:01<00:00, 69.95it/s] 91%|█████████ | 72/79 [00:01<00:00, 76.61it/s]100%|██████████| 79/79 [00:01<00:00, 42.07it/s]
10000 images processed, 1.9027035236358643 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:57,  1.36it/s] 11%|█▏        | 9/79 [00:00<00:05, 13.92it/s] 23%|██▎       | 18/79 [00:00<00:02, 27.42it/s] 33%|███▎      | 26/79 [00:01<00:01, 37.85it/s] 43%|████▎     | 34/79 [00:01<00:00, 46.74it/s] 53%|█████▎    | 42/79 [00:01<00:00, 54.34it/s] 63%|██████▎   | 50/79 [00:01<00:00, 60.76it/s] 76%|███████▌  | 60/79 [00:01<00:00, 70.42it/s] 89%|████████▊ | 70/79 [00:01<00:00, 77.72it/s]100%|██████████| 79/79 [00:01<00:00, 47.36it/s]
10000 images processed, 1.690805435180664 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:45,  1.51it/s]  4%|▍         | 3/70 [00:00<00:14,  4.74it/s] 17%|█▋        | 12/70 [00:00<00:02, 21.63it/s] 31%|███▏      | 22/70 [00:00<00:01, 37.81it/s] 44%|████▍     | 31/70 [00:01<00:00, 49.00it/s] 57%|█████▋    | 40/70 [00:01<00:00, 57.87it/s] 71%|███████▏  | 50/70 [00:01<00:00, 68.13it/s] 86%|████████▌ | 60/70 [00:01<00:00, 76.01it/s]100%|██████████| 70/70 [00:01<00:00, 80.54it/s]100%|██████████| 70/70 [00:01<00:00, 46.04it/s]
8925 images processed, 1.5501694679260254 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:57,  1.30s/it]  4%|▍         | 2/45 [00:01<00:26,  1.65it/s] 24%|██▍       | 11/45 [00:01<00:02, 12.32it/s] 38%|███▊      | 17/45 [00:01<00:01, 14.64it/s] 47%|████▋     | 21/45 [00:02<00:01, 16.08it/s] 67%|██████▋   | 30/45 [00:02<00:00, 26.24it/s] 78%|███████▊  | 35/45 [00:02<00:00, 15.28it/s]100%|██████████| 45/45 [00:02<00:00, 24.27it/s]100%|██████████| 45/45 [00:02<00:00, 15.28it/s]
5640 images processed, 2.9645497798919678 seconds used

20.110677003860474
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.57  99.36
places365     68.08  81.18
LSUN          17.82  96.07
iSUN          72.36  81.80
dtd           38.48  91.38
AVG           39.86  89.96
Retain-Acc: 0.7481
Forget-as-OOD (retain known vs forget novel):
  FPR: 54.60 AUROC: 89.30 AUIN: 98.64
15.316864967346191
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-seen_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-seen_rf.png
