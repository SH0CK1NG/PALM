nohup: ignoring input
[Full Grid] Searching over: lambdas(0.2) × lrs(0.001) × epochs(5 10 15 20 25 30 35 40 45 50)
[Run] lambda=0.2 lr=0.001 epochs=5
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:168: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:15<01:03, 15.89s/it] 40%|████      | 2/5 [00:24<00:35, 11.75s/it] 60%|██████    | 3/5 [00:35<00:22, 11.11s/it] 80%|████████  | 4/5 [00:44<00:10, 10.41s/it]100%|██████████| 5/5 [00:54<00:00, 10.24s/it]100%|██████████| 5/5 [00:54<00:00, 10.87s/it]
[loss] ep 0 it 0 total=9.3216 mle=1.5619 pcon=5.2951 forget=2.4646 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.3417 mle=1.6307 pcon=5.2880 forget=2.4229 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.5222 mle=1.7912 pcon=5.2813 forget=2.4497 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.4587 mle=1.7481 pcon=5.2750 forget=2.4357 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.5113 mle=1.8429 pcon=5.2683 forget=2.4001 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.4073 mle=1.6800 pcon=5.2618 forget=2.4655 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.3332 mle=1.6259 pcon=5.2557 forget=2.4515 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.5303 mle=1.8021 pcon=5.2498 forget=2.4784 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 1 it 10 total=9.1506 mle=1.4467 pcon=5.2437 forget=2.4601 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.3556 mle=1.6387 pcon=5.2378 forget=2.4791 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=9.1933 mle=1.5268 pcon=5.2322 forget=2.4342 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=9.3110 mle=1.6063 pcon=5.2265 forget=2.4782 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.3540 mle=1.6693 pcon=5.2211 forget=2.4636 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=9.1899 mle=1.5596 pcon=5.2158 forget=2.4145 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=9.2955 mle=1.6338 pcon=5.2102 forget=2.4515 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.3864 mle=1.7251 pcon=5.2050 forget=2.4563 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 2 it 20 total=9.0648 mle=1.4193 pcon=5.1998 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.5211 mle=1.8332 pcon=5.1947 forget=2.4932 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=9.3052 mle=1.6841 pcon=5.1895 forget=2.4315 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=9.2922 mle=1.6702 pcon=5.1848 forget=2.4372 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=9.4406 mle=1.8056 pcon=5.1798 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.3898 mle=1.7526 pcon=5.1751 forget=2.4621 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=9.0231 mle=1.4274 pcon=5.1706 forget=2.4251 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.2262 mle=1.6128 pcon=5.1666 forget=2.4468 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 3 it 30 total=9.2788 mle=1.6633 pcon=5.1623 forget=2.4531 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=9.1716 mle=1.5681 pcon=5.1582 forget=2.4453 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=9.1482 mle=1.5602 pcon=5.1540 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.3950 mle=1.8276 pcon=5.1498 forget=2.4176 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=9.3113 mle=1.7047 pcon=5.1462 forget=2.4604 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.3622 mle=1.7594 pcon=5.1425 forget=2.4602 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.4045 mle=1.8244 pcon=5.1385 forget=2.4417 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=9.2004 mle=1.6191 pcon=5.1348 forget=2.4466 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 4 it 40 total=9.1424 mle=1.5755 pcon=5.1311 forget=2.4358 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=9.1359 mle=1.5309 pcon=5.1276 forget=2.4774 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.2283 mle=1.7051 pcon=5.1245 forget=2.3986 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.2331 mle=1.6598 pcon=5.1210 forget=2.4523 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.3928 mle=1.8093 pcon=5.1177 forget=2.4658 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.0165 mle=1.4523 pcon=5.1146 forget=2.4496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.2409 mle=1.6859 pcon=5.1118 forget=2.4432 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[Run] lambda=0.2 lr=0.001 epochs=10
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:168: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:14<02:11, 14.64s/it] 20%|██        | 2/10 [00:24<01:36, 12.07s/it] 30%|███       | 3/10 [00:35<01:18, 11.19s/it] 40%|████      | 4/10 [00:44<01:02, 10.44s/it] 50%|█████     | 5/10 [00:53<00:49,  9.99s/it] 60%|██████    | 6/10 [01:03<00:40, 10.13s/it][loss] ep 0 it 0 total=9.3215 mle=1.5618 pcon=5.2951 forget=2.4646 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.3417 mle=1.6308 pcon=5.2880 forget=2.4229 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.5222 mle=1.7912 pcon=5.2813 forget=2.4496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.4588 mle=1.7481 pcon=5.2750 forget=2.4357 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.5113 mle=1.8428 pcon=5.2683 forget=2.4001 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.4072 mle=1.6799 pcon=5.2618 forget=2.4655 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.3328 mle=1.6256 pcon=5.2557 forget=2.4515 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.5303 mle=1.8021 pcon=5.2498 forget=2.4784 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 1 it 10 total=9.1505 mle=1.4466 pcon=5.2437 forget=2.4601 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.3555 mle=1.6387 pcon=5.2378 forget=2.4791 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=9.1933 mle=1.5268 pcon=5.2322 forget=2.4342 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=9.3108 mle=1.6062 pcon=5.2265 forget=2.4782 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.3542 mle=1.6694 pcon=5.2211 forget=2.4636 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=9.1898 mle=1.5595 pcon=5.2158 forget=2.4145 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=9.2953 mle=1.6336 pcon=5.2102 forget=2.4514 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.3863 mle=1.7250 pcon=5.2050 forget=2.4563 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 2 it 20 total=9.0647 mle=1.4191 pcon=5.1998 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.5211 mle=1.8333 pcon=5.1947 forget=2.4931 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=9.3053 mle=1.6842 pcon=5.1895 forget=2.4316 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=9.2923 mle=1.6702 pcon=5.1848 forget=2.4373 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=9.4408 mle=1.8058 pcon=5.1798 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.3899 mle=1.7527 pcon=5.1752 forget=2.4621 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=9.0231 mle=1.4274 pcon=5.1706 forget=2.4251 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.2262 mle=1.6128 pcon=5.1666 forget=2.4467 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 3 it 30 total=9.2787 mle=1.6632 pcon=5.1623 forget=2.4531 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=9.1716 mle=1.5682 pcon=5.1582 forget=2.4453 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=9.1482 mle=1.5602 pcon=5.1540 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.3951 mle=1.8276 pcon=5.1499 forget=2.4176 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=9.3114 mle=1.7048 pcon=5.1462 forget=2.4604 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.3623 mle=1.7596 pcon=5.1425 forget=2.4602 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.4045 mle=1.8244 pcon=5.1385 forget=2.4416 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=9.2005 mle=1.6191 pcon=5.1348 forget=2.4466 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 4 it 40 total=9.1426 mle=1.5757 pcon=5.1311 forget=2.4358 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=9.1359 mle=1.5309 pcon=5.1276 forget=2.4773 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.2283 mle=1.7052 pcon=5.1245 forget=2.3986 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.2331 mle=1.6599 pcon=5.1210 forget=2.4522 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.3928 mle=1.8093 pcon=5.1177 forget=2.4658 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.0165 mle=1.4522 pcon=5.1146 forget=2.4496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.2409 mle=1.6860 pcon=5.1118 forget=2.4432 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 5 it 0 total=9.2134 mle=1.6087 pcon=5.1085 forget=2.4962 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=9.2657 mle=1.6933 pcon=5.1056 forget=2.4668 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.3821 mle=1.7997 pcon=5.1025 forget=2.4799 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.5468 mle=1.9765 pcon=5.0994 forget=2.4709 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.3240 mle=1.7905 pcon=5.0965 forget=2.4370 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=9.2514 mle=1.7139 pcon=5.0935 forget=2.4439 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=9.2808 mle=1.7676 pcon=5.0905 forget=2.4226 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=9.2407 mle=1.7190 pcon=5.0876 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 6 it 10 total=9.1400 mle=1.5563 pcon=5.0850 forget=2.4987 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=9.2205 mle=1.7053 pcon=5.0823 forget=2.4329 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=9.2531 mle=1.6997 pcon=5.0799 forget=2.4735 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=9.0336 mle=1.5104 pcon=5.0775 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=9.2067 mle=1.6767 pcon=5.0748 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=9.2962 mle=1.7616 pcon=5.0724 forget=2.4622 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=8.9789 mle=1.4947 pcon=5.0702 forget=2.4140 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 70%|███████   | 7/10 [01:14<00:30, 10.28s/it] 80%|████████  | 8/10 [01:23<00:19,  9.93s/it] 90%|█████████ | 9/10 [01:32<00:09,  9.63s/it]100%|██████████| 10/10 [01:42<00:00,  9.80s/it]100%|██████████| 10/10 [01:42<00:00, 10.29s/it]
[loss] ep 6 it 360 total=9.1805 mle=1.7147 pcon=5.0677 forget=2.3980 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 7 it 20 total=9.0254 mle=1.5088 pcon=5.0654 forget=2.4511 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=9.0875 mle=1.5735 pcon=5.0629 forget=2.4511 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=9.2398 mle=1.7394 pcon=5.0607 forget=2.4398 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=9.2179 mle=1.7606 pcon=5.0585 forget=2.3988 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=9.1346 mle=1.6538 pcon=5.0567 forget=2.4240 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=9.3683 mle=1.8772 pcon=5.0548 forget=2.4363 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=9.0487 mle=1.5490 pcon=5.0525 forget=2.4473 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=9.3153 mle=1.7891 pcon=5.0503 forget=2.4758 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 8 it 30 total=9.2470 mle=1.7899 pcon=5.0485 forget=2.4086 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=9.0188 mle=1.5220 pcon=5.0465 forget=2.4502 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=9.2263 mle=1.7439 pcon=5.0447 forget=2.4377 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=9.1380 mle=1.6514 pcon=5.0428 forget=2.4439 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=9.0535 mle=1.5675 pcon=5.0410 forget=2.4450 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.9496 mle=1.4577 pcon=5.0392 forget=2.4528 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.1962 mle=1.7882 pcon=5.0373 forget=2.3708 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=9.1247 mle=1.6498 pcon=5.0357 forget=2.4392 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 9 it 40 total=9.2890 mle=1.7967 pcon=5.0340 forget=2.4583 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=9.1829 mle=1.7482 pcon=5.0322 forget=2.4024 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=9.1798 mle=1.6837 pcon=5.0310 forget=2.4651 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.9753 mle=1.4941 pcon=5.0295 forget=2.4517 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=9.1877 mle=1.7624 pcon=5.0282 forget=2.3971 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=9.1275 mle=1.6624 pcon=5.0265 forget=2.4386 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.9611 mle=1.5042 pcon=5.0252 forget=2.4318 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[Run] lambda=0.2 lr=0.001 epochs=15
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=15, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/15 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:168: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  7%|▋         | 1/15 [00:12<03:01, 13.00s/it] 13%|█▎        | 2/15 [00:20<02:09,  9.99s/it] 20%|██        | 3/15 [00:28<01:49,  9.10s/it] 27%|██▋       | 4/15 [00:38<01:40,  9.10s/it] 33%|███▎      | 5/15 [00:46<01:27,  8.75s/it] 40%|████      | 6/15 [00:55<01:20,  8.99s/it][loss] ep 0 it 0 total=9.3215 mle=1.5618 pcon=5.2951 forget=2.4646 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.3417 mle=1.6308 pcon=5.2880 forget=2.4229 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.5222 mle=1.7912 pcon=5.2813 forget=2.4496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.4588 mle=1.7481 pcon=5.2750 forget=2.4357 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.5113 mle=1.8428 pcon=5.2683 forget=2.4001 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.4072 mle=1.6799 pcon=5.2618 forget=2.4655 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.3328 mle=1.6256 pcon=5.2557 forget=2.4515 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.5303 mle=1.8021 pcon=5.2498 forget=2.4784 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 1 it 10 total=9.1505 mle=1.4466 pcon=5.2437 forget=2.4601 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.3555 mle=1.6387 pcon=5.2378 forget=2.4791 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=9.1933 mle=1.5268 pcon=5.2322 forget=2.4342 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=9.3108 mle=1.6062 pcon=5.2265 forget=2.4782 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.3542 mle=1.6694 pcon=5.2211 forget=2.4636 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=9.1898 mle=1.5595 pcon=5.2158 forget=2.4145 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=9.2953 mle=1.6336 pcon=5.2102 forget=2.4514 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.3863 mle=1.7250 pcon=5.2050 forget=2.4563 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 2 it 20 total=9.0647 mle=1.4191 pcon=5.1998 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.5211 mle=1.8333 pcon=5.1947 forget=2.4931 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=9.3053 mle=1.6842 pcon=5.1895 forget=2.4316 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=9.2923 mle=1.6702 pcon=5.1848 forget=2.4373 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=9.4408 mle=1.8058 pcon=5.1798 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.3899 mle=1.7527 pcon=5.1752 forget=2.4621 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=9.0231 mle=1.4274 pcon=5.1706 forget=2.4251 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.2262 mle=1.6128 pcon=5.1666 forget=2.4467 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 3 it 30 total=9.2787 mle=1.6632 pcon=5.1623 forget=2.4531 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=9.1716 mle=1.5682 pcon=5.1582 forget=2.4453 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=9.1483 mle=1.5602 pcon=5.1540 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.3951 mle=1.8276 pcon=5.1498 forget=2.4176 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=9.3114 mle=1.7047 pcon=5.1462 forget=2.4604 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.3622 mle=1.7595 pcon=5.1425 forget=2.4601 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.4046 mle=1.8245 pcon=5.1385 forget=2.4417 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=9.2007 mle=1.6193 pcon=5.1348 forget=2.4466 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 4 it 40 total=9.1425 mle=1.5756 pcon=5.1311 forget=2.4358 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=9.1359 mle=1.5309 pcon=5.1276 forget=2.4774 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.2282 mle=1.7051 pcon=5.1245 forget=2.3986 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.2331 mle=1.6598 pcon=5.1210 forget=2.4523 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.3928 mle=1.8093 pcon=5.1177 forget=2.4658 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.0164 mle=1.4522 pcon=5.1146 forget=2.4496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.2409 mle=1.6859 pcon=5.1118 forget=2.4432 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 5 it 0 total=9.2134 mle=1.6087 pcon=5.1085 forget=2.4962 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=9.2654 mle=1.6929 pcon=5.1056 forget=2.4669 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.3822 mle=1.7998 pcon=5.1025 forget=2.4799 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.5467 mle=1.9764 pcon=5.0994 forget=2.4709 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.3241 mle=1.7907 pcon=5.0965 forget=2.4369 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=9.2513 mle=1.7138 pcon=5.0935 forget=2.4440 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=9.2809 mle=1.7677 pcon=5.0905 forget=2.4226 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=9.2407 mle=1.7191 pcon=5.0876 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 6 it 10 total=9.1399 mle=1.5562 pcon=5.0849 forget=2.4987 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=9.2206 mle=1.7055 pcon=5.0823 forget=2.4328 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=9.2531 mle=1.6997 pcon=5.0799 forget=2.4735 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=9.0336 mle=1.5103 pcon=5.0775 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=9.2065 mle=1.6765 pcon=5.0747 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=9.2963 mle=1.7618 pcon=5.0724 forget=2.4622 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=8.9789 mle=1.4947 pcon=5.0702 forget=2.4140 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 47%|████▋     | 7/15 [01:03<01:09,  8.64s/it] 53%|█████▎    | 8/15 [01:13<01:02,  9.00s/it] 60%|██████    | 9/15 [01:22<00:54,  9.11s/it] 67%|██████▋   | 10/15 [01:31<00:45,  9.16s/it] 73%|███████▎  | 11/15 [01:40<00:36,  9.11s/it] 80%|████████  | 12/15 [01:50<00:27,  9.20s/it] 87%|████████▋ | 13/15 [02:00<00:18,  9.37s/it][loss] ep 6 it 360 total=9.1805 mle=1.7147 pcon=5.0677 forget=2.3980 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 7 it 20 total=9.0254 mle=1.5088 pcon=5.0654 forget=2.4511 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=9.0874 mle=1.5734 pcon=5.0629 forget=2.4510 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=9.2397 mle=1.7392 pcon=5.0607 forget=2.4398 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=9.2178 mle=1.7605 pcon=5.0585 forget=2.3988 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=9.1343 mle=1.6536 pcon=5.0567 forget=2.4240 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=9.3681 mle=1.8769 pcon=5.0548 forget=2.4363 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=9.0490 mle=1.5492 pcon=5.0525 forget=2.4473 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=9.3153 mle=1.7892 pcon=5.0503 forget=2.4758 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 8 it 30 total=9.2469 mle=1.7899 pcon=5.0485 forget=2.4086 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=9.0189 mle=1.5222 pcon=5.0465 forget=2.4502 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=9.2261 mle=1.7437 pcon=5.0447 forget=2.4377 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=9.1380 mle=1.6514 pcon=5.0428 forget=2.4438 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=9.0532 mle=1.5672 pcon=5.0410 forget=2.4450 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.9496 mle=1.4576 pcon=5.0392 forget=2.4528 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.1962 mle=1.7882 pcon=5.0372 forget=2.3708 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=9.1248 mle=1.6499 pcon=5.0357 forget=2.4392 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 9 it 40 total=9.2891 mle=1.7967 pcon=5.0340 forget=2.4583 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=9.1831 mle=1.7484 pcon=5.0322 forget=2.4024 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=9.1798 mle=1.6837 pcon=5.0310 forget=2.4651 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.9752 mle=1.4940 pcon=5.0295 forget=2.4517 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=9.1877 mle=1.7624 pcon=5.0282 forget=2.3970 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=9.1277 mle=1.6626 pcon=5.0265 forget=2.4386 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.9610 mle=1.5041 pcon=5.0252 forget=2.4318 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 10 it 0 total=8.9686 mle=1.4924 pcon=5.0232 forget=2.4530 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.9281 mle=1.4682 pcon=5.0221 forget=2.4378 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=9.1264 mle=1.6909 pcon=5.0206 forget=2.4149 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=9.1776 mle=1.7025 pcon=5.0192 forget=2.4559 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=9.2086 mle=1.7847 pcon=5.0180 forget=2.4059 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=9.2213 mle=1.8303 pcon=5.0167 forget=2.3743 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=9.0428 mle=1.7734 pcon=5.0153 forget=2.2540 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.8152 mle=1.5403 pcon=5.0144 forget=2.2605 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 11 it 10 total=8.9608 mle=1.7724 pcon=5.0130 forget=2.1753 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.7204 mle=1.7135 pcon=5.0120 forget=1.9948 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.5063 mle=1.5917 pcon=5.0112 forget=1.9033 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.3907 mle=1.5799 pcon=5.0103 forget=1.8005 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.3079 mle=1.5454 pcon=5.0098 forget=1.7526 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.2663 mle=1.5613 pcon=5.0095 forget=1.6956 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.5393 mle=1.7997 pcon=5.0095 forget=1.7301 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.4180 mle=1.6830 pcon=5.0099 forget=1.7252 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 12 it 20 total=8.2981 mle=1.5835 pcon=5.0099 forget=1.7048 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.2563 mle=1.5251 pcon=5.0102 forget=1.7210 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.4131 mle=1.7431 pcon=5.0108 forget=1.6591 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.4058 mle=1.7065 pcon=5.0116 forget=1.6878 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.2690 mle=1.5470 pcon=5.0127 forget=1.7093 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.3300 mle=1.5919 pcon=5.0140 forget=1.7240 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.4269 mle=1.6856 pcon=5.0154 forget=1.7258 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.4590 mle=1.7116 pcon=5.0167 forget=1.7307 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 13 it 30 total=8.4618 mle=1.7003 pcon=5.0183 forget=1.7432 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.6102 mle=1.8172 pcon=5.0199 forget=1.7731 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.4726 mle=1.6827 pcon=5.0213 forget=1.7686 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=8.5713 mle=1.7675 pcon=5.0226 forget=1.7812 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 93%|█████████▎| 14/15 [02:09<00:09,  9.48s/it]100%|██████████| 15/15 [02:18<00:00,  9.17s/it]100%|██████████| 15/15 [02:18<00:00,  9.22s/it]
[loss] ep 13 it 230 total=8.4807 mle=1.6559 pcon=5.0244 forget=1.8003 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.2898 mle=1.4559 pcon=5.0261 forget=1.8078 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.6130 mle=1.7628 pcon=5.0281 forget=1.8222 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.5112 mle=1.6542 pcon=5.0300 forget=1.8269 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 40 total=8.5642 mle=1.6868 pcon=5.0319 forget=1.8455 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.4263 mle=1.5291 pcon=5.0340 forget=1.8631 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.5839 mle=1.6795 pcon=5.0361 forget=1.8684 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.5279 mle=1.6128 pcon=5.0382 forget=1.8769 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.5965 mle=1.6649 pcon=5.0401 forget=1.8915 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.5110 mle=1.5629 pcon=5.0424 forget=1.9057 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.5271 mle=1.5656 pcon=5.0447 forget=1.9168 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[Run] lambda=0.2 lr=0.001 epochs=20
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:168: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:14<04:35, 14.48s/it] 10%|█         | 2/20 [00:24<03:35, 11.98s/it] 15%|█▌        | 3/20 [00:34<03:06, 10.96s/it] 20%|██        | 4/20 [00:43<02:44, 10.28s/it] 25%|██▌       | 5/20 [00:53<02:31, 10.09s/it] 30%|███       | 6/20 [01:03<02:22, 10.19s/it][loss] ep 0 it 0 total=9.3216 mle=1.5619 pcon=5.2951 forget=2.4646 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.3417 mle=1.6307 pcon=5.2880 forget=2.4229 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.5222 mle=1.7912 pcon=5.2813 forget=2.4497 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.4587 mle=1.7481 pcon=5.2750 forget=2.4357 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.5113 mle=1.8429 pcon=5.2683 forget=2.4001 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.4073 mle=1.6800 pcon=5.2618 forget=2.4655 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.3332 mle=1.6259 pcon=5.2557 forget=2.4515 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.5303 mle=1.8021 pcon=5.2498 forget=2.4784 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 1 it 10 total=9.1506 mle=1.4467 pcon=5.2437 forget=2.4601 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.3556 mle=1.6387 pcon=5.2378 forget=2.4791 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=9.1933 mle=1.5268 pcon=5.2322 forget=2.4342 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=9.3110 mle=1.6063 pcon=5.2265 forget=2.4782 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.3540 mle=1.6693 pcon=5.2211 forget=2.4636 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=9.1899 mle=1.5596 pcon=5.2158 forget=2.4145 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=9.2954 mle=1.6338 pcon=5.2102 forget=2.4514 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.3864 mle=1.7251 pcon=5.2050 forget=2.4564 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 2 it 20 total=9.0648 mle=1.4192 pcon=5.1998 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.5212 mle=1.8334 pcon=5.1947 forget=2.4932 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=9.3052 mle=1.6841 pcon=5.1895 forget=2.4315 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=9.2923 mle=1.6703 pcon=5.1848 forget=2.4373 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=9.4405 mle=1.8055 pcon=5.1798 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.3899 mle=1.7526 pcon=5.1752 forget=2.4621 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=9.0233 mle=1.4276 pcon=5.1706 forget=2.4251 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.2261 mle=1.6128 pcon=5.1666 forget=2.4468 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 3 it 30 total=9.2788 mle=1.6633 pcon=5.1623 forget=2.4531 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=9.1715 mle=1.5681 pcon=5.1582 forget=2.4452 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=9.1483 mle=1.5603 pcon=5.1540 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.3951 mle=1.8277 pcon=5.1498 forget=2.4176 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=9.3115 mle=1.7049 pcon=5.1462 forget=2.4604 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.3622 mle=1.7595 pcon=5.1425 forget=2.4602 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.4045 mle=1.8244 pcon=5.1384 forget=2.4416 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=9.2007 mle=1.6193 pcon=5.1348 forget=2.4466 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 4 it 40 total=9.1425 mle=1.5757 pcon=5.1311 forget=2.4358 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=9.1358 mle=1.5308 pcon=5.1276 forget=2.4773 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.2282 mle=1.7051 pcon=5.1245 forget=2.3986 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.2333 mle=1.6600 pcon=5.1210 forget=2.4523 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.3931 mle=1.8096 pcon=5.1177 forget=2.4658 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.0165 mle=1.4522 pcon=5.1146 forget=2.4496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.2410 mle=1.6861 pcon=5.1118 forget=2.4432 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 5 it 0 total=9.2134 mle=1.6087 pcon=5.1085 forget=2.4961 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=9.2656 mle=1.6931 pcon=5.1056 forget=2.4668 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.3823 mle=1.7999 pcon=5.1025 forget=2.4798 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.5467 mle=1.9763 pcon=5.0994 forget=2.4709 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.3239 mle=1.7905 pcon=5.0965 forget=2.4369 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=9.2513 mle=1.7139 pcon=5.0935 forget=2.4439 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=9.2807 mle=1.7675 pcon=5.0906 forget=2.4226 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=9.2406 mle=1.7189 pcon=5.0876 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 6 it 10 total=9.1399 mle=1.5563 pcon=5.0850 forget=2.4987 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=9.2203 mle=1.7051 pcon=5.0823 forget=2.4328 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=9.2529 mle=1.6996 pcon=5.0799 forget=2.4734 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=9.0337 mle=1.5103 pcon=5.0775 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=9.2065 mle=1.6765 pcon=5.0748 forget=2.4553 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=9.2963 mle=1.7617 pcon=5.0724 forget=2.4622 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=8.9788 mle=1.4947 pcon=5.0702 forget=2.4139 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 35%|███▌      | 7/20 [01:13<02:09,  9.95s/it] 40%|████      | 8/20 [01:23<01:59,  9.96s/it] 45%|████▌     | 9/20 [01:32<01:46,  9.68s/it] 50%|█████     | 10/20 [01:41<01:36,  9.60s/it] 55%|█████▌    | 11/20 [01:52<01:29,  9.91s/it] 60%|██████    | 12/20 [02:01<01:17,  9.70s/it] 65%|██████▌   | 13/20 [02:11<01:07,  9.71s/it][loss] ep 6 it 360 total=9.1804 mle=1.7147 pcon=5.0677 forget=2.3980 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 7 it 20 total=9.0254 mle=1.5088 pcon=5.0654 forget=2.4511 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=9.0875 mle=1.5735 pcon=5.0629 forget=2.4510 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=9.2397 mle=1.7393 pcon=5.0607 forget=2.4397 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=9.2180 mle=1.7607 pcon=5.0585 forget=2.3988 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=9.1344 mle=1.6536 pcon=5.0567 forget=2.4241 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=9.3682 mle=1.8770 pcon=5.0548 forget=2.4363 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=9.0491 mle=1.5492 pcon=5.0525 forget=2.4474 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=9.3154 mle=1.7893 pcon=5.0503 forget=2.4758 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 8 it 30 total=9.2469 mle=1.7899 pcon=5.0485 forget=2.4085 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=9.0191 mle=1.5223 pcon=5.0465 forget=2.4502 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=9.2261 mle=1.7437 pcon=5.0446 forget=2.4377 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=9.1380 mle=1.6514 pcon=5.0428 forget=2.4439 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=9.0534 mle=1.5674 pcon=5.0410 forget=2.4450 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.9497 mle=1.4578 pcon=5.0392 forget=2.4528 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.1963 mle=1.7883 pcon=5.0372 forget=2.3708 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=9.1245 mle=1.6497 pcon=5.0357 forget=2.4391 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 9 it 40 total=9.2891 mle=1.7969 pcon=5.0340 forget=2.4583 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=9.1831 mle=1.7484 pcon=5.0322 forget=2.4024 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=9.1798 mle=1.6837 pcon=5.0310 forget=2.4651 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.9754 mle=1.4942 pcon=5.0295 forget=2.4517 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=9.1875 mle=1.7623 pcon=5.0282 forget=2.3970 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=9.1277 mle=1.6625 pcon=5.0265 forget=2.4386 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.9611 mle=1.5041 pcon=5.0252 forget=2.4318 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 10 it 0 total=8.9686 mle=1.4924 pcon=5.0232 forget=2.4530 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.9283 mle=1.4684 pcon=5.0221 forget=2.4378 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=9.1263 mle=1.6909 pcon=5.0206 forget=2.4148 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=9.1775 mle=1.7025 pcon=5.0192 forget=2.4558 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=9.2083 mle=1.7848 pcon=5.0180 forget=2.4055 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=9.2193 mle=1.8303 pcon=5.0167 forget=2.3723 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=9.0379 mle=1.7733 pcon=5.0153 forget=2.2493 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.8037 mle=1.5405 pcon=5.0143 forget=2.2489 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 11 it 10 total=8.9344 mle=1.7711 pcon=5.0130 forget=2.1503 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.6722 mle=1.7142 pcon=5.0121 forget=1.9459 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.4600 mle=1.5929 pcon=5.0113 forget=1.8558 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.3559 mle=1.5843 pcon=5.0105 forget=1.7612 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.2768 mle=1.5479 pcon=5.0101 forget=1.7188 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.2533 mle=1.5719 pcon=5.0098 forget=1.6717 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.5096 mle=1.7997 pcon=5.0099 forget=1.7000 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.3814 mle=1.6840 pcon=5.0103 forget=1.6871 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 12 it 20 total=8.2501 mle=1.5775 pcon=5.0104 forget=1.6623 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.2062 mle=1.5180 pcon=5.0108 forget=1.6774 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.3842 mle=1.7317 pcon=5.0114 forget=1.6411 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.3855 mle=1.7080 pcon=5.0121 forget=1.6653 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.2323 mle=1.5427 pcon=5.0133 forget=1.6763 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.2827 mle=1.5777 pcon=5.0145 forget=1.6904 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.3877 mle=1.6712 pcon=5.0158 forget=1.7007 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.4259 mle=1.7025 pcon=5.0170 forget=1.7064 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 13 it 30 total=8.4328 mle=1.7029 pcon=5.0184 forget=1.7115 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.5570 mle=1.7986 pcon=5.0197 forget=1.7387 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.4209 mle=1.6711 pcon=5.0208 forget=1.7290 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=8.5181 mle=1.7549 pcon=5.0217 forget=1.7415 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 70%|███████   | 14/20 [02:19<00:54,  9.14s/it] 75%|███████▌  | 15/20 [02:29<00:47,  9.43s/it] 80%|████████  | 16/20 [02:36<00:35,  8.84s/it] 85%|████████▌ | 17/20 [02:46<00:27,  9.14s/it] 90%|█████████ | 18/20 [02:59<00:20, 10.17s/it] 95%|█████████▌| 19/20 [03:10<00:10, 10.57s/it]100%|██████████| 20/20 [03:19<00:00, 10.13s/it]100%|██████████| 20/20 [03:19<00:00,  9.99s/it]
[loss] ep 13 it 230 total=8.4294 mle=1.6557 pcon=5.0231 forget=1.7505 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.2306 mle=1.4528 pcon=5.0244 forget=1.7533 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.5461 mle=1.7606 pcon=5.0259 forget=1.7596 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.4343 mle=1.6442 pcon=5.0273 forget=1.7629 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 40 total=8.4904 mle=1.6847 pcon=5.0285 forget=1.7772 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.3376 mle=1.5230 pcon=5.0299 forget=1.7846 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.4749 mle=1.6638 pcon=5.0312 forget=1.7799 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.4189 mle=1.6024 pcon=5.0325 forget=1.7840 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.4859 mle=1.6534 pcon=5.0335 forget=1.7990 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.3728 mle=1.5427 pcon=5.0347 forget=1.7953 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.4035 mle=1.5663 pcon=5.0360 forget=1.8012 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 0 total=8.4712 mle=1.6330 pcon=5.0369 forget=1.8014 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.3935 mle=1.5492 pcon=5.0380 forget=1.8063 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.4892 mle=1.6421 pcon=5.0393 forget=1.8078 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.5507 mle=1.7025 pcon=5.0403 forget=1.8079 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.3762 mle=1.5165 pcon=5.0413 forget=1.8184 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.4129 mle=1.5483 pcon=5.0420 forget=1.8226 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.6709 mle=1.8014 pcon=5.0428 forget=1.8268 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.4966 mle=1.6269 pcon=5.0437 forget=1.8260 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 10 total=8.4336 mle=1.5391 pcon=5.0443 forget=1.8502 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.3979 mle=1.5129 pcon=5.0452 forget=1.8397 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.2958 mle=1.4055 pcon=5.0458 forget=1.8444 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.3923 mle=1.5057 pcon=5.0466 forget=1.8400 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.4783 mle=1.5834 pcon=5.0474 forget=1.8476 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.7874 mle=1.8934 pcon=5.0478 forget=1.8462 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.8272 mle=1.9105 pcon=5.0481 forget=1.8686 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.4114 mle=1.5011 pcon=5.0486 forget=1.8617 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.6600 mle=1.7503 pcon=5.0493 forget=1.8604 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.6677 mle=1.7448 pcon=5.0501 forget=1.8727 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.4337 mle=1.5131 pcon=5.0504 forget=1.8702 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.6772 mle=1.7290 pcon=5.0509 forget=1.8973 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.7818 mle=1.8491 pcon=5.0513 forget=1.8814 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.7181 mle=1.7722 pcon=5.0519 forget=1.8941 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.5634 mle=1.6035 pcon=5.0525 forget=1.9074 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.5908 mle=1.6365 pcon=5.0528 forget=1.9015 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.4517 mle=1.4891 pcon=5.0532 forget=1.9094 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.7071 mle=1.7267 pcon=5.0538 forget=1.9266 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.7210 mle=1.7419 pcon=5.0543 forget=1.9247 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.6125 mle=1.6244 pcon=5.0550 forget=1.9331 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.6086 mle=1.6209 pcon=5.0554 forget=1.9323 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.6673 mle=1.6672 pcon=5.0559 forget=1.9442 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.6831 mle=1.6883 pcon=5.0564 forget=1.9384 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.5915 mle=1.5831 pcon=5.0572 forget=1.9512 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.6307 mle=1.6135 pcon=5.0578 forget=1.9594 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.6459 mle=1.6107 pcon=5.0582 forget=1.9770 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.6053 mle=1.5614 pcon=5.0588 forget=1.9851 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.5747 mle=1.5439 pcon=5.0593 forget=1.9716 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.7130 mle=1.6727 pcon=5.0599 forget=1.9804 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.6786 mle=1.6242 pcon=5.0609 forget=1.9935 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.6416 mle=1.5882 pcon=5.0620 forget=1.9914 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[Run] lambda=0.2 lr=0.001 epochs=25
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=25, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/25 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:168: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  4%|▍         | 1/25 [00:14<05:53, 14.73s/it]  8%|▊         | 2/25 [00:24<04:36, 12.04s/it] 12%|█▏        | 3/25 [00:35<04:07, 11.24s/it] 16%|█▌        | 4/25 [00:45<03:49, 10.94s/it] 20%|██        | 5/25 [00:54<03:26, 10.34s/it] 24%|██▍       | 6/25 [01:05<03:19, 10.52s/it][loss] ep 0 it 0 total=9.3215 mle=1.5618 pcon=5.2951 forget=2.4646 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.3417 mle=1.6308 pcon=5.2880 forget=2.4229 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.5222 mle=1.7912 pcon=5.2813 forget=2.4496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.4588 mle=1.7481 pcon=5.2750 forget=2.4357 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.5113 mle=1.8428 pcon=5.2683 forget=2.4001 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.4072 mle=1.6799 pcon=5.2618 forget=2.4655 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.3328 mle=1.6256 pcon=5.2557 forget=2.4515 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.5303 mle=1.8021 pcon=5.2498 forget=2.4784 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 1 it 10 total=9.1505 mle=1.4466 pcon=5.2437 forget=2.4601 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.3555 mle=1.6387 pcon=5.2378 forget=2.4791 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=9.1933 mle=1.5268 pcon=5.2322 forget=2.4342 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=9.3108 mle=1.6062 pcon=5.2265 forget=2.4782 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.3542 mle=1.6694 pcon=5.2211 forget=2.4636 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=9.1898 mle=1.5595 pcon=5.2158 forget=2.4145 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=9.2953 mle=1.6336 pcon=5.2102 forget=2.4514 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.3863 mle=1.7250 pcon=5.2050 forget=2.4563 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 2 it 20 total=9.0648 mle=1.4192 pcon=5.1998 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.5212 mle=1.8334 pcon=5.1947 forget=2.4932 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=9.3052 mle=1.6841 pcon=5.1895 forget=2.4315 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=9.2922 mle=1.6702 pcon=5.1848 forget=2.4372 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=9.4408 mle=1.8057 pcon=5.1798 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.3900 mle=1.7527 pcon=5.1752 forget=2.4621 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=9.0232 mle=1.4275 pcon=5.1706 forget=2.4251 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.2261 mle=1.6127 pcon=5.1666 forget=2.4467 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 3 it 30 total=9.2788 mle=1.6634 pcon=5.1623 forget=2.4531 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=9.1715 mle=1.5681 pcon=5.1582 forget=2.4452 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=9.1480 mle=1.5600 pcon=5.1540 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.3952 mle=1.8278 pcon=5.1499 forget=2.4176 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=9.3115 mle=1.7049 pcon=5.1462 forget=2.4605 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.3623 mle=1.7596 pcon=5.1425 forget=2.4601 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.4044 mle=1.8243 pcon=5.1385 forget=2.4416 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=9.2006 mle=1.6193 pcon=5.1348 forget=2.4466 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 4 it 40 total=9.1423 mle=1.5754 pcon=5.1311 forget=2.4357 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=9.1358 mle=1.5309 pcon=5.1276 forget=2.4773 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.2282 mle=1.7051 pcon=5.1245 forget=2.3987 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.2333 mle=1.6600 pcon=5.1210 forget=2.4523 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.3930 mle=1.8095 pcon=5.1177 forget=2.4658 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.0164 mle=1.4522 pcon=5.1146 forget=2.4496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.2410 mle=1.6860 pcon=5.1118 forget=2.4432 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 5 it 0 total=9.2135 mle=1.6088 pcon=5.1085 forget=2.4961 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=9.2653 mle=1.6928 pcon=5.1056 forget=2.4669 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.3822 mle=1.7998 pcon=5.1025 forget=2.4799 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.5467 mle=1.9763 pcon=5.0994 forget=2.4709 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.3239 mle=1.7905 pcon=5.0965 forget=2.4369 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=9.2513 mle=1.7138 pcon=5.0935 forget=2.4440 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=9.2809 mle=1.7677 pcon=5.0905 forget=2.4227 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=9.2407 mle=1.7190 pcon=5.0876 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 6 it 10 total=9.1400 mle=1.5563 pcon=5.0849 forget=2.4987 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=9.2203 mle=1.7051 pcon=5.0823 forget=2.4329 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=9.2530 mle=1.6996 pcon=5.0799 forget=2.4735 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=9.0338 mle=1.5105 pcon=5.0775 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=9.2065 mle=1.6766 pcon=5.0748 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=9.2962 mle=1.7616 pcon=5.0724 forget=2.4622 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=8.9789 mle=1.4948 pcon=5.0702 forget=2.4140 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 28%|██▊       | 7/25 [01:15<03:03, 10.22s/it] 32%|███▏      | 8/25 [01:24<02:46,  9.82s/it] 36%|███▌      | 9/25 [01:33<02:31,  9.46s/it] 40%|████      | 10/25 [01:42<02:21,  9.44s/it] 44%|████▍     | 11/25 [01:52<02:15,  9.66s/it] 48%|████▊     | 12/25 [01:59<01:55,  8.91s/it] 52%|█████▏    | 13/25 [02:07<01:42,  8.51s/it][loss] ep 6 it 360 total=9.1805 mle=1.7148 pcon=5.0677 forget=2.3980 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 7 it 20 total=9.0256 mle=1.5090 pcon=5.0654 forget=2.4511 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=9.0876 mle=1.5736 pcon=5.0629 forget=2.4510 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=9.2397 mle=1.7393 pcon=5.0607 forget=2.4397 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=9.2180 mle=1.7607 pcon=5.0585 forget=2.3988 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=9.1345 mle=1.6537 pcon=5.0567 forget=2.4240 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=9.3683 mle=1.8772 pcon=5.0548 forget=2.4363 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=9.0491 mle=1.5493 pcon=5.0525 forget=2.4473 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=9.3152 mle=1.7890 pcon=5.0503 forget=2.4758 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 8 it 30 total=9.2469 mle=1.7899 pcon=5.0485 forget=2.4086 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=9.0188 mle=1.5221 pcon=5.0465 forget=2.4502 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=9.2263 mle=1.7440 pcon=5.0447 forget=2.4377 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=9.1380 mle=1.6514 pcon=5.0428 forget=2.4438 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=9.0534 mle=1.5674 pcon=5.0410 forget=2.4450 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.9495 mle=1.4576 pcon=5.0392 forget=2.4528 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.1964 mle=1.7884 pcon=5.0372 forget=2.3708 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=9.1247 mle=1.6499 pcon=5.0357 forget=2.4391 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 9 it 40 total=9.2892 mle=1.7969 pcon=5.0340 forget=2.4583 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=9.1831 mle=1.7484 pcon=5.0322 forget=2.4024 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=9.1800 mle=1.6838 pcon=5.0310 forget=2.4651 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.9753 mle=1.4940 pcon=5.0295 forget=2.4517 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=9.1877 mle=1.7624 pcon=5.0282 forget=2.3971 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=9.1276 mle=1.6624 pcon=5.0265 forget=2.4386 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.9612 mle=1.5042 pcon=5.0251 forget=2.4318 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 10 it 0 total=8.9687 mle=1.4925 pcon=5.0232 forget=2.4530 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.9285 mle=1.4685 pcon=5.0221 forget=2.4378 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=9.1264 mle=1.6910 pcon=5.0206 forget=2.4148 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=9.1775 mle=1.7025 pcon=5.0192 forget=2.4558 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=9.2080 mle=1.7846 pcon=5.0180 forget=2.4054 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=9.2189 mle=1.8303 pcon=5.0167 forget=2.3719 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=9.0370 mle=1.7732 pcon=5.0153 forget=2.2484 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.8015 mle=1.5405 pcon=5.0143 forget=2.2467 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 11 it 10 total=8.9289 mle=1.7707 pcon=5.0130 forget=2.1453 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.6635 mle=1.7146 pcon=5.0121 forget=1.9368 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.4519 mle=1.5932 pcon=5.0113 forget=1.8474 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.3502 mle=1.5847 pcon=5.0105 forget=1.7549 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.2717 mle=1.5481 pcon=5.0101 forget=1.7135 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.2470 mle=1.5695 pcon=5.0098 forget=1.6677 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.5040 mle=1.7996 pcon=5.0100 forget=1.6944 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.3741 mle=1.6841 pcon=5.0104 forget=1.6797 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 12 it 20 total=8.2424 mle=1.5765 pcon=5.0105 forget=1.6553 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.1993 mle=1.5175 pcon=5.0109 forget=1.6709 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.3803 mle=1.7305 pcon=5.0115 forget=1.6382 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.3821 mle=1.7080 pcon=5.0122 forget=1.6619 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.2269 mle=1.5419 pcon=5.0133 forget=1.6716 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.2748 mle=1.5748 pcon=5.0146 forget=1.6854 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.3820 mle=1.6701 pcon=5.0158 forget=1.6961 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.4176 mle=1.6995 pcon=5.0170 forget=1.7011 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 13 it 30 total=8.4265 mle=1.7028 pcon=5.0183 forget=1.7054 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.5465 mle=1.7955 pcon=5.0196 forget=1.7314 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.4100 mle=1.6688 pcon=5.0206 forget=1.7206 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=8.5060 mle=1.7524 pcon=5.0215 forget=1.7321 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 56%|█████▌    | 14/25 [02:15<01:31,  8.28s/it] 60%|██████    | 15/25 [02:24<01:25,  8.56s/it] 64%|██████▍   | 16/25 [02:34<01:21,  9.01s/it] 68%|██████▊   | 17/25 [02:41<01:08,  8.54s/it] 72%|███████▏  | 18/25 [02:49<00:57,  8.26s/it] 76%|███████▌  | 19/25 [02:58<00:50,  8.46s/it] 80%|████████  | 20/25 [03:06<00:41,  8.30s/it] 84%|████████▍ | 21/25 [03:14<00:33,  8.42s/it][loss] ep 13 it 230 total=8.4176 mle=1.6558 pcon=5.0228 forget=1.7390 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.2175 mle=1.4527 pcon=5.0240 forget=1.7408 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.5317 mle=1.7613 pcon=5.0253 forget=1.7452 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.4159 mle=1.6425 pcon=5.0266 forget=1.7469 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 40 total=8.4722 mle=1.6848 pcon=5.0276 forget=1.7597 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.3172 mle=1.5241 pcon=5.0289 forget=1.7642 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.4468 mle=1.6600 pcon=5.0300 forget=1.7568 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.3908 mle=1.5999 pcon=5.0311 forget=1.7598 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.4599 mle=1.6555 pcon=5.0318 forget=1.7726 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.3377 mle=1.5404 pcon=5.0328 forget=1.7646 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.3676 mle=1.5652 pcon=5.0337 forget=1.7687 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 0 total=8.4301 mle=1.6301 pcon=5.0343 forget=1.7657 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.3592 mle=1.5560 pcon=5.0351 forget=1.7681 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.4386 mle=1.6384 pcon=5.0359 forget=1.7642 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.5009 mle=1.7012 pcon=5.0366 forget=1.7631 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.3161 mle=1.5095 pcon=5.0371 forget=1.7696 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.3555 mle=1.5479 pcon=5.0374 forget=1.7701 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.5998 mle=1.7906 pcon=5.0377 forget=1.7715 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.4182 mle=1.6142 pcon=5.0380 forget=1.7660 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 10 total=8.3825 mle=1.5597 pcon=5.0380 forget=1.7848 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.3233 mle=1.5125 pcon=5.0383 forget=1.7725 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.2198 mle=1.4077 pcon=5.0382 forget=1.7739 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.3113 mle=1.5044 pcon=5.0382 forget=1.7686 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.3906 mle=1.5822 pcon=5.0381 forget=1.7703 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.6896 mle=1.8803 pcon=5.0377 forget=1.7716 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.7185 mle=1.8973 pcon=5.0371 forget=1.7840 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.3410 mle=1.5317 pcon=5.0367 forget=1.7726 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.5761 mle=1.7692 pcon=5.0364 forget=1.7705 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.5653 mle=1.7480 pcon=5.0362 forget=1.7810 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.3275 mle=1.5163 pcon=5.0354 forget=1.7758 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.5739 mle=1.7375 pcon=5.0348 forget=1.8017 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.6566 mle=1.8412 pcon=5.0340 forget=1.7814 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.6050 mle=1.7873 pcon=5.0333 forget=1.7845 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.4385 mle=1.6179 pcon=5.0325 forget=1.7881 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.4416 mle=1.6222 pcon=5.0316 forget=1.7879 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.3178 mle=1.4951 pcon=5.0307 forget=1.7920 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.5526 mle=1.7163 pcon=5.0299 forget=1.8064 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.5929 mle=1.7523 pcon=5.0290 forget=1.8116 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.4768 mle=1.6516 pcon=5.0283 forget=1.7969 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.4381 mle=1.6050 pcon=5.0272 forget=1.8059 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.4868 mle=1.6416 pcon=5.0262 forget=1.8190 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.5166 mle=1.6772 pcon=5.0251 forget=1.8143 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.4590 mle=1.6230 pcon=5.0243 forget=1.8117 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.4934 mle=1.6513 pcon=5.0233 forget=1.8187 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.4745 mle=1.6245 pcon=5.0222 forget=1.8278 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.4153 mle=1.5513 pcon=5.0213 forget=1.8427 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.3916 mle=1.5425 pcon=5.0201 forget=1.8290 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.5335 mle=1.6812 pcon=5.0191 forget=1.8332 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.5121 mle=1.6515 pcon=5.0184 forget=1.8423 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.4799 mle=1.6148 pcon=5.0177 forget=1.8474 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 0 total=8.3335 mle=1.4612 pcon=5.0169 forget=1.8554 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.5549 mle=1.6821 pcon=5.0159 forget=1.8569 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.4305 mle=1.5481 pcon=5.0150 forget=1.8674 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.5536 mle=1.6734 pcon=5.0141 forget=1.8661 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.4888 mle=1.6025 pcon=5.0134 forget=1.8729 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=8.6206 mle=1.7213 pcon=5.0126 forget=1.8867 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=8.6504 mle=1.7556 pcon=5.0119 forget=1.8829 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=8.6642 mle=1.7564 pcon=5.0111 forget=1.8966 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 10 total=8.3023 mle=1.3913 pcon=5.0103 forget=1.9007 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=8.5581 mle=1.6406 pcon=5.0095 forget=1.9080 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.5380 mle=1.6069 pcon=5.0089 forget=1.9223 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.5947 mle=1.6768 pcon=5.0082 forget=1.9097 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 88%|████████▊ | 22/25 [03:22<00:24,  8.12s/it] 92%|█████████▏| 23/25 [03:32<00:17,  8.75s/it] 96%|█████████▌| 24/25 [03:42<00:09,  9.20s/it]100%|██████████| 25/25 [03:50<00:00,  8.75s/it]100%|██████████| 25/25 [03:50<00:00,  9.22s/it]
[loss] ep 21 it 210 total=8.5342 mle=1.5949 pcon=5.0074 forget=1.9318 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.8794 mle=1.9333 pcon=5.0070 forget=1.9391 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.6775 mle=1.7209 pcon=5.0063 forget=1.9503 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.6853 mle=1.7375 pcon=5.0058 forget=1.9420 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 20 total=8.6267 mle=1.6651 pcon=5.0054 forget=1.9562 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.5870 mle=1.6289 pcon=5.0050 forget=1.9530 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.6983 mle=1.7194 pcon=5.0047 forget=1.9742 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.6165 mle=1.6374 pcon=5.0042 forget=1.9749 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.7157 mle=1.7228 pcon=5.0041 forget=1.9888 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.5971 mle=1.5977 pcon=5.0039 forget=1.9954 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.5525 mle=1.5452 pcon=5.0038 forget=2.0035 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=8.7324 mle=1.7246 pcon=5.0035 forget=2.0043 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 30 total=8.6142 mle=1.6017 pcon=5.0036 forget=2.0090 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.6577 mle=1.6376 pcon=5.0034 forget=2.0167 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=8.8424 mle=1.8053 pcon=5.0033 forget=2.0338 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.6478 mle=1.6127 pcon=5.0032 forget=2.0320 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.6268 mle=1.5894 pcon=5.0032 forget=2.0341 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=8.7893 mle=1.7463 pcon=5.0033 forget=2.0397 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=8.8531 mle=1.7894 pcon=5.0032 forget=2.0605 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.5946 mle=1.5262 pcon=5.0034 forget=2.0650 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 40 total=8.6985 mle=1.6320 pcon=5.0038 forget=2.0627 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=8.8531 mle=1.7860 pcon=5.0039 forget=2.0632 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.6814 mle=1.5963 pcon=5.0041 forget=2.0810 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.7192 mle=1.6226 pcon=5.0043 forget=2.0923 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=8.7315 mle=1.6401 pcon=5.0046 forget=2.0867 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.8356 mle=1.7322 pcon=5.0047 forget=2.0986 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.6924 mle=1.5894 pcon=5.0049 forget=2.0982 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[Run] lambda=0.2 lr=0.001 epochs=30
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=30, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/30 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:168: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  3%|▎         | 1/30 [00:15<07:34, 15.68s/it]  7%|▋         | 2/30 [00:25<05:47, 12.42s/it] 10%|█         | 3/30 [00:36<05:08, 11.44s/it] 13%|█▎        | 4/30 [00:46<04:42, 10.88s/it] 17%|█▋        | 5/30 [00:55<04:21, 10.46s/it] 20%|██        | 6/30 [01:05<04:01, 10.05s/it][loss] ep 0 it 0 total=9.3216 mle=1.5619 pcon=5.2951 forget=2.4646 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.3417 mle=1.6307 pcon=5.2880 forget=2.4229 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.5222 mle=1.7912 pcon=5.2813 forget=2.4497 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.4587 mle=1.7481 pcon=5.2750 forget=2.4357 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.5113 mle=1.8429 pcon=5.2683 forget=2.4001 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.4073 mle=1.6800 pcon=5.2618 forget=2.4655 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.3332 mle=1.6259 pcon=5.2557 forget=2.4515 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.5303 mle=1.8021 pcon=5.2498 forget=2.4784 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 1 it 10 total=9.1506 mle=1.4467 pcon=5.2437 forget=2.4601 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.3556 mle=1.6387 pcon=5.2378 forget=2.4791 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=9.1933 mle=1.5268 pcon=5.2322 forget=2.4342 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=9.3110 mle=1.6063 pcon=5.2265 forget=2.4782 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.3540 mle=1.6693 pcon=5.2211 forget=2.4636 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=9.1899 mle=1.5596 pcon=5.2158 forget=2.4145 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=9.2955 mle=1.6338 pcon=5.2102 forget=2.4515 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.3864 mle=1.7251 pcon=5.2050 forget=2.4563 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 2 it 20 total=9.0648 mle=1.4193 pcon=5.1998 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.5211 mle=1.8332 pcon=5.1947 forget=2.4932 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=9.3052 mle=1.6841 pcon=5.1895 forget=2.4315 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=9.2922 mle=1.6702 pcon=5.1848 forget=2.4372 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=9.4406 mle=1.8056 pcon=5.1798 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.3898 mle=1.7526 pcon=5.1751 forget=2.4621 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=9.0231 mle=1.4274 pcon=5.1706 forget=2.4251 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.2262 mle=1.6128 pcon=5.1666 forget=2.4468 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 3 it 30 total=9.2788 mle=1.6633 pcon=5.1623 forget=2.4531 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=9.1716 mle=1.5681 pcon=5.1582 forget=2.4453 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=9.1482 mle=1.5602 pcon=5.1540 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.3950 mle=1.8276 pcon=5.1498 forget=2.4176 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=9.3113 mle=1.7047 pcon=5.1462 forget=2.4604 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.3622 mle=1.7594 pcon=5.1425 forget=2.4602 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.4045 mle=1.8244 pcon=5.1385 forget=2.4417 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=9.2004 mle=1.6191 pcon=5.1348 forget=2.4466 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 4 it 40 total=9.1424 mle=1.5755 pcon=5.1311 forget=2.4358 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=9.1359 mle=1.5309 pcon=5.1276 forget=2.4774 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.2283 mle=1.7051 pcon=5.1245 forget=2.3986 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.2331 mle=1.6598 pcon=5.1210 forget=2.4523 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.3928 mle=1.8093 pcon=5.1177 forget=2.4658 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.0165 mle=1.4523 pcon=5.1146 forget=2.4496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.2409 mle=1.6859 pcon=5.1118 forget=2.4432 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 5 it 0 total=9.2136 mle=1.6090 pcon=5.1085 forget=2.4962 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=9.2658 mle=1.6933 pcon=5.1056 forget=2.4669 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.3822 mle=1.7998 pcon=5.1025 forget=2.4799 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.5469 mle=1.9765 pcon=5.0994 forget=2.4709 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.3241 mle=1.7907 pcon=5.0965 forget=2.4370 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=9.2513 mle=1.7138 pcon=5.0935 forget=2.4439 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=9.2808 mle=1.7676 pcon=5.0906 forget=2.4226 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=9.2406 mle=1.7189 pcon=5.0876 forget=2.4341 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 6 it 10 total=9.1402 mle=1.5565 pcon=5.0850 forget=2.4987 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=9.2207 mle=1.7054 pcon=5.0823 forget=2.4329 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=9.2530 mle=1.6996 pcon=5.0799 forget=2.4734 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=9.0338 mle=1.5104 pcon=5.0775 forget=2.4459 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=9.2066 mle=1.6766 pcon=5.0748 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=9.2963 mle=1.7617 pcon=5.0724 forget=2.4622 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=8.9790 mle=1.4948 pcon=5.0702 forget=2.4140 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 23%|██▎       | 7/30 [01:15<03:51, 10.05s/it] 27%|██▋       | 8/30 [01:26<03:50, 10.47s/it] 30%|███       | 9/30 [01:37<03:42, 10.57s/it] 33%|███▎      | 10/30 [01:47<03:27, 10.40s/it] 37%|███▋      | 11/30 [01:57<03:17, 10.41s/it] 40%|████      | 12/30 [02:06<02:56,  9.82s/it] 43%|████▎     | 13/30 [02:14<02:41,  9.48s/it][loss] ep 6 it 360 total=9.1805 mle=1.7147 pcon=5.0677 forget=2.3980 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 7 it 20 total=9.0255 mle=1.5089 pcon=5.0655 forget=2.4511 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=9.0875 mle=1.5736 pcon=5.0629 forget=2.4510 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=9.2396 mle=1.7392 pcon=5.0607 forget=2.4398 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=9.2178 mle=1.7605 pcon=5.0585 forget=2.3988 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=9.1344 mle=1.6536 pcon=5.0567 forget=2.4240 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=9.3685 mle=1.8774 pcon=5.0548 forget=2.4363 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=9.0490 mle=1.5492 pcon=5.0525 forget=2.4473 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=9.3153 mle=1.7892 pcon=5.0503 forget=2.4758 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 8 it 30 total=9.2470 mle=1.7900 pcon=5.0485 forget=2.4085 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=9.0191 mle=1.5224 pcon=5.0465 forget=2.4502 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=9.2263 mle=1.7439 pcon=5.0447 forget=2.4377 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=9.1381 mle=1.6515 pcon=5.0428 forget=2.4438 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=9.0531 mle=1.5671 pcon=5.0410 forget=2.4450 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.9495 mle=1.4576 pcon=5.0392 forget=2.4528 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.1964 mle=1.7884 pcon=5.0372 forget=2.3707 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=9.1248 mle=1.6500 pcon=5.0357 forget=2.4391 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 9 it 40 total=9.2892 mle=1.7969 pcon=5.0340 forget=2.4583 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=9.1829 mle=1.7482 pcon=5.0322 forget=2.4024 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=9.1801 mle=1.6840 pcon=5.0310 forget=2.4651 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.9753 mle=1.4940 pcon=5.0295 forget=2.4518 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=9.1878 mle=1.7625 pcon=5.0282 forget=2.3971 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=9.1277 mle=1.6625 pcon=5.0265 forget=2.4386 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.9610 mle=1.5041 pcon=5.0251 forget=2.4318 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 10 it 0 total=8.9685 mle=1.4923 pcon=5.0232 forget=2.4530 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.9282 mle=1.4682 pcon=5.0221 forget=2.4378 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=9.1264 mle=1.6910 pcon=5.0206 forget=2.4148 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=9.1777 mle=1.7027 pcon=5.0192 forget=2.4558 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=9.2081 mle=1.7848 pcon=5.0180 forget=2.4053 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=9.2188 mle=1.8302 pcon=5.0167 forget=2.3719 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=9.0365 mle=1.7731 pcon=5.0153 forget=2.2481 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.8004 mle=1.5403 pcon=5.0143 forget=2.2458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 11 it 10 total=8.9271 mle=1.7706 pcon=5.0130 forget=2.1435 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.6609 mle=1.7152 pcon=5.0121 forget=1.9337 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.4492 mle=1.5933 pcon=5.0113 forget=1.8446 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.3483 mle=1.5850 pcon=5.0105 forget=1.7528 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.2703 mle=1.5485 pcon=5.0101 forget=1.7117 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.2482 mle=1.5720 pcon=5.0099 forget=1.6663 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.5017 mle=1.7993 pcon=5.0100 forget=1.6924 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.3719 mle=1.6844 pcon=5.0104 forget=1.6771 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 12 it 20 total=8.2395 mle=1.5760 pcon=5.0105 forget=1.6530 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.1971 mle=1.5174 pcon=5.0109 forget=1.6688 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.3787 mle=1.7299 pcon=5.0115 forget=1.6372 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.3809 mle=1.7080 pcon=5.0122 forget=1.6607 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.2249 mle=1.5414 pcon=5.0134 forget=1.6701 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.2721 mle=1.5739 pcon=5.0146 forget=1.6837 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.3802 mle=1.6700 pcon=5.0158 forget=1.6945 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.4150 mle=1.6989 pcon=5.0169 forget=1.6992 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 13 it 30 total=8.4242 mle=1.7028 pcon=5.0183 forget=1.7031 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.5427 mle=1.7943 pcon=5.0196 forget=1.7288 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.4060 mle=1.6679 pcon=5.0205 forget=1.7175 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=8.5053 mle=1.7553 pcon=5.0213 forget=1.7287 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 47%|████▋     | 14/30 [02:25<02:37,  9.86s/it] 50%|█████     | 15/30 [02:36<02:32, 10.19s/it] 53%|█████▎    | 16/30 [02:43<02:10,  9.31s/it] 57%|█████▋    | 17/30 [02:54<02:04,  9.59s/it] 60%|██████    | 18/30 [03:04<01:56,  9.71s/it] 63%|██████▎   | 19/30 [03:14<01:47,  9.77s/it] 67%|██████▋   | 20/30 [03:24<01:39,  9.97s/it] 70%|███████   | 21/30 [03:34<01:29,  9.93s/it][loss] ep 13 it 230 total=8.4129 mle=1.6555 pcon=5.0226 forget=1.7348 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.2130 mle=1.4530 pcon=5.0238 forget=1.7362 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.5265 mle=1.7616 pcon=5.0251 forget=1.7399 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.4089 mle=1.6416 pcon=5.0263 forget=1.7410 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 40 total=8.4659 mle=1.6854 pcon=5.0273 forget=1.7532 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.3101 mle=1.5250 pcon=5.0285 forget=1.7566 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.4368 mle=1.6589 pcon=5.0295 forget=1.7483 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.3805 mle=1.5991 pcon=5.0305 forget=1.7509 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.4484 mle=1.6546 pcon=5.0311 forget=1.7627 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.3284 mle=1.5432 pcon=5.0320 forget=1.7532 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.3541 mle=1.5648 pcon=5.0328 forget=1.7565 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 0 total=8.4116 mle=1.6261 pcon=5.0333 forget=1.7523 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.3464 mle=1.5588 pcon=5.0339 forget=1.7537 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.4196 mle=1.6371 pcon=5.0346 forget=1.7480 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.4824 mle=1.7010 pcon=5.0350 forget=1.7464 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.2941 mle=1.5075 pcon=5.0353 forget=1.7513 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.3348 mle=1.5490 pcon=5.0354 forget=1.7504 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.5701 mle=1.7844 pcon=5.0354 forget=1.7504 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.3880 mle=1.6090 pcon=5.0354 forget=1.7435 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 10 total=8.3623 mle=1.5673 pcon=5.0351 forget=1.7599 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.2972 mle=1.5150 pcon=5.0350 forget=1.7472 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.1915 mle=1.4099 pcon=5.0345 forget=1.7471 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.2779 mle=1.5025 pcon=5.0341 forget=1.7412 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.3532 mle=1.5790 pcon=5.0336 forget=1.7406 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.6548 mle=1.8794 pcon=5.0327 forget=1.7427 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.6812 mle=1.8989 pcon=5.0316 forget=1.7508 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.3121 mle=1.5439 pcon=5.0306 forget=1.7376 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.5493 mle=1.7840 pcon=5.0297 forget=1.7356 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.5277 mle=1.7550 pcon=5.0288 forget=1.7438 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.2936 mle=1.5285 pcon=5.0274 forget=1.7378 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.5241 mle=1.7362 pcon=5.0260 forget=1.7619 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.6021 mle=1.8380 pcon=5.0245 forget=1.7397 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.5596 mle=1.7981 pcon=5.0229 forget=1.7386 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.3884 mle=1.6287 pcon=5.0214 forget=1.7383 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.3811 mle=1.6213 pcon=5.0196 forget=1.7402 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.2605 mle=1.5010 pcon=5.0179 forget=1.7416 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.4912 mle=1.7226 pcon=5.0162 forget=1.7524 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.5430 mle=1.7695 pcon=5.0144 forget=1.7590 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.4104 mle=1.6611 pcon=5.0127 forget=1.7365 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.3611 mle=1.6035 pcon=5.0107 forget=1.7468 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.4024 mle=1.6359 pcon=5.0087 forget=1.7578 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.4404 mle=1.6809 pcon=5.0067 forget=1.7529 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.3853 mle=1.6354 pcon=5.0049 forget=1.7450 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.4127 mle=1.6614 pcon=5.0030 forget=1.7484 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.3863 mle=1.6335 pcon=5.0008 forget=1.7519 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.3231 mle=1.5573 pcon=4.9989 forget=1.7669 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.3035 mle=1.5532 pcon=4.9968 forget=1.7534 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.4368 mle=1.6888 pcon=4.9947 forget=1.7533 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.4187 mle=1.6701 pcon=4.9930 forget=1.7556 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.3877 mle=1.6352 pcon=4.9912 forget=1.7613 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 0 total=8.2300 mle=1.4730 pcon=4.9894 forget=1.7676 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.4448 mle=1.6954 pcon=4.9874 forget=1.7620 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.2993 mle=1.5478 pcon=4.9855 forget=1.7661 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.4142 mle=1.6654 pcon=4.9835 forget=1.7654 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.3675 mle=1.6130 pcon=4.9817 forget=1.7729 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=8.4972 mle=1.7421 pcon=4.9798 forget=1.7753 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=8.5298 mle=1.7813 pcon=4.9781 forget=1.7705 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=8.5080 mle=1.7446 pcon=4.9763 forget=1.7871 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 10 total=8.1781 mle=1.4223 pcon=4.9743 forget=1.7815 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=8.4229 mle=1.6620 pcon=4.9725 forget=1.7884 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.3990 mle=1.6272 pcon=4.9708 forget=1.8010 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.4487 mle=1.6933 pcon=4.9690 forget=1.7863 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 73%|███████▎  | 22/30 [03:43<01:17,  9.72s/it] 77%|███████▋  | 23/30 [03:54<01:10, 10.03s/it] 80%|████████  | 24/30 [04:05<01:02, 10.36s/it] 83%|████████▎ | 25/30 [04:16<00:52, 10.51s/it] 87%|████████▋ | 26/30 [04:25<00:39,  9.99s/it] 90%|█████████ | 27/30 [04:33<00:28,  9.45s/it] 93%|█████████▎| 28/30 [04:45<00:20, 10.18s/it] 97%|█████████▋| 29/30 [04:54<00:09,  9.88s/it][loss] ep 21 it 210 total=8.3766 mle=1.5995 pcon=4.9672 forget=1.8100 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.7109 mle=1.9406 pcon=4.9657 forget=1.8046 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.5122 mle=1.7351 pcon=4.9641 forget=1.8130 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.4892 mle=1.7178 pcon=4.9624 forget=1.8090 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 20 total=8.4811 mle=1.7018 pcon=4.9610 forget=1.8183 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.4174 mle=1.6456 pcon=4.9596 forget=1.8122 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.5287 mle=1.7413 pcon=4.9583 forget=1.8292 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.4200 mle=1.6379 pcon=4.9568 forget=1.8253 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.5564 mle=1.7624 pcon=4.9556 forget=1.8384 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.4247 mle=1.6221 pcon=4.9544 forget=1.8481 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.3409 mle=1.5448 pcon=4.9534 forget=1.8427 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=8.5189 mle=1.7213 pcon=4.9520 forget=1.8455 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 30 total=8.4190 mle=1.6173 pcon=4.9511 forget=1.8506 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.4455 mle=1.6354 pcon=4.9499 forget=1.8602 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=8.6514 mle=1.8333 pcon=4.9488 forget=1.8692 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.4518 mle=1.6293 pcon=4.9477 forget=1.8748 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.4557 mle=1.6292 pcon=4.9468 forget=1.8797 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=8.5877 mle=1.7648 pcon=4.9459 forget=1.8770 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=8.6294 mle=1.7883 pcon=4.9449 forget=1.8962 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.3683 mle=1.5260 pcon=4.9441 forget=1.8982 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 40 total=8.4963 mle=1.6509 pcon=4.9435 forget=1.9019 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=8.6403 mle=1.7946 pcon=4.9426 forget=1.9031 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.4639 mle=1.6076 pcon=4.9418 forget=1.9145 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.4985 mle=1.6319 pcon=4.9411 forget=1.9255 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=8.5261 mle=1.6605 pcon=4.9404 forget=1.9252 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.5982 mle=1.7250 pcon=4.9396 forget=1.9336 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.4827 mle=1.6033 pcon=4.9389 forget=1.9405 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 0 total=8.6212 mle=1.7311 pcon=4.9383 forget=1.9518 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 50 total=8.5809 mle=1.6783 pcon=4.9377 forget=1.9650 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 100 total=8.6405 mle=1.7405 pcon=4.9371 forget=1.9629 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 150 total=8.6460 mle=1.7417 pcon=4.9368 forget=1.9675 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 200 total=8.7339 mle=1.8239 pcon=4.9364 forget=1.9736 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 250 total=8.5349 mle=1.6144 pcon=4.9361 forget=1.9843 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 300 total=8.6099 mle=1.6739 pcon=4.9358 forget=2.0001 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 350 total=8.7231 mle=1.7927 pcon=4.9354 forget=1.9951 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 10 total=8.6630 mle=1.7232 pcon=4.9352 forget=2.0046 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 60 total=8.6912 mle=1.7442 pcon=4.9348 forget=2.0123 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 110 total=8.7037 mle=1.7434 pcon=4.9345 forget=2.0258 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 160 total=8.5505 mle=1.5885 pcon=4.9342 forget=2.0278 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 210 total=8.7071 mle=1.7392 pcon=4.9341 forget=2.0338 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 260 total=8.6583 mle=1.6877 pcon=4.9342 forget=2.0364 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 310 total=8.8592 mle=1.8827 pcon=4.9340 forget=2.0426 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 360 total=8.8342 mle=1.8513 pcon=4.9340 forget=2.0489 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 20 total=8.5699 mle=1.5784 pcon=4.9339 forget=2.0576 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 70 total=8.6487 mle=1.6597 pcon=4.9337 forget=2.0553 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 120 total=8.7028 mle=1.6974 pcon=4.9339 forget=2.0716 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 170 total=8.5593 mle=1.5468 pcon=4.9340 forget=2.0785 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 220 total=8.7003 mle=1.6869 pcon=4.9342 forget=2.0792 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 270 total=8.6133 mle=1.5960 pcon=4.9343 forget=2.0830 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 320 total=8.7661 mle=1.7320 pcon=4.9346 forget=2.0996 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 370 total=8.6149 mle=1.5793 pcon=4.9348 forget=2.1008 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 30 total=8.7570 mle=1.7154 pcon=4.9351 forget=2.1065 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 80 total=8.8228 mle=1.7714 pcon=4.9352 forget=2.1162 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 130 total=8.5457 mle=1.4876 pcon=4.9356 forget=2.1225 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 180 total=8.5554 mle=1.4916 pcon=4.9356 forget=2.1282 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 230 total=8.6795 mle=1.6119 pcon=4.9360 forget=2.1316 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 280 total=8.7587 mle=1.6846 pcon=4.9364 forget=2.1377 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 330 total=8.6771 mle=1.5988 pcon=4.9365 forget=2.1419 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 380 total=8.7767 mle=1.6863 pcon=4.9367 forget=2.1537 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 40 total=8.7430 mle=1.6468 pcon=4.9370 forget=2.1592 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 90 total=8.7175 mle=1.6171 pcon=4.9373 forget=2.1631 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 140 total=8.6750 mle=1.5712 pcon=4.9377 forget=2.1661 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
100%|██████████| 30/30 [05:05<00:00, 10.14s/it]100%|██████████| 30/30 [05:05<00:00, 10.17s/it]
[loss] ep 29 it 190 total=9.0262 mle=1.9224 pcon=4.9380 forget=2.1659 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 240 total=8.8382 mle=1.7275 pcon=4.9386 forget=2.1721 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 290 total=8.7854 mle=1.6676 pcon=4.9387 forget=2.1791 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 340 total=8.8123 mle=1.6881 pcon=4.9393 forget=2.1849 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[Run] lambda=0.2 lr=0.001 epochs=35
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=35, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/35 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:168: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  3%|▎         | 1/35 [00:15<08:38, 15.24s/it]  6%|▌         | 2/35 [00:24<06:33, 11.92s/it]  9%|▊         | 3/35 [00:35<05:58, 11.22s/it] 11%|█▏        | 4/35 [00:45<05:39, 10.95s/it] 14%|█▍        | 5/35 [00:55<05:17, 10.58s/it] 17%|█▋        | 6/35 [01:03<04:42,  9.74s/it][loss] ep 0 it 0 total=9.3216 mle=1.5619 pcon=5.2951 forget=2.4646 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.3417 mle=1.6307 pcon=5.2880 forget=2.4229 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.5222 mle=1.7912 pcon=5.2813 forget=2.4497 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.4587 mle=1.7481 pcon=5.2750 forget=2.4357 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.5113 mle=1.8429 pcon=5.2683 forget=2.4001 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.4073 mle=1.6800 pcon=5.2618 forget=2.4655 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.3332 mle=1.6259 pcon=5.2557 forget=2.4515 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.5303 mle=1.8021 pcon=5.2498 forget=2.4784 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 1 it 10 total=9.1506 mle=1.4467 pcon=5.2437 forget=2.4601 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.3556 mle=1.6387 pcon=5.2378 forget=2.4791 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=9.1933 mle=1.5268 pcon=5.2322 forget=2.4342 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=9.3110 mle=1.6063 pcon=5.2265 forget=2.4782 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.3540 mle=1.6693 pcon=5.2211 forget=2.4636 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=9.1899 mle=1.5596 pcon=5.2158 forget=2.4145 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=9.2954 mle=1.6338 pcon=5.2102 forget=2.4514 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.3864 mle=1.7251 pcon=5.2050 forget=2.4564 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 2 it 20 total=9.0648 mle=1.4192 pcon=5.1998 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.5212 mle=1.8334 pcon=5.1947 forget=2.4932 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=9.3052 mle=1.6841 pcon=5.1895 forget=2.4315 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=9.2923 mle=1.6703 pcon=5.1848 forget=2.4373 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=9.4408 mle=1.8057 pcon=5.1798 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.3898 mle=1.7525 pcon=5.1752 forget=2.4621 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=9.0231 mle=1.4275 pcon=5.1706 forget=2.4251 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.2260 mle=1.6127 pcon=5.1666 forget=2.4467 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 3 it 30 total=9.2787 mle=1.6632 pcon=5.1623 forget=2.4531 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=9.1715 mle=1.5681 pcon=5.1582 forget=2.4452 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=9.1482 mle=1.5602 pcon=5.1540 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.3952 mle=1.8278 pcon=5.1498 forget=2.4176 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=9.3115 mle=1.7048 pcon=5.1462 forget=2.4605 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.3622 mle=1.7596 pcon=5.1425 forget=2.4602 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.4045 mle=1.8245 pcon=5.1384 forget=2.4416 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=9.2007 mle=1.6194 pcon=5.1348 forget=2.4466 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 4 it 40 total=9.1422 mle=1.5754 pcon=5.1311 forget=2.4357 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=9.1358 mle=1.5309 pcon=5.1276 forget=2.4773 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.2282 mle=1.7051 pcon=5.1245 forget=2.3986 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.2330 mle=1.6597 pcon=5.1210 forget=2.4523 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.3929 mle=1.8093 pcon=5.1177 forget=2.4658 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.0164 mle=1.4522 pcon=5.1146 forget=2.4496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.2409 mle=1.6860 pcon=5.1118 forget=2.4432 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 5 it 0 total=9.2135 mle=1.6088 pcon=5.1085 forget=2.4962 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=9.2656 mle=1.6931 pcon=5.1056 forget=2.4669 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.3821 mle=1.7997 pcon=5.1025 forget=2.4799 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.5468 mle=1.9764 pcon=5.0994 forget=2.4709 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.3238 mle=1.7903 pcon=5.0965 forget=2.4369 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=9.2515 mle=1.7139 pcon=5.0935 forget=2.4440 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=9.2807 mle=1.7675 pcon=5.0905 forget=2.4226 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=9.2406 mle=1.7189 pcon=5.0876 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 6 it 10 total=9.1400 mle=1.5563 pcon=5.0849 forget=2.4987 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=9.2207 mle=1.7054 pcon=5.0824 forget=2.4329 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=9.2530 mle=1.6996 pcon=5.0799 forget=2.4735 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=9.0336 mle=1.5103 pcon=5.0775 forget=2.4459 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=9.2064 mle=1.6765 pcon=5.0747 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=9.2962 mle=1.7616 pcon=5.0724 forget=2.4622 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=8.9789 mle=1.4948 pcon=5.0702 forget=2.4140 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 20%|██        | 7/35 [01:13<04:31,  9.70s/it] 23%|██▎       | 8/35 [01:22<04:19,  9.60s/it] 26%|██▌       | 9/35 [01:32<04:10,  9.64s/it] 29%|██▊       | 10/35 [01:42<04:04,  9.77s/it] 31%|███▏      | 11/35 [01:52<03:53,  9.74s/it] 34%|███▍      | 12/35 [02:02<03:46,  9.84s/it] 37%|███▋      | 13/35 [02:12<03:36,  9.86s/it][loss] ep 6 it 360 total=9.1805 mle=1.7148 pcon=5.0677 forget=2.3980 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 7 it 20 total=9.0255 mle=1.5089 pcon=5.0654 forget=2.4511 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=9.0875 mle=1.5735 pcon=5.0629 forget=2.4511 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=9.2397 mle=1.7393 pcon=5.0607 forget=2.4397 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=9.2180 mle=1.7607 pcon=5.0585 forget=2.3988 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=9.1345 mle=1.6538 pcon=5.0567 forget=2.4240 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=9.3683 mle=1.8771 pcon=5.0548 forget=2.4364 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=9.0489 mle=1.5491 pcon=5.0525 forget=2.4474 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=9.3154 mle=1.7893 pcon=5.0503 forget=2.4758 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 8 it 30 total=9.2468 mle=1.7898 pcon=5.0485 forget=2.4085 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=9.0188 mle=1.5220 pcon=5.0465 forget=2.4502 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=9.2261 mle=1.7438 pcon=5.0447 forget=2.4377 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=9.1381 mle=1.6515 pcon=5.0428 forget=2.4439 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=9.0533 mle=1.5673 pcon=5.0410 forget=2.4450 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.9496 mle=1.4577 pcon=5.0392 forget=2.4528 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.1963 mle=1.7883 pcon=5.0372 forget=2.3708 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=9.1248 mle=1.6499 pcon=5.0357 forget=2.4392 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 9 it 40 total=9.2893 mle=1.7970 pcon=5.0340 forget=2.4583 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=9.1831 mle=1.7484 pcon=5.0322 forget=2.4024 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=9.1799 mle=1.6838 pcon=5.0310 forget=2.4651 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.9753 mle=1.4941 pcon=5.0295 forget=2.4517 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=9.1878 mle=1.7625 pcon=5.0282 forget=2.3971 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=9.1277 mle=1.6626 pcon=5.0265 forget=2.4386 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.9610 mle=1.5041 pcon=5.0252 forget=2.4318 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 10 it 0 total=8.9685 mle=1.4922 pcon=5.0232 forget=2.4530 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.9283 mle=1.4683 pcon=5.0222 forget=2.4378 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=9.1263 mle=1.6909 pcon=5.0206 forget=2.4148 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=9.1775 mle=1.7026 pcon=5.0192 forget=2.4558 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=9.2081 mle=1.7848 pcon=5.0180 forget=2.4053 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=9.2189 mle=1.8304 pcon=5.0167 forget=2.3718 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=9.0367 mle=1.7734 pcon=5.0153 forget=2.2480 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.8003 mle=1.5406 pcon=5.0144 forget=2.2454 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 11 it 10 total=8.9262 mle=1.7706 pcon=5.0130 forget=2.1426 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.6594 mle=1.7150 pcon=5.0121 forget=1.9323 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.4478 mle=1.5932 pcon=5.0113 forget=1.8433 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.3474 mle=1.5850 pcon=5.0105 forget=1.7518 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.2693 mle=1.5483 pcon=5.0101 forget=1.7108 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.2477 mle=1.5721 pcon=5.0099 forget=1.6657 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.5012 mle=1.7997 pcon=5.0100 forget=1.6915 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.3704 mle=1.6840 pcon=5.0104 forget=1.6760 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 12 it 20 total=8.2398 mle=1.5773 pcon=5.0105 forget=1.6520 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.1959 mle=1.5172 pcon=5.0109 forget=1.6678 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.3781 mle=1.7298 pcon=5.0115 forget=1.6368 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.3806 mle=1.7082 pcon=5.0122 forget=1.6602 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.2241 mle=1.5413 pcon=5.0134 forget=1.6694 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.2710 mle=1.5734 pcon=5.0146 forget=1.6829 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.3792 mle=1.6696 pcon=5.0158 forget=1.6937 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.4138 mle=1.6985 pcon=5.0169 forget=1.6983 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 13 it 30 total=8.4234 mle=1.7030 pcon=5.0183 forget=1.7021 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.5411 mle=1.7940 pcon=5.0195 forget=1.7276 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.4043 mle=1.6677 pcon=5.0205 forget=1.7161 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=8.5018 mle=1.7535 pcon=5.0213 forget=1.7270 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 40%|████      | 14/35 [02:20<03:18,  9.43s/it] 43%|████▎     | 15/35 [02:28<02:58,  8.94s/it] 46%|████▌     | 16/35 [02:35<02:40,  8.43s/it] 49%|████▊     | 17/35 [02:43<02:26,  8.13s/it] 51%|█████▏    | 18/35 [02:50<02:14,  7.93s/it] 54%|█████▍    | 19/35 [02:57<02:01,  7.62s/it] 57%|█████▋    | 20/35 [03:04<01:52,  7.50s/it] 60%|██████    | 21/35 [03:12<01:47,  7.64s/it][loss] ep 13 it 230 total=8.4109 mle=1.6555 pcon=5.0225 forget=1.7329 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.2107 mle=1.4529 pcon=5.0237 forget=1.7341 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.5242 mle=1.7618 pcon=5.0249 forget=1.7375 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.3987 mle=1.6342 pcon=5.0261 forget=1.7383 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 40 total=8.4628 mle=1.6854 pcon=5.0272 forget=1.7502 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.3068 mle=1.5254 pcon=5.0283 forget=1.7531 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.4321 mle=1.6585 pcon=5.0293 forget=1.7444 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.3758 mle=1.5988 pcon=5.0302 forget=1.7467 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.4377 mle=1.6489 pcon=5.0308 forget=1.7580 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.3226 mle=1.5431 pcon=5.0316 forget=1.7479 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.3477 mle=1.5645 pcon=5.0324 forget=1.7508 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 0 total=8.4066 mle=1.6279 pcon=5.0328 forget=1.7460 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.3406 mle=1.5603 pcon=5.0333 forget=1.7469 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.4110 mle=1.6367 pcon=5.0339 forget=1.7404 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.4753 mle=1.7025 pcon=5.0342 forget=1.7386 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.2836 mle=1.5064 pcon=5.0344 forget=1.7428 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.3251 mle=1.5496 pcon=5.0344 forget=1.7411 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.5581 mle=1.7835 pcon=5.0342 forget=1.7404 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.3740 mle=1.6068 pcon=5.0342 forget=1.7330 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 10 total=8.3528 mle=1.5709 pcon=5.0336 forget=1.7483 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.2844 mle=1.5157 pcon=5.0333 forget=1.7353 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.1780 mle=1.4109 pcon=5.0326 forget=1.7345 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.2631 mle=1.5029 pcon=5.0320 forget=1.7282 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.3342 mle=1.5762 pcon=5.0313 forget=1.7267 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.6350 mle=1.8758 pcon=5.0301 forget=1.7292 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.6591 mle=1.8952 pcon=5.0287 forget=1.7352 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.2966 mle=1.5481 pcon=5.0274 forget=1.7211 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.5338 mle=1.7884 pcon=5.0262 forget=1.7192 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.5069 mle=1.7558 pcon=5.0250 forget=1.7261 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.2758 mle=1.5329 pcon=5.0232 forget=1.7197 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.5007 mle=1.7367 pcon=5.0214 forget=1.7426 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.5685 mle=1.8294 pcon=5.0195 forget=1.7197 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.5378 mle=1.8036 pcon=5.0176 forget=1.7166 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.3638 mle=1.6334 pcon=5.0156 forget=1.7147 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.3621 mle=1.6312 pcon=5.0134 forget=1.7175 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.2414 mle=1.5128 pcon=5.0113 forget=1.7173 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.4640 mle=1.7287 pcon=5.0091 forget=1.7261 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.5155 mle=1.7758 pcon=5.0069 forget=1.7327 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.3775 mle=1.6655 pcon=5.0048 forget=1.7072 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.3268 mle=1.6065 pcon=5.0024 forget=1.7179 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.3645 mle=1.6376 pcon=4.9999 forget=1.7270 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.4048 mle=1.6858 pcon=4.9975 forget=1.7216 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.3424 mle=1.6357 pcon=4.9952 forget=1.7115 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.3784 mle=1.6728 pcon=4.9928 forget=1.7128 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.3491 mle=1.6450 pcon=4.9902 forget=1.7138 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.2825 mle=1.5668 pcon=4.9878 forget=1.7279 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.2611 mle=1.5616 pcon=4.9852 forget=1.7143 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.3889 mle=1.6935 pcon=4.9827 forget=1.7126 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.3658 mle=1.6749 pcon=4.9804 forget=1.7105 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.3334 mle=1.6393 pcon=4.9782 forget=1.7159 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 0 total=8.1792 mle=1.4827 pcon=4.9758 forget=1.7207 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.3809 mle=1.6958 pcon=4.9733 forget=1.7118 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.2358 mle=1.5524 pcon=4.9709 forget=1.7125 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.3477 mle=1.6673 pcon=4.9684 forget=1.7120 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.3032 mle=1.6186 pcon=4.9660 forget=1.7186 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=8.4272 mle=1.7482 pcon=4.9636 forget=1.7154 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=8.4615 mle=1.7907 pcon=4.9613 forget=1.7095 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=8.4279 mle=1.7420 pcon=4.9590 forget=1.7269 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 10 total=8.1029 mle=1.4304 pcon=4.9565 forget=1.7161 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=8.3446 mle=1.6689 pcon=4.9541 forget=1.7216 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.3211 mle=1.6365 pcon=4.9518 forget=1.7328 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.3645 mle=1.6973 pcon=4.9495 forget=1.7177 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 63%|██████▎   | 22/35 [03:21<01:42,  7.92s/it] 66%|██████▌   | 23/35 [03:29<01:35,  7.94s/it] 69%|██████▊   | 24/35 [03:37<01:29,  8.16s/it] 71%|███████▏  | 25/35 [03:45<01:18,  7.88s/it] 74%|███████▍  | 26/35 [03:53<01:11,  7.98s/it] 77%|███████▋  | 27/35 [04:01<01:04,  8.06s/it] 80%|████████  | 28/35 [04:10<00:57,  8.25s/it] 83%|████████▎ | 29/35 [04:18<00:49,  8.28s/it][loss] ep 21 it 210 total=8.2873 mle=1.6013 pcon=4.9471 forget=1.7389 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.6081 mle=1.9357 pcon=4.9450 forget=1.7274 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.4072 mle=1.7314 pcon=4.9427 forget=1.7331 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.3923 mle=1.7220 pcon=4.9405 forget=1.7298 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 20 total=8.3813 mle=1.7074 pcon=4.9385 forget=1.7354 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.3124 mle=1.6479 pcon=4.9365 forget=1.7281 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.4201 mle=1.7450 pcon=4.9345 forget=1.7407 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.3026 mle=1.6354 pcon=4.9324 forget=1.7348 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.4469 mle=1.7729 pcon=4.9306 forget=1.7434 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.3095 mle=1.6278 pcon=4.9288 forget=1.7529 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.2074 mle=1.5402 pcon=4.9270 forget=1.7402 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=8.3832 mle=1.7142 pcon=4.9250 forget=1.7440 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 30 total=8.2948 mle=1.6267 pcon=4.9234 forget=1.7447 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.3134 mle=1.6380 pcon=4.9215 forget=1.7539 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=8.5131 mle=1.8356 pcon=4.9197 forget=1.7578 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.3205 mle=1.6390 pcon=4.9179 forget=1.7636 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.3221 mle=1.6375 pcon=4.9163 forget=1.7683 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=8.4396 mle=1.7655 pcon=4.9146 forget=1.7595 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=8.4763 mle=1.7888 pcon=4.9129 forget=1.7746 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.2200 mle=1.5349 pcon=4.9113 forget=1.7738 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 40 total=8.3420 mle=1.6540 pcon=4.9099 forget=1.7780 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=8.4784 mle=1.7934 pcon=4.9083 forget=1.7767 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.3002 mle=1.6125 pcon=4.9067 forget=1.7810 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.3247 mle=1.6315 pcon=4.9052 forget=1.7880 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=8.3457 mle=1.6533 pcon=4.9039 forget=1.7885 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.4069 mle=1.7137 pcon=4.9022 forget=1.7909 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.3171 mle=1.6158 pcon=4.9007 forget=1.8006 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 0 total=8.4419 mle=1.7378 pcon=4.8994 forget=1.8047 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 50 total=8.3816 mle=1.6649 pcon=4.8980 forget=1.8187 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 100 total=8.4447 mle=1.7346 pcon=4.8967 forget=1.8135 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 150 total=8.4454 mle=1.7365 pcon=4.8956 forget=1.8133 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 200 total=8.5143 mle=1.8023 pcon=4.8944 forget=1.8177 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 250 total=8.3314 mle=1.6083 pcon=4.8933 forget=1.8298 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 300 total=8.4148 mle=1.6831 pcon=4.8922 forget=1.8395 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 350 total=8.5328 mle=1.8109 pcon=4.8911 forget=1.8309 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 10 total=8.4551 mle=1.7246 pcon=4.8901 forget=1.8404 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 60 total=8.4746 mle=1.7452 pcon=4.8889 forget=1.8405 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 110 total=8.4846 mle=1.7343 pcon=4.8878 forget=1.8624 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 160 total=8.3297 mle=1.5873 pcon=4.8867 forget=1.8557 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 210 total=8.4820 mle=1.7366 pcon=4.8858 forget=1.8596 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 260 total=8.4401 mle=1.6949 pcon=4.8851 forget=1.8600 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 310 total=8.6191 mle=1.8648 pcon=4.8841 forget=1.8701 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 360 total=8.6252 mle=1.8683 pcon=4.8832 forget=1.8737 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 20 total=8.3456 mle=1.5878 pcon=4.8824 forget=1.8755 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 70 total=8.4207 mle=1.6610 pcon=4.8813 forget=1.8784 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 120 total=8.4770 mle=1.7036 pcon=4.8807 forget=1.8927 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 170 total=8.3321 mle=1.5527 pcon=4.8799 forget=1.8996 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 220 total=8.4531 mle=1.6806 pcon=4.8793 forget=1.8932 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 270 total=8.3699 mle=1.5948 pcon=4.8785 forget=1.8967 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 320 total=8.5167 mle=1.7185 pcon=4.8779 forget=1.9204 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 370 total=8.3613 mle=1.5731 pcon=4.8772 forget=1.9109 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 30 total=8.5133 mle=1.7153 pcon=4.8767 forget=1.9213 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 80 total=8.5667 mle=1.7663 pcon=4.8759 forget=1.9245 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 130 total=8.3085 mle=1.4969 pcon=4.8754 forget=1.9362 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 180 total=8.3075 mle=1.4926 pcon=4.8746 forget=1.9403 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 230 total=8.4346 mle=1.6195 pcon=4.8741 forget=1.9409 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 280 total=8.4951 mle=1.6760 pcon=4.8736 forget=1.9454 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 330 total=8.4000 mle=1.5872 pcon=4.8728 forget=1.9399 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 380 total=8.5070 mle=1.6777 pcon=4.8722 forget=1.9572 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 40 total=8.4634 mle=1.6324 pcon=4.8715 forget=1.9595 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 90 total=8.4601 mle=1.6163 pcon=4.8709 forget=1.9729 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 140 total=8.4033 mle=1.5562 pcon=4.8704 forget=1.9768 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 86%|████████▌ | 30/35 [04:27<00:41,  8.39s/it] 89%|████████▊ | 31/35 [04:37<00:35,  8.82s/it] 91%|█████████▏| 32/35 [04:45<00:26,  8.81s/it] 94%|█████████▍| 33/35 [04:55<00:18,  9.12s/it] 97%|█████████▋| 34/35 [05:05<00:09,  9.21s/it]100%|██████████| 35/35 [05:14<00:00,  9.25s/it]100%|██████████| 35/35 [05:14<00:00,  8.99s/it]
[loss] ep 29 it 190 total=8.7541 mle=1.9126 pcon=4.8697 forget=1.9719 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 240 total=8.5584 mle=1.7054 pcon=4.8694 forget=1.9836 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 290 total=8.5039 mle=1.6570 pcon=4.8687 forget=1.9782 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 340 total=8.5421 mle=1.6830 pcon=4.8683 forget=1.9908 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 0 total=8.5212 mle=1.6660 pcon=4.8677 forget=1.9876 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 50 total=8.4905 mle=1.6280 pcon=4.8670 forget=1.9956 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 100 total=8.5459 mle=1.6907 pcon=4.8667 forget=1.9885 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 150 total=8.4396 mle=1.5660 pcon=4.8663 forget=2.0073 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 200 total=8.5366 mle=1.6690 pcon=4.8659 forget=2.0017 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 250 total=8.5227 mle=1.6592 pcon=4.8657 forget=1.9979 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 300 total=8.6364 mle=1.7566 pcon=4.8651 forget=2.0147 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 350 total=8.5656 mle=1.6856 pcon=4.8646 forget=2.0155 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 10 total=8.6809 mle=1.7931 pcon=4.8641 forget=2.0237 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 60 total=8.4709 mle=1.5842 pcon=4.8638 forget=2.0229 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 110 total=8.5655 mle=1.6737 pcon=4.8635 forget=2.0283 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 160 total=8.4751 mle=1.5814 pcon=4.8631 forget=2.0306 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 210 total=8.7171 mle=1.8131 pcon=4.8628 forget=2.0412 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 260 total=8.5426 mle=1.6421 pcon=4.8625 forget=2.0381 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 310 total=8.4879 mle=1.5778 pcon=4.8621 forget=2.0480 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 360 total=8.5005 mle=1.5832 pcon=4.8619 forget=2.0554 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 20 total=8.4619 mle=1.5481 pcon=4.8619 forget=2.0520 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 70 total=8.4353 mle=1.5206 pcon=4.8618 forget=2.0530 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 120 total=8.4892 mle=1.5645 pcon=4.8614 forget=2.0634 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 170 total=8.4910 mle=1.5586 pcon=4.8611 forget=2.0713 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 220 total=8.5534 mle=1.6288 pcon=4.8607 forget=2.0638 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 270 total=8.5224 mle=1.5937 pcon=4.8605 forget=2.0683 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 320 total=8.4958 mle=1.5654 pcon=4.8601 forget=2.0702 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 370 total=8.4362 mle=1.4953 pcon=4.8600 forget=2.0808 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 30 total=8.7187 mle=1.7763 pcon=4.8598 forget=2.0826 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 80 total=8.8177 mle=1.8859 pcon=4.8596 forget=2.0722 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 130 total=8.6322 mle=1.6895 pcon=4.8596 forget=2.0832 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 180 total=8.5129 mle=1.5692 pcon=4.8595 forget=2.0841 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 230 total=8.7163 mle=1.7797 pcon=4.8596 forget=2.0771 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 280 total=8.5709 mle=1.6272 pcon=4.8594 forget=2.0843 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 330 total=8.7505 mle=1.7993 pcon=4.8593 forget=2.0919 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 380 total=8.5615 mle=1.6021 pcon=4.8592 forget=2.1002 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 40 total=8.6594 mle=1.7046 pcon=4.8591 forget=2.0957 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 90 total=8.6233 mle=1.6655 pcon=4.8591 forget=2.0987 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 140 total=8.5962 mle=1.6379 pcon=4.8590 forget=2.0993 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 190 total=8.6043 mle=1.6326 pcon=4.8589 forget=2.1128 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 240 total=8.6535 mle=1.6934 pcon=4.8589 forget=2.1012 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 290 total=8.5514 mle=1.5878 pcon=4.8590 forget=2.1046 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 340 total=8.5268 mle=1.5655 pcon=4.8589 forget=2.1024 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[Run] lambda=0.2 lr=0.001 epochs=40
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=40, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/40 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:168: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▎         | 1/40 [00:13<09:05, 13.99s/it]  5%|▌         | 2/40 [00:25<07:50, 12.39s/it]  8%|▊         | 3/40 [00:36<07:12, 11.69s/it] 10%|█         | 4/40 [00:48<07:09, 11.94s/it] 12%|█▎        | 5/40 [01:00<06:59, 11.99s/it] 15%|█▌        | 6/40 [01:10<06:18, 11.14s/it][loss] ep 0 it 0 total=9.3215 mle=1.5618 pcon=5.2951 forget=2.4646 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.3417 mle=1.6308 pcon=5.2880 forget=2.4229 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.5222 mle=1.7912 pcon=5.2813 forget=2.4496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.4588 mle=1.7481 pcon=5.2750 forget=2.4357 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.5113 mle=1.8428 pcon=5.2683 forget=2.4001 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.4072 mle=1.6799 pcon=5.2618 forget=2.4655 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.3328 mle=1.6256 pcon=5.2557 forget=2.4515 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.5303 mle=1.8021 pcon=5.2498 forget=2.4784 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 1 it 10 total=9.1505 mle=1.4466 pcon=5.2437 forget=2.4601 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.3555 mle=1.6387 pcon=5.2378 forget=2.4791 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=9.1933 mle=1.5268 pcon=5.2322 forget=2.4342 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=9.3108 mle=1.6062 pcon=5.2265 forget=2.4782 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.3542 mle=1.6694 pcon=5.2211 forget=2.4636 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=9.1898 mle=1.5595 pcon=5.2158 forget=2.4145 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=9.2953 mle=1.6336 pcon=5.2102 forget=2.4514 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.3863 mle=1.7250 pcon=5.2050 forget=2.4563 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 2 it 20 total=9.0647 mle=1.4191 pcon=5.1998 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.5211 mle=1.8333 pcon=5.1947 forget=2.4931 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=9.3053 mle=1.6842 pcon=5.1895 forget=2.4316 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=9.2923 mle=1.6702 pcon=5.1848 forget=2.4373 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=9.4408 mle=1.8058 pcon=5.1798 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.3899 mle=1.7527 pcon=5.1752 forget=2.4621 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=9.0231 mle=1.4274 pcon=5.1706 forget=2.4251 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.2262 mle=1.6128 pcon=5.1666 forget=2.4467 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 3 it 30 total=9.2787 mle=1.6632 pcon=5.1623 forget=2.4531 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=9.1716 mle=1.5682 pcon=5.1582 forget=2.4453 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=9.1482 mle=1.5602 pcon=5.1540 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.3951 mle=1.8276 pcon=5.1499 forget=2.4176 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=9.3114 mle=1.7048 pcon=5.1462 forget=2.4604 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.3623 mle=1.7596 pcon=5.1425 forget=2.4602 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.4045 mle=1.8244 pcon=5.1385 forget=2.4416 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=9.2005 mle=1.6191 pcon=5.1348 forget=2.4466 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 4 it 40 total=9.1426 mle=1.5757 pcon=5.1311 forget=2.4358 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=9.1359 mle=1.5309 pcon=5.1276 forget=2.4773 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.2283 mle=1.7052 pcon=5.1245 forget=2.3986 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.2331 mle=1.6599 pcon=5.1210 forget=2.4522 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.3928 mle=1.8093 pcon=5.1177 forget=2.4658 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.0165 mle=1.4522 pcon=5.1146 forget=2.4496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.2409 mle=1.6860 pcon=5.1118 forget=2.4432 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 5 it 0 total=9.2134 mle=1.6087 pcon=5.1085 forget=2.4962 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=9.2657 mle=1.6933 pcon=5.1056 forget=2.4668 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.3821 mle=1.7997 pcon=5.1025 forget=2.4799 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.5468 mle=1.9765 pcon=5.0994 forget=2.4709 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.3240 mle=1.7905 pcon=5.0965 forget=2.4370 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=9.2514 mle=1.7139 pcon=5.0935 forget=2.4439 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=9.2808 mle=1.7676 pcon=5.0905 forget=2.4226 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=9.2407 mle=1.7190 pcon=5.0876 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 6 it 10 total=9.1400 mle=1.5563 pcon=5.0850 forget=2.4987 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=9.2205 mle=1.7053 pcon=5.0823 forget=2.4329 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=9.2531 mle=1.6997 pcon=5.0799 forget=2.4735 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=9.0336 mle=1.5104 pcon=5.0775 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=9.2067 mle=1.6767 pcon=5.0748 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=9.2962 mle=1.7616 pcon=5.0724 forget=2.4622 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=8.9789 mle=1.4947 pcon=5.0702 forget=2.4140 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 18%|█▊        | 7/40 [01:20<05:57, 10.84s/it] 20%|██        | 8/40 [01:29<05:26, 10.19s/it] 22%|██▎       | 9/40 [01:40<05:23, 10.45s/it] 25%|██▌       | 10/40 [01:50<05:11, 10.40s/it] 28%|██▊       | 11/40 [02:01<05:07, 10.59s/it] 30%|███       | 12/40 [02:11<04:55, 10.56s/it] 32%|███▎      | 13/40 [02:22<04:45, 10.56s/it][loss] ep 6 it 360 total=9.1805 mle=1.7147 pcon=5.0677 forget=2.3980 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 7 it 20 total=9.0254 mle=1.5088 pcon=5.0654 forget=2.4511 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=9.0875 mle=1.5735 pcon=5.0629 forget=2.4511 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=9.2398 mle=1.7394 pcon=5.0607 forget=2.4398 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=9.2179 mle=1.7606 pcon=5.0585 forget=2.3988 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=9.1346 mle=1.6538 pcon=5.0567 forget=2.4240 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=9.3683 mle=1.8772 pcon=5.0548 forget=2.4363 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=9.0487 mle=1.5490 pcon=5.0525 forget=2.4473 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=9.3153 mle=1.7891 pcon=5.0503 forget=2.4758 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 8 it 30 total=9.2470 mle=1.7899 pcon=5.0485 forget=2.4086 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=9.0188 mle=1.5220 pcon=5.0465 forget=2.4502 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=9.2263 mle=1.7439 pcon=5.0447 forget=2.4377 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=9.1380 mle=1.6514 pcon=5.0428 forget=2.4439 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=9.0535 mle=1.5675 pcon=5.0410 forget=2.4450 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.9496 mle=1.4577 pcon=5.0392 forget=2.4528 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.1962 mle=1.7882 pcon=5.0373 forget=2.3708 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=9.1247 mle=1.6498 pcon=5.0357 forget=2.4392 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 9 it 40 total=9.2890 mle=1.7967 pcon=5.0340 forget=2.4583 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=9.1829 mle=1.7482 pcon=5.0322 forget=2.4024 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=9.1798 mle=1.6837 pcon=5.0310 forget=2.4651 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.9753 mle=1.4941 pcon=5.0295 forget=2.4517 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=9.1877 mle=1.7624 pcon=5.0282 forget=2.3971 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=9.1275 mle=1.6624 pcon=5.0265 forget=2.4386 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.9611 mle=1.5042 pcon=5.0252 forget=2.4318 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 10 it 0 total=8.9687 mle=1.4924 pcon=5.0232 forget=2.4530 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.9280 mle=1.4681 pcon=5.0221 forget=2.4378 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=9.1264 mle=1.6910 pcon=5.0206 forget=2.4148 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=9.1776 mle=1.7025 pcon=5.0192 forget=2.4558 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=9.2082 mle=1.7849 pcon=5.0180 forget=2.4053 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=9.2188 mle=1.8303 pcon=5.0167 forget=2.3717 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=9.0365 mle=1.7734 pcon=5.0153 forget=2.2478 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.8001 mle=1.5405 pcon=5.0143 forget=2.2452 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 11 it 10 total=8.9256 mle=1.7705 pcon=5.0130 forget=2.1421 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.6586 mle=1.7151 pcon=5.0121 forget=1.9314 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.4471 mle=1.5934 pcon=5.0113 forget=1.8425 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.3472 mle=1.5854 pcon=5.0105 forget=1.7513 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.2691 mle=1.5485 pcon=5.0101 forget=1.7104 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.2447 mle=1.5696 pcon=5.0099 forget=1.6653 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.5004 mle=1.7994 pcon=5.0100 forget=1.6910 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.3699 mle=1.6841 pcon=5.0104 forget=1.6753 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 12 it 20 total=8.2377 mle=1.5758 pcon=5.0105 forget=1.6514 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.1955 mle=1.5173 pcon=5.0109 forget=1.6673 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.3777 mle=1.7296 pcon=5.0116 forget=1.6365 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.3805 mle=1.7083 pcon=5.0122 forget=1.6599 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.2236 mle=1.5412 pcon=5.0134 forget=1.6690 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.2703 mle=1.5733 pcon=5.0146 forget=1.6825 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.3785 mle=1.6695 pcon=5.0158 forget=1.6933 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.4141 mle=1.6993 pcon=5.0169 forget=1.6978 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 13 it 30 total=8.4227 mle=1.7029 pcon=5.0183 forget=1.7015 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.5400 mle=1.7937 pcon=5.0195 forget=1.7269 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.4036 mle=1.6678 pcon=5.0205 forget=1.7153 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=8.5007 mle=1.7533 pcon=5.0213 forget=1.7262 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 35%|███▌      | 14/40 [02:32<04:34, 10.57s/it] 38%|███▊      | 15/40 [02:42<04:18, 10.34s/it] 40%|████      | 16/40 [02:53<04:09, 10.39s/it] 42%|████▎     | 17/40 [03:02<03:52, 10.11s/it] 45%|████▌     | 18/40 [03:14<03:50, 10.46s/it] 48%|████▊     | 19/40 [03:23<03:34, 10.21s/it] 50%|█████     | 20/40 [03:34<03:25, 10.27s/it] 52%|█████▎    | 21/40 [03:45<03:19, 10.50s/it][loss] ep 13 it 230 total=8.4098 mle=1.6556 pcon=5.0225 forget=1.7317 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.2097 mle=1.4531 pcon=5.0237 forget=1.7329 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.5227 mle=1.7618 pcon=5.0249 forget=1.7361 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.4040 mle=1.6412 pcon=5.0261 forget=1.7367 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 40 total=8.4613 mle=1.6857 pcon=5.0271 forget=1.7485 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.3052 mle=1.5258 pcon=5.0282 forget=1.7511 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.4293 mle=1.6580 pcon=5.0291 forget=1.7421 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.3729 mle=1.5984 pcon=5.0301 forget=1.7444 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.4346 mle=1.6486 pcon=5.0306 forget=1.7554 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.3191 mle=1.5427 pcon=5.0314 forget=1.7449 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.3447 mle=1.5650 pcon=5.0321 forget=1.7476 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 0 total=8.4030 mle=1.6280 pcon=5.0325 forget=1.7424 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.3374 mle=1.5612 pcon=5.0330 forget=1.7432 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.4057 mle=1.6359 pcon=5.0335 forget=1.7362 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.4704 mle=1.7024 pcon=5.0338 forget=1.7343 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.2779 mle=1.5059 pcon=5.0339 forget=1.7381 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.3196 mle=1.5497 pcon=5.0338 forget=1.7361 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.5513 mle=1.7827 pcon=5.0336 forget=1.7349 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.3662 mle=1.6056 pcon=5.0334 forget=1.7273 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 10 total=8.3478 mle=1.5732 pcon=5.0328 forget=1.7419 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.2779 mle=1.5167 pcon=5.0323 forget=1.7289 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.1706 mle=1.4114 pcon=5.0315 forget=1.7276 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.2544 mle=1.5024 pcon=5.0308 forget=1.7212 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.3239 mle=1.5750 pcon=5.0298 forget=1.7191 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.6236 mle=1.8733 pcon=5.0285 forget=1.7218 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.6495 mle=1.8960 pcon=5.0270 forget=1.7266 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.2878 mle=1.5502 pcon=5.0255 forget=1.7121 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.5255 mle=1.7912 pcon=5.0241 forget=1.7102 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.5039 mle=1.7647 pcon=5.0227 forget=1.7164 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.2653 mle=1.5347 pcon=5.0207 forget=1.7099 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.4813 mle=1.7306 pcon=5.0188 forget=1.7320 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.5515 mle=1.8262 pcon=5.0166 forget=1.7087 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.5265 mle=1.8075 pcon=5.0145 forget=1.7046 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.3501 mle=1.6361 pcon=5.0122 forget=1.7018 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.3482 mle=1.6334 pcon=5.0098 forget=1.7050 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.2270 mle=1.5156 pcon=5.0075 forget=1.7039 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.4460 mle=1.7295 pcon=5.0051 forget=1.7115 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.5010 mle=1.7805 pcon=5.0026 forget=1.7180 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.3572 mle=1.6659 pcon=5.0002 forget=1.6911 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.3078 mle=1.6083 pcon=4.9976 forget=1.7019 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.3410 mle=1.6365 pcon=4.9949 forget=1.7096 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.3776 mle=1.6814 pcon=4.9922 forget=1.7040 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.3180 mle=1.6355 pcon=4.9897 forget=1.6928 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.3585 mle=1.6786 pcon=4.9870 forget=1.6929 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.3242 mle=1.6474 pcon=4.9841 forget=1.6926 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.2557 mle=1.5681 pcon=4.9815 forget=1.7061 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.2331 mle=1.5620 pcon=4.9786 forget=1.6925 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.3634 mle=1.6974 pcon=4.9758 forget=1.6901 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.3346 mle=1.6760 pcon=4.9733 forget=1.6853 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.3094 mle=1.6482 pcon=4.9708 forget=1.6904 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 20 it 0 total=8.1490 mle=1.4867 pcon=4.9681 forget=1.6942 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.3454 mle=1.6964 pcon=4.9653 forget=1.6837 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.2001 mle=1.5549 pcon=4.9626 forget=1.6826 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.3058 mle=1.6639 pcon=4.9598 forget=1.6821 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.2653 mle=1.6202 pcon=4.9572 forget=1.6879 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=8.3944 mle=1.7580 pcon=4.9545 forget=1.6820 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=8.4207 mle=1.7935 pcon=4.9519 forget=1.6753 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=8.3794 mle=1.7373 pcon=4.9492 forget=1.6929 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 21 it 10 total=8.0594 mle=1.4336 pcon=4.9464 forget=1.6794 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 55%|█████▌    | 22/40 [03:56<03:12, 10.71s/it] 57%|█████▊    | 23/40 [04:06<02:59, 10.56s/it] 60%|██████    | 24/40 [04:16<02:46, 10.44s/it] 62%|██████▎   | 25/40 [04:27<02:37, 10.50s/it] 65%|██████▌   | 26/40 [04:37<02:26, 10.44s/it] 68%|██████▊   | 27/40 [04:45<02:07,  9.82s/it][loss] ep 21 it 60 total=8.3001 mle=1.6727 pcon=4.9437 forget=1.6837 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.2713 mle=1.6358 pcon=4.9411 forget=1.6944 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.3156 mle=1.6982 pcon=4.9385 forget=1.6790 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=8.2363 mle=1.6025 pcon=4.9358 forget=1.6980 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.5522 mle=1.9352 pcon=4.9333 forget=1.6837 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.3551 mle=1.7367 pcon=4.9307 forget=1.6878 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.3334 mle=1.7209 pcon=4.9281 forget=1.6844 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 22 it 20 total=8.3270 mle=1.7138 pcon=4.9257 forget=1.6876 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.2576 mle=1.6547 pcon=4.9233 forget=1.6796 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.3589 mle=1.7481 pcon=4.9208 forget=1.6899 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.2411 mle=1.6397 pcon=4.9184 forget=1.6829 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.3842 mle=1.7799 pcon=4.9162 forget=1.6881 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.2402 mle=1.6292 pcon=4.9140 forget=1.6970 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.1295 mle=1.5372 pcon=4.9118 forget=1.6805 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=8.3038 mle=1.7101 pcon=4.9094 forget=1.6843 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 23 it 30 total=8.2210 mle=1.6318 pcon=4.9073 forget=1.6819 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.2350 mle=1.6396 pcon=4.9050 forget=1.6904 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=8.4345 mle=1.8403 pcon=4.9027 forget=1.6915 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.2338 mle=1.6374 pcon=4.9005 forget=1.6959 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.2269 mle=1.6284 pcon=4.8984 forget=1.7001 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=8.3500 mle=1.7671 pcon=4.8963 forget=1.6866 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=8.3802 mle=1.7863 pcon=4.8940 forget=1.6998 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.1267 mle=1.5386 pcon=4.8919 forget=1.6961 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 24 it 40 total=8.2433 mle=1.6533 pcon=4.8901 forget=1.7000 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=8.3782 mle=1.7942 pcon=4.8879 forget=1.6961 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.1915 mle=1.6119 pcon=4.8858 forget=1.6938 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.2146 mle=1.6323 pcon=4.8838 forget=1.6986 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=8.2321 mle=1.6528 pcon=4.8819 forget=1.6974 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.2884 mle=1.7134 pcon=4.8797 forget=1.6954 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.2024 mle=1.6188 pcon=4.8776 forget=1.7060 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 25 it 0 total=8.3264 mle=1.7445 pcon=4.8757 forget=1.7061 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 50 total=8.2495 mle=1.6591 pcon=4.8738 forget=1.7166 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 100 total=8.3113 mle=1.7289 pcon=4.8718 forget=1.7106 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 150 total=8.3148 mle=1.7389 pcon=4.8701 forget=1.7059 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 200 total=8.3624 mle=1.7871 pcon=4.8683 forget=1.7070 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 250 total=8.1910 mle=1.6056 pcon=4.8665 forget=1.7189 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 300 total=8.2727 mle=1.6882 pcon=4.8648 forget=1.7198 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 350 total=8.3986 mle=1.8238 pcon=4.8630 forget=1.7118 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 10 total=8.2972 mle=1.7203 pcon=4.8612 forget=1.7157 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 60 total=8.3101 mle=1.7443 pcon=4.8593 forget=1.7065 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 110 total=8.3199 mle=1.7276 pcon=4.8575 forget=1.7348 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 160 total=8.1635 mle=1.5900 pcon=4.8556 forget=1.7178 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 210 total=8.2976 mle=1.7266 pcon=4.8539 forget=1.7171 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 260 total=8.2542 mle=1.6892 pcon=4.8524 forget=1.7127 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 310 total=8.4355 mle=1.8560 pcon=4.8505 forget=1.7291 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 360 total=8.4485 mle=1.8753 pcon=4.8487 forget=1.7245 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 27 it 20 total=8.1574 mle=1.5917 pcon=4.8469 forget=1.7188 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 70 total=8.2245 mle=1.6624 pcon=4.8449 forget=1.7172 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 120 total=8.2910 mle=1.7100 pcon=4.8432 forget=1.7377 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 170 total=8.1275 mle=1.5540 pcon=4.8414 forget=1.7321 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 220 total=8.2274 mle=1.6684 pcon=4.8397 forget=1.7193 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 270 total=8.1525 mle=1.5890 pcon=4.8378 forget=1.7257 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 320 total=8.2998 mle=1.7147 pcon=4.8360 forget=1.7491 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 370 total=8.1242 mle=1.5803 pcon=4.8343 forget=1.7096 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 70%|███████   | 28/40 [04:55<01:56,  9.72s/it] 72%|███████▎  | 29/40 [05:06<01:49, 10.00s/it] 75%|███████▌  | 30/40 [05:16<01:41, 10.16s/it] 78%|███████▊  | 31/40 [05:27<01:33, 10.36s/it] 80%|████████  | 32/40 [05:37<01:23, 10.38s/it] 82%|████████▎ | 33/40 [05:48<01:13, 10.51s/it] 85%|████████▌ | 34/40 [05:59<01:03, 10.52s/it][peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 28 it 30 total=8.2723 mle=1.7160 pcon=4.8325 forget=1.7238 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 80 total=8.3102 mle=1.7572 pcon=4.8305 forget=1.7225 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 130 total=8.0649 mle=1.4991 pcon=4.8288 forget=1.7370 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 180 total=8.0560 mle=1.5057 pcon=4.8267 forget=1.7236 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 230 total=8.1702 mle=1.6174 pcon=4.8248 forget=1.7280 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 280 total=8.2157 mle=1.6717 pcon=4.8229 forget=1.7210 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 330 total=8.1047 mle=1.5806 pcon=4.8208 forget=1.7033 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 380 total=8.2017 mle=1.6703 pcon=4.8186 forget=1.7128 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 29 it 40 total=8.1390 mle=1.6148 pcon=4.8165 forget=1.7078 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 90 total=8.1716 mle=1.6215 pcon=4.8144 forget=1.7357 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 140 total=8.0910 mle=1.5473 pcon=4.8123 forget=1.7314 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 190 total=8.4224 mle=1.9095 pcon=4.8101 forget=1.7028 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 240 total=8.2022 mle=1.6838 pcon=4.8081 forget=1.7103 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 290 total=8.1399 mle=1.6617 pcon=4.8058 forget=1.6724 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 340 total=8.1867 mle=1.6773 pcon=4.8038 forget=1.7056 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 30 it 0 total=8.1847 mle=1.6790 pcon=4.8016 forget=1.7042 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 50 total=8.1167 mle=1.6242 pcon=4.7992 forget=1.6933 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 100 total=8.1363 mle=1.6865 pcon=4.7973 forget=1.6525 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 150 total=8.0261 mle=1.5509 pcon=4.7953 forget=1.6799 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 200 total=8.1269 mle=1.6713 pcon=4.7933 forget=1.6623 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 250 total=8.1175 mle=1.6736 pcon=4.7914 forget=1.6525 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 300 total=8.1999 mle=1.7400 pcon=4.7892 forget=1.6707 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 350 total=8.1105 mle=1.6873 pcon=4.7871 forget=1.6360 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 31 it 10 total=8.1860 mle=1.7570 pcon=4.7851 forget=1.6440 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 60 total=8.0010 mle=1.5681 pcon=4.7832 forget=1.6496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 110 total=8.1924 mle=1.7199 pcon=4.7815 forget=1.6910 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 160 total=8.0638 mle=1.6230 pcon=4.7797 forget=1.6612 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 210 total=8.2982 mle=1.8263 pcon=4.7779 forget=1.6940 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 260 total=8.1089 mle=1.6460 pcon=4.7762 forget=1.6867 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 310 total=8.0120 mle=1.5665 pcon=4.7743 forget=1.6711 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 360 total=8.0672 mle=1.6081 pcon=4.7729 forget=1.6862 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 32 it 20 total=8.0502 mle=1.5665 pcon=4.7715 forget=1.7122 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 70 total=7.9769 mle=1.5363 pcon=4.7701 forget=1.6704 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 120 total=8.0268 mle=1.5694 pcon=4.7685 forget=1.6888 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 170 total=8.0263 mle=1.5551 pcon=4.7671 forget=1.7041 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 220 total=8.0395 mle=1.6152 pcon=4.7655 forget=1.6588 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 270 total=8.0282 mle=1.5849 pcon=4.7642 forget=1.6791 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 320 total=7.9711 mle=1.5208 pcon=4.7628 forget=1.6875 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 370 total=7.9463 mle=1.4899 pcon=4.7617 forget=1.6947 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 33 it 30 total=8.2225 mle=1.7631 pcon=4.7605 forget=1.6990 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 80 total=8.2790 mle=1.8571 pcon=4.7593 forget=1.6626 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 130 total=8.1586 mle=1.7058 pcon=4.7583 forget=1.6945 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 180 total=8.0305 mle=1.6058 pcon=4.7573 forget=1.6675 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 230 total=8.1727 mle=1.7612 pcon=4.7564 forget=1.6550 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 280 total=8.0417 mle=1.6195 pcon=4.7554 forget=1.6669 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 330 total=8.2416 mle=1.7947 pcon=4.7544 forget=1.6925 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 380 total=8.0473 mle=1.6081 pcon=4.7537 forget=1.6855 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 40 total=8.1525 mle=1.7116 pcon=4.7528 forget=1.6881 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 90 total=8.1106 mle=1.6603 pcon=4.7521 forget=1.6982 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 140 total=8.1016 mle=1.6423 pcon=4.7512 forget=1.7082 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 190 total=8.1139 mle=1.6492 pcon=4.7505 forget=1.7142 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 240 total=8.1351 mle=1.6661 pcon=4.7498 forget=1.7193 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 290 total=8.0253 mle=1.5775 pcon=4.7492 forget=1.6986 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 340 total=8.0524 mle=1.6044 pcon=4.7486 forget=1.6994 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA 88%|████████▊ | 35/40 [06:09<00:52, 10.55s/it] 90%|█████████ | 36/40 [06:21<00:43, 10.87s/it] 92%|█████████▎| 37/40 [06:30<00:31, 10.35s/it] 95%|█████████▌| 38/40 [06:38<00:19,  9.75s/it] 98%|█████████▊| 39/40 [06:46<00:09,  9.14s/it]100%|██████████| 40/40 [06:57<00:00,  9.76s/it]100%|██████████| 40/40 [06:57<00:00, 10.45s/it]

[loss] ep 35 it 0 total=8.0381 mle=1.5668 pcon=4.7481 forget=1.7232 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 50 total=8.2206 mle=1.7390 pcon=4.7478 forget=1.7338 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 100 total=7.9937 mle=1.5523 pcon=4.7475 forget=1.6940 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 150 total=8.0576 mle=1.5800 pcon=4.7469 forget=1.7307 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 200 total=8.0971 mle=1.6383 pcon=4.7464 forget=1.7123 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 250 total=8.2010 mle=1.7320 pcon=4.7462 forget=1.7228 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 300 total=8.1671 mle=1.6855 pcon=4.7458 forget=1.7358 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 350 total=8.1162 mle=1.6233 pcon=4.7456 forget=1.7473 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 10 total=8.2363 mle=1.7632 pcon=4.7452 forget=1.7279 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 60 total=8.3467 mle=1.8566 pcon=4.7448 forget=1.7453 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 110 total=8.3934 mle=1.8918 pcon=4.7446 forget=1.7569 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 160 total=8.2464 mle=1.7419 pcon=4.7444 forget=1.7601 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 210 total=8.3568 mle=1.8553 pcon=4.7440 forget=1.7574 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 260 total=8.3128 mle=1.8150 pcon=4.7438 forget=1.7540 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 310 total=8.0896 mle=1.5946 pcon=4.7438 forget=1.7513 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 360 total=8.0951 mle=1.5864 pcon=4.7439 forget=1.7648 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 20 total=8.3485 mle=1.8387 pcon=4.7438 forget=1.7660 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 70 total=8.2243 mle=1.7227 pcon=4.7437 forget=1.7579 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 120 total=8.1283 mle=1.6018 pcon=4.7434 forget=1.7831 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 170 total=8.1102 mle=1.5867 pcon=4.7435 forget=1.7800 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 220 total=8.1089 mle=1.5912 pcon=4.7434 forget=1.7743 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 270 total=8.0892 mle=1.5759 pcon=4.7434 forget=1.7699 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 320 total=8.2767 mle=1.7486 pcon=4.7435 forget=1.7846 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 370 total=8.1537 mle=1.6107 pcon=4.7436 forget=1.7994 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 30 total=8.1925 mle=1.6660 pcon=4.7436 forget=1.7829 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 80 total=8.1773 mle=1.6311 pcon=4.7436 forget=1.8026 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 130 total=8.1402 mle=1.6094 pcon=4.7436 forget=1.7872 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 180 total=8.0688 mle=1.5389 pcon=4.7434 forget=1.7864 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 230 total=8.2211 mle=1.6605 pcon=4.7436 forget=1.8170 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 280 total=8.4082 mle=1.8558 pcon=4.7436 forget=1.8088 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 330 total=8.4216 mle=1.8632 pcon=4.7436 forget=1.8149 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 380 total=8.3696 mle=1.8018 pcon=4.7435 forget=1.8243 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 40 total=8.2192 mle=1.6694 pcon=4.7436 forget=1.8062 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 90 total=8.2198 mle=1.6529 pcon=4.7438 forget=1.8232 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 140 total=8.2439 mle=1.6871 pcon=4.7440 forget=1.8129 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 190 total=8.2378 mle=1.6501 pcon=4.7441 forget=1.8436 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 240 total=8.2479 mle=1.6856 pcon=4.7443 forget=1.8180 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 290 total=8.2075 mle=1.6295 pcon=4.7443 forget=1.8338 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 340 total=8.3308 mle=1.7488 pcon=4.7446 forget=1.8375 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[Run] lambda=0.2 lr=0.001 epochs=45
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=45, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/45 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:168: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/45 [00:14<10:35, 14.44s/it]  4%|▍         | 2/45 [00:23<07:53, 11.01s/it]  7%|▋         | 3/45 [00:32<07:15, 10.37s/it]  9%|▉         | 4/45 [00:43<07:15, 10.62s/it] 11%|█         | 5/45 [00:53<06:56, 10.41s/it] 13%|█▎        | 6/45 [01:03<06:31, 10.05s/it][loss] ep 0 it 0 total=9.3217 mle=1.5619 pcon=5.2951 forget=2.4647 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.3417 mle=1.6308 pcon=5.2880 forget=2.4229 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.5223 mle=1.7913 pcon=5.2813 forget=2.4497 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.4588 mle=1.7481 pcon=5.2750 forget=2.4357 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.5113 mle=1.8429 pcon=5.2683 forget=2.4001 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.4073 mle=1.6799 pcon=5.2618 forget=2.4656 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.3329 mle=1.6257 pcon=5.2557 forget=2.4515 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.5301 mle=1.8019 pcon=5.2498 forget=2.4784 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 1 it 10 total=9.1504 mle=1.4465 pcon=5.2437 forget=2.4601 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.3554 mle=1.6386 pcon=5.2378 forget=2.4791 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=9.1932 mle=1.5269 pcon=5.2322 forget=2.4341 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=9.3110 mle=1.6064 pcon=5.2265 forget=2.4782 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.3541 mle=1.6693 pcon=5.2211 forget=2.4637 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=9.1899 mle=1.5596 pcon=5.2158 forget=2.4145 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=9.2953 mle=1.6336 pcon=5.2102 forget=2.4514 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.3864 mle=1.7251 pcon=5.2050 forget=2.4563 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 2 it 20 total=9.0649 mle=1.4193 pcon=5.1998 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.5211 mle=1.8333 pcon=5.1947 forget=2.4932 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=9.3052 mle=1.6842 pcon=5.1895 forget=2.4315 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=9.2922 mle=1.6702 pcon=5.1848 forget=2.4372 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=9.4409 mle=1.8058 pcon=5.1798 forget=2.4553 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.3898 mle=1.7526 pcon=5.1751 forget=2.4621 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=9.0231 mle=1.4274 pcon=5.1706 forget=2.4251 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.2261 mle=1.6127 pcon=5.1666 forget=2.4468 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 3 it 30 total=9.2787 mle=1.6633 pcon=5.1623 forget=2.4531 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=9.1715 mle=1.5681 pcon=5.1582 forget=2.4453 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=9.1481 mle=1.5601 pcon=5.1540 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.3952 mle=1.8278 pcon=5.1498 forget=2.4176 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=9.3114 mle=1.7048 pcon=5.1462 forget=2.4604 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.3622 mle=1.7595 pcon=5.1425 forget=2.4602 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.4045 mle=1.8245 pcon=5.1384 forget=2.4416 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=9.2006 mle=1.6193 pcon=5.1348 forget=2.4466 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 4 it 40 total=9.1424 mle=1.5755 pcon=5.1311 forget=2.4358 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=9.1358 mle=1.5309 pcon=5.1276 forget=2.4773 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.2282 mle=1.7051 pcon=5.1245 forget=2.3986 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.2332 mle=1.6599 pcon=5.1210 forget=2.4523 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.3931 mle=1.8095 pcon=5.1177 forget=2.4659 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.0165 mle=1.4523 pcon=5.1146 forget=2.4495 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.2410 mle=1.6861 pcon=5.1118 forget=2.4432 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 5 it 0 total=9.2135 mle=1.6089 pcon=5.1085 forget=2.4962 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=9.2652 mle=1.6928 pcon=5.1056 forget=2.4668 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.3823 mle=1.7999 pcon=5.1025 forget=2.4799 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.5467 mle=1.9764 pcon=5.0994 forget=2.4709 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.3238 mle=1.7903 pcon=5.0965 forget=2.4370 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=9.2512 mle=1.7138 pcon=5.0935 forget=2.4439 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=9.2809 mle=1.7677 pcon=5.0905 forget=2.4226 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=9.2406 mle=1.7190 pcon=5.0876 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 6 it 10 total=9.1400 mle=1.5564 pcon=5.0849 forget=2.4987 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=9.2204 mle=1.7052 pcon=5.0823 forget=2.4328 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=9.2530 mle=1.6996 pcon=5.0799 forget=2.4735 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=9.0338 mle=1.5105 pcon=5.0775 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=9.2066 mle=1.6766 pcon=5.0748 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=9.2962 mle=1.7617 pcon=5.0724 forget=2.4622 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=8.9789 mle=1.4948 pcon=5.0702 forget=2.4140 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 16%|█▌        | 7/45 [01:12<06:16,  9.92s/it] 18%|█▊        | 8/45 [01:23<06:12, 10.05s/it] 20%|██        | 9/45 [01:35<06:32, 10.90s/it] 22%|██▏       | 10/45 [01:45<06:13, 10.67s/it] 24%|██▍       | 11/45 [01:54<05:44, 10.13s/it] 27%|██▋       | 12/45 [02:06<05:51, 10.64s/it] 29%|██▉       | 13/45 [02:16<05:34, 10.44s/it][loss] ep 6 it 360 total=9.1807 mle=1.7149 pcon=5.0677 forget=2.3980 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 7 it 20 total=9.0254 mle=1.5089 pcon=5.0654 forget=2.4511 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=9.0875 mle=1.5735 pcon=5.0629 forget=2.4510 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=9.2396 mle=1.7392 pcon=5.0607 forget=2.4397 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=9.2179 mle=1.7606 pcon=5.0585 forget=2.3988 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=9.1345 mle=1.6538 pcon=5.0567 forget=2.4240 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=9.3684 mle=1.8772 pcon=5.0548 forget=2.4364 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=9.0488 mle=1.5490 pcon=5.0525 forget=2.4473 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=9.3153 mle=1.7892 pcon=5.0503 forget=2.4758 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 8 it 30 total=9.2469 mle=1.7899 pcon=5.0485 forget=2.4086 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=9.0189 mle=1.5221 pcon=5.0465 forget=2.4502 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=9.2262 mle=1.7438 pcon=5.0447 forget=2.4377 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=9.1381 mle=1.6516 pcon=5.0428 forget=2.4438 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=9.0533 mle=1.5673 pcon=5.0410 forget=2.4450 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.9495 mle=1.4576 pcon=5.0392 forget=2.4528 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.1963 mle=1.7883 pcon=5.0372 forget=2.3708 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=9.1248 mle=1.6500 pcon=5.0357 forget=2.4392 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 9 it 40 total=9.2890 mle=1.7967 pcon=5.0340 forget=2.4583 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=9.1830 mle=1.7484 pcon=5.0322 forget=2.4024 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=9.1799 mle=1.6837 pcon=5.0310 forget=2.4651 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.9752 mle=1.4940 pcon=5.0295 forget=2.4517 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=9.1878 mle=1.7625 pcon=5.0282 forget=2.3971 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=9.1274 mle=1.6623 pcon=5.0265 forget=2.4386 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.9611 mle=1.5041 pcon=5.0252 forget=2.4318 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 10 it 0 total=8.9686 mle=1.4924 pcon=5.0232 forget=2.4530 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.9284 mle=1.4684 pcon=5.0221 forget=2.4378 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=9.1263 mle=1.6909 pcon=5.0206 forget=2.4148 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=9.1776 mle=1.7026 pcon=5.0192 forget=2.4559 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=9.2081 mle=1.7849 pcon=5.0180 forget=2.4053 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=9.2188 mle=1.8304 pcon=5.0167 forget=2.3717 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=9.0366 mle=1.7734 pcon=5.0153 forget=2.2478 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.7999 mle=1.5405 pcon=5.0143 forget=2.2451 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 11 it 10 total=8.9254 mle=1.7706 pcon=5.0130 forget=2.1418 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.6582 mle=1.7152 pcon=5.0121 forget=1.9309 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.4466 mle=1.5932 pcon=5.0113 forget=1.8421 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.3468 mle=1.5853 pcon=5.0105 forget=1.7510 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.2687 mle=1.5485 pcon=5.0101 forget=1.7101 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.2472 mle=1.5722 pcon=5.0099 forget=1.6651 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.4999 mle=1.7992 pcon=5.0100 forget=1.6907 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.3694 mle=1.6840 pcon=5.0104 forget=1.6750 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 12 it 20 total=8.2388 mle=1.5772 pcon=5.0105 forget=1.6511 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.1952 mle=1.5173 pcon=5.0109 forget=1.6670 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.3776 mle=1.7297 pcon=5.0115 forget=1.6364 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.3804 mle=1.7084 pcon=5.0122 forget=1.6597 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.2233 mle=1.5411 pcon=5.0134 forget=1.6688 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.2700 mle=1.5731 pcon=5.0146 forget=1.6822 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.3782 mle=1.6694 pcon=5.0158 forget=1.6931 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.4126 mle=1.6981 pcon=5.0169 forget=1.6976 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 13 it 30 total=8.4223 mle=1.7029 pcon=5.0182 forget=1.7012 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.5394 mle=1.7935 pcon=5.0195 forget=1.7265 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.4028 mle=1.6675 pcon=5.0205 forget=1.7148 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=8.5003 mle=1.7534 pcon=5.0212 forget=1.7256 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 31%|███       | 14/45 [02:25<05:10, 10.02s/it] 33%|███▎      | 15/45 [02:35<05:03, 10.11s/it] 36%|███▌      | 16/45 [02:45<04:45,  9.84s/it] 38%|███▊      | 17/45 [02:55<04:40, 10.03s/it] 40%|████      | 18/45 [03:04<04:23,  9.76s/it] 42%|████▏     | 19/45 [03:14<04:11,  9.67s/it] 44%|████▍     | 20/45 [03:23<03:58,  9.55s/it] 47%|████▋     | 21/45 [03:33<03:50,  9.61s/it][loss] ep 13 it 230 total=8.4093 mle=1.6558 pcon=5.0225 forget=1.7311 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.2089 mle=1.4531 pcon=5.0236 forget=1.7322 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.5218 mle=1.7617 pcon=5.0248 forget=1.7353 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.3958 mle=1.6339 pcon=5.0260 forget=1.7358 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 40 total=8.4602 mle=1.6857 pcon=5.0270 forget=1.7475 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.3038 mle=1.5258 pcon=5.0281 forget=1.7500 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.4276 mle=1.6577 pcon=5.0290 forget=1.7408 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.3712 mle=1.5983 pcon=5.0300 forget=1.7430 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.4329 mle=1.6486 pcon=5.0305 forget=1.7539 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.3175 mle=1.5430 pcon=5.0313 forget=1.7432 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.3423 mle=1.5646 pcon=5.0320 forget=1.7458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 0 total=8.4003 mle=1.6276 pcon=5.0323 forget=1.7403 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.3353 mle=1.5616 pcon=5.0328 forget=1.7410 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.4024 mle=1.6354 pcon=5.0333 forget=1.7337 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.4675 mle=1.7023 pcon=5.0335 forget=1.7317 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.2745 mle=1.5057 pcon=5.0336 forget=1.7352 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.3165 mle=1.5500 pcon=5.0334 forget=1.7330 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.5518 mle=1.7870 pcon=5.0331 forget=1.7317 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.3614 mle=1.6048 pcon=5.0329 forget=1.7238 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 10 total=8.3448 mle=1.5745 pcon=5.0322 forget=1.7381 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.2737 mle=1.5170 pcon=5.0317 forget=1.7250 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.1661 mle=1.4118 pcon=5.0309 forget=1.7235 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.2493 mle=1.5024 pcon=5.0300 forget=1.7169 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.3175 mle=1.5740 pcon=5.0290 forget=1.7145 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.6173 mle=1.8725 pcon=5.0276 forget=1.7173 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.6418 mle=1.8945 pcon=5.0259 forget=1.7214 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.2821 mle=1.5511 pcon=5.0243 forget=1.7067 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.5205 mle=1.7928 pcon=5.0229 forget=1.7048 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.4963 mle=1.7644 pcon=5.0214 forget=1.7106 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.2608 mle=1.5377 pcon=5.0192 forget=1.7039 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.4791 mle=1.7365 pcon=5.0171 forget=1.7255 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.5410 mle=1.8242 pcon=5.0148 forget=1.7020 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.5199 mle=1.8101 pcon=5.0125 forget=1.6973 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.3460 mle=1.6420 pcon=5.0102 forget=1.6939 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.3399 mle=1.6348 pcon=5.0076 forget=1.6975 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.2185 mle=1.5172 pcon=5.0052 forget=1.6962 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.4355 mle=1.7300 pcon=5.0026 forget=1.7029 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.4918 mle=1.7826 pcon=5.0000 forget=1.7092 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.3453 mle=1.6662 pcon=4.9975 forget=1.6816 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.2952 mle=1.6081 pcon=4.9947 forget=1.6924 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.3254 mle=1.6346 pcon=4.9919 forget=1.6990 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.3613 mle=1.6791 pcon=4.9890 forget=1.6932 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.3032 mle=1.6355 pcon=4.9864 forget=1.6813 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.3370 mle=1.6729 pcon=4.9835 forget=1.6806 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.3137 mle=1.6535 pcon=4.9805 forget=1.6796 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.2401 mle=1.5697 pcon=4.9777 forget=1.6927 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.2158 mle=1.5622 pcon=4.9747 forget=1.6790 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.3467 mle=1.6986 pcon=4.9718 forget=1.6763 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.3106 mle=1.6719 pcon=4.9691 forget=1.6697 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.2878 mle=1.6468 pcon=4.9664 forget=1.6747 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 20 it 0 total=8.1308 mle=1.4893 pcon=4.9635 forget=1.6779 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.3236 mle=1.6967 pcon=4.9606 forget=1.6663 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.1774 mle=1.5555 pcon=4.9577 forget=1.6642 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.2803 mle=1.6618 pcon=4.9548 forget=1.6637 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.2414 mle=1.6205 pcon=4.9519 forget=1.6690 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=8.3700 mle=1.7594 pcon=4.9490 forget=1.6616 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=8.3954 mle=1.7947 pcon=4.9463 forget=1.6545 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=8.3507 mle=1.7353 pcon=4.9434 forget=1.6720 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 21 it 10 total=8.0337 mle=1.4363 pcon=4.9404 forget=1.6570 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 49%|████▉     | 22/45 [03:41<03:31,  9.18s/it] 51%|█████     | 23/45 [03:49<03:15,  8.90s/it] 53%|█████▎    | 24/45 [04:00<03:18,  9.43s/it] 56%|█████▌    | 25/45 [04:10<03:11,  9.56s/it] 58%|█████▊    | 26/45 [04:19<02:58,  9.38s/it] 60%|██████    | 27/45 [04:29<02:51,  9.52s/it][loss] ep 21 it 60 total=8.2736 mle=1.6755 pcon=4.9375 forget=1.6605 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.2445 mle=1.6390 pcon=4.9347 forget=1.6707 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.2854 mle=1.6983 pcon=4.9319 forget=1.6553 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=8.2047 mle=1.6030 pcon=4.9290 forget=1.6728 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.5170 mle=1.9338 pcon=4.9263 forget=1.6569 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.3203 mle=1.7369 pcon=4.9234 forget=1.6600 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.2959 mle=1.7190 pcon=4.9206 forget=1.6564 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 22 it 20 total=8.2913 mle=1.7152 pcon=4.9180 forget=1.6581 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.2224 mle=1.6575 pcon=4.9153 forget=1.6496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.3212 mle=1.7498 pcon=4.9127 forget=1.6587 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.1951 mle=1.6341 pcon=4.9100 forget=1.6510 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.3417 mle=1.7807 pcon=4.9075 forget=1.6535 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.1948 mle=1.6275 pcon=4.9051 forget=1.6622 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.0814 mle=1.5353 pcon=4.9027 forget=1.6434 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=8.2538 mle=1.7071 pcon=4.9000 forget=1.6468 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 23 it 30 total=8.1747 mle=1.6349 pcon=4.8976 forget=1.6422 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.1869 mle=1.6415 pcon=4.8950 forget=1.6503 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=8.3836 mle=1.8415 pcon=4.8925 forget=1.6496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.1803 mle=1.6376 pcon=4.8899 forget=1.6528 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.1733 mle=1.6291 pcon=4.8875 forget=1.6567 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=8.2894 mle=1.7644 pcon=4.8851 forget=1.6399 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=8.3243 mle=1.7895 pcon=4.8826 forget=1.6523 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.0653 mle=1.5391 pcon=4.8801 forget=1.6461 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 24 it 40 total=8.1807 mle=1.6531 pcon=4.8779 forget=1.6497 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=8.3128 mle=1.7937 pcon=4.8754 forget=1.6436 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.1217 mle=1.6118 pcon=4.8730 forget=1.6369 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.1387 mle=1.6289 pcon=4.8706 forget=1.6392 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=8.1551 mle=1.6498 pcon=4.8683 forget=1.6370 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.2086 mle=1.7110 pcon=4.8657 forget=1.6319 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.1246 mle=1.6194 pcon=4.8632 forget=1.6420 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 25 it 0 total=8.2527 mle=1.7512 pcon=4.8610 forget=1.6406 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 50 total=8.1623 mle=1.6566 pcon=4.8586 forget=1.6471 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 100 total=8.2278 mle=1.7318 pcon=4.8562 forget=1.6399 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 150 total=8.2258 mle=1.7381 pcon=4.8540 forget=1.6337 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 200 total=8.2571 mle=1.7730 pcon=4.8517 forget=1.6324 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 250 total=8.0898 mle=1.5976 pcon=4.8495 forget=1.6427 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 300 total=8.1720 mle=1.6890 pcon=4.8472 forget=1.6358 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 350 total=8.3077 mle=1.8329 pcon=4.8448 forget=1.6300 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 26 it 10 total=8.1907 mle=1.7191 pcon=4.8426 forget=1.6290 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 60 total=8.1900 mle=1.7403 pcon=4.8401 forget=1.6096 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 110 total=8.2069 mle=1.7258 pcon=4.8376 forget=1.6435 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 160 total=8.0427 mle=1.5879 pcon=4.8351 forget=1.6197 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 210 total=8.1661 mle=1.7201 pcon=4.8328 forget=1.6132 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 260 total=8.1162 mle=1.6763 pcon=4.8306 forget=1.6093 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 310 total=8.3081 mle=1.8540 pcon=4.8280 forget=1.6262 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 360 total=8.3231 mle=1.8824 pcon=4.8254 forget=1.6152 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 27 it 20 total=8.0178 mle=1.5925 pcon=4.8230 forget=1.6022 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 70 total=8.0687 mle=1.6538 pcon=4.8203 forget=1.5947 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 120 total=8.1618 mle=1.7172 pcon=4.8178 forget=1.6268 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 170 total=7.9809 mle=1.5584 pcon=4.8152 forget=1.6073 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 220 total=8.0684 mle=1.6631 pcon=4.8128 forget=1.5925 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 270 total=7.9945 mle=1.5883 pcon=4.8100 forget=1.5962 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 320 total=8.1281 mle=1.7126 pcon=4.8075 forget=1.6080 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 62%|██████▏   | 28/45 [04:38<02:40,  9.42s/it] 64%|██████▍   | 29/45 [04:47<02:31,  9.48s/it] 67%|██████▋   | 30/45 [04:57<02:24,  9.65s/it] 69%|██████▉   | 31/45 [05:09<02:24, 10.30s/it] 71%|███████   | 32/45 [05:19<02:12, 10.19s/it] 73%|███████▎  | 33/45 [05:28<01:56,  9.73s/it] 76%|███████▌  | 34/45 [05:40<01:55, 10.48s/it] 78%|███████▊  | 35/45 [05:54<01:54, 11.50s/it][loss] ep 27 it 370 total=7.9244 mle=1.5800 pcon=4.8049 forget=1.5395 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 28 it 30 total=8.0851 mle=1.7183 pcon=4.8024 forget=1.5644 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 80 total=8.1209 mle=1.7546 pcon=4.7996 forget=1.5667 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 130 total=7.8768 mle=1.5023 pcon=4.7971 forget=1.5775 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 180 total=7.8614 mle=1.5200 pcon=4.7943 forget=1.5472 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 230 total=7.9827 mle=1.6187 pcon=4.7917 forget=1.5724 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 280 total=8.0161 mle=1.6725 pcon=4.7892 forget=1.5544 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 330 total=7.9053 mle=1.5821 pcon=4.7864 forget=1.5368 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 380 total=7.9681 mle=1.6604 pcon=4.7837 forget=1.5239 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 29 it 40 total=7.9044 mle=1.6062 pcon=4.7811 forget=1.5172 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 90 total=7.9728 mle=1.6389 pcon=4.7785 forget=1.5555 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 140 total=7.8894 mle=1.5671 pcon=4.7760 forget=1.5462 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 190 total=8.2150 mle=1.9157 pcon=4.7735 forget=1.5258 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 240 total=7.9667 mle=1.6750 pcon=4.7713 forget=1.5205 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 290 total=7.9069 mle=1.6684 pcon=4.7688 forget=1.4697 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 340 total=7.9890 mle=1.6784 pcon=4.7667 forget=1.5439 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 30 it 0 total=7.9794 mle=1.6860 pcon=4.7645 forget=1.5289 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 50 total=7.9269 mle=1.6338 pcon=4.7622 forget=1.5308 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 100 total=7.9276 mle=1.6813 pcon=4.7604 forget=1.4859 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 150 total=7.8288 mle=1.5583 pcon=4.7586 forget=1.5120 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 200 total=7.9332 mle=1.6858 pcon=4.7568 forget=1.4906 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 250 total=7.9582 mle=1.6876 pcon=4.7552 forget=1.5155 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 300 total=8.0209 mle=1.7484 pcon=4.7532 forget=1.5193 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 350 total=7.9159 mle=1.6854 pcon=4.7516 forget=1.4790 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 31 it 10 total=8.0129 mle=1.7493 pcon=4.7499 forget=1.5137 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 60 total=7.8408 mle=1.5692 pcon=4.7484 forget=1.5232 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 110 total=8.0571 mle=1.7374 pcon=4.7471 forget=1.5726 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 160 total=7.8942 mle=1.6212 pcon=4.7458 forget=1.5273 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 210 total=8.1686 mle=1.8504 pcon=4.7446 forget=1.5737 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 260 total=7.9824 mle=1.6652 pcon=4.7433 forget=1.5738 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 310 total=7.8800 mle=1.5673 pcon=4.7420 forget=1.5707 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 360 total=7.9478 mle=1.6394 pcon=4.7410 forget=1.5673 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 20 total=7.9258 mle=1.5807 pcon=4.7401 forget=1.6050 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 70 total=7.8348 mle=1.5408 pcon=4.7392 forget=1.5548 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 120 total=7.9316 mle=1.5948 pcon=4.7381 forget=1.5987 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 170 total=7.8954 mle=1.5697 pcon=4.7371 forget=1.5886 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 220 total=7.9305 mle=1.6195 pcon=4.7361 forget=1.5748 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 270 total=7.9248 mle=1.5841 pcon=4.7352 forget=1.6055 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 320 total=7.8465 mle=1.5254 pcon=4.7342 forget=1.5868 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 370 total=7.8527 mle=1.5060 pcon=4.7336 forget=1.6131 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 30 total=8.1098 mle=1.7681 pcon=4.7328 forget=1.6089 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 80 total=8.1720 mle=1.8683 pcon=4.7320 forget=1.5717 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 130 total=8.0777 mle=1.7337 pcon=4.7314 forget=1.6126 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 180 total=7.9177 mle=1.5979 pcon=4.7308 forget=1.5891 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 230 total=8.0939 mle=1.7698 pcon=4.7303 forget=1.5938 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 280 total=7.9886 mle=1.6570 pcon=4.7296 forget=1.6020 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 330 total=8.1398 mle=1.7965 pcon=4.7289 forget=1.6143 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 380 total=8.0079 mle=1.6526 pcon=4.7285 forget=1.6268 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 40 total=8.0825 mle=1.7370 pcon=4.7279 forget=1.6176 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 90 total=8.0213 mle=1.6596 pcon=4.7274 forget=1.6343 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 140 total=8.0233 mle=1.6560 pcon=4.7269 forget=1.6404 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 190 total=8.0195 mle=1.6451 pcon=4.7264 forget=1.6480 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 240 total=8.0553 mle=1.6747 pcon=4.7258 forget=1.6547 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 290 total=7.9562 mle=1.5803 pcon=4.7254 forget=1.6505 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 340 total=7.9775 mle=1.5995 pcon=4.7250 forget=1.6530 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 0 total=7.9670 mle=1.5750 pcon=4.7247 forget=1.6673 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 80%|████████  | 36/45 [06:06<01:45, 11.69s/it] 82%|████████▏ | 37/45 [06:15<01:26, 10.76s/it] 84%|████████▍ | 38/45 [06:23<01:09, 10.00s/it] 87%|████████▋ | 39/45 [06:33<01:00, 10.09s/it] 89%|████████▉ | 40/45 [06:44<00:52, 10.44s/it] 91%|█████████ | 41/45 [06:56<00:42, 10.73s/it] 93%|█████████▎| 42/45 [07:06<00:32, 10.69s/it] 96%|█████████▌| 43/45 [07:18<00:21, 10.87s/it][loss] ep 35 it 50 total=8.1474 mle=1.7428 pcon=4.7244 forget=1.6802 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 100 total=7.9532 mle=1.5678 pcon=4.7242 forget=1.6612 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 150 total=7.9680 mle=1.5663 pcon=4.7237 forget=1.6780 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 200 total=8.0491 mle=1.6455 pcon=4.7234 forget=1.6802 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 250 total=8.1595 mle=1.7563 pcon=4.7232 forget=1.6800 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 300 total=8.1223 mle=1.7195 pcon=4.7228 forget=1.6800 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 350 total=8.0629 mle=1.6427 pcon=4.7226 forget=1.6976 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 10 total=8.1582 mle=1.7368 pcon=4.7222 forget=1.6992 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 60 total=8.2758 mle=1.8513 pcon=4.7218 forget=1.7027 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 110 total=8.3230 mle=1.8853 pcon=4.7217 forget=1.7160 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 160 total=8.2142 mle=1.7712 pcon=4.7214 forget=1.7216 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 210 total=8.3029 mle=1.8592 pcon=4.7209 forget=1.7227 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 260 total=8.2652 mle=1.8094 pcon=4.7207 forget=1.7351 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 310 total=8.0506 mle=1.6154 pcon=4.7206 forget=1.7147 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 360 total=8.0424 mle=1.5881 pcon=4.7206 forget=1.7337 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 20 total=8.2835 mle=1.8237 pcon=4.7204 forget=1.7395 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 70 total=8.2054 mle=1.7554 pcon=4.7201 forget=1.7298 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 120 total=8.0686 mle=1.5945 pcon=4.7197 forget=1.7544 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 170 total=8.0533 mle=1.5918 pcon=4.7197 forget=1.7418 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 220 total=8.0597 mle=1.5881 pcon=4.7194 forget=1.7523 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 270 total=8.0354 mle=1.5724 pcon=4.7192 forget=1.7438 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 320 total=8.2069 mle=1.7395 pcon=4.7191 forget=1.7483 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 370 total=8.1053 mle=1.6113 pcon=4.7190 forget=1.7751 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 30 total=8.1778 mle=1.6877 pcon=4.7188 forget=1.7713 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 80 total=8.1405 mle=1.6491 pcon=4.7186 forget=1.7728 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 130 total=8.1096 mle=1.6172 pcon=4.7184 forget=1.7740 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 180 total=8.0295 mle=1.5412 pcon=4.7181 forget=1.7702 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 230 total=8.1807 mle=1.6665 pcon=4.7180 forget=1.7963 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 280 total=8.3827 mle=1.8824 pcon=4.7178 forget=1.7825 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 330 total=8.3538 mle=1.8278 pcon=4.7175 forget=1.8084 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 380 total=8.3274 mle=1.7975 pcon=4.7172 forget=1.8126 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 40 total=8.2011 mle=1.6748 pcon=4.7171 forget=1.8092 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 90 total=8.1695 mle=1.6409 pcon=4.7170 forget=1.8115 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 140 total=8.2126 mle=1.7102 pcon=4.7169 forget=1.7855 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 190 total=8.2066 mle=1.6600 pcon=4.7168 forget=1.8299 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 240 total=8.1889 mle=1.6644 pcon=4.7167 forget=1.8078 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 290 total=8.1777 mle=1.6435 pcon=4.7164 forget=1.8178 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 340 total=8.3026 mle=1.7588 pcon=4.7165 forget=1.8274 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 0 total=8.2056 mle=1.6503 pcon=4.7162 forget=1.8391 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 50 total=8.1790 mle=1.6227 pcon=4.7161 forget=1.8402 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 100 total=8.2151 mle=1.6645 pcon=4.7160 forget=1.8345 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 150 total=8.3158 mle=1.7849 pcon=4.7159 forget=1.8150 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 200 total=8.2473 mle=1.6834 pcon=4.7157 forget=1.8481 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 250 total=8.2263 mle=1.6543 pcon=4.7155 forget=1.8565 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 300 total=8.3093 mle=1.7411 pcon=4.7154 forget=1.8528 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 350 total=8.1799 mle=1.6190 pcon=4.7152 forget=1.8456 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 10 total=8.3609 mle=1.8153 pcon=4.7151 forget=1.8304 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 60 total=8.2624 mle=1.6651 pcon=4.7152 forget=1.8821 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 110 total=8.3003 mle=1.7079 pcon=4.7151 forget=1.8772 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 160 total=8.1777 mle=1.5974 pcon=4.7149 forget=1.8654 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 210 total=8.2848 mle=1.6996 pcon=4.7147 forget=1.8705 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 260 total=8.2832 mle=1.6919 pcon=4.7147 forget=1.8766 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 310 total=8.2659 mle=1.6656 pcon=4.7147 forget=1.8857 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 360 total=8.3158 mle=1.6999 pcon=4.7146 forget=1.9013 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 20 total=8.2768 mle=1.6773 pcon=4.7146 forget=1.8849 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 70 total=8.2440 mle=1.6335 pcon=4.7145 forget=1.8960 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 120 total=8.3553 mle=1.7344 pcon=4.7144 forget=1.9065 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 170 total=8.4880 mle=1.8692 pcon=4.7143 forget=1.9045 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 220 total=8.1590 mle=1.5501 pcon=4.7142 forget=1.8947 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 270 total=8.2679 mle=1.6769 pcon=4.7141 forget=1.8768 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 320 total=8.5611 mle=1.9341 pcon=4.7140 forget=1.9130 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 370 total=8.2916 mle=1.6635 pcon=4.7138 forget=1.9142 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 98%|█████████▊| 44/45 [07:26<00:10, 10.05s/it]100%|██████████| 45/45 [07:33<00:00,  9.23s/it]100%|██████████| 45/45 [07:33<00:00, 10.08s/it]
[loss] ep 43 it 30 total=8.5204 mle=1.8830 pcon=4.7138 forget=1.9236 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 80 total=8.4196 mle=1.8155 pcon=4.7135 forget=1.8905 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 130 total=8.5949 mle=1.9561 pcon=4.7135 forget=1.9253 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 180 total=8.1708 mle=1.5029 pcon=4.7133 forget=1.9546 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 230 total=8.1362 mle=1.5086 pcon=4.7133 forget=1.9143 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 280 total=8.2158 mle=1.5896 pcon=4.7132 forget=1.9130 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 330 total=8.4053 mle=1.7569 pcon=4.7132 forget=1.9353 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 380 total=8.3943 mle=1.7692 pcon=4.7131 forget=1.9121 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 40 total=8.4235 mle=1.7603 pcon=4.7130 forget=1.9502 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 90 total=8.2969 mle=1.6257 pcon=4.7129 forget=1.9583 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 140 total=8.3485 mle=1.7138 pcon=4.7128 forget=1.9219 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 190 total=8.4558 mle=1.7626 pcon=4.7127 forget=1.9805 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 240 total=8.3590 mle=1.6593 pcon=4.7126 forget=1.9872 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 290 total=8.5387 mle=1.9055 pcon=4.7125 forget=1.9206 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 340 total=8.3134 mle=1.6521 pcon=4.7125 forget=1.9488 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[Run] lambda=0.2 lr=0.001 epochs=50
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:168: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:13<11:25, 13.99s/it]  4%|▍         | 2/50 [00:22<08:43, 10.90s/it]  6%|▌         | 3/50 [00:32<08:12, 10.48s/it]  8%|▊         | 4/50 [00:43<08:03, 10.51s/it] 10%|█         | 5/50 [00:52<07:39, 10.22s/it] 12%|█▏        | 6/50 [01:00<06:55,  9.45s/it][loss] ep 0 it 0 total=9.3215 mle=1.5618 pcon=5.2951 forget=2.4646 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.3417 mle=1.6308 pcon=5.2880 forget=2.4229 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.5222 mle=1.7912 pcon=5.2813 forget=2.4496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.4588 mle=1.7481 pcon=5.2750 forget=2.4357 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.5113 mle=1.8428 pcon=5.2683 forget=2.4001 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.4072 mle=1.6799 pcon=5.2618 forget=2.4655 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.3328 mle=1.6256 pcon=5.2557 forget=2.4515 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.5303 mle=1.8021 pcon=5.2498 forget=2.4784 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 1 it 10 total=9.1505 mle=1.4466 pcon=5.2437 forget=2.4601 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.3555 mle=1.6387 pcon=5.2378 forget=2.4791 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=9.1933 mle=1.5268 pcon=5.2322 forget=2.4342 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=9.3108 mle=1.6062 pcon=5.2265 forget=2.4782 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.3542 mle=1.6694 pcon=5.2211 forget=2.4636 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=9.1898 mle=1.5595 pcon=5.2158 forget=2.4145 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=9.2953 mle=1.6336 pcon=5.2102 forget=2.4514 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.3863 mle=1.7250 pcon=5.2050 forget=2.4563 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 2 it 20 total=9.0648 mle=1.4192 pcon=5.1998 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.5212 mle=1.8334 pcon=5.1947 forget=2.4932 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=9.3052 mle=1.6841 pcon=5.1895 forget=2.4315 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=9.2922 mle=1.6702 pcon=5.1848 forget=2.4372 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=9.4408 mle=1.8057 pcon=5.1798 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.3900 mle=1.7527 pcon=5.1752 forget=2.4621 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=9.0232 mle=1.4275 pcon=5.1706 forget=2.4251 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.2261 mle=1.6127 pcon=5.1666 forget=2.4467 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 3 it 30 total=9.2788 mle=1.6634 pcon=5.1623 forget=2.4531 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=9.1715 mle=1.5681 pcon=5.1582 forget=2.4452 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=9.1480 mle=1.5600 pcon=5.1540 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.3952 mle=1.8278 pcon=5.1499 forget=2.4176 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=9.3115 mle=1.7049 pcon=5.1462 forget=2.4605 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.3623 mle=1.7596 pcon=5.1425 forget=2.4601 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.4044 mle=1.8243 pcon=5.1385 forget=2.4416 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=9.2006 mle=1.6193 pcon=5.1348 forget=2.4466 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 4 it 40 total=9.1423 mle=1.5754 pcon=5.1311 forget=2.4357 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=9.1358 mle=1.5309 pcon=5.1276 forget=2.4773 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.2282 mle=1.7051 pcon=5.1245 forget=2.3987 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.2333 mle=1.6600 pcon=5.1210 forget=2.4523 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.3930 mle=1.8095 pcon=5.1177 forget=2.4658 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.0164 mle=1.4522 pcon=5.1146 forget=2.4496 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.2410 mle=1.6860 pcon=5.1118 forget=2.4432 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 5 it 0 total=9.2135 mle=1.6088 pcon=5.1085 forget=2.4961 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=9.2653 mle=1.6928 pcon=5.1056 forget=2.4669 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.3822 mle=1.7998 pcon=5.1025 forget=2.4799 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.5467 mle=1.9763 pcon=5.0994 forget=2.4709 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.3239 mle=1.7905 pcon=5.0965 forget=2.4369 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=9.2513 mle=1.7138 pcon=5.0935 forget=2.4440 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=9.2809 mle=1.7677 pcon=5.0905 forget=2.4227 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=9.2407 mle=1.7190 pcon=5.0876 forget=2.4340 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 6 it 10 total=9.1400 mle=1.5563 pcon=5.0849 forget=2.4987 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=9.2203 mle=1.7051 pcon=5.0823 forget=2.4329 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=9.2530 mle=1.6996 pcon=5.0799 forget=2.4735 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=9.0338 mle=1.5105 pcon=5.0775 forget=2.4458 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=9.2065 mle=1.6766 pcon=5.0748 forget=2.4552 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=9.2962 mle=1.7616 pcon=5.0724 forget=2.4622 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=8.9789 mle=1.4948 pcon=5.0702 forget=2.4140 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 14%|█▍        | 7/50 [01:10<06:43,  9.38s/it] 16%|█▌        | 8/50 [01:19<06:30,  9.30s/it] 18%|█▊        | 9/50 [01:28<06:18,  9.23s/it] 20%|██        | 10/50 [01:37<06:08,  9.21s/it] 22%|██▏       | 11/50 [01:46<06:01,  9.26s/it] 24%|██▍       | 12/50 [01:56<05:59,  9.45s/it] 26%|██▌       | 13/50 [02:06<05:48,  9.42s/it][loss] ep 6 it 360 total=9.1805 mle=1.7148 pcon=5.0677 forget=2.3980 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 7 it 20 total=9.0256 mle=1.5090 pcon=5.0654 forget=2.4511 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=9.0876 mle=1.5736 pcon=5.0629 forget=2.4510 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=9.2397 mle=1.7393 pcon=5.0607 forget=2.4397 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=9.2180 mle=1.7607 pcon=5.0585 forget=2.3988 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=9.1345 mle=1.6537 pcon=5.0567 forget=2.4240 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=9.3683 mle=1.8772 pcon=5.0548 forget=2.4363 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=9.0491 mle=1.5493 pcon=5.0525 forget=2.4473 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=9.3152 mle=1.7890 pcon=5.0503 forget=2.4758 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 8 it 30 total=9.2469 mle=1.7899 pcon=5.0485 forget=2.4086 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=9.0188 mle=1.5221 pcon=5.0465 forget=2.4502 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=9.2263 mle=1.7440 pcon=5.0447 forget=2.4377 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=9.1380 mle=1.6514 pcon=5.0428 forget=2.4438 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=9.0534 mle=1.5674 pcon=5.0410 forget=2.4450 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.9495 mle=1.4576 pcon=5.0392 forget=2.4528 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.1964 mle=1.7884 pcon=5.0372 forget=2.3708 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=9.1247 mle=1.6499 pcon=5.0357 forget=2.4391 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 9 it 40 total=9.2892 mle=1.7969 pcon=5.0340 forget=2.4583 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=9.1831 mle=1.7484 pcon=5.0322 forget=2.4024 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=9.1800 mle=1.6838 pcon=5.0310 forget=2.4651 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.9753 mle=1.4940 pcon=5.0295 forget=2.4517 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=9.1877 mle=1.7624 pcon=5.0282 forget=2.3971 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=9.1276 mle=1.6624 pcon=5.0265 forget=2.4386 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.9612 mle=1.5042 pcon=5.0251 forget=2.4318 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 10 it 0 total=8.9687 mle=1.4925 pcon=5.0232 forget=2.4530 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.9283 mle=1.4683 pcon=5.0221 forget=2.4378 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=9.1262 mle=1.6908 pcon=5.0206 forget=2.4148 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=9.1776 mle=1.7025 pcon=5.0192 forget=2.4558 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=9.2079 mle=1.7846 pcon=5.0180 forget=2.4054 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=9.2189 mle=1.8305 pcon=5.0167 forget=2.3717 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=9.0363 mle=1.7733 pcon=5.0153 forget=2.2477 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.7998 mle=1.5405 pcon=5.0143 forget=2.2450 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 11 it 10 total=8.9251 mle=1.7704 pcon=5.0130 forget=2.1418 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.6583 mle=1.7155 pcon=5.0121 forget=1.9307 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.4464 mle=1.5934 pcon=5.0113 forget=1.8417 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.3466 mle=1.5853 pcon=5.0105 forget=1.7508 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.2686 mle=1.5486 pcon=5.0101 forget=1.7099 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.2468 mle=1.5720 pcon=5.0099 forget=1.6650 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.5001 mle=1.7995 pcon=5.0100 forget=1.6905 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.3693 mle=1.6841 pcon=5.0104 forget=1.6747 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 12 it 20 total=8.2370 mle=1.5756 pcon=5.0105 forget=1.6508 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.1950 mle=1.5172 pcon=5.0109 forget=1.6668 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.3775 mle=1.7296 pcon=5.0115 forget=1.6363 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.3802 mle=1.7084 pcon=5.0122 forget=1.6596 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.2230 mle=1.5411 pcon=5.0134 forget=1.6686 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.2698 mle=1.5732 pcon=5.0146 forget=1.6821 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.3780 mle=1.6694 pcon=5.0158 forget=1.6929 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.4125 mle=1.6982 pcon=5.0169 forget=1.6974 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 13 it 30 total=8.4221 mle=1.7029 pcon=5.0182 forget=1.7010 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.5390 mle=1.7934 pcon=5.0195 forget=1.7262 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.4024 mle=1.6674 pcon=5.0204 forget=1.7146 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=8.4998 mle=1.7533 pcon=5.0212 forget=1.7253 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 28%|██▊       | 14/50 [02:14<05:28,  9.11s/it] 30%|███       | 15/50 [02:24<05:30,  9.44s/it] 32%|███▏      | 16/50 [02:33<05:15,  9.27s/it] 34%|███▍      | 17/50 [02:43<05:09,  9.37s/it] 36%|███▌      | 18/50 [02:52<05:00,  9.38s/it] 38%|███▊      | 19/50 [03:02<04:53,  9.47s/it] 40%|████      | 20/50 [03:11<04:41,  9.37s/it] 42%|████▏     | 21/50 [03:20<04:32,  9.38s/it][loss] ep 13 it 230 total=8.4090 mle=1.6559 pcon=5.0225 forget=1.7306 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.2083 mle=1.4530 pcon=5.0236 forget=1.7317 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.5212 mle=1.7616 pcon=5.0248 forget=1.7347 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.3952 mle=1.6339 pcon=5.0260 forget=1.7352 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 40 total=8.4595 mle=1.6857 pcon=5.0270 forget=1.7468 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.3032 mle=1.5259 pcon=5.0281 forget=1.7492 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.4265 mle=1.6575 pcon=5.0290 forget=1.7400 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.3701 mle=1.5981 pcon=5.0299 forget=1.7421 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.4314 mle=1.6482 pcon=5.0304 forget=1.7528 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.3161 mle=1.5428 pcon=5.0312 forget=1.7421 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.3408 mle=1.5644 pcon=5.0319 forget=1.7445 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 0 total=8.3957 mle=1.6246 pcon=5.0322 forget=1.7390 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.3343 mle=1.5621 pcon=5.0327 forget=1.7395 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.4004 mle=1.6352 pcon=5.0331 forget=1.7321 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.4656 mle=1.7023 pcon=5.0333 forget=1.7300 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.2729 mle=1.5061 pcon=5.0334 forget=1.7334 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.3145 mle=1.5502 pcon=5.0332 forget=1.7311 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.5493 mle=1.7868 pcon=5.0329 forget=1.7295 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.3624 mle=1.6083 pcon=5.0326 forget=1.7215 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 10 total=8.3427 mle=1.5753 pcon=5.0319 forget=1.7356 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.2707 mle=1.5169 pcon=5.0313 forget=1.7225 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.1635 mle=1.4123 pcon=5.0304 forget=1.7208 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.2461 mle=1.5025 pcon=5.0295 forget=1.7141 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.3132 mle=1.5733 pcon=5.0284 forget=1.7116 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.6127 mle=1.8714 pcon=5.0269 forget=1.7144 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.6368 mle=1.8935 pcon=5.0252 forget=1.7180 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.2787 mle=1.5520 pcon=5.0236 forget=1.7032 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.5174 mle=1.7941 pcon=5.0220 forget=1.7013 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.4928 mle=1.7656 pcon=5.0204 forget=1.7068 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.2554 mle=1.5373 pcon=5.0182 forget=1.6999 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.4733 mle=1.7361 pcon=5.0160 forget=1.7212 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.5346 mle=1.8233 pcon=5.0136 forget=1.6977 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.5157 mle=1.8120 pcon=5.0113 forget=1.6925 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.3406 mle=1.6430 pcon=5.0088 forget=1.6887 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.3351 mle=1.6364 pcon=5.0061 forget=1.6926 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.2127 mle=1.5182 pcon=5.0036 forget=1.6909 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.4286 mle=1.7304 pcon=5.0010 forget=1.6972 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.4856 mle=1.7841 pcon=4.9983 forget=1.7033 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.3377 mle=1.6668 pcon=4.9957 forget=1.6753 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.2870 mle=1.6082 pcon=4.9928 forget=1.6861 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.3183 mle=1.6364 pcon=4.9899 forget=1.6920 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.3511 mle=1.6781 pcon=4.9869 forget=1.6861 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.2933 mle=1.6353 pcon=4.9842 forget=1.6738 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 19 it 40 total=8.3283 mle=1.6744 pcon=4.9812 forget=1.6727 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.3024 mle=1.6531 pcon=4.9781 forget=1.6712 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.2301 mle=1.5709 pcon=4.9752 forget=1.6840 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.2040 mle=1.5617 pcon=4.9721 forget=1.6703 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.3363 mle=1.7000 pcon=4.9690 forget=1.6674 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.2971 mle=1.6713 pcon=4.9662 forget=1.6596 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.2770 mle=1.6491 pcon=4.9634 forget=1.6644 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 20 it 0 total=8.1189 mle=1.4912 pcon=4.9605 forget=1.6672 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.3097 mle=1.6973 pcon=4.9574 forget=1.6550 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.1623 mle=1.5556 pcon=4.9544 forget=1.6522 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.2640 mle=1.6608 pcon=4.9513 forget=1.6518 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.2268 mle=1.6218 pcon=4.9484 forget=1.6567 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=8.3452 mle=1.7516 pcon=4.9454 forget=1.6483 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=8.3788 mle=1.7954 pcon=4.9425 forget=1.6408 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=8.3315 mle=1.7337 pcon=4.9395 forget=1.6583 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
 44%|████▍     | 22/50 [03:30<04:24,  9.44s/it] 46%|████▌     | 23/50 [03:40<04:20,  9.65s/it] 48%|████▊     | 24/50 [03:49<04:08,  9.55s/it] 50%|█████     | 25/50 [03:59<03:58,  9.55s/it] 52%|█████▏    | 26/50 [04:08<03:42,  9.27s/it] 54%|█████▍    | 27/50 [04:18<03:39,  9.54s/it][loss] ep 21 it 10 total=8.0164 mle=1.4378 pcon=4.9363 forget=1.6423 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=8.2555 mle=1.6768 pcon=4.9333 forget=1.6454 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.2330 mle=1.6474 pcon=4.9304 forget=1.6551 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.2649 mle=1.6979 pcon=4.9274 forget=1.6396 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=8.1774 mle=1.5970 pcon=4.9244 forget=1.6560 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.4949 mle=1.9341 pcon=4.9216 forget=1.6392 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.2952 mle=1.7349 pcon=4.9185 forget=1.6417 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.2711 mle=1.7178 pcon=4.9155 forget=1.6378 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 22 it 20 total=8.2663 mle=1.7150 pcon=4.9128 forget=1.6385 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.1969 mle=1.6573 pcon=4.9100 forget=1.6295 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.2951 mle=1.7500 pcon=4.9072 forget=1.6380 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.1734 mle=1.6395 pcon=4.9043 forget=1.6297 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.3117 mle=1.7796 pcon=4.9017 forget=1.6304 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.1646 mle=1.6265 pcon=4.8991 forget=1.6390 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.0503 mle=1.5351 pcon=4.8965 forget=1.6187 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=8.2208 mle=1.7056 pcon=4.8936 forget=1.6216 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 23 it 30 total=8.1430 mle=1.6365 pcon=4.8911 forget=1.6155 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.1548 mle=1.6431 pcon=4.8883 forget=1.6234 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=8.3544 mle=1.8475 pcon=4.8855 forget=1.6214 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.1442 mle=1.6377 pcon=4.8828 forget=1.6237 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.1411 mle=1.6335 pcon=4.8802 forget=1.6274 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=8.2561 mle=1.7699 pcon=4.8776 forget=1.6087 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=8.2841 mle=1.7887 pcon=4.8748 forget=1.6206 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.0231 mle=1.5386 pcon=4.8721 forget=1.6123 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 24 it 40 total=8.1358 mle=1.6499 pcon=4.8697 forget=1.6161 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=8.2684 mle=1.7936 pcon=4.8670 forget=1.6078 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.0754 mle=1.6123 pcon=4.8643 forget=1.5988 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.0919 mle=1.6311 pcon=4.8616 forget=1.5992 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=8.1034 mle=1.6475 pcon=4.8591 forget=1.5969 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.1591 mle=1.7144 pcon=4.8562 forget=1.5885 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.0704 mle=1.6193 pcon=4.8535 forget=1.5977 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 25 it 0 total=8.2022 mle=1.7551 pcon=4.8509 forget=1.5961 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 50 total=8.1043 mle=1.6556 pcon=4.8482 forget=1.6005 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 100 total=8.1681 mle=1.7315 pcon=4.8455 forget=1.5910 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 150 total=8.1646 mle=1.7370 pcon=4.8431 forget=1.5846 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 200 total=8.1868 mle=1.7638 pcon=4.8404 forget=1.5826 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 250 total=8.0235 mle=1.5953 pcon=4.8378 forget=1.5904 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 300 total=8.1019 mle=1.6868 pcon=4.8352 forget=1.5799 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 350 total=8.2448 mle=1.8364 pcon=4.8325 forget=1.5758 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 26 it 10 total=8.1182 mle=1.7171 pcon=4.8299 forget=1.5712 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 60 total=8.1094 mle=1.7401 pcon=4.8270 forget=1.5423 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 110 total=8.1310 mle=1.7272 pcon=4.8242 forget=1.5796 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 160 total=7.9639 mle=1.5897 pcon=4.8213 forget=1.5529 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 210 total=8.0785 mle=1.7161 pcon=4.8186 forget=1.5437 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 260 total=8.0279 mle=1.6722 pcon=4.8161 forget=1.5396 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 310 total=8.2180 mle=1.8529 pcon=4.8130 forget=1.5521 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 360 total=8.2430 mle=1.8925 pcon=4.8101 forget=1.5404 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 27 it 20 total=7.9216 mle=1.5937 pcon=4.8073 forget=1.5205 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 70 total=7.9745 mle=1.6582 pcon=4.8042 forget=1.5121 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 120 total=8.0750 mle=1.7212 pcon=4.8014 forget=1.5524 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 170 total=7.8854 mle=1.5637 pcon=4.7984 forget=1.5233 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 220 total=7.9670 mle=1.6633 pcon=4.7956 forget=1.5081 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 270 total=7.8947 mle=1.5903 pcon=4.7926 forget=1.5119 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 320 total=8.0247 mle=1.7147 pcon=4.7897 forget=1.5203 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA 56%|█████▌    | 28/50 [04:27<03:29,  9.52s/it] 58%|█████▊    | 29/50 [04:36<03:16,  9.36s/it] 60%|██████    | 30/50 [04:47<03:13,  9.67s/it] 62%|██████▏   | 31/50 [04:56<03:04,  9.73s/it] 64%|██████▍   | 32/50 [05:07<02:58,  9.89s/it] 66%|██████▌   | 33/50 [05:15<02:41,  9.51s/it] 68%|██████▊   | 34/50 [05:25<02:32,  9.55s/it] 70%|███████   | 35/50 [05:35<02:26,  9.75s/it]
[loss] ep 27 it 370 total=7.8119 mle=1.5820 pcon=4.7870 forget=1.4429 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 28 it 30 total=7.9729 mle=1.7149 pcon=4.7844 forget=1.4736 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 80 total=8.0142 mle=1.7544 pcon=4.7815 forget=1.4783 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 130 total=7.7805 mle=1.5049 pcon=4.7789 forget=1.4967 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 180 total=7.7632 mle=1.5268 pcon=4.7762 forget=1.4603 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 230 total=7.8954 mle=1.6226 pcon=4.7736 forget=1.4992 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 280 total=7.9275 mle=1.6725 pcon=4.7712 forget=1.4838 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 330 total=7.8218 mle=1.5854 pcon=4.7687 forget=1.4678 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 380 total=7.8864 mle=1.6640 pcon=4.7661 forget=1.4563 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 29 it 40 total=7.8087 mle=1.6139 pcon=4.7637 forget=1.4312 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 90 total=7.8841 mle=1.6390 pcon=4.7614 forget=1.4837 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 140 total=7.8184 mle=1.5725 pcon=4.7592 forget=1.4867 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 190 total=8.1490 mle=1.9220 pcon=4.7569 forget=1.4702 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 240 total=7.8886 mle=1.6691 pcon=4.7550 forget=1.4645 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 290 total=7.8501 mle=1.6669 pcon=4.7529 forget=1.4304 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 340 total=7.9372 mle=1.6854 pcon=4.7510 forget=1.5007 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-epochgrid-lambda0.2-planB_adapter
[loss] ep 30 it 0 total=7.9168 mle=1.6865 pcon=4.7490 forget=1.4812 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 50 total=7.8586 mle=1.6294 pcon=4.7470 forget=1.4822 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 100 total=7.8746 mle=1.6827 pcon=4.7455 forget=1.4463 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 150 total=7.7933 mle=1.5745 pcon=4.7440 forget=1.4749 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 200 total=7.9029 mle=1.6945 pcon=4.7424 forget=1.4660 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 250 total=7.9266 mle=1.7003 pcon=4.7411 forget=1.4853 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 300 total=7.9744 mle=1.7540 pcon=4.7394 forget=1.4809 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 350 total=7.8705 mle=1.6804 pcon=4.7380 forget=1.4522 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 10 total=7.9761 mle=1.7589 pcon=4.7365 forget=1.4806 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 60 total=7.8059 mle=1.5733 pcon=4.7353 forget=1.4973 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 110 total=8.0418 mle=1.7584 pcon=4.7342 forget=1.5492 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 160 total=7.8510 mle=1.6155 pcon=4.7330 forget=1.5025 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 210 total=8.1364 mle=1.8627 pcon=4.7320 forget=1.5418 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 260 total=7.9487 mle=1.6675 pcon=4.7309 forget=1.5502 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 310 total=7.8726 mle=1.6011 pcon=4.7298 forget=1.5418 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 360 total=7.9292 mle=1.6586 pcon=4.7290 forget=1.5416 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 20 total=7.8847 mle=1.5808 pcon=4.7282 forget=1.5757 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 70 total=7.8065 mle=1.5438 pcon=4.7274 forget=1.5353 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 120 total=7.9197 mle=1.6240 pcon=4.7265 forget=1.5693 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 170 total=7.8456 mle=1.5699 pcon=4.7257 forget=1.5500 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 220 total=7.9176 mle=1.6310 pcon=4.7247 forget=1.5619 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 270 total=7.8657 mle=1.5774 pcon=4.7239 forget=1.5644 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 320 total=7.8173 mle=1.5348 pcon=4.7231 forget=1.5594 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 370 total=7.8310 mle=1.5259 pcon=4.7224 forget=1.5826 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 30 total=8.0552 mle=1.7560 pcon=4.7217 forget=1.5775 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 80 total=8.1521 mle=1.8862 pcon=4.7209 forget=1.5449 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 130 total=8.0529 mle=1.7503 pcon=4.7204 forget=1.5823 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 180 total=7.8880 mle=1.5970 pcon=4.7198 forget=1.5712 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 230 total=8.0603 mle=1.7636 pcon=4.7193 forget=1.5775 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 280 total=7.9783 mle=1.6927 pcon=4.7186 forget=1.5671 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 330 total=8.1037 mle=1.8031 pcon=4.7179 forget=1.5827 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 380 total=7.9961 mle=1.6722 pcon=4.7174 forget=1.6064 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 40 total=8.0634 mle=1.7531 pcon=4.7168 forget=1.5935 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 90 total=7.9717 mle=1.6593 pcon=4.7163 forget=1.5961 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 140 total=7.9841 mle=1.6606 pcon=4.7157 forget=1.6078 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 190 total=8.0006 mle=1.6593 pcon=4.7152 forget=1.6261 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 240 total=7.9986 mle=1.6665 pcon=4.7146 forget=1.6175 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 290 total=7.9161 mle=1.5789 pcon=4.7141 forget=1.6231 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 340 total=7.9289 mle=1.5896 pcon=4.7136 forget=1.6257 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 0 total=7.9124 mle=1.5727 pcon=4.7131 forget=1.6266 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 50 total=8.1176 mle=1.7544 pcon=4.7128 forget=1.6504 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 100 total=7.9277 mle=1.5811 pcon=4.7125 forget=1.6342 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 72%|███████▏  | 36/50 [05:46<02:22, 10.15s/it] 74%|███████▍  | 37/50 [05:57<02:13, 10.23s/it] 76%|███████▌  | 38/50 [06:05<01:57,  9.77s/it] 78%|███████▊  | 39/50 [06:15<01:45,  9.61s/it] 80%|████████  | 40/50 [06:24<01:35,  9.58s/it] 82%|████████▏ | 41/50 [06:33<01:24,  9.39s/it] 84%|████████▍ | 42/50 [06:41<01:11,  8.96s/it] 86%|████████▌ | 43/50 [06:50<01:02,  9.00s/it][loss] ep 35 it 150 total=7.9146 mle=1.5623 pcon=4.7118 forget=1.6404 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 200 total=8.0254 mle=1.6587 pcon=4.7114 forget=1.6553 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 250 total=8.1143 mle=1.7588 pcon=4.7111 forget=1.6445 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 300 total=8.0780 mle=1.7278 pcon=4.7105 forget=1.6397 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 350 total=8.0278 mle=1.6689 pcon=4.7102 forget=1.6487 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 10 total=8.1047 mle=1.7328 pcon=4.7096 forget=1.6623 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 60 total=8.2192 mle=1.8477 pcon=4.7091 forget=1.6624 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 110 total=8.2653 mle=1.8827 pcon=4.7088 forget=1.6738 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 160 total=8.1585 mle=1.7774 pcon=4.7084 forget=1.6727 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 210 total=8.2337 mle=1.8499 pcon=4.7078 forget=1.6760 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 260 total=8.2055 mle=1.8146 pcon=4.7075 forget=1.6834 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 310 total=8.0144 mle=1.6290 pcon=4.7071 forget=1.6782 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 360 total=8.0050 mle=1.6007 pcon=4.7070 forget=1.6973 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 20 total=8.2229 mle=1.8190 pcon=4.7066 forget=1.6973 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 70 total=8.1978 mle=1.8031 pcon=4.7063 forget=1.6884 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 120 total=8.0120 mle=1.6029 pcon=4.7057 forget=1.7035 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 170 total=8.0022 mle=1.5942 pcon=4.7055 forget=1.7026 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 220 total=8.0152 mle=1.6045 pcon=4.7050 forget=1.7057 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 270 total=7.9757 mle=1.5750 pcon=4.7048 forget=1.6960 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 320 total=8.1433 mle=1.7363 pcon=4.7045 forget=1.7025 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 370 total=8.0417 mle=1.6111 pcon=4.7042 forget=1.7264 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 30 total=8.1192 mle=1.6963 pcon=4.7039 forget=1.7190 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 80 total=8.0839 mle=1.6534 pcon=4.7036 forget=1.7269 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 130 total=8.0419 mle=1.6132 pcon=4.7032 forget=1.7255 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 180 total=7.9741 mle=1.5390 pcon=4.7027 forget=1.7324 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 230 total=8.1034 mle=1.6686 pcon=4.7025 forget=1.7323 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 280 total=8.3175 mle=1.8828 pcon=4.7022 forget=1.7325 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 330 total=8.2759 mle=1.8251 pcon=4.7018 forget=1.7491 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 380 total=8.2350 mle=1.7837 pcon=4.7014 forget=1.7499 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 40 total=8.1286 mle=1.6775 pcon=4.7011 forget=1.7499 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 90 total=8.1087 mle=1.6534 pcon=4.7009 forget=1.7544 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 140 total=8.1519 mle=1.7135 pcon=4.7006 forget=1.7378 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 190 total=8.1272 mle=1.6589 pcon=4.7004 forget=1.7679 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 240 total=8.1121 mle=1.6677 pcon=4.7002 forget=1.7442 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 290 total=8.1046 mle=1.6510 pcon=4.6999 forget=1.7538 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 340 total=8.2314 mle=1.7589 pcon=4.6998 forget=1.7728 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 0 total=8.1433 mle=1.6619 pcon=4.6994 forget=1.7819 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 50 total=8.0950 mle=1.6273 pcon=4.6992 forget=1.7685 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 100 total=8.1475 mle=1.6709 pcon=4.6991 forget=1.7775 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 150 total=8.2268 mle=1.7666 pcon=4.6989 forget=1.7614 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 200 total=8.1605 mle=1.6886 pcon=4.6986 forget=1.7733 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 250 total=8.1425 mle=1.6623 pcon=4.6983 forget=1.7818 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 300 total=8.2204 mle=1.7319 pcon=4.6981 forget=1.7904 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 350 total=8.0934 mle=1.6144 pcon=4.6979 forget=1.7811 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 10 total=8.2717 mle=1.8028 pcon=4.6977 forget=1.7711 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 60 total=8.1575 mle=1.6632 pcon=4.6977 forget=1.7965 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 110 total=8.1866 mle=1.6825 pcon=4.6976 forget=1.8065 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 160 total=8.1058 mle=1.6034 pcon=4.6974 forget=1.8050 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 210 total=8.2349 mle=1.7275 pcon=4.6972 forget=1.8102 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 260 total=8.2062 mle=1.7067 pcon=4.6971 forget=1.8024 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 310 total=8.1536 mle=1.6482 pcon=4.6970 forget=1.8085 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 360 total=8.2340 mle=1.7089 pcon=4.6969 forget=1.8282 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 20 total=8.1849 mle=1.6725 pcon=4.6969 forget=1.8155 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 70 total=8.1629 mle=1.6395 pcon=4.6968 forget=1.8265 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 120 total=8.2604 mle=1.7331 pcon=4.6967 forget=1.8306 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 170 total=8.3988 mle=1.8712 pcon=4.6966 forget=1.8310 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 220 total=8.0623 mle=1.5487 pcon=4.6965 forget=1.8171 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 270 total=8.1956 mle=1.6849 pcon=4.6964 forget=1.8143 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 320 total=8.4834 mle=1.9458 pcon=4.6963 forget=1.8414 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 370 total=8.2061 mle=1.6657 pcon=4.6961 forget=1.8444 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 30 total=8.3940 mle=1.8496 pcon=4.6961 forget=1.8483 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 80 total=8.3441 mle=1.8151 pcon=4.6959 forget=1.8331 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 88%|████████▊ | 44/50 [06:58<00:51,  8.58s/it] 90%|█████████ | 45/50 [07:06<00:42,  8.45s/it] 92%|█████████▏| 46/50 [07:14<00:32,  8.23s/it] 94%|█████████▍| 47/50 [07:23<00:25,  8.63s/it] 96%|█████████▌| 48/50 [07:32<00:17,  8.55s/it] 98%|█████████▊| 49/50 [07:39<00:08,  8.27s/it]100%|██████████| 50/50 [07:48<00:00,  8.33s/it]100%|██████████| 50/50 [07:48<00:00,  9.36s/it]
[loss] ep 43 it 130 total=8.4739 mle=1.9162 pcon=4.6959 forget=1.8617 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 180 total=8.0845 mle=1.5161 pcon=4.6958 forget=1.8727 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 230 total=8.0661 mle=1.5168 pcon=4.6958 forget=1.8535 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 280 total=8.1471 mle=1.6043 pcon=4.6958 forget=1.8470 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 330 total=8.3035 mle=1.7497 pcon=4.6958 forget=1.8581 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 380 total=8.3071 mle=1.7541 pcon=4.6957 forget=1.8573 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 40 total=8.3413 mle=1.7660 pcon=4.6957 forget=1.8796 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 90 total=8.1969 mle=1.6121 pcon=4.6957 forget=1.8891 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 140 total=8.2704 mle=1.7125 pcon=4.6957 forget=1.8622 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 190 total=8.3685 mle=1.7620 pcon=4.6956 forget=1.9109 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 240 total=8.2734 mle=1.6638 pcon=4.6956 forget=1.9140 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 290 total=8.4317 mle=1.8733 pcon=4.6956 forget=1.8628 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 340 total=8.2344 mle=1.6514 pcon=4.6957 forget=1.8873 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 0 total=8.1529 mle=1.5774 pcon=4.6957 forget=1.8798 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 50 total=8.2113 mle=1.6384 pcon=4.6956 forget=1.8773 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 100 total=8.3555 mle=1.7294 pcon=4.6958 forget=1.9303 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 150 total=8.3664 mle=1.7780 pcon=4.6957 forget=1.8927 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 200 total=8.3241 mle=1.7134 pcon=4.6957 forget=1.9149 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 250 total=8.2009 mle=1.6005 pcon=4.6957 forget=1.9046 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 300 total=8.2790 mle=1.6766 pcon=4.6957 forget=1.9066 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 350 total=8.2380 mle=1.6392 pcon=4.6961 forget=1.9027 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 10 total=8.2167 mle=1.5948 pcon=4.6962 forget=1.9257 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 60 total=8.4158 mle=1.7839 pcon=4.6963 forget=1.9356 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 110 total=8.2284 mle=1.6038 pcon=4.6964 forget=1.9282 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 160 total=8.3805 mle=1.7805 pcon=4.6963 forget=1.9037 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 210 total=8.4088 mle=1.8055 pcon=4.6963 forget=1.9070 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 260 total=8.3405 mle=1.7256 pcon=4.6964 forget=1.9186 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 310 total=8.2005 mle=1.5852 pcon=4.6964 forget=1.9188 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 360 total=8.5351 mle=1.9087 pcon=4.6966 forget=1.9298 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 20 total=8.3209 mle=1.6883 pcon=4.6967 forget=1.9358 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 70 total=8.3440 mle=1.7218 pcon=4.6967 forget=1.9255 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 120 total=8.3927 mle=1.7471 pcon=4.6967 forget=1.9489 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 170 total=8.4930 mle=1.8786 pcon=4.6967 forget=1.9177 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 220 total=8.1940 mle=1.5575 pcon=4.6968 forget=1.9398 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 270 total=8.4658 mle=1.8048 pcon=4.6968 forget=1.9642 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 320 total=8.3949 mle=1.7332 pcon=4.6969 forget=1.9648 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 370 total=8.2530 mle=1.6328 pcon=4.6969 forget=1.9233 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 30 total=8.3405 mle=1.6612 pcon=4.6970 forget=1.9824 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 80 total=8.4071 mle=1.7517 pcon=4.6971 forget=1.9583 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 130 total=8.3753 mle=1.7178 pcon=4.6973 forget=1.9601 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 180 total=8.3988 mle=1.6843 pcon=4.6973 forget=2.0172 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 230 total=8.3380 mle=1.6595 pcon=4.6974 forget=1.9811 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 280 total=8.3450 mle=1.6761 pcon=4.6977 forget=1.9712 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 330 total=8.3115 mle=1.6473 pcon=4.6978 forget=1.9664 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 380 total=8.4894 mle=1.7738 pcon=4.6980 forget=2.0176 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 40 total=8.3496 mle=1.6525 pcon=4.6981 forget=1.9991 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 90 total=8.3464 mle=1.6860 pcon=4.6982 forget=1.9621 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 140 total=8.3634 mle=1.6682 pcon=4.6983 forget=1.9969 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 190 total=8.3365 mle=1.6370 pcon=4.6984 forget=2.0010 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 240 total=8.3603 mle=1.6956 pcon=4.6985 forget=1.9662 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 290 total=8.4681 mle=1.7410 pcon=4.6985 forget=2.0286 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 340 total=8.3861 mle=1.6887 pcon=4.6989 forget=1.9985 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[Full Grid] Completed. Manifest saved to: evaluation_results/CIFAR-100-resnet34-top5-palm-cache6-ema0.999--epochgrid-lambda0.2.csv
