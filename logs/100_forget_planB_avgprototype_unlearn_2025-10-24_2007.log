nohup: ignoring input
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter', adapter_load_path=None, adapter_load_paths=None, lora_new_adapter_name=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=True, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast( 2  2%|▏         | 1/50 [00:25<21:02, 25.77s/i   4%|▍         | 2/50 [00:46<18:12, 22.77s/it]  6%|▌         | 3/50 [01:07<17:13, 21.99s/it]  8%|▊         | 4/50 [01:28<16:32, 21.58s/it] 10%|█         | 5/50 [01:42<14:08, 18.86s/it] 12%|█▏        | 6/50 [01:58<13:03, 17.81s/it] 14%|█▍        | 7/50 [02:08<11:01, 15.39s/it][loss] ep 0 it 0 total=9.1226 mle=1.5803 pcon=5.2951 forget=2.3854 favg=-0.1382 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=8.9081 mle=1.4909 pcon=5.2914 forget=2.3537 favg=-0.2279 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.1772 mle=1.6391 pcon=5.2875 forget=2.3816 favg=-0.1311 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2657 mle=1.8130 pcon=5.2841 forget=2.3525 favg=-0.1838 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2345 mle=1.8135 pcon=5.2802 forget=2.3236 favg=-0.1829 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=8.8564 mle=1.4461 pcon=5.2762 forget=2.3792 favg=-0.2451 nr=128 nf=101 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=8.9972 mle=1.4831 pcon=5.2726 forget=2.3775 favg=-0.1360 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1585 mle=1.4994 pcon=5.2693 forget=2.4357 favg=-0.0458 nr=128 nf=93 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 1 it 10 total=9.1162 mle=1.7197 pcon=5.2657 forget=2.3580 favg=-0.2272 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=8.8507 mle=1.2957 pcon=5.2618 forget=2.4521 favg=-0.1589 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.8876 mle=1.4153 pcon=5.2582 forget=2.3926 favg=-0.1785 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.6608 mle=1.1732 pcon=5.2546 forget=2.4271 favg=-0.1941 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0908 mle=1.5839 pcon=5.2510 forget=2.3914 favg=-0.1355 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9684 mle=1.5961 pcon=5.2475 forget=2.3748 favg=-0.2500 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9578 mle=1.6190 pcon=5.2441 forget=2.3531 favg=-0.2583 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0017 mle=1.6114 pcon=5.2407 forget=2.3750 favg=-0.2253 nr=128 nf=97 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 2 it 20 total=8.8866 mle=1.5294 pcon=5.2372 forget=2.3616 favg=-0.2416 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=8.8260 mle=1.3554 pcon=5.2341 forget=2.4297 favg=-0.1932 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.7298 mle=1.4283 pcon=5.2308 forget=2.3529 favg=-0.2822 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=9.0183 mle=1.6473 pcon=5.2275 forget=2.3772 favg=-0.2338 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.6925 mle=1.3786 pcon=5.2240 forget=2.3835 favg=-0.2937 nr=128 nf=89 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0017 mle=1.7361 pcon=5.2205 forget=2.3696 favg=-0.3245 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.3980 mle=1.3940 pcon=5.2170 forget=2.3569 favg=-0.5698 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=8.1629 mle=1.4041 pcon=5.2133 forget=2.3360 favg=-0.7905 nr=128 nf=96 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 3 it 30 total=7.9790 mle=1.6233 pcon=5.2090 forget=2.2541 favg=-1.1074 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=7.4344 mle=1.4908 pcon=5.2043 forget=2.1817 favg=-1.4424 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=7.5213 mle=1.7710 pcon=5.1992 forget=2.1146 favg=-1.5635 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=7.3365 mle=1.8279 pcon=5.1944 forget=2.0779 favg=-1.7637 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=7.3967 mle=1.8662 pcon=5.1896 forget=2.0910 favg=-1.7500 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=7.7224 mle=1.8925 pcon=5.1847 forget=2.1315 favg=-1.4863 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=8.5444 mle=1.8687 pcon=5.1799 forget=2.1414 favg=-0.6455 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=9.9748 mle=1.7480 pcon=5.1753 forget=2.1872 favg=0.8643 nr=128 nf=97 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 4 it 40 total=10.4234 mle=1.7643 pcon=5.1709 forget=2.2050 favg=1.2832 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=10.0948 mle=1.3217 pcon=5.1670 forget=2.3356 favg=1.2705 nr=128 nf=91 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=10.3227 mle=1.7975 pcon=5.1635 forget=2.3012 favg=1.0605 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.8332 mle=1.5507 pcon=5.1604 forget=2.3399 favg=0.7822 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.4420 mle=1.3866 pcon=5.1579 forget=2.3462 favg=0.5513 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.1583 mle=1.1967 pcon=5.1556 forget=2.3580 favg=0.4480 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.3868 mle=1.5700 pcon=5.1535 forget=2.3171 favg=0.3462 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 5 it 0 total=9.0910 mle=1.1700 pcon=5.1515 forget=2.3784 favg=0.3911 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=9.3148 mle=1.5287 pcon=5.1493 forget=2.3419 favg=0.2949 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.2898 mle=1.5962 pcon=5.1474 forget=2.3523 favg=0.1940 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.1030 mle=1.5052 pcon=5.1456 forget=2.3393 favg=0.1129 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=8.6457 mle=1.5710 pcon=5.1436 forget=2.2607 favg=-0.3296 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=7.9332 mle=1.5224 pcon=5.1421 forget=2.0788 favg=-0.8101 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=9.4359 mle=1.7956 pcon=5.1405 forget=1.8954 favg=0.6045 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=9.9278 mle=1.6779 pcon=5.1379 forget=2.0836 favg=1.0283 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 6 it 10 total=9.6377 mle=1.3227 pcon=5.1352 forget=2.2901 favg=0.8896 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=9.4253 mle=1.3920 pcon=5.1323 forget=2.3844 favg=0.5166 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=9.0799 mle=1.2953 pcon=5.1293 forget=2.3941 favg=0.2612 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.8083 mle=1.3463 pcon=5.1262 forget=2.3483 favg=-0.0125 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9131 mle=1.7123 pcon=5.1233 forget=2.2428 favg=-0.1653 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=8.4179 mle=1.4069 pcon=5.1212 forget=2.1684 favg=-0.2786 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=8.8002 mle=1.7539 pcon=5.1195 forget=1.9934 favg=-0.0666 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 6 it 360 total=9.5351 mle=1.8449 pcon=5.1181 forget=1.9076 favg=0.6646 nr=128 nf=89 protos=600 fproto_sim=NA
[loss] ep 7 it 20 total=9.6036 mle=1.6248 pcon=5.1175 forget=1.9174 favg=0.9438 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=9.3204 mle=1.5641 pcon=5.1167 forget=2.0068 favg=0.6328 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.7259 mle=1.6435 pcon=5.1157 forget=2.0422 favg=-0.0756 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.0197 mle=1.9491 pcon=5.1144 forget=1.7969 favg=-0.8408 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=7.4782 mle=1.6157 pcon=5.1126 forget=1.7666 favg=-1.0166 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=7.9274 mle=1.7316 pcon=5.1102 forget=1.7820 favg=-0.6963 nr=128 nf=94 protos=600 fproto_sim=NA
 16%|█▌        | 8/50 [02:18<09:37, 13.75s/it] 18%|█▊        | 9/50 [02:29<08:37, 12.61s/it] 20%|██        | 10/50 [02:38<07:50, 11.77s/it] 22%|██▏       | 11/50 [02:49<07:23, 11.38s/it] 24%|██▍       | 12/50 [02:59<07:01, 11.09s/it] 26%|██▌       | 13/50 [03:09<06:37, 10.75s/it] 28%|██▊       | 14/50 [03:20<06:23, 10.65s/it] 30%|███       | 15/50 [03:30<06:07, 10.49s/it] 32%|███▏      | 16/50 [03:40<05:53, 10.39s/it][loss] ep 7 it 320 total=8.6556 mle=1.6386 pcon=5.1072 forget=1.8613 favg=0.0485 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=9.2604 mle=1.5188 pcon=5.1039 forget=2.0128 favg=0.6250 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 8 it 30 total=9.5899 mle=1.6158 pcon=5.1007 forget=1.9774 favg=0.8960 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=9.4383 mle=1.4692 pcon=5.0977 forget=2.0160 favg=0.8555 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=9.5592 mle=1.7246 pcon=5.0954 forget=1.9374 favg=0.8018 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=9.3778 mle=1.7070 pcon=5.0937 forget=1.8208 favg=0.7563 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=9.3764 mle=1.7798 pcon=5.0928 forget=1.7762 favg=0.7275 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=9.2220 mle=1.8335 pcon=5.0923 forget=1.7479 favg=0.5483 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.1516 mle=2.2196 pcon=5.0917 forget=1.6853 favg=0.1549 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.3059 mle=1.8339 pcon=5.0916 forget=1.7361 favg=-0.3557 nr=128 nf=101 protos=600 fproto_sim=NA
[loss] ep 9 it 40 total=7.6064 mle=1.8789 pcon=5.0913 forget=1.6655 favg=-1.0293 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=7.3575 mle=1.9152 pcon=5.0903 forget=1.6303 favg=-1.2783 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=7.6763 mle=1.5513 pcon=5.0892 forget=1.7239 favg=-0.6880 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=9.0363 mle=1.4911 pcon=5.0874 forget=1.8533 favg=0.6045 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=9.8901 mle=1.7714 pcon=5.0855 forget=1.9843 favg=1.0488 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=9.5846 mle=1.4602 pcon=5.0834 forget=2.1050 favg=0.9360 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=9.2166 mle=1.3766 pcon=5.0817 forget=2.0987 favg=0.6597 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 10 it 0 total=8.9025 mle=1.5802 pcon=5.0801 forget=1.9827 favg=0.2595 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=7.9123 mle=1.5394 pcon=5.0796 forget=1.7523 favg=-0.4590 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=7.6574 mle=1.8825 pcon=5.0791 forget=1.6050 favg=-0.9092 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=7.5363 mle=1.9274 pcon=5.0786 forget=1.5714 favg=-1.0410 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=7.8829 mle=1.7483 pcon=5.0780 forget=1.6005 favg=-0.5439 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=8.7523 mle=1.6490 pcon=5.0771 forget=1.6885 favg=0.3376 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=9.1268 mle=1.6973 pcon=5.0756 forget=1.7533 favg=0.6006 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.8399 mle=1.2906 pcon=5.0739 forget=1.9036 favg=0.5718 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 11 it 10 total=8.7618 mle=1.4241 pcon=5.0719 forget=1.9303 favg=0.3354 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.2877 mle=1.4533 pcon=5.0703 forget=1.7801 favg=-0.0160 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.1854 mle=1.5217 pcon=5.0691 forget=1.7271 favg=-0.1326 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.5804 mle=1.6433 pcon=5.0681 forget=1.7198 favg=0.1492 nr=128 nf=101 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.8065 mle=1.5822 pcon=5.0672 forget=1.7352 favg=0.4219 nr=128 nf=91 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.9161 mle=1.5052 pcon=5.0664 forget=1.7674 favg=0.5771 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.9542 mle=1.5523 pcon=5.0657 forget=1.8386 favg=0.4976 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.5812 mle=1.4050 pcon=5.0649 forget=1.9081 favg=0.2031 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 12 it 20 total=8.1857 mle=1.4279 pcon=5.0639 forget=1.8886 favg=-0.1947 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=7.9516 mle=1.5590 pcon=5.0625 forget=1.8735 favg=-0.5435 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=7.4694 mle=1.4823 pcon=5.0606 forget=1.7043 favg=-0.7778 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=7.3294 mle=1.4583 pcon=5.0585 forget=1.7370 favg=-0.9243 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=7.4832 mle=1.4358 pcon=5.0561 forget=1.7232 favg=-0.7319 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.1737 mle=1.4815 pcon=5.0536 forget=1.6903 favg=-0.0518 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.9791 mle=1.5064 pcon=5.0516 forget=1.7033 favg=0.7178 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=9.3843 mle=1.5746 pcon=5.0496 forget=1.7640 favg=0.9961 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 13 it 30 total=9.3415 mle=1.4693 pcon=5.0483 forget=1.7575 favg=1.0664 nr=128 nf=102 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=9.2828 mle=1.6350 pcon=5.0471 forget=1.7295 favg=0.8711 nr=128 nf=90 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.6658 mle=1.5362 pcon=5.0462 forget=1.7154 favg=0.3679 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=8.1702 mle=1.5251 pcon=5.0452 forget=1.7369 favg=-0.1370 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 13 it 230 total=7.5019 mle=1.5566 pcon=5.0443 forget=1.6603 favg=-0.7593 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=7.1643 mle=1.4415 pcon=5.0429 forget=1.6896 favg=-1.0098 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=7.3383 mle=1.5878 pcon=5.0413 forget=1.6575 favg=-0.9482 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.2099 mle=1.4955 pcon=5.0393 forget=1.7090 favg=-0.0340 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 14 it 40 total=9.0965 mle=1.5016 pcon=5.0372 forget=1.8262 favg=0.7314 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=9.2689 mle=1.4111 pcon=5.0350 forget=1.9624 favg=0.8604 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=9.2127 mle=1.4497 pcon=5.0328 forget=1.9430 favg=0.7871 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.7260 mle=1.3463 pcon=5.0305 forget=1.8262 favg=0.5229 nr=128 nf=102 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.4233 mle=1.4543 pcon=5.0285 forget=1.7983 favg=0.1422 nr=128 nf=101 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=7.8617 mle=1.5044 pcon=5.0268 forget=1.6542 favg=-0.3237 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=7.5456 mle=1.6039 pcon=5.0251 forget=1.6114 favg=-0.6948 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 15 it 0 total=7.4088 mle=1.6520 pcon=5.0232 forget=1.6139 favg=-0.8804 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=7.3090 mle=1.5314 pcon=5.0216 forget=1.6007 favg=-0.8447 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=7.9415 mle=1.4803 pcon=5.0201 forget=1.6118 favg=-0.1707 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.8263 mle=1.4443 pcon=5.0183 forget=1.7095 favg=0.6543 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=9.1606 mle=1.3745/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.56  99.36
places365     68.08  81.18
LSUN          17.76  96.09
iSUN          72.35  81.79
dtd           38.55  91.37
AVG           39.86  89.96
Retain-Acc: 0.7482
Forget-as-OOD (retain known vs forget novel):
  FPR: 54.70 AUROC: 89.29 AUIN: 98.64
9.588337421417236
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0_rf.png
m=NA
[loss] ep 16 it 310 total=8.5603 mle=1.5671 pcon=4.9997 forget=1.7538 favg=0.2396 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=9.0106 mle=1.5179 pcon=4.9975 forget=1.8097 favg=0.6855 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=9.0951 mle=1.4894 pcon=4.9955 forget=1.8070 favg=0.8032 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.9862 mle=1.4818 pcon=4.9934 forget=1.8020 favg=0.7090 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.7795 mle=1.4648 pcon=4.9913 forget=1.8141 favg=0.5093 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.3680 mle=1.3803 pcon=4.9895 forget=1.7272 favg=0.2710 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.2210 mle=1.4969 pcon=4.9879 forget=1.6707 favg=0.0655 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.0321 mle=1.6106 pcon=4.9864 forget=1.5993 favg=-0.1641 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=7.9777 mle=1.6235 pcon=4.9849 forget=1.5724 favg=-0.2031 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=7.7979 mle=1.5566 pcon=4.9833 forget=1.5612 favg=-0.3032 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=7.7524 mle=1.5721 pcon=4.9816 forget=1.5774 favg=-0.3787 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.1333 mle=1.7730 pcon=4.9799 forget=1.5981 favg=-0.2178 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.0252 mle=1.5544 pcon=4.9785 forget=1.6613 favg=-0.1689 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.1507 mle=1.4541 pcon=4.9767 forget=1.6180 favg=0.1019 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.5627 mle=1.5312 pcon=4.9749 forget=1.6970 favg=0.3596 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.7125 mle=1.4947 pcon=4.9732 forget=1.7304 favg=0.5142 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.6690 mle=1.3691 pcon=4.9718 forget=1.7461 favg=0.5820 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.6940 mle=1.4391 pcon=4.9703 forget=1.7044 favg=0.5801 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.6813 mle=1.5045 pcon=4.9688 forget=1.7031 favg=0.5049 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.6079 mle=1.5759 pcon=4.9674 forget=1.6649 favg=0.3997 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.3149 mle=1.4552 pcon=4.9662 forget=1.6969 favg=0.1965 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.2677 mle=1.6197 pcon=4.9649 forget=1.6473 favg=0.0358 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=7.8838 mle=1.4311 pcon=4.9633 forget=1.6283 favg=-0.1389 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=7.8482 mle=1.5653 pcon=4.9621 forget=1.6064 favg=-0.2856 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=7.8624 mle=1.4744 pcon=4.9608 forget=1.6524 favg=-0.2252 nr=128 nf=102 protos=600 fproto_sim=NA
[loss] ep 20 it 0 total=7.8733 mle=1.4057 pcon=4.9592 forget=1.6684 favg=-0.1600 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.1571 mle=1.5129 pcon=4.9576 forget=1.6563 favg=0.0304 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.2319 mle=1.4414 pcon=4.9557 forget=1.6725 favg=0.1622 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.5821 mle=1.5343 pcon=4.9540 forget=1.7029 favg=0.3909 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.5559 mle=1.4586 pcon=4.9526 forget=1.7006 favg=0.4441 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=8.6170 mle=1.5338 pcon=4.9513 forget=1.6831 favg=0.4487 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=8.4774 mle=1.5126 pcon=4.9499 forget=1.6682 favg=0.3467 nr=128 nf=91 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=8.4412 mle=1.6060 pcon=4.9489 forget=1.6502 favg=0.2361 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 21 it 10 total=8.0979 mle=1.5592 pcon=4.9477 forget=1.5851 favg=0.0060 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=7.9923 mle=1.6346 pcon=4.9466 forget=1.5852 favg=-0.1741 nr=128 nf=89 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=7.9771 mle=1.6974 pcon=4.9452 forget=1.6097 favg=-0.2751 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.0178 mle=1.6080 pcon=4.9439 forget=1.6143 favg=-0.1484 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=7.9249 mle=1.5131 pcon=4.9423 forget=1.6084 favg=-0.1388 nr=128 nf=91 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.2147 mle=1.6236 pcon=4.9410 forget=1.6445 favg=0.0056 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.3514 mle=1.4978 pcon=4.9395 forget=1.6921 favg=0.2220 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.5338 mle=1.4975 pcon=4.9381 forget=1.6788 favg=0.4194 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 22 it 20 total=8.5845 mle=1.4775 pcon=4.9367 forget=1.7166 favg=0.4536 nr=128 nf=102 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.6473 mle=1.5070 pcon=4.9351 forget=1.7287 favg=0.4766 nr=128 nf=101 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.5088 mle=1.4569 pcon=4.9338 forget=1.7329 favg=0.3853 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.3912 mle=1.4826 pcon=4.9324 forget=1.6618 favg=0.3145 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.2632 mle=1.4855 pcon=4.9311 forget=1.6584 favg=0.1881 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.2017 mle=1.5162 pcon=4.9300 forget=1.6361 favg=0.1194 nr=128 nf=90 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.0869 mle=1.5385 pcon=4.9289 forget=1.5857 favg=0.0338 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=8.1104 mle=1.6324 pcon=4.9277 forget=1.5687 favg=-0.0183 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 23 it 30 total=7.9744 mle=1.5190 pcon=4.9265 forget=1.5859 favg=-0.0570 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.0116 mle=1.5235 pcon=4.9253 forget=1.6315 favg=-0.0687 nr=128 nf=90 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=7.9662 mle=1.5381 pcon=4.9241 forget=1.5926 favg=-0.0886 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.0001 mle=1.5892 pcon=4.9225 forget=1.5990 favg=-0.1105 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.0224 mle=1.6181 pcon=4.9210 forget=1.6120 favg=-0.1287 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=7.9618 mle=1.6324 pcon=4.9194 forget=1.6156 favg=-0.2057 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=7.9416 mle=1.6026 pcon=4.9178 forget=1.6273 favg=-0.2061 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.1277 mle=1.5885 pcon=4.9164 forget=1.6438 favg=-0.0210 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 24 it 40 total=8.1781 mle=1.5370 pcon=4.9150 forget=1.6264 favg=0.0997 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=8.5830 mle=1.6370 pcon=4.9137 forget=1.6452 favg=0.3872 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.7283 mle=1.5229 pcon=4.9125 forget=1.6743 favg=0.6187 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.9475 mle=1.6041 pcon=4.9112 forget=1.6792 favg=0.7529 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=9.0510 mle=1.5747 pcon=4.9103 forget=1.7135 favg=0.8525 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.9546 mle=1.4706 pcon=4.9095 forget=1.7011 favg=0.8735 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.9307 mle=1.4652 pcon=4.9084 forget=1.7456 favg=0.8115 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 25 it 0 total=8.8269 mle=1.5276 pcon=4.9076 forget=1.7193 favg=0.6724 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 25 it 50 total=8.6383 mle=1.5430 pcon=4.9067 forget=1.6788 favg=0.5098 nr=128 nf=98 protos=600 fproto_sim=NA
 52%|█████▏    | 26/50 [05:25<04:12, 10.54s/it] 54%|█████▍    | 27/50 [05:35<04:04, 10.63s/it] 56%|█████▌    | 28/50 [05:46<03:51, 10.52s/it] 58%|█████▊    | 29/50 [05:56<03:39, 10.45s/it] 60%|██████    | 30/50 [06:07<03:34, 10.71s/it] 62%|██████▏   | 31/50 [06:18<03:25, 10.82s/it] 64%|██████▍   | 32/50 [06:29<03:14, 10.82s/it] 66%|██████▌   | 33/50 [06:40<03:04, 10.87s/it][loss] ep 25 it 100 total=8.4768 mle=1.5947 pcon=4.9059 forget=1.6546 favg=0.3215 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 25 it 150 total=8.1576 mle=1.5846 pcon=4.9052 forget=1.6396 favg=0.0282 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 25 it 200 total=7.9653 mle=1.6377 pcon=4.9042 forget=1.6147 favg=-0.1913 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 25 it 250 total=7.7755 mle=1.5667 pcon=4.9032 forget=1.6066 favg=-0.3010 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 25 it 300 total=7.6411 mle=1.5854 pcon=4.9018 forget=1.6109 favg=-0.4570 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 25 it 350 total=7.6379 mle=1.5280 pcon=4.9005 forget=1.6049 favg=-0.3955 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 26 it 10 total=7.7047 mle=1.5331 pcon=4.8989 forget=1.5993 favg=-0.3267 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 26 it 60 total=7.8313 mle=1.5576 pcon=4.8973 forget=1.5902 favg=-0.2139 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 26 it 110 total=8.0380 mle=1.5352 pcon=4.8960 forget=1.6393 favg=-0.0325 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 26 it 160 total=8.0473 mle=1.4784 pcon=4.8944 forget=1.6286 favg=0.0458 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 26 it 210 total=8.2898 mle=1.5638 pcon=4.8930 forget=1.6434 favg=0.1896 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 26 it 260 total=8.3213 mle=1.4987 pcon=4.8918 forget=1.6545 favg=0.2764 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 26 it 310 total=8.4882 mle=1.6785 pcon=4.8904 forget=1.6210 favg=0.2983 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 26 it 360 total=8.3292 mle=1.5473 pcon=4.8891 forget=1.6362 favg=0.2566 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 27 it 20 total=8.2333 mle=1.5280 pcon=4.8878 forget=1.6173 favg=0.2002 nr=128 nf=101 protos=600 fproto_sim=NA
[loss] ep 27 it 70 total=8.1867 mle=1.5990 pcon=4.8866 forget=1.5942 favg=0.1069 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 27 it 120 total=8.1446 mle=1.6075 pcon=4.8854 forget=1.6185 favg=0.0331 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 27 it 170 total=8.0556 mle=1.5268 pcon=4.8843 forget=1.6081 favg=0.0363 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 27 it 220 total=8.0123 mle=1.5166 pcon=4.8833 forget=1.6064 favg=0.0061 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 27 it 270 total=7.9914 mle=1.5062 pcon=4.8821 forget=1.6058 favg=-0.0028 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 27 it 320 total=8.1413 mle=1.6218 pcon=4.8812 forget=1.6166 favg=0.0217 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 27 it 370 total=7.9896 mle=1.4533 pcon=4.8801 forget=1.6243 favg=0.0319 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 28 it 30 total=8.1087 mle=1.5068 pcon=4.8791 forget=1.6526 favg=0.0701 nr=128 nf=90 protos=600 fproto_sim=NA
[loss] ep 28 it 80 total=8.2940 mle=1.6707 pcon=4.8780 forget=1.6313 favg=0.1141 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 28 it 130 total=8.1649 mle=1.4932 pcon=4.8771 forget=1.6571 favg=0.1375 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 28 it 180 total=8.1941 mle=1.4039 pcon=4.8760 forget=1.6511 favg=0.2632 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 28 it 230 total=8.2894 mle=1.4518 pcon=4.8749 forget=1.6721 favg=0.2905 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 28 it 280 total=8.4127 mle=1.4419 pcon=4.8740 forget=1.7142 favg=0.3826 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 28 it 330 total=8.4299 mle=1.4769 pcon=4.8730 forget=1.6850 favg=0.3950 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 28 it 380 total=8.4396 mle=1.3977 pcon=4.8720 forget=1.7266 favg=0.4434 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 29 it 40 total=8.4324 mle=1.4293 pcon=4.8709 forget=1.6824 favg=0.4497 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 29 it 90 total=8.7102 mle=1.5618 pcon=4.8701 forget=1.7172 favg=0.5610 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 29 it 140 total=8.4880 mle=1.4395 pcon=4.8692 forget=1.7288 favg=0.4504 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 29 it 190 total=8.5862 mle=1.5728 pcon=4.8683 forget=1.7421 favg=0.4031 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 29 it 240 total=8.5684 mle=1.5311 pcon=4.8674 forget=1.7312 favg=0.4387 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 29 it 290 total=8.4820 mle=1.5674 pcon=4.8665 forget=1.6599 favg=0.3882 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 29 it 340 total=8.4793 mle=1.5299 pcon=4.8656 forget=1.7015 favg=0.3823 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 30 it 0 total=8.4780 mle=1.5323 pcon=4.8648 forget=1.6950 favg=0.3860 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 30 it 50 total=8.3682 mle=1.4887 pcon=4.8639 forget=1.6793 favg=0.3362 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 30 it 100 total=8.3544 mle=1.6121 pcon=4.8630 forget=1.6326 favg=0.2467 nr=128 nf=101 protos=600 fproto_sim=NA
[loss] ep 30 it 150 total=8.2563 mle=1.5485 pcon=4.8620 forget=1.6404 favg=0.2054 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 30 it 200 total=8.2128 mle=1.5263 pcon=4.8610 forget=1.6447 favg=0.1807 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 30 it 250 total=8.1697 mle=1.6108 pcon=4.8601 forget=1.6098 favg=0.0890 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 30 it 300 total=8.0254 mle=1.5398 pcon=4.8589 forget=1.6059 favg=0.0207 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 30 it 350 total=7.9578 mle=1.5407 pcon=4.8578 forget=1.6431 favg=-0.0838 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 31 it 10 total=7.9272 mle=1.5610 pcon=4.8565 forget=1.6363 favg=-0.1267 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 31 it 60 total=7.7767 mle=1.5789 pcon=4.8554 forget=1.5887 favg=-0.2463 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 31 it 110 total=7.7619 mle=1.5660 pcon=4.8542 forget=1.5784 favg=-0.2367 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 31 it 160 total=7.7356 mle=1.5451 pcon=4.8529 forget=1.6091 favg=-0.2715 nr=128 nf=91 protos=600 fproto_sim=NA
[loss] ep 31 it 210 total=7.8034 mle=1.6217 pcon=4.8515 forget=1.5927 favg=-0.2625 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 31 it 260 total=7.7354 mle=1.5697 pcon=4.8504 forget=1.6098 favg=-0.2944 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 31 it 310 total=7.7737 mle=1.5115 pcon=4.8489 forget=1.5959 favg=-0.1826 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 31 it 360 total=7.8982 mle=1.5353 pcon=4.8479 forget=1.6141 favg=-0.0990 nr=128 nf=94 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 32 it 20 total=8.1007 mle=1.5541 pcon=4.8468 forget=1.6279 favg=0.0719 nr=128 nf=91 protos=600 fproto_sim=NA
[loss] ep 32 it 70 total=8.1322 mle=1.5346 pcon=4.8456 forget=1.6122 favg=0.1398 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 32 it 120 total=8.2339 mle=1.4452 pcon=4.8445 forget=1.6664 favg=0.2778 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 32 it 170 total=8.4880 mle=1.5939 pcon=4.8433 forget=1.6387 favg=0.4121 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 32 it 220 total=8.4891 mle=1.4841 pcon=4.8423 forget=1.6769 favg=0.4858 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 32 it 270 total=8.6314 mle=1.4235 pcon=4.8413 forget=1.6908 favg=0.6758 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 32 it 320 total=8.7340 mle=1.4482 pcon=4.8404 forget=1.6901 favg=0.7554 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 32 it 370 total=8.6552 mle=1.4156 pcon=4.8399 forget=1.6947 favg=0.7051 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 33 it 30 total=8.9577 mle=1.6150 pcon=4.8390 forget=1.7077 favg=0.7959 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 33 it 80 total=8.7550 mle=1.4311 pcon=4.8383 forget=1.7233 favg=0.7622 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 33 it 130 total=8.7712 mle=1.4936 pcon=4.8377 forget=1.7217 favg=0.7183 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 33 it 180 total=8.7035 mle=1.5423 pcon=4.8369 forget=1.6676 favg=0.6567 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 33 it 230 total=8.8468 mle=1.6002 pcon=4.8365 forget=1.7290 favg=0.6812 nr=128 nf=96 protos=600 fproto_sim=NA
 68%|██████▊   | 34/50 [06:51<02:51, 10.74s/it] 70%|███████   | 35/50 [07:01<02:39, 10.60s/it] 72%|███████▏  | 36/50 [07:11<02:26, 10.48s/it] 74%|███████▍  | 37/50 [07:21<02:14, 10.36s/it] 76%|███████▌  | 38/50 [07:31<02:04, 10.35s/it] 78%|███████▊  | 39/50 [07:42<01:53, 10.28s/it] 80%|████████  | 40/50 [07:52<01:42, 10.23s/it] 82%|████████▏ | 41/50 [08:02<01:32, 10.25s/it] 84%|████████▍ | 42/50 [08:12<01:21, 10.24s/it][loss] ep 33 it 280 total=8.5818 mle=1.5219 pcon=4.8359 forget=1.6649 favg=0.5591 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 33 it 330 total=8.6776 mle=1.5401 pcon=4.8352 forget=1.7426 favg=0.5596 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 33 it 380 total=8.4514 mle=1.4707 pcon=4.8347 forget=1.7012 favg=0.4448 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 34 it 40 total=8.3971 mle=1.4339 pcon=4.8342 forget=1.7240 favg=0.4050 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 34 it 90 total=8.3787 mle=1.5256 pcon=4.8337 forget=1.6629 favg=0.3564 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 34 it 140 total=8.3481 mle=1.4815 pcon=4.8330 forget=1.7157 favg=0.3179 nr=128 nf=102 protos=600 fproto_sim=NA
[loss] ep 34 it 190 total=8.2258 mle=1.4433 pcon=4.8322 forget=1.6725 favg=0.2778 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 34 it 240 total=8.1790 mle=1.4393 pcon=4.8316 forget=1.6782 favg=0.2299 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 34 it 290 total=8.2415 mle=1.4988 pcon=4.8309 forget=1.6799 favg=0.2318 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 34 it 340 total=8.1648 mle=1.4334 pcon=4.8301 forget=1.7101 favg=0.1912 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 35 it 0 total=8.1546 mle=1.4558 pcon=4.8295 forget=1.6921 favg=0.1771 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 35 it 50 total=8.2725 mle=1.5760 pcon=4.8288 forget=1.6492 favg=0.2185 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 35 it 100 total=8.3080 mle=1.6113 pcon=4.8281 forget=1.6670 favg=0.2015 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 35 it 150 total=8.1391 mle=1.3827 pcon=4.8272 forget=1.7228 favg=0.2063 nr=128 nf=90 protos=600 fproto_sim=NA
[loss] ep 35 it 200 total=8.1713 mle=1.4581 pcon=4.8263 forget=1.6914 favg=0.1956 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 35 it 250 total=8.1707 mle=1.4024 pcon=4.8254 forget=1.7365 favg=0.2063 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 35 it 300 total=8.2632 mle=1.4968 pcon=4.8246 forget=1.6969 favg=0.2449 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 35 it 350 total=8.2814 mle=1.4957 pcon=4.8239 forget=1.7079 favg=0.2539 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 36 it 10 total=8.2630 mle=1.5186 pcon=4.8230 forget=1.7040 favg=0.2174 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 36 it 60 total=8.3267 mle=1.5344 pcon=4.8220 forget=1.7352 favg=0.2350 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 36 it 110 total=8.3231 mle=1.5505 pcon=4.8211 forget=1.7131 favg=0.2383 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 36 it 160 total=8.1091 mle=1.4319 pcon=4.8205 forget=1.7031 favg=0.1537 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 36 it 210 total=8.4277 mle=1.6302 pcon=4.8195 forget=1.6972 favg=0.2808 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 36 it 260 total=8.2322 mle=1.5034 pcon=4.8188 forget=1.6968 favg=0.2133 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 36 it 310 total=8.3745 mle=1.4826 pcon=4.8180 forget=1.7297 favg=0.3442 nr=128 nf=90 protos=600 fproto_sim=NA
[loss] ep 36 it 360 total=8.1645 mle=1.4484 pcon=4.8174 forget=1.6963 favg=0.2024 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 37 it 20 total=8.3248 mle=1.5196 pcon=4.8165 forget=1.7265 favg=0.2622 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 37 it 70 total=8.1828 mle=1.5514 pcon=4.8156 forget=1.6702 favg=0.1455 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 37 it 120 total=8.3253 mle=1.5147 pcon=4.8146 forget=1.7155 favg=0.2805 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 37 it 170 total=8.3277 mle=1.4716 pcon=4.8137 forget=1.6933 favg=0.3491 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 37 it 220 total=8.4225 mle=1.5655 pcon=4.8129 forget=1.6711 favg=0.3730 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 37 it 270 total=8.3636 mle=1.5576 pcon=4.8122 forget=1.6649 favg=0.3289 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 37 it 320 total=8.4465 mle=1.5079 pcon=4.8114 forget=1.6960 favg=0.4312 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 37 it 370 total=8.4258 mle=1.5418 pcon=4.8108 forget=1.6635 favg=0.4097 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 38 it 30 total=8.4665 mle=1.4984 pcon=4.8102 forget=1.7087 favg=0.4492 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 38 it 80 total=8.6146 mle=1.5134 pcon=4.8096 forget=1.7335 favg=0.5581 nr=128 nf=101 protos=600 fproto_sim=NA
[loss] ep 38 it 130 total=8.5779 mle=1.5799 pcon=4.8090 forget=1.6982 favg=0.4907 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 38 it 180 total=8.4933 mle=1.5133 pcon=4.8083 forget=1.7108 favg=0.4609 nr=128 nf=90 protos=600 fproto_sim=NA
[loss] ep 38 it 230 total=8.7804 mle=1.5723 pcon=4.8077 forget=1.7451 favg=0.6553 nr=128 nf=102 protos=600 fproto_sim=NA
[loss] ep 38 it 280 total=8.7126 mle=1.5262 pcon=4.8073 forget=1.7453 favg=0.6338 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 38 it 330 total=8.7989 mle=1.6301 pcon=4.8067 forget=1.7493 favg=0.6128 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 38 it 380 total=8.6386 mle=1.4278 pcon=4.8060 forget=1.7656 favg=0.6392 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 39 it 40 total=8.8328 mle=1.5012 pcon=4.8056 forget=1.7687 favg=0.7573 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 39 it 90 total=8.8365 mle=1.5555 pcon=4.8051 forget=1.7992 favg=0.6768 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 39 it 140 total=8.9653 mle=1.5857 pcon=4.8046 forget=1.7943 favg=0.7808 nr=128 nf=102 protos=600 fproto_sim=NA
[loss] ep 39 it 190 total=8.7379 mle=1.3972 pcon=4.8043 forget=1.8063 favg=0.7300 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 39 it 240 total=8.7672 mle=1.3971 pcon=4.8040 forget=1.8396 favg=0.7266 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 39 it 290 total=8.9154 mle=1.5175 pcon=4.8036 forget=1.8183 favg=0.7759 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 39 it 340 total=8.8036 mle=1.4426 pcon=4.8033 forget=1.8688 favg=0.6890 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 40 it 0 total=8.6371 mle=1.3250 pcon=4.8030 forget=1.8362 favg=0.6729 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 40 it 50 total=8.9867 mle=1.5384 pcon=4.8026 forget=1.8708 favg=0.7749 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 40 it 100 total=8.6692 mle=1.3371 pcon=4.8022 forget=1.8591 favg=0.6709 nr=128 nf=101 protos=600 fproto_sim=NA
[loss] ep 40 it 150 total=8.8972 mle=1.4615 pcon=4.8017 forget=1.8738 favg=0.7603 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 40 it 200 total=8.8659 mle=1.3665 pcon=4.8014 forget=1.8933 favg=0.8047 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 40 it 250 total=8.9048 mle=1.4583 pcon=4.8011 forget=1.9013 favg=0.7441 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 40 it 300 total=8.9253 mle=1.4810 pcon=4.8008 forget=1.8915 favg=0.7520 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 40 it 350 total=8.8192 mle=1.3751 pcon=4.8005 forget=1.8867 favg=0.7568 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 41 it 10 total=8.8652 mle=1.4096 pcon=4.8000 forget=1.9154 favg=0.7402 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 41 it 60 total=8.9326 mle=1.4205 pcon=4.7999 forget=1.9378 favg=0.7744 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 41 it 110 total=8.8738 mle=1.3844 pcon=4.7997 forget=1.9329 favg=0.7568 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 41 it 160 total=8.9218 mle=1.3695 pcon=4.7994 forget=1.9277 favg=0.8252 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 41 it 210 total=8.8317 mle=1.4030 pcon=4.7991 forget=1.9108 favg=0.7188 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 41 it 260 total=8.8316 mle=1.2860 pcon=4.7989 forget=1.9973 favg=0.7495 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 41 it 310 total=8.9258 mle=1.4146 pcon=4.7987 forget=1.9762 favg=0.7363 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 41 it 360 total=8.9503 mle=1.4325 pcon=4.7984 forget=1.9607 favg=0.7588 nr=128 nf=91 protos=600 fproto_sim=NA
[loss] ep 42 it 20 total=8.9853 mle=1.3654 pcon=4.7981 forget=1.9751 favg=0.8467 nr=128 nf=101 protos=600 fproto_sim=NA
[loss] ep 42 it 70 total=9.0125 mle=1.4240 pcon=4.7980 forget=1.9844 favg=0.8062 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 42 it 120 total=9.0057 mle=1.3672 pcon=4.7977 forget=1.9708 favg=0.8701 nr=128 nf=92 protos=600 fproto_sim=NA
 86%|████████▌ | 43/50 [08:23<01:11, 10.27s/it] 88%|████████▊ | 44/50 [08:33<01:01, 10.30s/it] 90%|█████████ | 45/50 [08:43<00:50, 10.17s/it] 92%|█████████▏| 46/50 [08:53<00:40, 10.23s/it] 94%|█████████▍| 47/50 [09:03<00:30, 10.18s/it] 96%|█████████▌| 48/50 [09:13<00:20, 10.07s/it] 98%|█████████▊| 49/50 [09:23<00:10, 10.15s/it]100%|██████████| 50/50 [09:33<00:00, 10.09s/it]100%|██████████| 50/50 [09:33<00:00, 11.48s/it]
[loss] ep 42 it 170 total=9.0272 mle=1.4209 pcon=4.7974 forget=2.0237 favg=0.7852 nr=128 nf=90 protos=600 fproto_sim=NA
[loss] ep 42 it 220 total=9.0232 mle=1.3610 pcon=4.7971 forget=2.0145 favg=0.8506 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 42 it 270 total=9.1610 mle=1.4554 pcon=4.7969 forget=2.0230 favg=0.8857 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 42 it 320 total=9.1383 mle=1.4302 pcon=4.7968 forget=2.0271 favg=0.8843 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 42 it 370 total=9.1547 mle=1.4904 pcon=4.7965 forget=2.0148 favg=0.8530 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 43 it 30 total=9.1635 mle=1.5075 pcon=4.7964 forget=2.0364 favg=0.8232 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 43 it 80 total=9.3422 mle=1.5316 pcon=4.7961 forget=2.0833 favg=0.9312 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 43 it 130 total=9.2241 mle=1.4680 pcon=4.7959 forget=2.0813 favg=0.8789 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 43 it 180 total=9.1940 mle=1.3747 pcon=4.7956 forget=2.1033 favg=0.9204 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 43 it 230 total=9.2652 mle=1.3718 pcon=4.7953 forget=2.1440 favg=0.9541 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 43 it 280 total=9.2432 mle=1.3762 pcon=4.7951 forget=2.1369 favg=0.9351 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 43 it 330 total=9.3309 mle=1.4162 pcon=4.7949 forget=2.1140 favg=1.0059 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 43 it 380 total=9.3112 mle=1.4206 pcon=4.7948 forget=2.1348 favg=0.9609 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 44 it 40 total=9.3395 mle=1.3381 pcon=4.7948 forget=2.1832 favg=1.0234 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 44 it 90 total=9.3865 mle=1.4103 pcon=4.7947 forget=2.1997 favg=0.9819 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 44 it 140 total=9.3836 mle=1.4447 pcon=4.7946 forget=2.2004 favg=0.9438 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 44 it 190 total=9.6073 mle=1.5260 pcon=4.7944 forget=2.1746 favg=1.1123 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 44 it 240 total=9.5250 mle=1.4695 pcon=4.7944 forget=2.2347 favg=1.0264 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 44 it 290 total=9.5596 mle=1.4797 pcon=4.7945 forget=2.2404 favg=1.0449 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 44 it 340 total=9.5242 mle=1.3544 pcon=4.7944 forget=2.2758 favg=1.0996 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 45 it 0 total=9.4743 mle=1.3542 pcon=4.7945 forget=2.2709 favg=1.0547 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 45 it 50 total=9.5924 mle=1.4431 pcon=4.7943 forget=2.2925 favg=1.0625 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 45 it 100 total=9.8155 mle=1.5019 pcon=4.7946 forget=2.3022 favg=1.2168 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 45 it 150 total=9.6698 mle=1.3956 pcon=4.7947 forget=2.3135 favg=1.1660 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 45 it 200 total=9.6834 mle=1.3555 pcon=4.7949 forget=2.3602 favg=1.1729 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 45 it 250 total=9.6662 mle=1.3656 pcon=4.7949 forget=2.3435 favg=1.1621 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 45 it 300 total=9.6245 mle=1.3515 pcon=4.7952 forget=2.3469 favg=1.1309 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 45 it 350 total=9.5570 mle=1.2697 pcon=4.7955 forget=2.3873 favg=1.1045 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 46 it 10 total=9.7629 mle=1.3407 pcon=4.7958 forget=2.3941 favg=1.2324 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 46 it 60 total=9.9762 mle=1.4714 pcon=4.7959 forget=2.4218 favg=1.2871 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 46 it 110 total=9.8516 mle=1.3767 pcon=4.7960 forget=2.4114 favg=1.2676 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 46 it 160 total=9.9943 mle=1.4792 pcon=4.7962 forget=2.4122 favg=1.3066 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 46 it 210 total=9.8538 mle=1.3812 pcon=4.7963 forget=2.4634 favg=1.2129 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 46 it 260 total=9.9541 mle=1.4732 pcon=4.7964 forget=2.4384 favg=1.2461 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 46 it 310 total=9.8437 mle=1.3398 pcon=4.7968 forget=2.4581 favg=1.2490 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 46 it 360 total=10.0350 mle=1.4413 pcon=4.7972 forget=2.4889 favg=1.3076 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 47 it 20 total=10.0126 mle=1.4203 pcon=4.7978 forget=2.4683 favg=1.3262 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 47 it 70 total=10.0226 mle=1.3798 pcon=4.7982 forget=2.5039 favg=1.3408 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 47 it 120 total=10.1065 mle=1.4239 pcon=4.7986 forget=2.5237 favg=1.3604 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 47 it 170 total=10.2535 mle=1.5478 pcon=4.7989 forget=2.5298 favg=1.3770 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 47 it 220 total=10.0285 mle=1.3983 pcon=4.7993 forget=2.5183 favg=1.3125 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 47 it 270 total=10.1492 mle=1.4394 pcon=4.7997 forget=2.5067 favg=1.4033 nr=128 nf=102 protos=600 fproto_sim=NA
[loss] ep 47 it 320 total=10.0746 mle=1.3440 pcon=4.8004 forget=2.5338 favg=1.3965 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 47 it 370 total=10.0950 mle=1.3383 pcon=4.8008 forget=2.5360 favg=1.4199 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 48 it 30 total=10.3425 mle=1.5716 pcon=4.8014 forget=2.5192 favg=1.4502 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 48 it 80 total=10.2132 mle=1.4841 pcon=4.8020 forget=2.5492 favg=1.3779 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 48 it 130 total=10.2199 mle=1.3527 pcon=4.8027 forget=2.5695 favg=1.4951 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 48 it 180 total=10.2273 mle=1.4717 pcon=4.8033 forget=2.5638 favg=1.3887 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 48 it 230 total=10.1056 mle=1.3170 pcon=4.8039 forget=2.5784 favg=1.4062 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 48 it 280 total=10.1129 mle=1.3069 pcon=4.8049 forget=2.5871 favg=1.4141 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 48 it 330 total=10.1517 mle=1.3119 pcon=4.8057 forget=2.6025 favg=1.4316 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 48 it 380 total=10.3281 mle=1.4421 pcon=4.8064 forget=2.5727 favg=1.5068 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 49 it 40 total=10.2798 mle=1.3154 pcon=4.8070 forget=2.6184 favg=1.5391 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 49 it 90 total=10.2962 mle=1.3977 pcon=4.8079 forget=2.6043 favg=1.4863 nr=128 nf=91 protos=600 fproto_sim=NA
[loss] ep 49 it 140 total=10.2511 mle=1.3572 pcon=4.8087 forget=2.6066 favg=1.4785 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 49 it 190 total=10.3734 mle=1.4062 pcon=4.8093 forget=2.6246 favg=1.5332 nr=128 nf=91 protos=600 fproto_sim=NA
[loss] ep 49 it 240 total=10.1963 mle=1.3243 pcon=4.8100 forget=2.5962 favg=1.4658 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 49 it 290 total=10.2758 mle=1.3715 pcon=4.8108 forget=2.6032 favg=1.4902 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 49 it 340 total=10.4205 mle=1.4244 pcon=4.8119 forget=2.6275 favg=1.5566 nr=128 nf=99 protos=600 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[peft] set_adapter(list) failed: unhashable type: 'list'; try adapter fusion
[peft] adapter fusion failed: adapter fusion API not available or single adapter provided; fallback to last adapter
[peft] active adapter set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:01<08:08,  1.25s/it]  3%|▎         | 11/391 [00:01<00:34, 10.86it/s]  5%|▌         | 21/391 [00:01<00:16, 21.90it/s]  8%|▊         | 31/391 [00:01<00:10, 33.43it/s] 10%|█         | 41/391 [00:01<00:07, 44.78it/s] 13%|█▎        | 51/391 [00:01<00:06, 55.42it/s] 16%|█▌        | 61/391 [00:01<00:05, 64.65it/s] 18%|█▊        | 71/391 [00:01<00:04, 72.48it/s] 21%|██        | 81/391 [00:02<00:03, 78.53it/s] 23%|██▎       | 91/391 [00:02<00:03, 83.10it/s] 26%|██▌       | 101/391 [00:02<00:03, 86.69it/s] 28%|██▊       | 111/391 [00:02<00:03, 89.09it/s] 31%|███       | 121/391 [00:02<00:02, 90.91it/s] 34%|███▎      | 131/391 [00:02<00:02, 92.32it/s] 36%|███▌      | 141/391 [00:02<00:02, 93.32it/s] 39%|███▊      | 151/391 [00:02<00:02, 94.00it/s] 41%|████      | 161/391 [00:02<00:02, 94.42it/s] 44%|████▎     | 171/391 [00:03<00:02, 94.76it/s] 46%|████▋     | 181/391 [00:03<00:02, 95.02it/s] 49%|████▉     | 191/391 [00:03<00:02, 95.17it/s] 51%|█████▏    | 201/391 [00:03<00:01, 95.24it/s] 54%|█████▍    | 211/391 [00:03<00:01, 95.53it/s] 57%|█████▋    | 221/391 [00:03<00:01, 95.66it/s] 59%|█████▉    | 231/391 [00:03<00:01, 95.65it/s] 62%|██████▏   | 241/391 [00:03<00:01, 95.72it/s] 64%|██████▍   | 251/391 [00:03<00:01, 95.77it/s] 67%|██████▋   | 261/391 [00:03<00:01, 95.83it/s] 69%|██████▉   | 271/391 [00:04<00:01, 95.86it/s] 72%|███████▏  | 281/391 [00:04<00:01, 95.86it/s] 74%|███████▍  | 291/391 [00:04<00:01, 95.79it/s] 77%|███████▋  | 301/391 [00:04<00:00, 95.85it/s] 80%|███████▉  | 311/391 [00:04<00:00, 96.06it/s] 82%|████████▏ | 321/391 [00:04<00:00, 95.72it/s] 85%|████████▍ | 331/391 [00:04<00:00, 95.81it/s] 87%|████████▋ | 341/391 [00:04<00:00, 95.89it/s] 90%|████████▉ | 351/391 [00:04<00:00, 95.95it/s] 92%|█████████▏| 361/391 [00:05<00:00, 95.71it/s] 95%|█████████▍| 371/391 [00:05<00:00, 96.42it/s] 97%|█████████▋| 381/391 [00:05<00:00, 96.85it/s]100%|██████████| 391/391 [00:05<00:00, 95.00it/s]100%|██████████| 391/391 [00:05<00:00, 73.26it/s]
50000 images processed, 5.6745765209198 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:34,  2.24it/s] 14%|█▍        | 11/79 [00:00<00:02, 25.44it/s] 27%|██▋       | 21/79 [00:00<00:01, 43.50it/s] 39%|███▉      | 31/79 [00:00<00:00, 56.99it/s] 52%|█████▏    | 41/79 [00:00<00:00, 67.17it/s] 65%|██████▍   | 51/79 [00:00<00:00, 74.85it/s] 77%|███████▋  | 61/79 [00:01<00:00, 80.96it/s] 90%|████████▉ | 71/79 [00:01<00:00, 85.41it/s]100%|██████████| 79/79 [00:02<00:00, 35.64it/s]
10000 images processed, 2.3441388607025146 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:36,  2.10it/s]  5%|▍         | 10/204 [00:00<00:08, 22.24it/s] 10%|▉         | 20/204 [00:00<00:04, 40.91it/s] 15%|█▍        | 30/204 [00:00<00:03, 55.19it/s] 20%|█▉        | 40/204 [00:00<00:02, 65.87it/s] 25%|██▍       | 50/204 [00:01<00:02, 74.00it/s] 29%|██▉       | 60/204 [00:01<00:01, 79.60it/s] 34%|███▍      | 70/204 [00:01<00:01, 84.21it/s] 39%|███▉      | 80/204 [00:01<00:01, 87.18it/s] 44%|████▍     | 90/204 [00:01<00:01, 88.90it/s] 49%|████▉     | 100/204 [00:01<00:01, 90.36it/s] 54%|█████▍    | 110/204 [00:01<00:01, 91.42it/s] 59%|█████▉    | 120/204 [00:01<00:00, 92.28it/s] 64%|██████▎   | 130/204 [00:01<00:00, 92.79it/s] 69%|██████▊   | 140/204 [00:01<00:00, 93.29it/s] 74%|███████▎  | 150/204 [00:02<00:00, 94.02it/s] 78%|███████▊  | 160/204 [00:02<00:00, 94.38it/s] 83%|████████▎ | 170/204 [00:02<00:00, 94.71it/s] 88%|████████▊ | 180/204 [00:02<00:00, 95.33it/s] 93%|█████████▎| 190/204 [00:02<00:00, 95.91it/s] 98%|█████████▊| 200/204 [00:02<00:00, 96.30it/s]100%|██████████| 204/204 [00:02<00:00, 77.60it/s]
26032 images processed, 2.803407907485962 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:43,  1.81it/s] 10%|█         | 8/79 [00:00<00:04, 15.74it/s] 23%|██▎       | 18/79 [00:00<00:01, 34.32it/s] 35%|███▌      | 28/79 [00:00<00:01, 49.56it/s] 48%|████▊     | 38/79 [00:00<00:00, 61.39it/s] 61%|██████    | 48/79 [00:01<00:00, 70.11it/s] 73%|███████▎  | 58/79 [00:01<00:00, 77.13it/s] 86%|████████▌ | 68/79 [00:01<00:00, 82.34it/s] 99%|█████████▊| 78/79 [00:01<00:00, 86.56it/s]100%|██████████| 79/79 [00:01<00:00, 56.55it/s]
10000 images processed, 1.47251558303833 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:35,  2.18it/s] 14%|█▍        | 11/79 [00:00<00:02, 24.94it/s] 27%|██▋       | 21/79 [00:00<00:01, 42.93it/s] 39%|███▉      | 31/79 [00:00<00:00, 56.73it/s] 52%|█████▏    | 41/79 [00:00<00:00, 67.51it/s] 65%|██████▍   | 51/79 [00:00<00:00, 75.29it/s] 77%|███████▋  | 61/79 [00:01<00:00, 81.48it/s] 90%|████████▉ | 71/79 [00:01<00:00, 85.99it/s]100%|██████████| 79/79 [00:01<00:00, 62.09it/s]
10000 images processed, 1.3409521579742432 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:33,  2.04it/s] 14%|█▍        | 10/70 [00:00<00:02, 21.37it/s] 29%|██▊       | 20/70 [00:00<00:01, 39.60it/s] 43%|████▎     | 30/70 [00:00<00:00, 54.03it/s] 57%|█████▋    | 40/70 [00:00<00:00, 65.07it/s] 71%|███████▏  | 50/70 [00:01<00:00, 73.87it/s] 86%|████████▌ | 60/70 [00:01<00:00, 80.44it/s]100%|██████████| 70/70 [00:01<00:00, 84.33it/s]100%|██████████| 70/70 [00:01<00:00, 56.82it/s]
8925 images processed, 1.3122007846832275 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:40,  1.10it/s]  4%|▍         | 2/45 [00:01<00:21,  2.00it/s] 20%|██        | 9/45 [00:01<00:03, 11.75it/s] 38%|███▊      | 17/45 [00:01<00:01, 21.17it/s] 47%|████▋     | 21/45 [00:01<00:01, 19.43it/s] 56%|█████▌    | 25/45 [00:01<00:01, 17.85it/s] 76%|███████▌  | 34/45 [00:02<00:00, 20.89it/s] 87%|████████▋ | 39/45 [00:02<00:00, 24.15it/s]100%|██████████| 45/45 [00:02<00:00, 18.33it/s]
5640 images processed, 2.536862850189209 seconds used

19.046534776687622
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.32  99.00
places365     77.71  79.47
LSUN          29.22  95.39
iSUN          88.38  73.94
dtd           35.71  92.70
AVG           47.07  88.10
Retain-Acc: 0.7060
Forget-as-OOD (retain known vs forget novel):
  FPR: 88.10 AUROC: 78.09 AUIN: 97.04
7.554735422134399
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0_rf.png
