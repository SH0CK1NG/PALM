nohup: ignoring input
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=30, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.2, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=True, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/30 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  3%|▎         | 1/30 [01:02<30:07, 62.32s/it]  7%|▋         | 2/30 [01:57<27:14, 58.39s/it] 10%|█         | 3/30 [02:54<25:50, 57.44s/it] 13%|█▎        | 4/30 [03:51<24:48, 57.27s/it] 17%|█▋        | 5/30 [04:48<23:49, 57.18s/it] 20%|██        | 6/30 [05:45<22:52, 57.18s/it] 23%|██▎       | 7/30 [06:42<21:51, 57.01s/it][loss] ep 0 it 0 total=7.8809 mle=1.5710 pcon=5.2950 forget=1.3755 favg=-0.3606 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=7.7875 mle=1.5424 pcon=5.2879 forget=1.4013 favg=-0.4441 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=7.9331 mle=1.7006 pcon=5.2809 forget=1.3740 favg=-0.4224 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8.0965 mle=1.8996 pcon=5.2738 forget=1.3696 favg=-0.4465 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=7.9674 mle=1.7131 pcon=5.2670 forget=1.3826 favg=-0.3953 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=7.7254 mle=1.5020 pcon=5.2603 forget=1.3784 favg=-0.4153 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=7.8313 mle=1.5596 pcon=5.2540 forget=1.3819 favg=-0.3643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=7.8703 mle=1.6808 pcon=5.2476 forget=1.3896 favg=-0.4478 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 1 it 10 total=7.8555 mle=1.6714 pcon=5.2409 forget=1.4100 favg=-0.4668 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=7.8238 mle=1.6371 pcon=5.2346 forget=1.3670 favg=-0.4148 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=7.7389 mle=1.5167 pcon=5.2284 forget=1.3893 favg=-0.3955 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=7.9847 mle=1.7800 pcon=5.2224 forget=1.3892 favg=-0.4070 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=7.9262 mle=1.7646 pcon=5.2167 forget=1.3851 favg=-0.4402 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=7.8241 mle=1.6046 pcon=5.2112 forget=1.3679 favg=-0.3596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=7.9437 mle=1.7234 pcon=5.2056 forget=1.3777 favg=-0.3630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=7.9443 mle=1.7940 pcon=5.2002 forget=1.3856 favg=-0.4355 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 2 it 20 total=7.6889 mle=1.5530 pcon=5.1949 forget=1.3760 favg=-0.4351 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=7.9764 mle=1.8881 pcon=5.1898 forget=1.3562 favg=-0.4578 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=7.9107 mle=1.7421 pcon=5.1845 forget=1.3742 favg=-0.3901 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=7.6295 mle=1.5263 pcon=5.1794 forget=1.3617 favg=-0.4380 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=7.6261 mle=1.5862 pcon=5.1742 forget=1.3832 favg=-0.5176 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=7.9281 mle=1.8823 pcon=5.1694 forget=1.3635 favg=-0.4871 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=7.5113 mle=1.5841 pcon=5.1645 forget=1.3500 favg=-0.5874 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=7.6496 mle=1.8073 pcon=5.1596 forget=1.3521 favg=-0.6694 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 3 it 30 total=7.3522 mle=1.6855 pcon=5.1546 forget=1.3612 favg=-0.8491 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=7.1130 mle=1.6678 pcon=5.1499 forget=1.3714 favg=-1.0762 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=6.9764 mle=1.7410 pcon=5.1447 forget=1.3788 favg=-1.2881 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=6.9585 mle=1.9236 pcon=5.1397 forget=1.4069 favg=-1.5117 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=6.4885 mle=1.5771 pcon=5.1346 forget=1.4409 favg=-1.6641 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=6.7058 mle=1.8897 pcon=5.1293 forget=1.4710 favg=-1.7842 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=6.5684 mle=1.8098 pcon=5.1240 forget=1.5038 favg=-1.8691 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=6.4993 mle=1.7712 pcon=5.1194 forget=1.5227 favg=-1.9141 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 4 it 40 total=6.4377 mle=1.6667 pcon=5.1147 forget=1.5362 favg=-1.8799 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=6.8118 mle=1.8157 pcon=5.1100 forget=1.5834 favg=-1.6973 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8.3942 mle=1.6673 pcon=5.1057 forget=1.6185 favg=0.0027 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=9.9047 mle=1.4748 pcon=5.1016 forget=1.6487 favg=1.6797 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=10.2387 mle=1.7693 pcon=5.0970 forget=1.5970 favg=1.7754 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=9.7297 mle=1.4708 pcon=5.0928 forget=1.5470 favg=1.6191 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=9.8859 mle=1.9415 pcon=5.0887 forget=1.4866 favg=1.3691 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 0 total=9.4080 mle=1.8695 pcon=5.0850 forget=1.4369 favg=1.0166 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=8.8009 mle=1.7834 pcon=5.0812 forget=1.3767 favg=0.5596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=8.6426 mle=1.5915 pcon=5.0781 forget=1.3978 favg=0.5752 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=8.5249 mle=1.6060 pcon=5.0746 forget=1.3885 favg=0.4558 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=8.5338 mle=1.6846 pcon=5.0719 forget=1.3735 favg=0.4038 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=8.5768 mle=1.7826 pcon=5.0690 forget=1.3619 favg=0.3633 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=8.3412 mle=1.5949 pcon=5.0661 forget=1.3711 favg=0.3091 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=8.6021 mle=1.7656 pcon=5.0634 forget=1.3767 favg=0.3965 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 10 total=8.1549 mle=1.4142 pcon=5.0607 forget=1.3963 favg=0.2837 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.8870 mle=1.5576 pcon=5.0583 forget=1.3622 favg=-0.0912 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=8.0103 mle=1.5965 pcon=5.0560 forget=1.3985 favg=-0.0406 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=8.0781 mle=1.6689 pcon=5.0534 forget=1.3689 favg=-0.0131 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=7.7273 mle=1.6167 pcon=5.0512 forget=1.3683 favg=-0.3088 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=7.7205 mle=1.7971 pcon=5.0483 forget=1.3840 favg=-0.5088 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.2642 mle=1.5501 pcon=5.0457 forget=1.4067 favg=-0.7383 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=7.1367 mle=1.6702 pcon=5.0429 forget=1.4143 favg=-0.9907 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 20 total=6.9004 mle=1.6666 pcon=5.0399 forget=1.4449 favg=-1.2510 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=7.6761 mle=1.7840 pcon=5.0373 forget=1.4437 favg=-0.5889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=9.6226 mle=1.6326 pcon=5.0347 forget=1.4162 favg=1.5391 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=9.0529 mle=1.5737 pcon=5.0325 forget=1.3832 favg=1.0635 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=8.6771 mle=1.6448 pcon=5.0306 forget=1.3806 favg=0.6211 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=8.5290 mle=1.7961 pcon=5.0286 forget=1.3921 favg=0.3123 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=8.2051 mle=1.7366 pcon=5.0263 forget=1.3899 favg=0.0523 nr=64 nf=64 protos=540 fproto_sim=NA
 27%|██▋       | 8/30 [07:40<21:03, 57.43s/it] 30%|███       | 9/30 [08:37<20:02, 57.25s/it] 33%|███▎      | 10/30 [09:35<19:09, 57.48s/it] 37%|███▋      | 11/30 [10:32<18:10, 57.38s/it] 40%|████      | 12/30 [11:32<17:25, 58.07s/it] 43%|████▎     | 13/30 [12:35<16:55, 59.73s/it] 47%|████▋     | 14/30 [13:34<15:52, 59.50s/it] 50%|█████     | 15/30 [14:30<14:35, 58.37s/it] 53%|█████▎    | 16/30 [15:20<13:03, 55.95s/it][loss] ep 7 it 370 total=7.6624 mle=1.6449 pcon=5.0244 forget=1.3739 favg=-0.3809 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 30 total=7.0273 mle=1.5412 pcon=5.0226 forget=1.3854 favg=-0.9219 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=7.2111 mle=1.5839 pcon=5.0206 forget=1.4112 favg=-0.8047 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.8686 mle=1.4953 pcon=5.0182 forget=1.4405 favg=-0.0853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=8.2234 mle=1.7185 pcon=5.0158 forget=1.4461 favg=0.0431 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.8377 mle=1.5206 pcon=5.0139 forget=1.4154 favg=-0.1122 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=7.8601 mle=1.6893 pcon=5.0117 forget=1.3822 favg=-0.2230 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=7.9542 mle=1.7359 pcon=5.0095 forget=1.3926 favg=-0.1838 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=8.6704 mle=1.5075 pcon=5.0078 forget=1.3753 favg=0.7798 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 40 total=9.1990 mle=1.6528 pcon=5.0064 forget=1.3689 favg=1.1709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=8.3014 mle=1.6817 pcon=5.0047 forget=1.3665 favg=0.2484 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=7.3666 mle=1.8204 pcon=5.0034 forget=1.3797 favg=-0.8369 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.0579 mle=1.5663 pcon=5.0020 forget=1.3617 favg=-0.8721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.9166 mle=1.5945 pcon=5.0004 forget=1.4207 favg=-0.0991 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=8.2556 mle=1.5556 pcon=4.9984 forget=1.4598 favg=0.2418 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=8.1106 mle=1.5628 pcon=4.9966 forget=1.4357 favg=0.1154 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 0 total=7.8419 mle=1.6109 pcon=4.9946 forget=1.4000 favg=-0.1637 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=7.6275 mle=1.5900 pcon=4.9929 forget=1.3852 favg=-0.3406 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=8.4665 mle=1.7588 pcon=4.9917 forget=1.3769 favg=0.3391 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=9.5279 mle=1.8440 pcon=4.9909 forget=1.3863 favg=1.3066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=8.6757 mle=1.8601 pcon=4.9900 forget=1.3720 favg=0.4536 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=7.5003 mle=1.5869 pcon=4.9889 forget=1.3632 favg=-0.4387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=7.1273 mle=1.5873 pcon=4.9880 forget=1.3490 favg=-0.7969 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=7.1911 mle=1.6622 pcon=4.9867 forget=1.3781 favg=-0.8359 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 10 total=7.4141 mle=1.5951 pcon=4.9852 forget=1.4110 favg=-0.5771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=7.8418 mle=1.6637 pcon=4.9835 forget=1.4487 favg=-0.2542 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=7.7735 mle=1.5978 pcon=4.9818 forget=1.4512 favg=-0.2573 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=7.8286 mle=1.7002 pcon=4.9798 forget=1.3974 favg=-0.2487 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=9.0725 mle=1.7976 pcon=4.9786 forget=1.3754 favg=0.9209 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=9.5178 mle=1.6498 pcon=4.9781 forget=1.3938 favg=1.4961 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=9.1860 mle=1.9958 pcon=4.9771 forget=1.3958 favg=0.8174 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=7.7718 mle=1.6819 pcon=4.9760 forget=1.3513 favg=-0.2373 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 20 total=6.9416 mle=1.6259 pcon=4.9744 forget=1.3452 favg=-1.0039 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=6.3764 mle=1.7330 pcon=4.9728 forget=1.3406 favg=-1.6699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=6.1616 mle=1.6243 pcon=4.9713 forget=1.3765 favg=-1.8105 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=8.4755 mle=1.7660 pcon=4.9700 forget=1.3958 favg=0.3438 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=9.6870 mle=1.6277 pcon=4.9688 forget=1.4206 favg=1.6699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=9.5395 mle=1.5318 pcon=4.9681 forget=1.4282 favg=1.6113 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=9.2126 mle=1.5074 pcon=4.9668 forget=1.4044 favg=1.3340 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=9.1674 mle=1.8372 pcon=4.9654 forget=1.4014 favg=0.9634 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 30 total=8.2494 mle=1.4185 pcon=4.9641 forget=1.3790 favg=0.4878 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=7.6373 mle=1.4818 pcon=4.9625 forget=1.3924 favg=-0.1995 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=6.5645 mle=1.6159 pcon=4.9608 forget=1.3530 favg=-1.3652 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=7.9920 mle=1.6429 pcon=4.9597 forget=1.3655 favg=0.0238 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=9.1311 mle=1.7052 pcon=4.9589 forget=1.3742 favg=1.0928 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=8.4998 mle=1.5531 pcon=4.9579 forget=1.3560 favg=0.6328 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=7.8819 mle=1.6005 pcon=4.9568 forget=1.3560 favg=-0.0313 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=7.4630 mle=1.5808 pcon=4.9556 forget=1.3433 favg=-0.4167 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 40 total=7.2951 mle=1.7710 pcon=4.9539 forget=1.3387 favg=-0.7686 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=6.9907 mle=1.7787 pcon=4.9526 forget=1.3551 favg=-1.0957 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=6.7132 mle=1.6277 pcon=4.9513 forget=1.3783 favg=-1.2441 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=8.6767 mle=1.8641 pcon=4.9494 forget=1.4027 favg=0.4604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=9.7685 mle=1.6940 pcon=4.9478 forget=1.4470 favg=1.6797 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=9.2931 mle=1.6477 pcon=4.9458 forget=1.4301 favg=1.2695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=8.3095 mle=1.6835 pcon=4.9438 forget=1.3858 favg=0.2964 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 0 total=7.7141 mle=1.7011 pcon=4.9421 forget=1.3636 favg=-0.2927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=6.9300 mle=1.4814 pcon=4.9401 forget=1.3375 favg=-0.8291 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=6.5341 mle=1.5594 pcon=4.9384 forget=1.3243 favg=-1.2881 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=6.3994 mle=1.7078 pcon=4.9369 forget=1.3318 favg=-1.5771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=6.7264 mle=1.6363 pcon=4.9358 forget=1.3301 favg=-1.1758 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=9.7177 mle=1.9367 pcon=4.9347 forget=1.3444 favg=1.5020 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=9.5419 mle=1.5952 pcon=4.9336 forget=1.3628 favg=1.6504 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=9.3409 mle=1.5422 pcon=4.9320 forget=1.3794 favg=1.4873 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 10 total=9.1606 mle=1.5873 pcon=4.9301 forget=1.3972 favg=1.2461 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=8.7697 mle=1.4934 pcon=4.9279 forget=1.3826 favg=0.9658 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=8.4532 mle=1.5561 pcon=4.9256 forget=1.3748 favg=0.5967 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=8.1533 mle=1.6443 pcon=4.9226 forget=1.3689 favg=0.2175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=7.3636 mle=1.5794 pcon=4.9195 forget=1.3518 favg=-0.4871 nr=64 nf=64 protos=540 fproto_sim=NA
 57%|█████▋    | 17/30 [16:08<11:33, 53.38s/it] 60%|██████    | 18/30 [17:10<11:14, 56.20s/it] 63%|██████▎   | 19/30 [18:01<10:01, 54.67s/it] 67%|██████▋   | 20/30 [18:51<08:50, 53.02s/it] 70%|███████   | 21/30 [19:37<07:39, 51.10s/it] 73%|███████▎  | 22/30 [20:13<06:12, 46.51s/it] 77%|███████▋  | 23/30 [20:53<05:10, 44.39s/it] 80%|████████  | 24/30 [21:33<04:18, 43.06s/it] 83%|████████▎ | 25/30 [22:18<03:38, 43.70s/it][loss] ep 16 it 260 total=6.8992 mle=1.8797 pcon=4.9162 forget=1.3337 favg=-1.2305 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=6.6189 mle=1.8031 pcon=4.9133 forget=1.3458 favg=-1.4434 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=7.9807 mle=1.6508 pcon=4.9104 forget=1.3813 favg=0.0383 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 20 total=9.0935 mle=1.7767 pcon=4.9073 forget=1.4261 favg=0.9834 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=9.0277 mle=1.7483 pcon=4.9047 forget=1.4304 favg=0.9443 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=8.2779 mle=1.5502 pcon=4.9017 forget=1.4164 favg=0.4097 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=7.6720 mle=1.5491 pcon=4.8982 forget=1.3753 favg=-0.1506 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=7.4354 mle=1.6128 pcon=4.8950 forget=1.3619 favg=-0.4343 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=6.9276 mle=1.4680 pcon=4.8923 forget=1.3359 favg=-0.7686 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=7.2617 mle=1.8125 pcon=4.8894 forget=1.3323 favg=-0.7725 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=7.8241 mle=1.7510 pcon=4.8863 forget=1.3381 favg=-0.1512 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=8.9512 mle=1.5728 pcon=4.8841 forget=1.3489 favg=1.1455 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=9.4226 mle=1.6796 pcon=4.8816 forget=1.3751 favg=1.4863 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=9.1750 mle=1.6749 pcon=4.8790 forget=1.3838 favg=1.2373 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=8.7077 mle=1.5855 pcon=4.8762 forget=1.3695 favg=0.8765 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=7.9460 mle=1.6506 pcon=4.8732 forget=1.3527 favg=0.0695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=7.3359 mle=1.6102 pcon=4.8702 forget=1.3391 favg=-0.4836 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=6.8394 mle=1.7015 pcon=4.8672 forget=1.3088 favg=-1.0381 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=6.5690 mle=1.6848 pcon=4.8644 forget=1.3205 favg=-1.3008 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 40 total=6.1988 mle=1.5420 pcon=4.8616 forget=1.3216 favg=-1.5264 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=6.3943 mle=1.6248 pcon=4.8590 forget=1.3451 favg=-1.4346 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=6.7485 mle=1.6018 pcon=4.8561 forget=1.3590 favg=-1.0684 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=7.4266 mle=1.6783 pcon=4.8533 forget=1.3885 favg=-0.4934 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=8.3374 mle=1.8507 pcon=4.8506 forget=1.4271 favg=0.2090 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=8.5162 mle=1.7025 pcon=4.8481 forget=1.4460 favg=0.5195 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=8.5904 mle=1.5388 pcon=4.8454 forget=1.4552 favg=0.7510 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 0 total=8.7887 mle=1.6358 pcon=4.8426 forget=1.4416 favg=0.8687 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=8.7402 mle=1.6495 pcon=4.8398 forget=1.4135 favg=0.8374 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=8.7849 mle=1.7148 pcon=4.8371 forget=1.4098 favg=0.8232 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=9.0345 mle=1.8143 pcon=4.8341 forget=1.3861 favg=1.0000 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=8.8056 mle=1.5625 pcon=4.8312 forget=1.3651 favg=1.0469 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=9.1539 mle=1.9062 pcon=4.8285 forget=1.3695 favg=1.0498 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=8.8470 mle=1.7896 pcon=4.8258 forget=1.3786 favg=0.8530 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=8.1871 mle=1.4657 pcon=4.8233 forget=1.3697 favg=0.5283 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 10 total=7.7923 mle=1.4734 pcon=4.8208 forget=1.3655 favg=0.1326 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=7.6002 mle=1.6054 pcon=4.8184 forget=1.3648 favg=-0.1885 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=7.3540 mle=1.5474 pcon=4.8162 forget=1.3630 favg=-0.3726 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=6.8317 mle=1.5553 pcon=4.8139 forget=1.3272 favg=-0.8647 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=6.8390 mle=1.7833 pcon=4.8117 forget=1.3436 favg=-1.0996 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=6.5410 mle=1.6508 pcon=4.8093 forget=1.3328 favg=-1.2520 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=6.2577 mle=1.5484 pcon=4.8068 forget=1.3371 favg=-1.4346 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=6.1776 mle=1.5669 pcon=4.8044 forget=1.3698 favg=-1.5635 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 20 total=6.3344 mle=1.6082 pcon=4.8019 forget=1.3794 favg=-1.4551 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=6.5968 mle=1.6585 pcon=4.7996 forget=1.4112 favg=-1.2725 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=7.0642 mle=1.5553 pcon=4.7971 forget=1.4495 favg=-0.7378 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=7.7455 mle=1.6103 pcon=4.7950 forget=1.4689 favg=-0.1287 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=8.3494 mle=1.5846 pcon=4.7929 forget=1.4803 favg=0.4917 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=8.5995 mle=1.5004 pcon=4.7906 forget=1.4935 favg=0.8149 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=9.0628 mle=1.7048 pcon=4.7887 forget=1.4786 favg=1.0908 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=9.4888 mle=1.9291 pcon=4.7864 forget=1.4862 favg=1.2871 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 30 total=9.0471 mle=1.5171 pcon=4.7847 forget=1.4543 favg=1.2910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=9.1205 mle=1.6233 pcon=4.7826 forget=1.4314 favg=1.2832 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=9.0413 mle=1.5379 pcon=4.7805 forget=1.4300 favg=1.2930 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=9.1648 mle=1.7157 pcon=4.7784 forget=1.3992 favg=1.2715 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=9.0001 mle=1.6061 pcon=4.7766 forget=1.4016 favg=1.2158 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=8.8132 mle=1.5671 pcon=4.7746 forget=1.3944 favg=1.0771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=8.6924 mle=1.6078 pcon=4.7726 forget=1.3736 favg=0.9385 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=8.4884 mle=1.5612 pcon=4.7707 forget=1.3885 favg=0.7681 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 40 total=8.3259 mle=1.5309 pcon=4.7690 forget=1.3624 favg=0.6636 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=7.9768 mle=1.6035 pcon=4.7672 forget=1.3636 favg=0.2424 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=7.6998 mle=1.5640 pcon=4.7655 forget=1.3607 favg=0.0096 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=7.4962 mle=1.6425 pcon=4.7640 forget=1.3499 favg=-0.2603 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 240 total=7.0862 mle=1.4806 pcon=4.7624 forget=1.3545 favg=-0.5112 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=7.1767 mle=1.6900 pcon=4.7609 forget=1.3548 favg=-0.6289 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=6.9226 mle=1.5874 pcon=4.7594 forget=1.3425 favg=-0.7666 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 0 total=6.7910 mle=1.5978 pcon=4.7579 forget=1.3446 favg=-0.9092 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=7.1652 mle=1.7786 pcon=4.7567 forget=1.3628 favg=-0.7329 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=6.9776 mle=1.6306 pcon=4.7553 forget=1.3407 favg=-0.7490 nr=64 nf=64 protos=540 fproto_sim=NA
 87%|████████▋ | 26/30 [22:54<02:45, 41.38s/it] 90%|█████████ | 27/30 [23:23<01:53, 37.88s/it] 93%|█████████▎| 28/30 [23:53<01:10, 35.41s/it] 97%|█████████▋| 29/30 [24:22<00:33, 33.63s/it]100%|██████████| 30/30 [24:52<00:00, 32.41s/it]100%|██████████| 30/30 [24:52<00:00, 49.75s/it]
[loss] ep 25 it 150 total=7.2889 mle=1.7431 pcon=4.7542 forget=1.3414 favg=-0.5498 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 200 total=7.3601 mle=1.8100 pcon=4.7528 forget=1.3647 favg=-0.5674 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=7.4334 mle=1.7109 pcon=4.7515 forget=1.3741 favg=-0.4031 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=7.4148 mle=1.6108 pcon=4.7502 forget=1.3660 favg=-0.3123 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=7.3258 mle=1.5123 pcon=4.7492 forget=1.3773 favg=-0.3130 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 10 total=7.4466 mle=1.6046 pcon=4.7479 forget=1.3912 favg=-0.2971 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=7.5501 mle=1.5629 pcon=4.7467 forget=1.3858 favg=-0.1454 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=7.6368 mle=1.6817 pcon=4.7455 forget=1.3913 favg=-0.1818 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=7.4364 mle=1.5002 pcon=4.7442 forget=1.3914 favg=-0.1995 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=7.5417 mle=1.6396 pcon=4.7428 forget=1.3998 favg=-0.2405 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=7.4765 mle=1.4922 pcon=4.7416 forget=1.4085 favg=-0.1658 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 310 total=7.8186 mle=1.8487 pcon=4.7404 forget=1.4008 favg=-0.1713 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=7.4263 mle=1.5633 pcon=4.7390 forget=1.4048 favg=-0.2808 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 20 total=7.6100 mle=1.6569 pcon=4.7378 forget=1.4054 favg=-0.1901 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=7.4868 mle=1.5642 pcon=4.7363 forget=1.4146 favg=-0.2283 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=7.3774 mle=1.5501 pcon=4.7350 forget=1.4169 favg=-0.3247 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=7.4458 mle=1.5307 pcon=4.7337 forget=1.4219 favg=-0.2405 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=7.5225 mle=1.4649 pcon=4.7324 forget=1.4161 favg=-0.0909 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=7.5923 mle=1.5840 pcon=4.7311 forget=1.4179 favg=-0.1406 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=7.6945 mle=1.7202 pcon=4.7300 forget=1.4086 favg=-0.1643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=7.9042 mle=1.7033 pcon=4.7289 forget=1.4255 favg=0.0465 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 30 total=7.7334 mle=1.6543 pcon=4.7279 forget=1.4034 favg=-0.0522 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=7.9104 mle=1.6831 pcon=4.7269 forget=1.4158 favg=0.0846 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=7.8330 mle=1.5810 pcon=4.7260 forget=1.4202 favg=0.1058 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=7.8593 mle=1.5374 pcon=4.7251 forget=1.4105 favg=0.1863 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=7.8648 mle=1.4166 pcon=4.7238 forget=1.4097 favg=0.3147 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=8.0767 mle=1.5749 pcon=4.7228 forget=1.4198 favg=0.3591 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=8.4056 mle=1.7763 pcon=4.7217 forget=1.4276 favg=0.4800 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=8.4145 mle=1.6751 pcon=4.7207 forget=1.4338 favg=0.5850 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 40 total=8.4662 mle=1.7286 pcon=4.7196 forget=1.4179 favg=0.6001 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=8.3382 mle=1.5608 pcon=4.7185 forget=1.4134 favg=0.6455 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=8.4984 mle=1.6448 pcon=4.7177 forget=1.4220 favg=0.7139 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=8.4252 mle=1.5989 pcon=4.7169 forget=1.4214 favg=0.6880 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=8.3976 mle=1.5373 pcon=4.7161 forget=1.4201 favg=0.7241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=8.6450 mle=1.7018 pcon=4.7154 forget=1.4207 favg=0.8071 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=8.4790 mle=1.5587 pcon=4.7148 forget=1.4228 favg=0.7827 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:55,  2.22it/s]  2%|▏         | 8/391 [00:00<00:20, 18.36it/s]  4%|▎         | 14/391 [00:00<00:13, 28.32it/s]  5%|▌         | 20/391 [00:00<00:10, 35.88it/s]  7%|▋         | 26/391 [00:00<00:08, 41.31it/s]  8%|▊         | 33/391 [00:00<00:07, 47.61it/s] 10%|▉         | 39/391 [00:01<00:07, 50.16it/s] 12%|█▏        | 45/391 [00:01<00:06, 51.73it/s] 13%|█▎        | 52/391 [00:01<00:06, 54.77it/s] 15%|█▌        | 60/391 [00:01<00:05, 59.71it/s] 17%|█▋        | 67/391 [00:01<00:05, 62.26it/s] 19%|█▉        | 76/391 [00:01<00:04, 68.14it/s] 22%|██▏       | 85/391 [00:01<00:04, 72.37it/s] 24%|██▍       | 93/391 [00:01<00:04, 70.91it/s] 26%|██▌       | 101/391 [00:02<00:04, 63.29it/s] 28%|██▊       | 108/391 [00:02<00:04, 60.68it/s] 29%|██▉       | 115/391 [00:02<00:04, 61.32it/s] 31%|███▏      | 123/391 [00:02<00:04, 64.59it/s] 33%|███▎      | 130/391 [00:02<00:03, 65.48it/s] 35%|███▌      | 137/391 [00:02<00:03, 63.81it/s] 37%|███▋      | 144/391 [00:02<00:03, 64.26it/s] 39%|███▉      | 152/391 [00:02<00:03, 66.57it/s] 41%|████      | 159/391 [00:02<00:03, 61.22it/s] 42%|████▏     | 166/391 [00:03<00:03, 58.55it/s] 44%|████▍     | 173/391 [00:03<00:03, 59.40it/s] 46%|████▌     | 180/391 [00:03<00:03, 58.76it/s] 48%|████▊     | 186/391 [00:03<00:03, 56.99it/s] 49%|████▉     | 192/391 [00:03<00:03, 56.11it/s] 51%|█████     | 199/391 [00:03<00:03, 56.46it/s] 52%|█████▏    | 205/391 [00:03<00:03, 54.99it/s] 54%|█████▍    | 211/391 [00:03<00:03, 56.10it/s] 56%|█████▌    | 218/391 [00:03<00:02, 59.90it/s] 58%|█████▊    | 226/391 [00:04<00:02, 63.38it/s] 60%|█████▉    | 233/391 [00:04<00:02, 60.57it/s] 61%|██████▏   | 240/391 [00:04<00:02, 62.51it/s] 63%|██████▎   | 247/391 [00:04<00:02, 62.09it/s] 65%|██████▍   | 254/391 [00:04<00:02, 62.13it/s] 67%|██████▋   | 262/391 [00:04<00:01, 66.34it/s] 69%|██████▉   | 269/391 [00:04<00:01, 64.91it/s] 71%|███████   | 276/391 [00:04<00:01, 65.07it/s] 72%|███████▏  | 283/391 [00:05<00:01, 61.32it/s] 74%|███████▍  | 290/391 [00:05<00:01, 62.04it/s] 76%|███████▌  | 297/391 [00:05<00:01, 63.10it/s] 78%|███████▊  | 304/391 [00:05<00:01, 64.03it/s] 80%|███████▉  | 311/391 [00:05<00:01, 64.91it/s] 81%|████████▏ | 318/391 [00:05<00:01, 63.48it/s] 83%|████████▎ | 325/391 [00:05<00:01, 63.17it/s] 85%|████████▍ | 332/391 [00:05<00:00, 63.36it/s] 87%|████████▋ | 339/391 [00:05<00:00, 61.31it/s] 88%|████████▊ | 346/391 [00:06<00:00, 61.62it/s] 90%|█████████ | 353/391 [00:06<00:00, 63.86it/s] 92%|█████████▏| 360/391 [00:06<00:00, 65.40it/s] 94%|█████████▍| 367/391 [00:06<00:00, 65.87it/s] 96%|█████████▌| 374/391 [00:06<00:00, 66.95it/s] 98%|█████████▊| 382/391 [00:06<00:00, 69.27it/s]100%|█████████▉| 390/391 [00:06<00:00, 71.88it/s]100%|██████████| 391/391 [00:06<00:00, 58.81it/s]
50000 images processed, 7.1009461879730225 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:23,  3.26it/s] 11%|█▏        | 9/79 [00:00<00:02, 26.19it/s] 20%|██        | 16/79 [00:00<00:01, 38.80it/s] 29%|██▉       | 23/79 [00:00<00:01, 45.87it/s] 38%|███▊      | 30/79 [00:00<00:00, 51.06it/s] 47%|████▋     | 37/79 [00:00<00:00, 55.43it/s] 56%|█████▌    | 44/79 [00:00<00:00, 57.82it/s] 65%|██████▍   | 51/79 [00:01<00:00, 60.95it/s] 73%|███████▎  | 58/79 [00:01<00:00, 53.97it/s] 81%|████████  | 64/79 [00:01<00:00, 54.74it/s] 91%|█████████ | 72/79 [00:01<00:00, 59.50it/s]100%|██████████| 79/79 [00:02<00:00, 25.57it/s]100%|██████████| 79/79 [00:02<00:00, 37.80it/s]
10000 images processed, 2.179918050765991 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:24,  2.41it/s]  4%|▍         | 9/204 [00:00<00:09, 21.29it/s]  7%|▋         | 15/204 [00:00<00:06, 30.59it/s] 10%|█         | 21/204 [00:00<00:04, 37.45it/s] 13%|█▎        | 27/204 [00:00<00:04, 41.14it/s] 17%|█▋        | 34/204 [00:00<00:03, 47.52it/s] 21%|██        | 42/204 [00:01<00:02, 55.81it/s] 25%|██▍       | 50/204 [00:01<00:02, 57.86it/s] 28%|██▊       | 57/204 [00:01<00:02, 53.78it/s] 31%|███▏      | 64/204 [00:01<00:02, 57.39it/s] 35%|███▍      | 71/204 [00:01<00:02, 58.53it/s] 38%|███▊      | 78/204 [00:01<00:02, 58.68it/s] 42%|████▏     | 86/204 [00:01<00:01, 62.66it/s] 46%|████▌     | 93/204 [00:01<00:01, 63.77it/s] 49%|████▉     | 100/204 [00:02<00:01, 64.61it/s] 53%|█████▎    | 108/204 [00:02<00:01, 66.47it/s] 57%|█████▋    | 116/204 [00:02<00:01, 67.36it/s] 60%|██████    | 123/204 [00:02<00:01, 65.90it/s] 64%|██████▎   | 130/204 [00:02<00:01, 64.60it/s] 68%|██████▊   | 139/204 [00:02<00:00, 69.34it/s] 72%|███████▏  | 146/204 [00:02<00:00, 69.32it/s] 75%|███████▌  | 154/204 [00:02<00:00, 70.49it/s] 79%|███████▉  | 162/204 [00:02<00:00, 71.42it/s] 83%|████████▎ | 170/204 [00:03<00:00, 71.36it/s] 87%|████████▋ | 178/204 [00:03<00:00, 70.60it/s] 91%|█████████ | 186/204 [00:03<00:00, 70.47it/s] 95%|█████████▌| 194/204 [00:03<00:00, 71.41it/s]100%|█████████▉| 203/204 [00:03<00:00, 74.57it/s]100%|██████████| 204/204 [00:03<00:00, 58.53it/s]
26032 images processed, 3.7022526264190674 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:50,  1.56it/s] 10%|█         | 8/79 [00:00<00:05, 13.71it/s] 20%|██        | 16/79 [00:00<00:02, 26.81it/s] 28%|██▊       | 22/79 [00:01<00:02, 28.30it/s] 34%|███▍      | 27/79 [00:01<00:01, 27.21it/s] 43%|████▎     | 34/79 [00:01<00:01, 30.82it/s] 53%|█████▎    | 42/79 [00:01<00:01, 33.43it/s] 61%|██████    | 48/79 [00:01<00:00, 37.97it/s] 67%|██████▋   | 53/79 [00:01<00:00, 40.42it/s] 73%|███████▎  | 58/79 [00:02<00:00, 37.03it/s] 84%|████████▎ | 66/79 [00:02<00:00, 42.95it/s] 91%|█████████ | 72/79 [00:02<00:00, 45.38it/s] 97%|█████████▋| 77/79 [00:02<00:00, 40.35it/s]100%|██████████| 79/79 [00:02<00:00, 32.22it/s]
10000 images processed, 2.592149257659912 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:32,  2.40it/s] 10%|█         | 8/79 [00:00<00:03, 19.43it/s] 19%|█▉        | 15/79 [00:00<00:02, 31.55it/s] 28%|██▊       | 22/79 [00:00<00:01, 41.75it/s] 37%|███▋      | 29/79 [00:00<00:01, 48.66it/s] 46%|████▌     | 36/79 [00:00<00:00, 52.97it/s] 54%|█████▍    | 43/79 [00:01<00:00, 56.80it/s] 63%|██████▎   | 50/79 [00:01<00:00, 58.84it/s] 72%|███████▏  | 57/79 [00:01<00:00, 59.72it/s] 84%|████████▎ | 66/79 [00:01<00:00, 66.88it/s] 92%|█████████▏| 73/79 [00:01<00:00, 65.62it/s]100%|██████████| 79/79 [00:01<00:00, 50.21it/s]
10000 images processed, 1.663111686706543 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:30,  2.29it/s]  6%|▌         | 4/70 [00:00<00:07,  8.74it/s] 14%|█▍        | 10/70 [00:00<00:02, 20.99it/s] 26%|██▌       | 18/70 [00:00<00:01, 35.12it/s] 36%|███▌      | 25/70 [00:00<00:01, 43.46it/s] 47%|████▋     | 33/70 [00:00<00:00, 51.93it/s] 60%|██████    | 42/70 [00:01<00:00, 60.73it/s] 71%|███████▏  | 50/70 [00:01<00:00, 63.46it/s] 83%|████████▎ | 58/70 [00:01<00:00, 66.19it/s] 96%|█████████▌| 67/70 [00:01<00:00, 68.99it/s]100%|██████████| 70/70 [00:01<00:00, 46.94it/s]
8925 images processed, 1.5773155689239502 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:44,  1.01s/it]  4%|▍         | 2/45 [00:01<00:20,  2.07it/s] 20%|██        | 9/45 [00:01<00:04,  8.80it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.57it/s] 38%|███▊      | 17/45 [00:02<00:02, 11.70it/s] 42%|████▏     | 19/45 [00:02<00:02, 10.94it/s] 56%|█████▌    | 25/45 [00:02<00:01, 12.91it/s] 60%|██████    | 27/45 [00:02<00:01, 12.14it/s] 73%|███████▎  | 33/45 [00:03<00:00, 12.09it/s] 78%|███████▊  | 35/45 [00:03<00:00, 10.70it/s] 91%|█████████ | 41/45 [00:03<00:00, 14.32it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.50it/s]100%|██████████| 45/45 [00:04<00:00, 10.02it/s]
5640 images processed, 4.54895281791687 seconds used

25.118278741836548
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Traceback (most recent call last):
  File "/home/shaokun/PALM/eval_cifar.py", line 19, in <module>
    eval_maha(args)
  File "/home/shaokun/PALM/util/evaluations/mahalanobis.py", line 21, in eval_maha
    feat_log = np.load(f"cache/{args.backbone}-{args.method}/{args.in_dataset}/train_{args.backbone}-{args.method}_features.npy", allow_pickle=True)
  File "/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'cache/resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0/CIFAR-100/train_resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0_features.npy'
