nohup: ignoring input
[resume] detected last completed stage = 2; seen={0,8,11,40,51,66,67,88,94,57}
==== Stage 3: inc={59,58,44,93,10}; seen={0,8,11,40,51,66,67,88,94,57}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2', lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='59,58,44,93,10', forget_classes_seen='0,8,11,40,51,66,67,88,94,57', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:10<08:14, 10.09s/it]  4%|▍         | 2/50 [00:12<04:15,  5.32s/it]  6%|▌         | 3/50 [00:13<02:49,  3.60s/it]  8%|▊         | 4/50 [00:15<02:07,  2.78s/it] 10%|█         | 5/50 [00:16<01:36,  2.15s/it] 12%|█▏        | 6/50 [00:17<01:23,  1.90s/it] 14%|█▍        | 7/50 [00:18<01:14,  1.74s/it] 16%|█▌        | 8/50 [00:20<01:07,  1.62s/it] 18%|█▊        | 9/50 [00:21<01:05,  1.59s/it] 20%|██        | 10/50 [00:22<00:52,  1.31s/it] 22%|██▏       | 11/50 [00:23<00:44,  1.14s/it] 24%|██▍       | 12/50 [00:24<00:46,  1.22s/it] 26%|██▌       | 13/50 [00:25<00:40,  1.09s/it] 28%|██▊       | 14/50 [00:26<00:34,  1.03it/s] 30%|███       | 15/50 [00:26<00:30,  1.14it/s] 32%|███▏      | 16/50 [00:28<00:37,  1.11s/it] 34%|███▍      | 17/50 [00:29<00:32,  1.02it/s] 36%|███▌      | 18/50 [00:30<00:35,  1.11s/it] 38%|███▊      | 19/50 [00:31<00:30,  1.02it/s] 40%|████      | 20/50 [00:32<00:29,  1.02it/s] 42%|████▏     | 21/50 [00:33<00:27,  1.04it/s] 44%|████▍     | 22/50 [00:34<00:25,  1.08it/s] 46%|████▌     | 23/50 [00:34<00:25,  1.08it/s] 48%|████▊     | 24/50 [00:35<00:21,  1.21it/s] 50%|█████     | 25/50 [00:36<00:19,  1.30it/s] 52%|█████▏    | 26/50 [00:36<00:17,  1.39it/s] 54%|█████▍    | 27/50 [00:37<00:15,  1.47it/s] 56%|█████▌    | 28/50 [00:37<00:14,  1.52it/s] 58%|█████▊    | 29/50 [00:38<00:12,  1.63it/s] 60%|██████    | 30/50 [00:39<00:13,  1.50it/s][loss] ep 0 it 0 total=15.1291 mle=5.4403 pcon=9.6888 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 2 it 10 total=14.7192 mle=5.0427 pcon=9.6765 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 5 it 0 total=14.3103 mle=4.6488 pcon=9.6615 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 7 it 10 total=13.9806 mle=4.3368 pcon=9.6438 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 10 it 0 total=13.6889 mle=4.0649 pcon=9.6240 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 12 it 10 total=13.4913 mle=3.8885 pcon=9.6027 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 15 it 0 total=13.3262 mle=3.7458 pcon=9.5804 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 17 it 10 total=13.2238 mle=3.6665 pcon=9.5572 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 20 it 0 total=13.1176 mle=3.5841 pcon=9.5335 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 22 it 10 total=13.0137 mle=3.5042 pcon=9.5095 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 25 it 0 total=12.9759 mle=3.4905 pcon=9.4854 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 27 it 10 total=12.8714 mle=3.4101 pcon=9.4614 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 30 it 0 total=12.8650 mle=3.4276 pcon=9.4374 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
 62%|██████▏   | 31/50 [00:40<00:13,  1.39it/s] 64%|██████▍   | 32/50 [00:40<00:11,  1.50it/s] 66%|██████▌   | 33/50 [00:41<00:10,  1.59it/s] 68%|██████▊   | 34/50 [00:41<00:10,  1.59it/s] 70%|███████   | 35/50 [00:42<00:09,  1.62it/s] 72%|███████▏  | 36/50 [00:43<00:08,  1.63it/s] 74%|███████▍  | 37/50 [00:43<00:07,  1.67it/s] 76%|███████▌  | 38/50 [00:44<00:07,  1.69it/s] 78%|███████▊  | 39/50 [00:44<00:06,  1.72it/s] 80%|████████  | 40/50 [00:45<00:05,  1.73it/s] 82%|████████▏ | 41/50 [00:45<00:05,  1.73it/s] 84%|████████▍ | 42/50 [00:46<00:04,  1.69it/s] 86%|████████▌ | 43/50 [00:47<00:04,  1.70it/s] 88%|████████▊ | 44/50 [00:47<00:03,  1.58it/s] 90%|█████████ | 45/50 [00:48<00:03,  1.64it/s] 92%|█████████▏| 46/50 [00:49<00:02,  1.62it/s] 94%|█████████▍| 47/50 [00:49<00:01,  1.62it/s] 96%|█████████▌| 48/50 [00:50<00:01,  1.65it/s] 98%|█████████▊| 49/50 [00:50<00:00,  1.71it/s]100%|██████████| 50/50 [00:51<00:00,  1.75it/s]100%|██████████| 50/50 [00:51<00:00,  1.03s/it]
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 32 it 10 total=12.8295 mle=3.4155 pcon=9.4139 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 35 it 0 total=12.7847 mle=3.3939 pcon=9.3909 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 37 it 10 total=12.7476 mle=3.3791 pcon=9.3684 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 40 it 0 total=12.7259 mle=3.3793 pcon=9.3466 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 42 it 10 total=12.7152 mle=3.3895 pcon=9.3257 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 45 it 0 total=12.6736 mle=3.3681 pcon=9.3055 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 47 it 10 total=12.6563 mle=3.3702 pcon=9.2861 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stage3-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<04:32,  1.58it/s]  3%|▎         | 11/430 [00:00<00:21, 19.49it/s]  5%|▍         | 21/430 [00:00<00:11, 35.78it/s]  7%|▋         | 31/430 [00:00<00:08, 49.36it/s] 10%|▉         | 41/430 [00:01<00:06, 60.64it/s] 12%|█▏        | 51/430 [00:01<00:05, 69.74it/s] 14%|█▍        | 61/430 [00:01<00:04, 76.69it/s] 17%|█▋        | 71/430 [00:01<00:04, 81.73it/s] 19%|█▉        | 81/430 [00:01<00:04, 85.60it/s] 21%|██        | 91/430 [00:01<00:03, 86.53it/s] 23%|██▎       | 101/430 [00:01<00:03, 88.84it/s] 26%|██▌       | 111/430 [00:01<00:03, 90.85it/s] 28%|██▊       | 121/430 [00:01<00:03, 91.01it/s] 30%|███       | 131/430 [00:02<00:03, 91.77it/s] 33%|███▎      | 141/430 [00:02<00:03, 93.01it/s] 35%|███▌      | 151/430 [00:02<00:02, 93.18it/s] 37%|███▋      | 161/430 [00:02<00:02, 93.71it/s] 40%|███▉      | 171/430 [00:02<00:02, 93.74it/s] 42%|████▏     | 181/430 [00:02<00:02, 93.71it/s] 44%|████▍     | 191/430 [00:02<00:02, 93.46it/s] 47%|████▋     | 201/430 [00:02<00:02, 93.39it/s] 49%|████▉     | 211/430 [00:02<00:02, 93.08it/s] 51%|█████▏    | 221/430 [00:02<00:02, 92.37it/s] 54%|█████▎    | 231/430 [00:03<00:02, 92.71it/s] 56%|█████▌    | 241/430 [00:03<00:02, 93.16it/s] 58%|█████▊    | 251/430 [00:03<00:01, 94.11it/s] 61%|██████    | 261/430 [00:03<00:01, 94.70it/s] 63%|██████▎   | 271/430 [00:03<00:01, 95.15it/s] 65%|██████▌   | 281/430 [00:03<00:01, 95.31it/s] 68%|██████▊   | 291/430 [00:03<00:01, 95.68it/s] 70%|███████   | 301/430 [00:03<00:01, 95.83it/s] 72%|███████▏  | 311/430 [00:03<00:01, 95.63it/s] 75%|███████▍  | 321/430 [00:04<00:01, 95.86it/s] 77%|███████▋  | 331/430 [00:04<00:01, 95.51it/s] 79%|███████▉  | 341/430 [00:04<00:00, 95.49it/s] 82%|████████▏ | 351/430 [00:04<00:00, 95.61it/s] 84%|████████▍ | 361/430 [00:04<00:00, 95.73it/s] 86%|████████▋ | 371/430 [00:04<00:00, 95.62it/s] 89%|████████▊ | 381/430 [00:04<00:00, 95.14it/s] 91%|█████████ | 391/430 [00:04<00:00, 95.52it/s] 93%|█████████▎| 401/430 [00:04<00:00, 95.46it/s] 96%|█████████▌| 411/430 [00:04<00:00, 96.15it/s] 98%|█████████▊| 421/430 [00:05<00:00, 96.65it/s]100%|██████████| 430/430 [00:05<00:00, 83.15it/s]
55000 images processed, 5.254627227783203 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:40,  2.12it/s] 10%|█         | 9/86 [00:00<00:03, 19.71it/s] 21%|██        | 18/86 [00:00<00:01, 35.44it/s] 31%|███▏      | 27/86 [00:00<00:01, 48.09it/s] 42%|████▏     | 36/86 [00:00<00:00, 58.43it/s] 52%|█████▏    | 45/86 [00:01<00:00, 66.12it/s] 62%|██████▏   | 53/86 [00:01<00:00, 65.69it/s] 72%|███████▏  | 62/86 [00:01<00:00, 71.23it/s] 83%|████████▎ | 71/86 [00:01<00:00, 74.63it/s] 93%|█████████▎| 80/86 [00:01<00:00, 76.85it/s]100%|██████████| 86/86 [00:01<00:00, 55.87it/s]
11000 images processed, 1.5677943229675293 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:57,  1.14it/s]  4%|▍         | 9/204 [00:00<00:16, 12.01it/s]  7%|▋         | 15/204 [00:01<00:09, 19.83it/s] 11%|█         | 22/204 [00:01<00:06, 29.15it/s] 15%|█▍        | 30/204 [00:01<00:04, 38.40it/s] 19%|█▊        | 38/204 [00:01<00:03, 47.53it/s] 22%|██▏       | 45/204 [00:01<00:03, 51.83it/s] 25%|██▌       | 52/204 [00:01<00:02, 55.02it/s] 29%|██▉       | 60/204 [00:01<00:02, 59.88it/s] 33%|███▎      | 68/204 [00:01<00:02, 64.24it/s] 37%|███▋      | 76/204 [00:01<00:01, 67.01it/s] 41%|████      | 84/204 [00:02<00:01, 65.41it/s] 45%|████▍     | 91/204 [00:02<00:01, 63.44it/s] 48%|████▊     | 98/204 [00:02<00:02, 52.48it/s] 51%|█████     | 104/204 [00:02<00:01, 52.86it/s] 55%|█████▍    | 112/204 [00:02<00:01, 58.08it/s] 59%|█████▉    | 121/204 [00:02<00:01, 64.26it/s] 64%|██████▎   | 130/204 [00:02<00:01, 69.79it/s] 68%|██████▊   | 138/204 [00:02<00:00, 70.65it/s] 72%|███████▏  | 146/204 [00:03<00:00, 73.05it/s] 75%|███████▌  | 154/204 [00:03<00:00, 69.63it/s] 79%|███████▉  | 162/204 [00:03<00:00, 71.48it/s] 83%|████████▎ | 170/204 [00:03<00:00, 67.76it/s] 87%|████████▋ | 177/204 [00:03<00:00, 66.13it/s] 91%|█████████ | 186/204 [00:03<00:00, 70.79it/s] 96%|█████████▌| 195/204 [00:03<00:00, 73.44it/s]100%|█████████▉| 203/204 [00:03<00:00, 75.11it/s]100%|██████████| 204/204 [00:03<00:00, 52.79it/s]
26032 images processed, 4.121156215667725 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:53,  1.45it/s] 10%|█         | 8/79 [00:00<00:05, 13.11it/s] 20%|██        | 16/79 [00:00<00:02, 25.97it/s] 30%|███       | 24/79 [00:01<00:01, 36.94it/s] 41%|████      | 32/79 [00:01<00:01, 46.15it/s] 51%|█████     | 40/79 [00:01<00:00, 52.86it/s] 61%|██████    | 48/79 [00:01<00:00, 58.02it/s] 71%|███████   | 56/79 [00:01<00:00, 62.34it/s] 81%|████████  | 64/79 [00:01<00:00, 66.47it/s] 91%|█████████ | 72/79 [00:01<00:00, 68.97it/s]100%|██████████| 79/79 [00:02<00:00, 32.27it/s]
10000 images processed, 2.5339696407318115 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:56,  1.38it/s] 11%|█▏        | 9/79 [00:00<00:04, 14.10it/s] 22%|██▏       | 17/79 [00:00<00:02, 26.05it/s] 33%|███▎      | 26/79 [00:01<00:01, 38.76it/s] 43%|████▎     | 34/79 [00:01<00:00, 47.90it/s] 53%|█████▎    | 42/79 [00:01<00:00, 53.96it/s] 63%|██████▎   | 50/79 [00:01<00:00, 59.57it/s] 73%|███████▎  | 58/79 [00:01<00:00, 62.83it/s] 84%|████████▎ | 66/79 [00:01<00:00, 67.13it/s] 94%|█████████▎| 74/79 [00:01<00:00, 67.78it/s]100%|██████████| 79/79 [00:01<00:00, 45.01it/s]
10000 images processed, 1.8363449573516846 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:43,  1.59it/s]  9%|▊         | 6/70 [00:00<00:06,  9.92it/s] 14%|█▍        | 10/70 [00:00<00:03, 15.40it/s] 20%|██        | 14/70 [00:00<00:02, 19.97it/s] 26%|██▌       | 18/70 [00:01<00:02, 23.29it/s] 34%|███▍      | 24/70 [00:01<00:01, 31.32it/s] 44%|████▍     | 31/70 [00:01<00:00, 40.84it/s] 56%|█████▌    | 39/70 [00:01<00:00, 49.82it/s] 67%|██████▋   | 47/70 [00:01<00:00, 57.38it/s] 80%|████████  | 56/70 [00:01<00:00, 64.27it/s] 91%|█████████▏| 64/70 [00:01<00:00, 67.58it/s]100%|██████████| 70/70 [00:01<00:00, 37.97it/s]
8925 images processed, 1.9255425930023193 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<01:00,  1.39s/it] 22%|██▏       | 10/45 [00:01<00:03,  8.98it/s] 38%|███▊      | 17/45 [00:01<00:02, 11.28it/s] 53%|█████▎    | 24/45 [00:02<00:01, 16.40it/s] 71%|███████   | 32/45 [00:02<00:00, 24.00it/s] 84%|████████▍ | 38/45 [00:02<00:00, 16.73it/s]100%|██████████| 45/45 [00:02<00:00, 15.36it/s]
5640 images processed, 2.9880175590515137 seconds used

22.085158824920654
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.87  98.76  81.70
places365     61.15  83.07  47.88
LSUN          45.27  89.33  68.46
iSUN          40.54  90.58  71.27
dtd           35.43  90.87  75.72
AVG           37.25  90.52  69.01
[incremental] Overall: 0.6973 New: 0.7000 Old: 0.6960
[incremental] Final(Top-1): 0.5083  Average: 0.6621
4.570682048797607
==== Stage 4: inc={64,22,42,9,90}; seen={0,8,11,40,51,66,67,88,94,57,59,58,44,93,10}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3', lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='64,22,42,9,90', forget_classes_seen='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:09<07:32,  9.24s/it]  4%|▍         | 2/50 [00:11<04:03,  5.08s/it]  6%|▌         | 3/50 [00:12<02:22,  3.04s/it]  8%|▊         | 4/50 [00:12<01:34,  2.05s/it] 10%|█         | 5/50 [00:13<01:08,  1.53s/it] 12%|█▏        | 6/50 [00:13<00:56,  1.27s/it] 14%|█▍        | 7/50 [00:14<00:44,  1.03s/it] 16%|█▌        | 8/50 [00:14<00:36,  1.15it/s] 18%|█▊        | 9/50 [00:15<00:31,  1.31it/s] 20%|██        | 10/50 [00:16<00:35,  1.13it/s] 22%|██▏       | 11/50 [00:18<00:40,  1.05s/it] 24%|██▍       | 12/50 [00:19<00:42,  1.13s/it] 26%|██▌       | 13/50 [00:20<00:42,  1.16s/it] 28%|██▊       | 14/50 [00:21<00:42,  1.18s/it] 30%|███       | 15/50 [00:22<00:34,  1.02it/s] 32%|███▏      | 16/50 [00:23<00:35,  1.03s/it] 34%|███▍      | 17/50 [00:24<00:31,  1.06it/s] 36%|███▌      | 18/50 [00:24<00:27,  1.19it/s] 38%|███▊      | 19/50 [00:26<00:30,  1.02it/s] 40%|████      | 20/50 [00:27<00:34,  1.14s/it] 42%|████▏     | 21/50 [00:28<00:32,  1.12s/it] 44%|████▍     | 22/50 [00:29<00:27,  1.04it/s] 46%|████▌     | 23/50 [00:29<00:23,  1.17it/s] 48%|████▊     | 24/50 [00:31<00:24,  1.07it/s] 50%|█████     | 25/50 [00:32<00:23,  1.06it/s] 52%|█████▏    | 26/50 [00:32<00:22,  1.08it/s] 54%|█████▍    | 27/50 [00:34<00:23,  1.02s/it] 56%|█████▌    | 28/50 [00:34<00:20,  1.05it/s] 58%|█████▊    | 29/50 [00:36<00:21,  1.04s/it] 60%|██████    | 30/50 [00:36<00:18,  1.06it/s][loss] ep 0 it 0 total=15.7636 mle=5.6741 pcon=10.0896 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 2 it 10 total=15.2931 mle=5.2130 pcon=10.0801 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 5 it 0 total=14.8621 mle=4.7948 pcon=10.0673 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 7 it 10 total=14.4834 mle=4.4317 pcon=10.0517 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 10 it 0 total=14.2375 mle=4.2039 pcon=10.0336 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 12 it 10 total=13.9737 mle=3.9601 pcon=10.0136 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 15 it 0 total=13.7853 mle=3.7930 pcon=9.9923 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 17 it 10 total=13.6487 mle=3.6787 pcon=9.9700 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 20 it 0 total=13.5186 mle=3.5716 pcon=9.9470 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 22 it 10 total=13.4588 mle=3.5354 pcon=9.9235 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 25 it 0 total=13.3979 mle=3.4982 pcon=9.8996 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 27 it 10 total=13.3265 mle=3.4508 pcon=9.8757 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 30 it 0 total=13.2966 mle=3.4447 pcon=9.8519 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
 62%|██████▏   | 31/50 [00:37<00:16,  1.14it/s] 64%|██████▍   | 32/50 [00:38<00:15,  1.17it/s] 66%|██████▌   | 33/50 [00:39<00:14,  1.14it/s] 68%|██████▊   | 34/50 [00:39<00:12,  1.27it/s] 70%|███████   | 35/50 [00:40<00:11,  1.35it/s] 72%|███████▏  | 36/50 [00:41<00:09,  1.42it/s] 74%|███████▍  | 37/50 [00:41<00:08,  1.45it/s] 76%|███████▌  | 38/50 [00:42<00:08,  1.45it/s] 78%|███████▊  | 39/50 [00:43<00:07,  1.51it/s] 80%|████████  | 40/50 [00:43<00:06,  1.53it/s] 82%|████████▏ | 41/50 [00:44<00:05,  1.55it/s] 84%|████████▍ | 42/50 [00:45<00:05,  1.54it/s] 86%|████████▌ | 43/50 [00:45<00:04,  1.45it/s] 88%|████████▊ | 44/50 [00:46<00:04,  1.50it/s] 90%|█████████ | 45/50 [00:47<00:03,  1.47it/s] 92%|█████████▏| 46/50 [00:47<00:02,  1.52it/s] 94%|█████████▍| 47/50 [00:48<00:01,  1.52it/s] 96%|█████████▌| 48/50 [00:49<00:01,  1.53it/s] 98%|█████████▊| 49/50 [00:49<00:00,  1.39it/s]100%|██████████| 50/50 [00:50<00:00,  1.51it/s]100%|██████████| 50/50 [00:50<00:00,  1.01s/it]
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 32 it 10 total=13.2584 mle=3.4301 pcon=9.8283 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 35 it 0 total=13.2226 mle=3.4174 pcon=9.8052 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 37 it 10 total=13.1705 mle=3.3880 pcon=9.7825 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 40 it 0 total=13.1617 mle=3.4012 pcon=9.7604 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 42 it 10 total=13.1391 mle=3.3999 pcon=9.7392 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 45 it 0 total=13.1079 mle=3.3894 pcon=9.7186 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 47 it 10 total=13.0880 mle=3.3891 pcon=9.6989 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stage4-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<03:30,  2.04it/s]  3%|▎         | 11/430 [00:00<00:17, 23.82it/s]  5%|▍         | 21/430 [00:00<00:09, 40.96it/s]  7%|▋         | 31/430 [00:00<00:07, 55.09it/s] 10%|▉         | 41/430 [00:00<00:05, 65.83it/s] 12%|█▏        | 51/430 [00:01<00:05, 73.97it/s] 14%|█▍        | 61/430 [00:01<00:04, 80.13it/s] 17%|█▋        | 71/430 [00:01<00:04, 84.56it/s] 19%|█▉        | 81/430 [00:01<00:04, 87.17it/s] 21%|██        | 91/430 [00:01<00:03, 89.64it/s] 23%|██▎       | 101/430 [00:01<00:03, 90.56it/s] 26%|██▌       | 111/430 [00:01<00:03, 91.95it/s] 28%|██▊       | 121/430 [00:01<00:03, 92.94it/s] 30%|███       | 131/430 [00:01<00:03, 93.96it/s] 33%|███▎      | 141/430 [00:01<00:03, 94.57it/s] 35%|███▌      | 151/430 [00:02<00:02, 95.09it/s] 37%|███▋      | 161/430 [00:02<00:02, 95.32it/s] 40%|███▉      | 171/430 [00:02<00:02, 95.23it/s] 42%|████▏     | 181/430 [00:02<00:02, 94.79it/s] 44%|████▍     | 191/430 [00:02<00:02, 95.20it/s] 47%|████▋     | 201/430 [00:02<00:02, 95.40it/s] 49%|████▉     | 211/430 [00:02<00:02, 95.66it/s] 51%|█████▏    | 221/430 [00:02<00:02, 94.63it/s] 54%|█████▎    | 231/430 [00:02<00:02, 94.59it/s] 56%|█████▌    | 241/430 [00:03<00:01, 95.36it/s] 58%|█████▊    | 251/430 [00:03<00:01, 92.25it/s] 61%|██████    | 261/430 [00:03<00:01, 93.37it/s] 63%|██████▎   | 271/430 [00:03<00:01, 94.02it/s] 65%|██████▌   | 281/430 [00:03<00:01, 94.45it/s] 68%|██████▊   | 291/430 [00:03<00:01, 95.01it/s] 70%|███████   | 301/430 [00:03<00:01, 94.57it/s] 72%|███████▏  | 311/430 [00:03<00:01, 94.45it/s] 75%|███████▍  | 321/430 [00:03<00:01, 94.97it/s] 77%|███████▋  | 331/430 [00:03<00:01, 95.27it/s] 79%|███████▉  | 341/430 [00:04<00:00, 95.36it/s] 82%|████████▏ | 351/430 [00:04<00:00, 94.86it/s] 84%|████████▍ | 361/430 [00:04<00:00, 95.17it/s] 86%|████████▋ | 371/430 [00:04<00:00, 95.43it/s] 89%|████████▊ | 381/430 [00:04<00:00, 95.05it/s] 91%|█████████ | 391/430 [00:04<00:00, 92.48it/s] 93%|█████████▎| 401/430 [00:04<00:00, 93.21it/s] 96%|█████████▌| 411/430 [00:04<00:00, 94.47it/s] 98%|█████████▊| 421/430 [00:04<00:00, 95.41it/s]100%|██████████| 430/430 [00:05<00:00, 85.44it/s]
55000 images processed, 5.108402252197266 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:54,  1.56it/s] 13%|█▎        | 11/86 [00:00<00:03, 19.24it/s] 24%|██▍       | 21/86 [00:00<00:01, 35.22it/s] 36%|███▌      | 31/86 [00:00<00:01, 49.03it/s] 48%|████▊     | 41/86 [00:01<00:00, 60.25it/s] 58%|█████▊    | 50/86 [00:01<00:00, 67.01it/s] 69%|██████▊   | 59/86 [00:01<00:00, 71.39it/s] 80%|████████  | 69/86 [00:01<00:00, 78.36it/s] 92%|█████████▏| 79/86 [00:01<00:00, 83.58it/s]100%|██████████| 86/86 [00:01<00:00, 54.92it/s]
11000 images processed, 1.5849742889404297 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:39,  2.04it/s]  5%|▌         | 11/204 [00:00<00:08, 23.62it/s] 10%|█         | 21/204 [00:00<00:04, 41.23it/s] 15%|█▌        | 31/204 [00:00<00:03, 54.93it/s] 20%|██        | 41/204 [00:00<00:02, 65.72it/s] 25%|██▌       | 51/204 [00:01<00:02, 73.58it/s] 30%|██▉       | 61/204 [00:01<00:01, 78.86it/s] 35%|███▍      | 71/204 [00:01<00:01, 83.41it/s] 40%|███▉      | 81/204 [00:01<00:01, 85.87it/s] 45%|████▍     | 91/204 [00:01<00:01, 88.30it/s] 50%|████▉     | 101/204 [00:01<00:01, 90.04it/s] 54%|█████▍    | 111/204 [00:01<00:01, 91.35it/s] 59%|█████▉    | 121/204 [00:01<00:00, 92.13it/s] 64%|██████▍   | 131/204 [00:01<00:00, 92.61it/s] 69%|██████▉   | 141/204 [00:02<00:00, 89.17it/s] 74%|███████▍  | 151/204 [00:02<00:00, 90.17it/s] 79%|███████▉  | 161/204 [00:02<00:00, 91.42it/s] 84%|████████▍ | 171/204 [00:02<00:00, 92.54it/s] 89%|████████▊ | 181/204 [00:02<00:00, 93.13it/s] 94%|█████████▎| 191/204 [00:02<00:00, 94.19it/s] 99%|█████████▊| 201/204 [00:02<00:00, 94.94it/s]100%|██████████| 204/204 [00:02<00:00, 76.39it/s]
26032 images processed, 2.7111318111419678 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:06,  1.17it/s] 14%|█▍        | 11/79 [00:00<00:04, 15.15it/s] 23%|██▎       | 18/79 [00:01<00:02, 24.36it/s] 34%|███▍      | 27/79 [00:01<00:01, 36.81it/s] 47%|████▋     | 37/79 [00:01<00:00, 49.81it/s] 59%|█████▉    | 47/79 [00:01<00:00, 60.08it/s] 72%|███████▏  | 57/79 [00:01<00:00, 66.03it/s] 85%|████████▍ | 67/79 [00:01<00:00, 73.36it/s] 97%|█████████▋| 77/79 [00:01<00:00, 78.60it/s]100%|██████████| 79/79 [00:01<00:00, 45.11it/s]
10000 images processed, 1.7917842864990234 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:14,  1.05it/s] 13%|█▎        | 10/79 [00:01<00:05, 12.53it/s] 24%|██▍       | 19/79 [00:01<00:02, 24.59it/s] 37%|███▋      | 29/79 [00:01<00:01, 37.95it/s] 49%|████▉     | 39/79 [00:01<00:00, 49.95it/s] 62%|██████▏   | 49/79 [00:01<00:00, 60.49it/s] 75%|███████▍  | 59/79 [00:01<00:00, 69.35it/s] 87%|████████▋ | 69/79 [00:01<00:00, 76.35it/s]100%|██████████| 79/79 [00:01<00:00, 44.31it/s]
10000 images processed, 1.805724859237671 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:48,  1.43it/s] 14%|█▍        | 10/70 [00:00<00:03, 16.49it/s] 27%|██▋       | 19/70 [00:00<00:01, 30.19it/s] 39%|███▊      | 27/70 [00:01<00:01, 40.52it/s] 53%|█████▎    | 37/70 [00:01<00:00, 53.56it/s] 67%|██████▋   | 47/70 [00:01<00:00, 64.31it/s] 81%|████████▏ | 57/70 [00:01<00:00, 72.80it/s] 96%|█████████▌| 67/70 [00:01<00:00, 79.34it/s]100%|██████████| 70/70 [00:01<00:00, 47.63it/s]
8925 images processed, 1.5041992664337158 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:57,  1.30s/it]  4%|▍         | 2/45 [00:01<00:25,  1.67it/s] 20%|██        | 9/45 [00:01<00:03, 10.00it/s] 38%|███▊      | 17/45 [00:01<00:01, 15.79it/s] 47%|████▋     | 21/45 [00:01<00:01, 17.64it/s] 53%|█████▎    | 24/45 [00:02<00:01, 17.49it/s] 64%|██████▍   | 29/45 [00:02<00:00, 22.73it/s] 73%|███████▎  | 33/45 [00:02<00:00, 20.60it/s] 89%|████████▉ | 40/45 [00:02<00:00, 20.51it/s]100%|██████████| 45/45 [00:02<00:00, 15.55it/s]
5640 images processed, 2.914985179901123 seconds used

19.056537866592407
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.14  98.72  84.70
places365     73.32  79.76  49.30
LSUN          48.39  88.79  71.90
iSUN          50.46  88.43  72.07
dtd           40.94  90.07  78.16
AVG           43.45  89.15  71.23
[incremental] Overall: 0.6205 New: 0.6200 Old: 0.6207
[incremental] Final(Top-1): 0.5083  Average: 0.6619
5.342894077301025
==== Stage 5: inc={100,101,102,103,104}; seen={0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4', lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='100,101,102,103,104', forget_classes_seen='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:11<08:59, 11.01s/it]  4%|▍         | 2/50 [00:11<04:05,  5.11s/it]  6%|▌         | 3/50 [00:13<02:33,  3.26s/it]  8%|▊         | 4/50 [00:14<01:48,  2.35s/it] 10%|█         | 5/50 [00:14<01:17,  1.73s/it] 12%|█▏        | 6/50 [00:15<00:58,  1.33s/it] 14%|█▍        | 7/50 [00:16<00:50,  1.17s/it] 16%|█▌        | 8/50 [00:16<00:41,  1.00it/s] 18%|█▊        | 9/50 [00:17<00:34,  1.18it/s] 20%|██        | 10/50 [00:17<00:30,  1.31it/s] 22%|██▏       | 11/50 [00:18<00:27,  1.42it/s] 24%|██▍       | 12/50 [00:18<00:25,  1.50it/s] 26%|██▌       | 13/50 [00:19<00:23,  1.57it/s] 28%|██▊       | 14/50 [00:19<00:21,  1.66it/s] 30%|███       | 15/50 [00:20<00:21,  1.64it/s] 32%|███▏      | 16/50 [00:21<00:22,  1.49it/s] 34%|███▍      | 17/50 [00:22<00:26,  1.22it/s] 36%|███▌      | 18/50 [00:23<00:24,  1.33it/s] 38%|███▊      | 19/50 [00:23<00:21,  1.43it/s] 40%|████      | 20/50 [00:25<00:26,  1.12it/s] 42%|████▏     | 21/50 [00:26<00:28,  1.02it/s] 44%|████▍     | 22/50 [00:27<00:30,  1.10s/it] 46%|████▌     | 23/50 [00:29<00:31,  1.17s/it] 48%|████▊     | 24/50 [00:30<00:29,  1.12s/it] 50%|█████     | 25/50 [00:30<00:26,  1.05s/it] 52%|█████▏    | 26/50 [00:31<00:25,  1.06s/it] 54%|█████▍    | 27/50 [00:32<00:21,  1.09it/s] 56%|█████▌    | 28/50 [00:33<00:19,  1.11it/s] 58%|█████▊    | 29/50 [00:34<00:19,  1.05it/s] 60%|██████    | 30/50 [00:35<00:19,  1.01it/s][loss] ep 0 it 0 total=15.9796 mle=5.5963 pcon=10.3833 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 2 it 10 total=15.5035 mle=5.1303 pcon=10.3732 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 5 it 0 total=15.0861 mle=4.7261 pcon=10.3600 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 7 it 10 total=14.7598 mle=4.4161 pcon=10.3436 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 10 it 0 total=14.4130 mle=4.0880 pcon=10.3249 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 12 it 10 total=14.2188 mle=3.9144 pcon=10.3044 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 15 it 0 total=14.0556 mle=3.7732 pcon=10.2824 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 17 it 10 total=13.9255 mle=3.6661 pcon=10.2594 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 20 it 0 total=13.8095 mle=3.5738 pcon=10.2357 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 22 it 10 total=13.7311 mle=3.5196 pcon=10.2115 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 25 it 0 total=13.6742 mle=3.4872 pcon=10.1870 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 27 it 10 total=13.6239 mle=3.4614 pcon=10.1625 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 30 it 0 total=13.5864 mle=3.4484 pcon=10.1380 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
 62%|██████▏   | 31/50 [00:36<00:17,  1.09it/s] 64%|██████▍   | 32/50 [00:36<00:14,  1.23it/s] 66%|██████▌   | 33/50 [00:37<00:12,  1.37it/s] 68%|██████▊   | 34/50 [00:38<00:12,  1.27it/s] 70%|███████   | 35/50 [00:39<00:11,  1.26it/s] 72%|███████▏  | 36/50 [00:39<00:10,  1.35it/s] 74%|███████▍  | 37/50 [00:40<00:11,  1.16it/s] 76%|███████▌  | 38/50 [00:41<00:09,  1.23it/s] 78%|███████▊  | 39/50 [00:42<00:10,  1.08it/s] 80%|████████  | 40/50 [00:43<00:09,  1.08it/s] 82%|████████▏ | 41/50 [00:44<00:08,  1.11it/s] 84%|████████▍ | 42/50 [00:45<00:07,  1.14it/s] 86%|████████▌ | 43/50 [00:46<00:06,  1.07it/s] 88%|████████▊ | 44/50 [00:47<00:05,  1.08it/s] 90%|█████████ | 45/50 [00:48<00:04,  1.08it/s] 92%|█████████▏| 46/50 [00:49<00:03,  1.16it/s] 94%|█████████▍| 47/50 [00:49<00:02,  1.23it/s] 96%|█████████▌| 48/50 [00:50<00:01,  1.20it/s] 98%|█████████▊| 49/50 [00:51<00:00,  1.10it/s]100%|██████████| 50/50 [00:52<00:00,  1.16it/s]100%|██████████| 50/50 [00:52<00:00,  1.05s/it]
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 32 it 10 total=13.5452 mle=3.4311 pcon=10.1140 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 35 it 0 total=13.5073 mle=3.4169 pcon=10.0905 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 37 it 10 total=13.4802 mle=3.4128 pcon=10.0674 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 40 it 0 total=13.4437 mle=3.3986 pcon=10.0451 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 42 it 10 total=13.4291 mle=3.4056 pcon=10.0235 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 45 it 0 total=13.4018 mle=3.3990 pcon=10.0028 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 47 it 10 total=13.3781 mle=3.3953 pcon=9.9828 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stage5-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<04:06,  1.74it/s]  2%|▏         | 9/430 [00:00<00:24, 17.09it/s]  4%|▍         | 18/430 [00:00<00:12, 32.70it/s]  6%|▌         | 26/430 [00:00<00:09, 43.68it/s]  8%|▊         | 33/430 [00:00<00:07, 49.83it/s]  9%|▉         | 40/430 [00:01<00:07, 54.97it/s] 11%|█         | 48/430 [00:01<00:06, 60.18it/s] 13%|█▎        | 56/430 [00:01<00:05, 64.41it/s] 15%|█▍        | 64/430 [00:01<00:05, 65.29it/s] 17%|█▋        | 72/430 [00:01<00:05, 65.44it/s] 19%|█▊        | 80/430 [00:01<00:05, 67.89it/s] 20%|██        | 88/430 [00:01<00:04, 70.41it/s] 23%|██▎       | 97/430 [00:01<00:04, 73.27it/s] 24%|██▍       | 105/430 [00:01<00:04, 71.53it/s] 26%|██▋       | 113/430 [00:02<00:04, 71.89it/s] 28%|██▊       | 121/430 [00:02<00:04, 72.63it/s] 30%|███       | 129/430 [00:02<00:04, 70.15it/s] 32%|███▏      | 137/430 [00:02<00:04, 69.22it/s] 34%|███▎      | 145/430 [00:02<00:04, 70.74it/s] 36%|███▌      | 154/430 [00:02<00:03, 73.79it/s] 38%|███▊      | 163/430 [00:02<00:03, 75.86it/s] 40%|███▉      | 171/430 [00:02<00:03, 76.50it/s] 42%|████▏     | 179/430 [00:02<00:03, 75.09it/s] 43%|████▎     | 187/430 [00:03<00:03, 76.10it/s] 46%|████▌     | 196/430 [00:03<00:03, 76.92it/s] 47%|████▋     | 204/430 [00:03<00:02, 75.94it/s] 49%|████▉     | 212/430 [00:03<00:02, 74.96it/s] 51%|█████     | 220/430 [00:03<00:02, 74.96it/s] 53%|█████▎    | 228/430 [00:03<00:02, 75.93it/s] 55%|█████▌    | 237/430 [00:03<00:02, 79.12it/s] 57%|█████▋    | 245/430 [00:03<00:02, 78.28it/s] 59%|█████▉    | 253/430 [00:03<00:02, 77.16it/s] 61%|██████    | 261/430 [00:04<00:02, 77.75it/s] 63%|██████▎   | 269/430 [00:04<00:02, 74.35it/s] 64%|██████▍   | 277/430 [00:04<00:02, 73.35it/s] 66%|██████▋   | 285/430 [00:04<00:01, 73.72it/s] 68%|██████▊   | 293/430 [00:04<00:01, 73.51it/s] 70%|███████   | 302/430 [00:04<00:01, 75.66it/s] 72%|███████▏  | 310/430 [00:04<00:01, 76.04it/s] 74%|███████▍  | 318/430 [00:04<00:01, 74.84it/s] 76%|███████▌  | 326/430 [00:04<00:01, 73.55it/s] 78%|███████▊  | 335/430 [00:05<00:01, 75.65it/s] 80%|███████▉  | 343/430 [00:05<00:01, 75.42it/s] 82%|████████▏ | 351/430 [00:05<00:01, 74.77it/s] 84%|████████▎ | 360/430 [00:05<00:00, 75.65it/s] 86%|████████▌ | 369/430 [00:05<00:00, 77.50it/s] 88%|████████▊ | 377/430 [00:05<00:00, 75.74it/s] 90%|████████▉ | 385/430 [00:05<00:00, 76.59it/s] 91%|█████████▏| 393/430 [00:05<00:00, 77.02it/s] 93%|█████████▎| 401/430 [00:05<00:00, 76.18it/s] 95%|█████████▌| 409/430 [00:06<00:00, 74.53it/s] 97%|█████████▋| 418/430 [00:06<00:00, 76.42it/s] 99%|█████████▉| 426/430 [00:06<00:00, 77.23it/s]100%|██████████| 430/430 [00:06<00:00, 68.19it/s]
55000 images processed, 6.3789002895355225 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:40,  2.07it/s]  8%|▊         | 7/86 [00:00<00:05, 15.21it/s] 15%|█▌        | 13/86 [00:00<00:02, 25.43it/s] 23%|██▎       | 20/86 [00:00<00:01, 36.10it/s] 33%|███▎      | 28/86 [00:00<00:01, 47.30it/s] 42%|████▏     | 36/86 [00:01<00:00, 55.01it/s] 51%|█████     | 44/86 [00:01<00:00, 60.44it/s] 59%|█████▉    | 51/86 [00:01<00:00, 62.42it/s] 70%|██████▉   | 60/86 [00:01<00:00, 68.18it/s] 80%|████████  | 69/86 [00:01<00:00, 72.73it/s] 91%|█████████ | 78/86 [00:01<00:00, 75.12it/s]100%|██████████| 86/86 [00:01<00:00, 73.87it/s]100%|██████████| 86/86 [00:01<00:00, 51.49it/s]
11000 images processed, 1.6893742084503174 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:36,  1.30it/s]  4%|▍         | 8/204 [00:00<00:16, 12.06it/s]  7%|▋         | 15/204 [00:00<00:08, 22.02it/s] 11%|█▏        | 23/204 [00:01<00:05, 33.04it/s] 15%|█▌        | 31/204 [00:01<00:04, 41.28it/s] 19%|█▊        | 38/204 [00:01<00:03, 47.15it/s] 22%|██▏       | 45/204 [00:01<00:03, 52.43it/s] 25%|██▌       | 52/204 [00:01<00:02, 55.89it/s] 29%|██▉       | 60/204 [00:01<00:02, 61.20it/s] 33%|███▎      | 67/204 [00:01<00:02, 63.32it/s] 37%|███▋      | 75/204 [00:01<00:01, 66.68it/s] 42%|████▏     | 85/204 [00:01<00:01, 74.23it/s] 47%|████▋     | 95/204 [00:02<00:01, 79.98it/s] 51%|█████▏    | 105/204 [00:02<00:01, 83.87it/s] 56%|█████▋    | 115/204 [00:02<00:01, 86.78it/s] 61%|██████▏   | 125/204 [00:02<00:00, 88.85it/s] 66%|██████▌   | 135/204 [00:02<00:00, 90.29it/s] 71%|███████   | 145/204 [00:02<00:00, 91.56it/s] 76%|███████▌  | 155/204 [00:02<00:00, 92.66it/s] 81%|████████  | 165/204 [00:02<00:00, 92.23it/s] 86%|████████▌ | 175/204 [00:02<00:00, 92.80it/s] 91%|█████████ | 185/204 [00:03<00:00, 93.73it/s] 96%|█████████▌| 195/204 [00:03<00:00, 94.41it/s]100%|██████████| 204/204 [00:03<00:00, 63.18it/s]
26032 images processed, 3.2747139930725098 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:03,  1.23it/s] 14%|█▍        | 11/79 [00:00<00:04, 15.86it/s] 27%|██▋       | 21/79 [00:01<00:01, 30.10it/s] 38%|███▊      | 30/79 [00:01<00:01, 41.91it/s] 51%|█████     | 40/79 [00:01<00:00, 53.82it/s] 63%|██████▎   | 50/79 [00:01<00:00, 63.37it/s] 76%|███████▌  | 60/79 [00:01<00:00, 71.26it/s] 89%|████████▊ | 70/79 [00:01<00:00, 77.80it/s]100%|██████████| 79/79 [00:01<00:00, 41.45it/s]
10000 images processed, 1.9423987865447998 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:58,  1.33it/s] 14%|█▍        | 11/79 [00:00<00:04, 16.95it/s] 27%|██▋       | 21/79 [00:00<00:01, 31.89it/s] 39%|███▉      | 31/79 [00:01<00:01, 45.42it/s] 52%|█████▏    | 41/79 [00:01<00:00, 56.95it/s] 65%|██████▍   | 51/79 [00:01<00:00, 66.48it/s] 77%|███████▋  | 61/79 [00:01<00:00, 74.28it/s] 90%|████████▉ | 71/79 [00:01<00:00, 80.34it/s]100%|██████████| 79/79 [00:01<00:00, 50.51it/s]
10000 images processed, 1.5831012725830078 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:38,  1.81it/s] 11%|█▏        | 8/70 [00:00<00:03, 15.66it/s] 26%|██▌       | 18/70 [00:00<00:01, 34.18it/s] 40%|████      | 28/70 [00:00<00:00, 49.27it/s] 54%|█████▍    | 38/70 [00:00<00:00, 61.29it/s] 69%|██████▊   | 48/70 [00:01<00:00, 70.59it/s] 83%|████████▎ | 58/70 [00:01<00:00, 77.80it/s] 97%|█████████▋| 68/70 [00:01<00:00, 82.98it/s]100%|██████████| 70/70 [00:01<00:00, 53.21it/s]
8925 images processed, 1.349660873413086 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:53,  1.22s/it]  4%|▍         | 2/45 [00:01<00:25,  1.71it/s] 27%|██▋       | 12/45 [00:01<00:02, 13.89it/s] 38%|███▊      | 17/45 [00:01<00:01, 16.05it/s] 47%|████▋     | 21/45 [00:01<00:01, 18.46it/s] 56%|█████▌    | 25/45 [00:02<00:01, 19.32it/s] 73%|███████▎  | 33/45 [00:02<00:00, 22.37it/s] 80%|████████  | 36/45 [00:02<00:00, 23.46it/s] 98%|█████████▊| 44/45 [00:02<00:00, 29.79it/s]100%|██████████| 45/45 [00:02<00:00, 17.18it/s]
5640 images processed, 2.6428110599517822 seconds used

20.736842155456543
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.28  98.72  87.13
places365     77.94  79.12  53.49
LSUN          49.35  89.16  75.82
iSUN          53.24  88.35  75.44
dtd           41.76  90.30  81.75
AVG           45.31  89.13  74.73
[incremental] Overall: 0.5556 New: 0.4620 Old: 0.5790
[incremental] Final(Top-1): 0.5083  Average: 0.6616
4.695640802383423
==== Stage 6: inc={105,106,107,108,109}; seen={0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90,100,101,102,103,104}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5', lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='105,106,107,108,109', forget_classes_seen='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90,100,101,102,103,104', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:08<06:36,  8.09s/it]  4%|▍         | 2/50 [00:08<02:58,  3.72s/it]  6%|▌         | 3/50 [00:09<01:46,  2.26s/it]  8%|▊         | 4/50 [00:09<01:13,  1.60s/it] 10%|█         | 5/50 [00:10<00:55,  1.23s/it] 12%|█▏        | 6/50 [00:10<00:43,  1.01it/s] 14%|█▍        | 7/50 [00:11<00:36,  1.19it/s] 16%|█▌        | 8/50 [00:12<00:31,  1.33it/s] 18%|█▊        | 9/50 [00:12<00:27,  1.47it/s] 20%|██        | 10/50 [00:13<00:26,  1.53it/s] 22%|██▏       | 11/50 [00:13<00:24,  1.61it/s] 24%|██▍       | 12/50 [00:14<00:22,  1.66it/s] 26%|██▌       | 13/50 [00:14<00:21,  1.74it/s] 28%|██▊       | 14/50 [00:15<00:20,  1.78it/s] 30%|███       | 15/50 [00:15<00:20,  1.74it/s] 32%|███▏      | 16/50 [00:16<00:20,  1.66it/s] 34%|███▍      | 17/50 [00:17<00:19,  1.67it/s] 36%|███▌      | 18/50 [00:17<00:19,  1.67it/s] 38%|███▊      | 19/50 [00:18<00:18,  1.67it/s] 40%|████      | 20/50 [00:19<00:18,  1.59it/s] 42%|████▏     | 21/50 [00:19<00:18,  1.58it/s] 44%|████▍     | 22/50 [00:20<00:17,  1.58it/s] 46%|████▌     | 23/50 [00:21<00:17,  1.51it/s] 48%|████▊     | 24/50 [00:21<00:17,  1.51it/s] 50%|█████     | 25/50 [00:22<00:16,  1.48it/s] 52%|█████▏    | 26/50 [00:23<00:16,  1.50it/s] 54%|█████▍    | 27/50 [00:23<00:15,  1.50it/s] 56%|█████▌    | 28/50 [00:24<00:14,  1.54it/s] 58%|█████▊    | 29/50 [00:25<00:13,  1.54it/s] 60%|██████    | 30/50 [00:25<00:12,  1.54it/s][loss] ep 0 it 0 total=16.1350 mle=5.5311 pcon=10.6039 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 2 it 10 total=15.6688 mle=5.0750 pcon=10.5938 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 5 it 0 total=15.2480 mle=4.6674 pcon=10.5806 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 7 it 10 total=14.9078 mle=4.3434 pcon=10.5644 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 10 it 0 total=14.6109 mle=4.0651 pcon=10.5459 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 12 it 10 total=14.4040 mle=3.8784 pcon=10.5256 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 15 it 0 total=14.2278 mle=3.7240 pcon=10.5038 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 17 it 10 total=14.1076 mle=3.6267 pcon=10.4810 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 20 it 0 total=14.0123 mle=3.5549 pcon=10.4574 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 22 it 10 total=13.9451 mle=3.5118 pcon=10.4334 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 25 it 0 total=13.8785 mle=3.4694 pcon=10.4091 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 27 it 10 total=13.8337 mle=3.4490 pcon=10.3847 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 30 it 0 total=13.7941 mle=3.4337 pcon=10.3604 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
 62%|██████▏   | 31/50 [00:26<00:12,  1.58it/s] 64%|██████▍   | 32/50 [00:26<00:11,  1.53it/s] 66%|██████▌   | 33/50 [00:27<00:10,  1.55it/s] 68%|██████▊   | 34/50 [00:28<00:10,  1.55it/s] 70%|███████   | 35/50 [00:28<00:09,  1.55it/s] 72%|███████▏  | 36/50 [00:29<00:08,  1.59it/s] 74%|███████▍  | 37/50 [00:30<00:08,  1.60it/s] 76%|███████▌  | 38/50 [00:30<00:07,  1.54it/s] 78%|███████▊  | 39/50 [00:31<00:07,  1.53it/s] 80%|████████  | 40/50 [00:32<00:06,  1.52it/s] 82%|████████▏ | 41/50 [00:32<00:05,  1.54it/s] 84%|████████▍ | 42/50 [00:33<00:04,  1.65it/s] 86%|████████▌ | 43/50 [00:33<00:04,  1.73it/s] 88%|████████▊ | 44/50 [00:34<00:03,  1.80it/s] 90%|█████████ | 45/50 [00:34<00:02,  1.74it/s] 92%|█████████▏| 46/50 [00:35<00:02,  1.67it/s] 94%|█████████▍| 47/50 [00:36<00:01,  1.77it/s] 96%|█████████▌| 48/50 [00:36<00:01,  1.80it/s] 98%|█████████▊| 49/50 [00:37<00:00,  1.72it/s]100%|██████████| 50/50 [00:37<00:00,  1.81it/s]100%|██████████| 50/50 [00:37<00:00,  1.33it/s]
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 32 it 10 total=13.7604 mle=3.4239 pcon=10.3364 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 35 it 0 total=13.7151 mle=3.4022 pcon=10.3129 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 37 it 10 total=13.6904 mle=3.4007 pcon=10.2897 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 40 it 0 total=13.6673 mle=3.3999 pcon=10.2674 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 42 it 10 total=13.6388 mle=3.3930 pcon=10.2458 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 45 it 0 total=13.6113 mle=3.3864 pcon=10.2249 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 47 it 10 total=13.5884 mle=3.3835 pcon=10.2049 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stage6-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<03:27,  2.06it/s]  3%|▎         | 11/430 [00:00<00:17, 24.15it/s]  5%|▍         | 21/430 [00:00<00:09, 42.26it/s]  7%|▋         | 31/430 [00:00<00:07, 55.84it/s] 10%|▉         | 41/430 [00:00<00:05, 66.86it/s] 12%|█▏        | 51/430 [00:01<00:05, 74.74it/s] 14%|█▍        | 61/430 [00:01<00:04, 80.61it/s] 17%|█▋        | 71/430 [00:01<00:04, 84.82it/s] 19%|█▉        | 81/430 [00:01<00:03, 87.40it/s] 21%|██        | 91/430 [00:01<00:03, 89.90it/s] 23%|██▎       | 101/430 [00:01<00:03, 91.30it/s] 26%|██▌       | 111/430 [00:01<00:03, 91.78it/s] 28%|██▊       | 121/430 [00:01<00:03, 86.87it/s] 30%|███       | 130/430 [00:01<00:03, 85.07it/s] 33%|███▎      | 140/430 [00:01<00:03, 88.32it/s] 35%|███▍      | 149/430 [00:02<00:03, 86.57it/s] 37%|███▋      | 159/430 [00:02<00:03, 88.69it/s] 39%|███▉      | 169/430 [00:02<00:02, 90.57it/s] 42%|████▏     | 179/430 [00:02<00:02, 84.27it/s] 44%|████▍     | 189/430 [00:02<00:02, 87.11it/s] 46%|████▋     | 199/430 [00:02<00:02, 88.51it/s] 49%|████▊     | 209/430 [00:02<00:02, 90.71it/s] 51%|█████     | 219/430 [00:02<00:02, 92.47it/s] 53%|█████▎    | 229/430 [00:02<00:02, 93.52it/s] 56%|█████▌    | 239/430 [00:03<00:02, 94.41it/s] 58%|█████▊    | 249/430 [00:03<00:01, 94.91it/s] 60%|██████    | 259/430 [00:03<00:01, 95.35it/s] 63%|██████▎   | 269/430 [00:03<00:01, 94.80it/s] 65%|██████▍   | 279/430 [00:03<00:01, 93.63it/s] 67%|██████▋   | 289/430 [00:03<00:01, 91.88it/s] 70%|██████▉   | 299/430 [00:03<00:01, 92.16it/s] 72%|███████▏  | 309/430 [00:03<00:01, 92.33it/s] 74%|███████▍  | 319/430 [00:03<00:01, 92.29it/s] 77%|███████▋  | 329/430 [00:04<00:01, 93.01it/s] 79%|███████▉  | 339/430 [00:04<00:00, 92.86it/s] 81%|████████  | 349/430 [00:04<00:00, 93.31it/s] 83%|████████▎ | 359/430 [00:04<00:00, 93.48it/s] 86%|████████▌ | 369/430 [00:04<00:00, 93.41it/s] 88%|████████▊ | 379/430 [00:04<00:00, 93.91it/s] 90%|█████████ | 389/430 [00:04<00:00, 94.11it/s] 93%|█████████▎| 399/430 [00:04<00:00, 93.58it/s] 95%|█████████▌| 409/430 [00:04<00:00, 94.01it/s] 97%|█████████▋| 419/430 [00:04<00:00, 95.05it/s]100%|█████████▉| 429/430 [00:05<00:00, 95.68it/s]100%|██████████| 430/430 [00:05<00:00, 83.95it/s]
55000 images processed, 5.204737186431885 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:51,  1.66it/s] 12%|█▏        | 10/86 [00:00<00:04, 18.49it/s] 23%|██▎       | 20/86 [00:00<00:01, 35.24it/s] 34%|███▎      | 29/86 [00:00<00:01, 47.87it/s] 45%|████▌     | 39/86 [00:01<00:00, 59.53it/s] 57%|█████▋    | 49/86 [00:01<00:00, 68.41it/s] 69%|██████▊   | 59/86 [00:01<00:00, 75.95it/s] 80%|████████  | 69/86 [00:01<00:00, 81.95it/s] 92%|█████████▏| 79/86 [00:01<00:00, 86.34it/s]100%|██████████| 86/86 [00:01<00:00, 56.65it/s]
11000 images processed, 1.562014102935791 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:38,  1.28it/s]  3%|▎         | 7/204 [00:00<00:19, 10.30it/s]  7%|▋         | 15/204 [00:00<00:08, 22.82it/s] 10%|█         | 21/204 [00:01<00:06, 29.64it/s] 14%|█▎        | 28/204 [00:01<00:04, 38.41it/s] 17%|█▋        | 35/204 [00:01<00:03, 45.76it/s] 22%|██▏       | 45/204 [00:01<00:02, 58.29it/s] 26%|██▌       | 53/204 [00:01<00:02, 62.95it/s] 31%|███       | 63/204 [00:01<00:01, 71.27it/s] 36%|███▌      | 73/204 [00:01<00:01, 77.41it/s] 40%|████      | 82/204 [00:01<00:01, 72.69it/s] 44%|████▍     | 90/204 [00:01<00:01, 72.80it/s] 48%|████▊     | 98/204 [00:02<00:01, 70.84it/s] 52%|█████▏    | 107/204 [00:02<00:01, 74.68it/s] 57%|█████▋    | 117/204 [00:02<00:01, 79.80it/s] 62%|██████▏   | 127/204 [00:02<00:00, 83.63it/s] 67%|██████▋   | 137/204 [00:02<00:00, 85.41it/s] 72%|███████▏  | 147/204 [00:02<00:00, 87.48it/s] 77%|███████▋  | 157/204 [00:02<00:00, 89.35it/s] 82%|████████▏ | 167/204 [00:02<00:00, 91.08it/s] 87%|████████▋ | 177/204 [00:02<00:00, 92.32it/s] 92%|█████████▏| 187/204 [00:03<00:00, 93.67it/s] 97%|█████████▋| 197/204 [00:03<00:00, 94.35it/s]100%|██████████| 204/204 [00:03<00:00, 62.84it/s]
26032 images processed, 3.301569700241089 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:51,  1.52it/s]  5%|▌         | 4/79 [00:00<00:11,  6.52it/s] 15%|█▌        | 12/79 [00:00<00:03, 21.14it/s] 27%|██▋       | 21/79 [00:00<00:01, 36.02it/s] 39%|███▉      | 31/79 [00:01<00:00, 50.46it/s] 52%|█████▏    | 41/79 [00:01<00:00, 61.82it/s] 65%|██████▍   | 51/79 [00:01<00:00, 70.21it/s] 77%|███████▋  | 61/79 [00:01<00:00, 76.98it/s] 89%|████████▊ | 70/79 [00:01<00:00, 77.83it/s]100%|██████████| 79/79 [00:01<00:00, 75.42it/s]100%|██████████| 79/79 [00:01<00:00, 48.11it/s]
10000 images processed, 1.678356409072876 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:45,  1.73it/s] 13%|█▎        | 10/79 [00:00<00:03, 18.86it/s] 23%|██▎       | 18/79 [00:00<00:01, 31.95it/s] 32%|███▏      | 25/79 [00:00<00:01, 40.69it/s] 41%|████      | 32/79 [00:01<00:01, 46.17it/s] 49%|████▉     | 39/79 [00:01<00:00, 51.06it/s] 59%|█████▉    | 47/79 [00:01<00:00, 58.40it/s] 70%|██████▉   | 55/79 [00:01<00:00, 63.68it/s] 80%|███████▉  | 63/79 [00:01<00:00, 66.99it/s] 90%|████████▉ | 71/79 [00:01<00:00, 66.95it/s]100%|██████████| 79/79 [00:01<00:00, 48.10it/s]
10000 images processed, 1.6621065139770508 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:53,  1.29it/s] 13%|█▎        | 9/70 [00:00<00:04, 13.17it/s] 24%|██▍       | 17/70 [00:01<00:02, 24.20it/s] 33%|███▎      | 23/70 [00:01<00:01, 29.47it/s] 41%|████▏     | 29/70 [00:01<00:01, 35.79it/s] 50%|█████     | 35/70 [00:01<00:00, 39.21it/s] 60%|██████    | 42/70 [00:01<00:00, 46.40it/s] 70%|███████   | 49/70 [00:01<00:00, 52.09it/s] 80%|████████  | 56/70 [00:01<00:00, 54.22it/s] 90%|█████████ | 63/70 [00:01<00:00, 52.73it/s]100%|██████████| 70/70 [00:01<00:00, 36.38it/s]
8925 images processed, 1.9518470764160156 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:46,  1.06s/it]  4%|▍         | 2/45 [00:01<00:21,  1.99it/s] 20%|██        | 9/45 [00:01<00:03, 11.74it/s] 38%|███▊      | 17/45 [00:01<00:01, 16.87it/s] 47%|████▋     | 21/45 [00:01<00:01, 19.25it/s] 56%|█████▌    | 25/45 [00:01<00:01, 18.30it/s] 71%|███████   | 32/45 [00:02<00:00, 26.39it/s] 80%|████████  | 36/45 [00:02<00:00, 20.09it/s] 91%|█████████ | 41/45 [00:02<00:00, 24.71it/s]100%|██████████| 45/45 [00:02<00:00, 17.47it/s]
5640 images processed, 2.6014304161071777 seconds used

19.530810594558716
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.17  98.80  89.51
places365     79.69  78.52  56.58
LSUN          47.63  89.90  79.41
iSUN          53.96  88.23  77.81
dtd           41.15  90.81  84.71
AVG           45.32  89.25  77.60
[incremental] Overall: 0.5070 New: 0.4860 Old: 0.5112
[incremental] Final(Top-1): 0.5070  Average: 0.6614
4.671527624130249
[done] continual incremental run finished. Adapters at: checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-continual-from-c80-to-CIFAR-110-pcon_split-stack
