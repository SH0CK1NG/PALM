nohup: ignoring input
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=25, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/25 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:168: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  4%|▍         | 1/25 [00:14<05:50, 14.61s/it]  8%|▊         | 2/25 [00:24<04:28, 11.67s/it] 12%|█▏        | 3/25 [00:32<03:47, 10.33s/it] 16%|█▌        | 4/25 [00:41<03:23,  9.70s/it] 20%|██        | 5/25 [00:51<03:16,  9.84s/it] 24%|██▍       | 6/25 [01:05<03:29, 11.03s/it][loss] ep 0 it 0 total=9.3574 mle=1.5917 pcon=5.2950 forget=2.4707 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.3798 mle=1.6528 pcon=5.2879 forget=2.4391 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.5058 mle=1.7263 pcon=5.2810 forget=2.4984 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.3454 mle=1.6582 pcon=5.2747 forget=2.4125 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.5192 mle=1.8000 pcon=5.2680 forget=2.4512 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.4207 mle=1.6840 pcon=5.2618 forget=2.4749 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.4121 mle=1.6676 pcon=5.2558 forget=2.4887 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.5270 mle=1.7989 pcon=5.2502 forget=2.4779 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 1 it 10 total=9.4999 mle=1.7853 pcon=5.2440 forget=2.4705 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.1627 mle=1.4598 pcon=5.2384 forget=2.4645 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=9.3038 mle=1.5882 pcon=5.2330 forget=2.4827 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=9.3528 mle=1.6098 pcon=5.2274 forget=2.5155 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.2354 mle=1.5484 pcon=5.2220 forget=2.4650 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=9.2164 mle=1.5658 pcon=5.2167 forget=2.4339 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=9.3368 mle=1.6466 pcon=5.2114 forget=2.4788 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.3146 mle=1.6363 pcon=5.2061 forget=2.4722 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 2 it 20 total=9.3265 mle=1.6321 pcon=5.2011 forget=2.4933 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.3507 mle=1.6428 pcon=5.1960 forget=2.5119 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=9.5166 mle=1.8840 pcon=5.1914 forget=2.4412 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=9.2544 mle=1.6095 pcon=5.1867 forget=2.4582 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=9.5505 mle=1.8828 pcon=5.1821 forget=2.4856 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.4457 mle=1.7690 pcon=5.1778 forget=2.4989 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=9.3918 mle=1.7388 pcon=5.1735 forget=2.4795 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.2570 mle=1.6018 pcon=5.1691 forget=2.4861 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 3 it 30 total=9.2566 mle=1.5919 pcon=5.1649 forget=2.4998 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=9.3548 mle=1.7090 pcon=5.1608 forget=2.4850 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=9.3117 mle=1.6768 pcon=5.1566 forget=2.4783 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.1727 mle=1.5348 pcon=5.1526 forget=2.4853 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=9.2716 mle=1.6278 pcon=5.1486 forget=2.4952 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.2096 mle=1.6058 pcon=5.1451 forget=2.4588 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.2905 mle=1.6825 pcon=5.1414 forget=2.4666 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=9.4948 mle=1.8874 pcon=5.1379 forget=2.4696 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 4 it 40 total=9.2588 mle=1.6425 pcon=5.1345 forget=2.4818 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=9.1767 mle=1.5945 pcon=5.1310 forget=2.4512 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.2393 mle=1.6443 pcon=5.1277 forget=2.4673 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.3714 mle=1.7785 pcon=5.1244 forget=2.4685 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.3820 mle=1.8137 pcon=5.1214 forget=2.4469 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.2591 mle=1.6604 pcon=5.1183 forget=2.4803 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.1175 mle=1.5092 pcon=5.1152 forget=2.4930 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 5 it 0 total=9.2015 mle=1.6144 pcon=5.1124 forget=2.4747 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=9.1557 mle=1.5756 pcon=5.1094 forget=2.4708 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.3247 mle=1.7750 pcon=5.1061 forget=2.4435 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.1341 mle=1.5635 pcon=5.1037 forget=2.4669 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.1002 mle=1.5021 pcon=5.1012 forget=2.4969 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=9.3438 mle=1.7580 pcon=5.0981 forget=2.4876 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=9.1066 mle=1.5418 pcon=5.0960 forget=2.4687 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=9.1272 mle=1.5427 pcon=5.0937 forget=2.4908 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 6 it 10 total=9.1892 mle=1.5935 pcon=5.0912 forget=2.5045 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=9.3657 mle=1.8112 pcon=5.0888 forget=2.4657 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=9.0027 mle=1.4283 pcon=5.0862 forget=2.4882 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=9.1001 mle=1.5361 pcon=5.0835 forget=2.4805 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=9.2953 mle=1.7548 pcon=5.0813 forget=2.4592 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=9.3137 mle=1.7465 pcon=5.0794 forget=2.4878 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=9.1975 mle=1.6477 pcon=5.0770 forget=2.4728 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 28%|██▊       | 7/25 [01:18<03:30, 11.67s/it] 32%|███▏      | 8/25 [01:31<03:28, 12.24s/it] 36%|███▌      | 9/25 [01:45<03:24, 12.76s/it] 40%|████      | 10/25 [01:58<03:12, 12.86s/it] 44%|████▍     | 11/25 [02:11<03:02, 13.02s/it] 48%|████▊     | 12/25 [02:25<02:51, 13.17s/it] 52%|█████▏    | 13/25 [02:38<02:37, 13.13s/it][loss] ep 6 it 360 total=9.0601 mle=1.5423 pcon=5.0749 forget=2.4429 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 7 it 20 total=9.0289 mle=1.4917 pcon=5.0729 forget=2.4643 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=9.2123 mle=1.6394 pcon=5.0708 forget=2.5021 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=9.1122 mle=1.5381 pcon=5.0687 forget=2.5053 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=9.0146 mle=1.5276 pcon=5.0664 forget=2.4206 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=9.0615 mle=1.5346 pcon=5.0645 forget=2.4624 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=9.2998 mle=1.7632 pcon=5.0628 forget=2.4738 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=9.1276 mle=1.5671 pcon=5.0609 forget=2.4996 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=9.0574 mle=1.5159 pcon=5.0589 forget=2.4825 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 8 it 30 total=9.1022 mle=1.5835 pcon=5.0570 forget=2.4617 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=9.3218 mle=1.8167 pcon=5.0551 forget=2.4500 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=9.2920 mle=1.7700 pcon=5.0532 forget=2.4688 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=9.1556 mle=1.6152 pcon=5.0515 forget=2.4890 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=9.1353 mle=1.6017 pcon=5.0490 forget=2.4845 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=9.1225 mle=1.5977 pcon=5.0476 forget=2.4772 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.2893 mle=1.8016 pcon=5.0458 forget=2.4419 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=9.0774 mle=1.6051 pcon=5.0442 forget=2.4281 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 9 it 40 total=9.2131 mle=1.6981 pcon=5.0427 forget=2.4724 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=9.0736 mle=1.5712 pcon=5.0409 forget=2.4614 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=9.0163 mle=1.5213 pcon=5.0394 forget=2.4556 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=9.1062 mle=1.5914 pcon=5.0379 forget=2.4769 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=9.2036 mle=1.7325 pcon=5.0362 forget=2.4349 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=9.1065 mle=1.5973 pcon=5.0347 forget=2.4745 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.9403 mle=1.4601 pcon=5.0338 forget=2.4464 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 10 it 0 total=9.3882 mle=1.8847 pcon=5.0324 forget=2.4711 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=9.3372 mle=1.8145 pcon=5.0313 forget=2.4914 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=9.0679 mle=1.5723 pcon=5.0299 forget=2.4656 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=9.1385 mle=1.6254 pcon=5.0285 forget=2.4846 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=9.3412 mle=1.8229 pcon=5.0271 forget=2.4912 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=9.2589 mle=1.7821 pcon=5.0258 forget=2.4510 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=9.1224 mle=1.6386 pcon=5.0244 forget=2.4593 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.9539 mle=1.4628 pcon=5.0233 forget=2.4678 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 11 it 10 total=8.9743 mle=1.4401 pcon=5.0225 forget=2.5117 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.8736 mle=1.3763 pcon=5.0214 forget=2.4759 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.9685 mle=1.5111 pcon=5.0202 forget=2.4372 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.8871 mle=1.3877 pcon=5.0190 forget=2.4804 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=9.0681 mle=1.5972 pcon=5.0177 forget=2.4532 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=9.1874 mle=1.7231 pcon=5.0168 forget=2.4475 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=9.1143 mle=1.6530 pcon=5.0156 forget=2.4457 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=9.1995 mle=1.6993 pcon=5.0146 forget=2.4856 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 12 it 20 total=9.1502 mle=1.6786 pcon=5.0133 forget=2.4582 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=9.0956 mle=1.6153 pcon=5.0121 forget=2.4682 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.9723 mle=1.5504 pcon=5.0108 forget=2.4111 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=9.2551 mle=1.8007 pcon=5.0095 forget=2.4449 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=9.0469 mle=1.6440 pcon=5.0082 forget=2.3947 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.8541 mle=1.5123 pcon=5.0071 forget=2.3347 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=9.0483 mle=1.7821 pcon=5.0058 forget=2.2603 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.9871 mle=1.7196 pcon=5.0045 forget=2.2629 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 13 it 30 total=8.6544 mle=1.5825 pcon=5.0034 forget=2.0685 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.9098 mle=1.9088 pcon=5.0022 forget=1.9988 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.4869 mle=1.6001 pcon=5.0014 forget=1.8854 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=8.4278 mle=1.6791 pcon=5.0007 forget=1.7481 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 56%|█████▌    | 14/25 [02:52<02:27, 13.42s/it] 60%|██████    | 15/25 [03:06<02:16, 13.67s/it] 64%|██████▍   | 16/25 [03:21<02:04, 13.89s/it] 68%|██████▊   | 17/25 [03:35<01:52, 14.01s/it] 72%|███████▏  | 18/25 [03:48<01:36, 13.75s/it] 76%|███████▌  | 19/25 [04:01<01:20, 13.46s/it] 80%|████████  | 20/25 [04:16<01:09, 13.81s/it] 84%|████████▍ | 21/25 [04:27<00:52, 13.07s/it][loss] ep 13 it 230 total=8.4027 mle=1.6571 pcon=5.0001 forget=1.7454 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.3054 mle=1.6003 pcon=5.0001 forget=1.7050 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.2290 mle=1.5455 pcon=5.0003 forget=1.6831 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.2497 mle=1.5856 pcon=5.0006 forget=1.6636 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 14 it 40 total=8.5502 mle=1.8616 pcon=5.0009 forget=1.6876 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.4031 mle=1.7086 pcon=5.0017 forget=1.6927 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.4362 mle=1.7185 pcon=5.0023 forget=1.7155 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.3099 mle=1.6027 pcon=5.0032 forget=1.7040 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.4532 mle=1.7206 pcon=5.0041 forget=1.7285 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.2767 mle=1.5468 pcon=5.0049 forget=1.7250 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.5537 mle=1.8015 pcon=5.0060 forget=1.7462 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 15 it 0 total=8.3197 mle=1.5610 pcon=5.0069 forget=1.7517 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.3312 mle=1.5574 pcon=5.0082 forget=1.7656 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.4984 mle=1.7109 pcon=5.0092 forget=1.7783 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.3558 mle=1.5541 pcon=5.0104 forget=1.7913 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.4841 mle=1.6766 pcon=5.0119 forget=1.7956 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.3332 mle=1.5113 pcon=5.0130 forget=1.8090 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.3158 mle=1.4941 pcon=5.0139 forget=1.8079 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.6559 mle=1.8306 pcon=5.0149 forget=1.8104 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 10 total=8.4106 mle=1.5827 pcon=5.0160 forget=1.8118 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.5331 mle=1.6894 pcon=5.0168 forget=1.8268 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.3373 mle=1.4892 pcon=5.0175 forget=1.8306 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.3379 mle=1.4892 pcon=5.0183 forget=1.8304 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.4305 mle=1.5767 pcon=5.0185 forget=1.8353 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.4926 mle=1.6207 pcon=5.0190 forget=1.8530 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.4855 mle=1.6275 pcon=5.0193 forget=1.8387 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.3905 mle=1.5172 pcon=5.0197 forget=1.8537 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.4412 mle=1.5690 pcon=5.0198 forget=1.8524 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.5139 mle=1.6495 pcon=5.0198 forget=1.8445 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.4632 mle=1.5936 pcon=5.0198 forget=1.8497 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.5282 mle=1.6460 pcon=5.0198 forget=1.8625 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.3987 mle=1.5321 pcon=5.0199 forget=1.8467 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.5284 mle=1.6443 pcon=5.0197 forget=1.8644 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.5094 mle=1.6249 pcon=5.0193 forget=1.8652 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.2863 mle=1.3941 pcon=5.0191 forget=1.8731 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.4766 mle=1.5909 pcon=5.0186 forget=1.8671 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.5290 mle=1.6423 pcon=5.0179 forget=1.8687 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.4086 mle=1.5205 pcon=5.0175 forget=1.8707 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.6444 mle=1.7683 pcon=5.0169 forget=1.8593 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.5654 mle=1.6755 pcon=5.0162 forget=1.8737 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.4850 mle=1.6030 pcon=5.0154 forget=1.8666 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.4845 mle=1.5988 pcon=5.0147 forget=1.8710 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.6866 mle=1.7898 pcon=5.0136 forget=1.8831 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.4316 mle=1.5417 pcon=5.0129 forget=1.8770 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.5422 mle=1.6622 pcon=5.0120 forget=1.8680 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.5934 mle=1.7049 pcon=5.0110 forget=1.8775 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.4185 mle=1.5289 pcon=5.0101 forget=1.8795 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.5399 mle=1.6428 pcon=5.0092 forget=1.8878 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.4810 mle=1.5899 pcon=5.0083 forget=1.8828 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.4837 mle=1.5766 pcon=5.0075 forget=1.8996 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 0 total=8.5645 mle=1.6708 pcon=5.0065 forget=1.8872 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.6689 mle=1.7772 pcon=5.0056 forget=1.8860 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.6832 mle=1.7848 pcon=5.0047 forget=1.8937 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.5715 mle=1.6737 pcon=5.0037 forget=1.8942 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.6305 mle=1.7314 pcon=5.0025 forget=1.8966 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=8.4431 mle=1.5434 pcon=5.0016 forget=1.8981 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=8.5027 mle=1.5900 pcon=5.0006 forget=1.9121 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=8.6623 mle=1.7471 pcon=4.9997 forget=1.9155 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 10 total=8.5227 mle=1.6087 pcon=4.9986 forget=1.9154 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
 88%|████████▊ | 22/25 [04:41<00:39, 13.30s/it] 92%|█████████▏| 23/25 [04:55<00:26, 13.44s/it] 96%|█████████▌| 24/25 [05:08<00:13, 13.41s/it]100%|██████████| 25/25 [05:23<00:00, 13.81s/it]100%|██████████| 25/25 [05:23<00:00, 12.92s/it]
[loss] ep 21 it 60 total=8.5437 mle=1.6212 pcon=4.9979 forget=1.9246 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.5100 mle=1.6008 pcon=4.9971 forget=1.9121 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.5374 mle=1.6120 pcon=4.9962 forget=1.9291 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=8.7843 mle=1.8619 pcon=4.9955 forget=1.9269 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.6568 mle=1.7199 pcon=4.9946 forget=1.9423 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.5050 mle=1.5753 pcon=4.9937 forget=1.9360 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.4780 mle=1.5446 pcon=4.9927 forget=1.9407 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 20 total=8.5592 mle=1.6203 pcon=4.9919 forget=1.9470 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.5257 mle=1.5799 pcon=4.9909 forget=1.9549 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.4299 mle=1.4863 pcon=4.9902 forget=1.9534 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.6571 mle=1.7110 pcon=4.9893 forget=1.9568 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.6642 mle=1.7012 pcon=4.9887 forget=1.9744 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.8097 mle=1.8559 pcon=4.9880 forget=1.9658 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.5343 mle=1.5819 pcon=4.9874 forget=1.9650 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=8.4869 mle=1.5192 pcon=4.9868 forget=1.9810 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 30 total=8.5694 mle=1.6082 pcon=4.9861 forget=1.9751 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.4659 mle=1.5001 pcon=4.9857 forget=1.9801 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=8.5601 mle=1.5776 pcon=4.9852 forget=1.9973 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.6434 mle=1.6541 pcon=4.9846 forget=2.0047 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.7314 mle=1.7430 pcon=4.9841 forget=2.0043 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=8.6332 mle=1.6531 pcon=4.9835 forget=1.9966 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=8.6449 mle=1.6576 pcon=4.9829 forget=2.0044 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.5264 mle=1.5369 pcon=4.9827 forget=2.0069 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 40 total=8.6250 mle=1.6297 pcon=4.9823 forget=2.0131 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=8.5340 mle=1.5235 pcon=4.9817 forget=2.0288 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.7061 mle=1.6907 pcon=4.9815 forget=2.0339 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.6235 mle=1.6080 pcon=4.9810 forget=2.0345 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=8.5851 mle=1.5677 pcon=4.9808 forget=2.0366 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.6477 mle=1.6254 pcon=4.9806 forget=2.0416 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.5692 mle=1.5529 pcon=4.9802 forget=2.0361 orth=0.0000 favg=0.0000 nr=64 nf=64 protos=600 fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<05:05,  1.28it/s]  2%|▏         | 6/391 [00:00<00:44,  8.72it/s]  3%|▎         | 12/391 [00:00<00:21, 17.75it/s]  5%|▍         | 18/391 [00:01<00:14, 25.46it/s]  7%|▋         | 26/391 [00:01<00:09, 36.67it/s]  9%|▊         | 34/391 [00:01<00:07, 45.87it/s] 11%|█         | 42/391 [00:01<00:06, 52.80it/s] 13%|█▎        | 49/391 [00:01<00:06, 56.92it/s] 14%|█▍        | 56/391 [00:01<00:05, 57.78it/s] 16%|█▋        | 64/391 [00:01<00:05, 61.49it/s] 18%|█▊        | 71/391 [00:01<00:05, 63.61it/s] 20%|██        | 79/391 [00:01<00:04, 65.99it/s] 22%|██▏       | 87/391 [00:02<00:04, 68.51it/s] 24%|██▍       | 95/391 [00:02<00:04, 70.98it/s] 26%|██▋       | 103/391 [00:02<00:04, 70.66it/s] 28%|██▊       | 111/391 [00:02<00:03, 70.40it/s] 30%|███       | 119/391 [00:02<00:03, 70.40it/s] 32%|███▏      | 127/391 [00:02<00:03, 70.46it/s] 35%|███▍      | 135/391 [00:02<00:03, 70.42it/s] 37%|███▋      | 143/391 [00:02<00:03, 70.32it/s] 39%|███▊      | 151/391 [00:02<00:03, 70.35it/s] 41%|████      | 159/391 [00:03<00:03, 70.20it/s] 43%|████▎     | 167/391 [00:03<00:03, 68.45it/s] 45%|████▍     | 175/391 [00:03<00:03, 68.96it/s] 47%|████▋     | 183/391 [00:03<00:02, 70.29it/s] 49%|████▉     | 191/391 [00:03<00:02, 69.31it/s] 51%|█████     | 198/391 [00:03<00:02, 68.37it/s] 52%|█████▏    | 205/391 [00:03<00:02, 68.26it/s] 54%|█████▍    | 213/391 [00:03<00:02, 69.80it/s] 57%|█████▋    | 221/391 [00:04<00:02, 69.99it/s] 59%|█████▊    | 229/391 [00:04<00:02, 70.20it/s] 61%|██████    | 237/391 [00:04<00:02, 70.24it/s] 63%|██████▎   | 245/391 [00:04<00:02, 70.33it/s] 65%|██████▍   | 253/391 [00:04<00:01, 69.99it/s] 67%|██████▋   | 261/391 [00:04<00:01, 70.39it/s] 69%|██████▉   | 269/391 [00:04<00:01, 69.60it/s] 71%|███████   | 277/391 [00:04<00:01, 69.83it/s] 73%|███████▎  | 285/391 [00:04<00:01, 70.01it/s] 75%|███████▍  | 293/391 [00:05<00:01, 70.96it/s] 77%|███████▋  | 301/391 [00:05<00:01, 69.92it/s] 79%|███████▉  | 309/391 [00:05<00:01, 70.96it/s] 81%|████████  | 317/391 [00:05<00:01, 69.86it/s] 83%|████████▎ | 325/391 [00:05<00:00, 70.65it/s] 85%|████████▌ | 333/391 [00:05<00:00, 70.45it/s] 87%|████████▋ | 341/391 [00:05<00:00, 70.86it/s] 89%|████████▉ | 349/391 [00:05<00:00, 69.80it/s] 91%|█████████▏| 357/391 [00:05<00:00, 70.80it/s] 93%|█████████▎| 365/391 [00:06<00:00, 70.56it/s] 95%|█████████▌| 373/391 [00:06<00:00, 70.66it/s] 97%|█████████▋| 381/391 [00:06<00:00, 70.64it/s] 99%|█████████▉| 389/391 [00:06<00:00, 70.36it/s]100%|██████████| 391/391 [00:06<00:00, 60.57it/s]
50000 images processed, 6.756139278411865 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:04,  1.20it/s] 10%|█         | 8/79 [00:00<00:06, 11.09it/s] 19%|█▉        | 15/79 [00:01<00:03, 20.94it/s] 28%|██▊       | 22/79 [00:01<00:01, 30.18it/s] 37%|███▋      | 29/79 [00:01<00:01, 38.30it/s] 46%|████▌     | 36/79 [00:01<00:00, 45.20it/s] 56%|█████▌    | 44/79 [00:01<00:00, 51.48it/s] 65%|██████▍   | 51/79 [00:01<00:00, 55.27it/s] 73%|███████▎  | 58/79 [00:01<00:00, 58.53it/s] 82%|████████▏ | 65/79 [00:01<00:00, 60.94it/s] 92%|█████████▏| 73/79 [00:01<00:00, 63.54it/s]100%|██████████| 79/79 [00:02<00:00, 39.43it/s]
10000 images processed, 2.0376574993133545 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:58,  1.72it/s]  4%|▍         | 8/204 [00:00<00:12, 15.15it/s]  7%|▋         | 15/204 [00:00<00:07, 26.44it/s] 11%|█         | 22/204 [00:00<00:05, 36.03it/s] 14%|█▎        | 28/204 [00:01<00:04, 41.20it/s] 17%|█▋        | 35/204 [00:01<00:03, 48.38it/s] 21%|██        | 42/204 [00:01<00:03, 48.26it/s] 24%|██▎       | 48/204 [00:01<00:03, 46.18it/s] 27%|██▋       | 56/204 [00:01<00:02, 52.31it/s] 31%|███       | 63/204 [00:01<00:02, 56.01it/s] 34%|███▍      | 70/204 [00:01<00:02, 58.75it/s] 38%|███▊      | 78/204 [00:01<00:02, 61.26it/s] 42%|████▏     | 85/204 [00:01<00:01, 60.27it/s] 45%|████▌     | 92/204 [00:02<00:01, 59.51it/s] 49%|████▊     | 99/204 [00:02<00:01, 57.06it/s] 52%|█████▏    | 106/204 [00:02<00:01, 58.78it/s] 55%|█████▍    | 112/204 [00:02<00:01, 58.57it/s] 58%|█████▊    | 118/204 [00:02<00:01, 58.60it/s] 61%|██████    | 124/204 [00:02<00:01, 55.73it/s] 64%|██████▍   | 131/204 [00:02<00:01, 56.11it/s] 67%|██████▋   | 137/204 [00:02<00:01, 53.98it/s] 71%|███████   | 144/204 [00:03<00:01, 56.83it/s] 74%|███████▍  | 151/204 [00:03<00:00, 58.27it/s] 77%|███████▋  | 157/204 [00:03<00:00, 55.88it/s] 80%|███████▉  | 163/204 [00:03<00:00, 56.70it/s] 84%|████████▍ | 171/204 [00:03<00:00, 62.29it/s] 88%|████████▊ | 179/204 [00:03<00:00, 65.23it/s] 92%|█████████▏| 187/204 [00:03<00:00, 69.20it/s] 95%|█████████▌| 194/204 [00:03<00:00, 68.91it/s] 99%|█████████▊| 201/204 [00:03<00:00, 68.76it/s]100%|██████████| 204/204 [00:03<00:00, 52.16it/s]
26032 images processed, 3.9577903747558594 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:52,  1.49it/s]  5%|▌         | 4/79 [00:00<00:11,  6.39it/s] 14%|█▍        | 11/79 [00:00<00:03, 18.63it/s] 23%|██▎       | 18/79 [00:01<00:02, 28.70it/s] 32%|███▏      | 25/79 [00:01<00:01, 37.55it/s] 41%|████      | 32/79 [00:01<00:01, 44.53it/s] 49%|████▉     | 39/79 [00:01<00:00, 48.93it/s] 58%|█████▊    | 46/79 [00:01<00:00, 53.56it/s] 71%|███████   | 56/79 [00:01<00:00, 64.21it/s] 84%|████████▎ | 66/79 [00:01<00:00, 73.11it/s] 96%|█████████▌| 76/79 [00:01<00:00, 79.36it/s]100%|██████████| 79/79 [00:01<00:00, 43.92it/s]
10000 images processed, 1.8283612728118896 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.92it/s]  9%|▉         | 7/79 [00:00<00:04, 14.40it/s] 18%|█▊        | 14/79 [00:00<00:02, 27.10it/s] 27%|██▋       | 21/79 [00:00<00:01, 37.11it/s] 37%|███▋      | 29/79 [00:00<00:01, 46.40it/s] 46%|████▌     | 36/79 [00:01<00:00, 51.61it/s] 54%|█████▍    | 43/79 [00:01<00:00, 56.06it/s] 65%|██████▍   | 51/79 [00:01<00:00, 60.13it/s] 75%|███████▍  | 59/79 [00:01<00:00, 63.10it/s] 85%|████████▍ | 67/79 [00:01<00:00, 65.16it/s] 95%|█████████▍| 75/79 [00:01<00:00, 66.52it/s]100%|██████████| 79/79 [00:01<00:00, 47.32it/s]
10000 images processed, 1.6968963146209717 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:36,  1.91it/s] 13%|█▎        | 9/70 [00:00<00:03, 18.04it/s] 24%|██▍       | 17/70 [00:00<00:01, 31.83it/s] 36%|███▌      | 25/70 [00:00<00:01, 43.19it/s] 46%|████▌     | 32/70 [00:00<00:00, 49.87it/s] 57%|█████▋    | 40/70 [00:01<00:00, 57.45it/s] 69%|██████▊   | 48/70 [00:01<00:00, 61.42it/s] 80%|████████  | 56/70 [00:01<00:00, 66.18it/s] 91%|█████████▏| 64/70 [00:01<00:00, 67.49it/s]100%|██████████| 70/70 [00:01<00:00, 47.79it/s]
8925 images processed, 1.5039658546447754 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:47,  1.07s/it] 11%|█         | 5/45 [00:01<00:07,  5.21it/s] 27%|██▋       | 12/45 [00:01<00:02, 13.18it/s] 38%|███▊      | 17/45 [00:01<00:01, 16.65it/s] 47%|████▋     | 21/45 [00:01<00:01, 15.50it/s] 62%|██████▏   | 28/45 [00:01<00:00, 23.48it/s] 73%|███████▎  | 33/45 [00:02<00:00, 24.08it/s] 89%|████████▉ | 40/45 [00:02<00:00, 31.92it/s]100%|██████████| 45/45 [00:02<00:00, 35.52it/s]100%|██████████| 45/45 [00:02<00:00, 19.05it/s]
5640 images processed, 2.3910486698150635 seconds used

21.90242576599121
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.75  99.34  97.83
places365     68.67  81.18  78.09
LSUN          19.40  95.83  95.62
iSUN          74.29  80.92  82.91
dtd           39.70  91.09  93.87
AVG           40.96  89.67  89.66
Retain-Acc: 0.7567
Forget-as-OOD (retain known vs forget novel):
  FPR: 58.33 AUROC: 87.60 AUIN: 97.50
6.965394973754883
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08_rf.png
