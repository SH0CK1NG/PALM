nohup: ignoring input
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter', adapter_load_path=None, adapter_load_paths=None, lora_new_adapter_name=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=True, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:25<01:43, 25.90s/it] 40%|████      | 2/5 [00:46<01:08, 22.91s/it] 60%|██████    | 3/5 [01:06<00:43, 21.70s/it] 80%|████████  | 4/5 [01:27<00:21, 21.35s/it]100%|██████████| 5/5 [01:48<00:00, 20.99s/it]100%|██████████| 5/5 [01:48<00:00, 21.63s/it]
[loss] ep 0 it 0 total=9.1228 mle=1.5806 pcon=5.2951 forget=2.3854 favg=-0.1382 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=8.9082 mle=1.4910 pcon=5.2914 forget=2.3537 favg=-0.2279 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.1771 mle=1.6391 pcon=5.2875 forget=2.3816 favg=-0.1311 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2658 mle=1.8131 pcon=5.2840 forget=2.3525 favg=-0.1838 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2345 mle=1.8135 pcon=5.2802 forget=2.3236 favg=-0.1829 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=8.8564 mle=1.4462 pcon=5.2762 forget=2.3791 favg=-0.2451 nr=128 nf=101 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=8.9972 mle=1.4830 pcon=5.2726 forget=2.3775 favg=-0.1360 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1586 mle=1.4994 pcon=5.2693 forget=2.4357 favg=-0.0458 nr=128 nf=93 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 1 it 10 total=9.1161 mle=1.7196 pcon=5.2657 forget=2.3580 favg=-0.2272 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=8.8507 mle=1.2957 pcon=5.2618 forget=2.4521 favg=-0.1589 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.8874 mle=1.4152 pcon=5.2582 forget=2.3925 favg=-0.1785 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.6609 mle=1.1733 pcon=5.2546 forget=2.4271 favg=-0.1941 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0907 mle=1.5840 pcon=5.2510 forget=2.3914 favg=-0.1356 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9683 mle=1.5959 pcon=5.2475 forget=2.3748 favg=-0.2500 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9578 mle=1.6189 pcon=5.2441 forget=2.3531 favg=-0.2583 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0017 mle=1.6114 pcon=5.2407 forget=2.3750 favg=-0.2253 nr=128 nf=97 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 2 it 20 total=8.8869 mle=1.5297 pcon=5.2372 forget=2.3616 favg=-0.2416 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=8.8260 mle=1.3554 pcon=5.2341 forget=2.4298 favg=-0.1932 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.7296 mle=1.4282 pcon=5.2308 forget=2.3528 favg=-0.2822 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=9.0182 mle=1.6472 pcon=5.2275 forget=2.3772 favg=-0.2338 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.6925 mle=1.3786 pcon=5.2240 forget=2.3836 favg=-0.2937 nr=128 nf=89 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0016 mle=1.7360 pcon=5.2205 forget=2.3696 favg=-0.3245 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.3981 mle=1.3941 pcon=5.2170 forget=2.3569 favg=-0.5698 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=8.1628 mle=1.4040 pcon=5.2133 forget=2.3360 favg=-0.7905 nr=128 nf=96 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 3 it 30 total=7.9791 mle=1.6233 pcon=5.2090 forget=2.2541 favg=-1.1074 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=7.4343 mle=1.4908 pcon=5.2042 forget=2.1816 favg=-1.4424 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=7.5213 mle=1.7709 pcon=5.1992 forget=2.1146 favg=-1.5635 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=7.3362 mle=1.8276 pcon=5.1944 forget=2.0779 favg=-1.7637 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=7.3968 mle=1.8663 pcon=5.1896 forget=2.0909 favg=-1.7500 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=7.7223 mle=1.8924 pcon=5.1846 forget=2.1316 favg=-1.4863 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=8.5441 mle=1.8684 pcon=5.1799 forget=2.1414 favg=-0.6455 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=9.9751 mle=1.7484 pcon=5.1753 forget=2.1872 favg=0.8643 nr=128 nf=97 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 4 it 40 total=10.4232 mle=1.7641 pcon=5.1709 forget=2.2050 favg=1.2832 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=10.0947 mle=1.3217 pcon=5.1670 forget=2.3356 favg=1.2705 nr=128 nf=91 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=10.3227 mle=1.7975 pcon=5.1635 forget=2.3012 favg=1.0605 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.8332 mle=1.5508 pcon=5.1604 forget=2.3398 favg=0.7822 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.4419 mle=1.3865 pcon=5.1579 forget=2.3462 favg=0.5513 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.1583 mle=1.1966 pcon=5.1556 forget=2.3581 favg=0.4480 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.3868 mle=1.5700 pcon=5.1535 forget=2.3171 favg=0.3462 nr=128 nf=93 protos=600 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[peft] set_adapter(list) failed: unhashable type: 'list'; try adapter fusion
[peft] adapter fusion failed: adapter fusion API not available or single adapter provided; fallback to last adapter
[peft] active adapter set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:12,  2.02it/s]  2%|▏         | 9/391 [00:00<00:20, 19.04it/s]  4%|▍         | 16/391 [00:00<00:12, 31.00it/s]  6%|▌         | 24/391 [00:00<00:08, 42.47it/s]  8%|▊         | 32/391 [00:00<00:07, 50.99it/s] 10%|█         | 40/391 [00:01<00:06, 56.92it/s] 12%|█▏        | 48/391 [00:01<00:05, 60.90it/s] 14%|█▍        | 56/391 [00:01<00:05, 63.82it/s] 16%|█▋        | 64/391 [00:01<00:04, 67.88it/s] 18%|█▊        | 72/391 [00:01<00:04, 70.79it/s] 20%|██        | 80/391 [00:01<00:04, 71.31it/s] 23%|██▎       | 88/391 [00:01<00:04, 73.24it/s] 25%|██▍       | 96/391 [00:01<00:04, 72.86it/s] 27%|██▋       | 104/391 [00:01<00:03, 74.44it/s] 29%|██▊       | 112/391 [00:02<00:03, 73.46it/s] 31%|███       | 120/391 [00:02<00:03, 73.02it/s] 33%|███▎      | 128/391 [00:02<00:03, 74.71it/s] 35%|███▍      | 136/391 [00:02<00:03, 73.73it/s] 37%|███▋      | 144/391 [00:02<00:03, 74.81it/s] 39%|███▉      | 152/391 [00:02<00:03, 73.46it/s] 41%|████      | 160/391 [00:02<00:03, 74.61it/s] 43%|████▎     | 168/391 [00:02<00:03, 74.05it/s] 45%|████▌     | 176/391 [00:02<00:02, 73.03it/s] 47%|████▋     | 184/391 [00:02<00:02, 74.32it/s] 49%|████▉     | 192/391 [00:03<00:02, 73.61it/s] 51%|█████     | 200/391 [00:03<00:02, 72.97it/s] 53%|█████▎    | 208/391 [00:03<00:02, 74.10it/s] 55%|█████▌    | 216/391 [00:03<00:02, 73.70it/s] 57%|█████▋    | 224/391 [00:03<00:02, 74.77it/s] 59%|█████▉    | 232/391 [00:03<00:02, 73.86it/s] 61%|██████▏   | 240/391 [00:03<00:02, 73.34it/s] 63%|██████▎   | 248/391 [00:03<00:01, 74.76it/s] 65%|██████▌   | 256/391 [00:03<00:01, 73.54it/s] 68%|██████▊   | 264/391 [00:04<00:01, 74.59it/s] 70%|██████▉   | 272/391 [00:04<00:01, 73.79it/s] 72%|███████▏  | 280/391 [00:04<00:01, 72.77it/s] 74%|███████▎  | 288/391 [00:04<00:01, 74.13it/s] 76%|███████▌  | 296/391 [00:04<00:01, 73.59it/s] 78%|███████▊  | 304/391 [00:04<00:01, 72.81it/s] 80%|███████▉  | 312/391 [00:04<00:01, 74.05it/s] 82%|████████▏ | 320/391 [00:04<00:00, 73.58it/s] 84%|████████▍ | 328/391 [00:04<00:00, 73.50it/s] 86%|████████▌ | 336/391 [00:05<00:00, 74.51it/s] 88%|████████▊ | 344/391 [00:05<00:00, 73.41it/s] 90%|█████████ | 352/391 [00:05<00:00, 74.45it/s] 92%|█████████▏| 360/391 [00:05<00:00, 73.15it/s] 94%|█████████▍| 368/391 [00:05<00:00, 72.50it/s] 96%|█████████▌| 376/391 [00:05<00:00, 72.39it/s] 98%|█████████▊| 384/391 [00:05<00:00, 74.24it/s]100%|██████████| 391/391 [00:05<00:00, 67.49it/s]
50000 images processed, 5.885209083557129 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.93it/s] 11%|█▏        | 9/79 [00:00<00:03, 18.19it/s] 22%|██▏       | 17/79 [00:00<00:01, 31.98it/s] 32%|███▏      | 25/79 [00:00<00:01, 42.42it/s] 42%|████▏     | 33/79 [00:00<00:00, 50.13it/s] 52%|█████▏    | 41/79 [00:01<00:00, 55.55it/s] 62%|██████▏   | 49/79 [00:01<00:00, 61.18it/s] 71%|███████   | 56/79 [00:01<00:00, 63.53it/s] 81%|████████  | 64/79 [00:01<00:00, 65.63it/s] 91%|█████████ | 72/79 [00:01<00:00, 68.87it/s]100%|██████████| 79/79 [00:01<00:00, 48.87it/s]
10000 images processed, 1.646888017654419 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:55,  1.76it/s]  4%|▍         | 9/204 [00:00<00:11, 17.11it/s]  8%|▊         | 17/204 [00:00<00:06, 30.26it/s] 12%|█▏        | 25/204 [00:00<00:04, 40.52it/s] 16%|█▌        | 33/204 [00:01<00:03, 49.03it/s] 20%|█▉        | 40/204 [00:01<00:03, 54.34it/s] 24%|██▎       | 48/204 [00:01<00:02, 58.96it/s] 27%|██▋       | 56/204 [00:01<00:02, 62.37it/s] 31%|███▏      | 64/204 [00:01<00:02, 65.77it/s] 35%|███▌      | 72/204 [00:01<00:01, 67.79it/s] 39%|███▉      | 80/204 [00:01<00:01, 70.38it/s] 43%|████▎     | 88/204 [00:01<00:01, 70.60it/s] 47%|████▋     | 96/204 [00:01<00:01, 70.59it/s] 51%|█████     | 104/204 [00:01<00:01, 72.39it/s] 55%|█████▍    | 112/204 [00:02<00:01, 72.02it/s] 59%|█████▉    | 120/204 [00:02<00:01, 71.23it/s] 63%|██████▎   | 128/204 [00:02<00:01, 71.00it/s] 67%|██████▋   | 136/204 [00:02<00:00, 73.01it/s] 71%|███████   | 144/204 [00:02<00:00, 72.49it/s] 75%|███████▍  | 152/204 [00:02<00:00, 72.72it/s] 78%|███████▊  | 160/204 [00:02<00:00, 71.33it/s] 82%|████████▏ | 168/204 [00:02<00:00, 72.01it/s] 86%|████████▋ | 176/204 [00:02<00:00, 72.08it/s] 90%|█████████ | 184/204 [00:03<00:00, 73.57it/s] 94%|█████████▍| 192/204 [00:03<00:00, 73.19it/s] 98%|█████████▊| 200/204 [00:03<00:00, 74.02it/s]100%|██████████| 204/204 [00:03<00:00, 60.47it/s]
26032 images processed, 3.410550355911255 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:00,  1.30it/s] 11%|█▏        | 9/79 [00:00<00:05, 13.52it/s] 20%|██        | 16/79 [00:00<00:02, 23.54it/s] 30%|███       | 24/79 [00:01<00:01, 34.45it/s] 41%|████      | 32/79 [00:01<00:01, 43.49it/s] 51%|█████     | 40/79 [00:01<00:00, 50.79it/s] 61%|██████    | 48/79 [00:01<00:00, 57.12it/s] 71%|███████   | 56/79 [00:01<00:00, 61.71it/s] 81%|████████  | 64/79 [00:01<00:00, 65.81it/s] 91%|█████████ | 72/79 [00:01<00:00, 67.59it/s]100%|██████████| 79/79 [00:01<00:00, 43.27it/s]
10000 images processed, 1.8449945449829102 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:45,  1.71it/s] 11%|█▏        | 9/79 [00:00<00:04, 17.05it/s] 22%|██▏       | 17/79 [00:00<00:02, 30.46it/s] 32%|███▏      | 25/79 [00:00<00:01, 41.02it/s] 42%|████▏     | 33/79 [00:01<00:00, 49.40it/s] 52%|█████▏    | 41/79 [00:01<00:00, 56.49it/s] 62%|██████▏   | 49/79 [00:01<00:00, 60.66it/s] 72%|███████▏  | 57/79 [00:01<00:00, 64.19it/s] 82%|████████▏ | 65/79 [00:01<00:00, 67.53it/s] 92%|█████████▏| 73/79 [00:01<00:00, 68.99it/s]100%|██████████| 79/79 [00:01<00:00, 48.77it/s]
10000 images processed, 1.6401889324188232 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:38,  1.81it/s] 10%|█         | 7/70 [00:00<00:04, 13.66it/s] 21%|██▏       | 15/70 [00:00<00:01, 28.35it/s] 31%|███▏      | 22/70 [00:00<00:01, 38.29it/s] 43%|████▎     | 30/70 [00:00<00:00, 48.47it/s] 53%|█████▎    | 37/70 [00:01<00:00, 54.14it/s] 63%|██████▎   | 44/70 [00:01<00:00, 57.83it/s] 74%|███████▍  | 52/70 [00:01<00:00, 62.96it/s] 86%|████████▌ | 60/70 [00:01<00:00, 66.08it/s] 97%|█████████▋| 68/70 [00:01<00:00, 67.43it/s]100%|██████████| 70/70 [00:01<00:00, 45.59it/s]
8925 images processed, 1.5655345916748047 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:44,  1.02s/it]  4%|▍         | 2/45 [00:01<00:25,  1.68it/s] 22%|██▏       | 10/45 [00:01<00:03, 11.19it/s] 40%|████      | 18/45 [00:01<00:01, 13.55it/s] 60%|██████    | 27/45 [00:02<00:00, 22.33it/s] 73%|███████▎  | 33/45 [00:02<00:00, 27.57it/s] 87%|████████▋ | 39/45 [00:02<00:00, 19.05it/s]100%|██████████| 45/45 [00:02<00:00, 16.34it/s]
5640 images processed, 2.7753944396972656 seconds used

20.38460373878479
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.56  99.36
places365     68.08  81.18
LSUN          17.76  96.09
iSUN          72.35  81.79
dtd           38.55  91.37
AVG           39.86  89.96
Retain-Acc: 0.7482
Traceback (most recent call last):
  File "/home/shaokun/PALM/eval_cifar.py", line 19, in <module>
    eval_maha(args)
  File "/home/shaokun/PALM/util/evaluations/mahalanobis.py", line 173, in eval_maha
    fr_results = metrics.cal_metric(retain_scores, forget_scores)
  File "/home/shaokun/PALM/util/evaluations/metrics.py", line 16, in cal_metric
    tp, fp, fpr_at_tpr95 = get_curve(known, novel, method)
  File "/home/shaokun/PALM/util/evaluations/metrics.py", line 85, in get_curve
    end = np.max([np.max(known), np.max(novel)])
  File "/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/numpy/core/fromnumeric.py", line 2810, in max
    return _wrapreduction(a, np.maximum, 'max', axis, None, out,
  File "/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/numpy/core/fromnumeric.py", line 88, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
ValueError: zero-size array to reduction operation maximum which has no identity
