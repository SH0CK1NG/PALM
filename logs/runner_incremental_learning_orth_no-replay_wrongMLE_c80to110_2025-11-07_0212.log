nohup: ignoring input
==== Stage 1: inc={0,8,11,40,51}; seen={}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='retain_only', palm_retain_only=False, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage1', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='0,8,11,40,51', forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:07<05:52,  7.20s/it]  4%|▍         | 2/50 [00:07<02:41,  3.37s/it]  6%|▌         | 3/50 [00:08<01:36,  2.06s/it]  8%|▊         | 4/50 [00:08<01:06,  1.45s/it] 10%|█         | 5/50 [00:09<00:49,  1.11s/it] 12%|█▏        | 6/50 [00:09<00:39,  1.13it/s] 14%|█▍        | 7/50 [00:10<00:32,  1.33it/s] 16%|█▌        | 8/50 [00:10<00:27,  1.53it/s] 18%|█▊        | 9/50 [00:11<00:24,  1.69it/s] 20%|██        | 10/50 [00:11<00:21,  1.84it/s] 22%|██▏       | 11/50 [00:12<00:21,  1.80it/s] 24%|██▍       | 12/50 [00:12<00:20,  1.90it/s] 26%|██▌       | 13/50 [00:13<00:19,  1.93it/s] 28%|██▊       | 14/50 [00:13<00:18,  2.00it/s] 30%|███       | 15/50 [00:14<00:17,  2.04it/s] 32%|███▏      | 16/50 [00:14<00:16,  2.10it/s] 34%|███▍      | 17/50 [00:15<00:16,  2.05it/s] 36%|███▌      | 18/50 [00:15<00:15,  2.02it/s] 38%|███▊      | 19/50 [00:16<00:15,  1.98it/s] 40%|████      | 20/50 [00:16<00:14,  2.00it/s] 42%|████▏     | 21/50 [00:17<00:14,  1.94it/s] 44%|████▍     | 22/50 [00:17<00:14,  1.98it/s] 46%|████▌     | 23/50 [00:18<00:13,  2.01it/s] 48%|████▊     | 24/50 [00:18<00:12,  2.01it/s] 50%|█████     | 25/50 [00:19<00:12,  2.06it/s] 52%|█████▏    | 26/50 [00:19<00:11,  2.04it/s] 54%|█████▍    | 27/50 [00:20<00:11,  2.06it/s] 56%|█████▌    | 28/50 [00:20<00:10,  2.03it/s] 58%|█████▊    | 29/50 [00:21<00:10,  2.00it/s] 60%|██████    | 30/50 [00:21<00:10,  2.00it/s] 62%|██████▏   | 31/50 [00:22<00:09,  2.01it/s] 64%|██████▍   | 32/50 [00:22<00:09,  1.90it/s] 66%|██████▌   | 33/50 [00:23<00:08,  1.98it/s] 68%|██████▊   | 34/50 [00:23<00:07,  2.01it/s] 70%|███████   | 35/50 [00:24<00:07,  2.04it/s] 72%|███████▏  | 36/50 [00:24<00:07,  1.89it/s] 74%|███████▍  | 37/50 [00:25<00:06,  1.91it/s] 76%|███████▌  | 38/50 [00:25<00:06,  1.97it/s] 78%|███████▊  | 39/50 [00:26<00:05,  1.99it/s] 80%|████████  | 40/50 [00:26<00:05,  1.95it/s] 82%|████████▏ | 41/50 [00:27<00:04,  1.98it/s] 84%|████████▍ | 42/50 [00:27<00:04,  1.95it/s] 86%|████████▌ | 43/50 [00:28<00:03,  2.04it/s] 88%|████████▊ | 44/50 [00:28<00:02,  2.06it/s] 90%|█████████ | 45/50 [00:29<00:02,  2.03it/s] 92%|█████████▏| 46/50 [00:29<00:01,  2.06it/s] 94%|█████████▍| 47/50 [00:30<00:01,  2.03it/s] 96%|█████████▌| 48/50 [00:30<00:01,  1.90it/s] 98%|█████████▊| 49/50 [00:31<00:00,  1.93it/s]100%|██████████| 50/50 [00:31<00:00,  1.94it/s]100%|██████████| 50/50 [00:31<00:00,  1.57it/s]
[loss] ep 0 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage1
[loss] ep 2 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 5 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 7 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 10 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 12 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 15 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 17 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 20 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 22 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 25 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 27 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 30 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 32 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 35 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 37 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 40 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 42 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 45 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 47 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage1
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stage1-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<03:53,  1.84it/s]  3%|▎         | 11/430 [00:00<00:19, 22.02it/s]  5%|▍         | 20/430 [00:00<00:11, 37.26it/s]  7%|▋         | 30/430 [00:00<00:07, 51.82it/s]  9%|▉         | 40/430 [00:00<00:06, 63.23it/s] 12%|█▏        | 50/430 [00:01<00:05, 72.23it/s] 14%|█▎        | 59/430 [00:01<00:04, 76.25it/s] 16%|█▌        | 69/430 [00:01<00:04, 81.63it/s] 18%|█▊        | 79/430 [00:01<00:04, 85.33it/s] 21%|██        | 89/430 [00:01<00:03, 87.66it/s] 23%|██▎       | 99/430 [00:01<00:03, 89.44it/s] 25%|██▌       | 109/430 [00:01<00:03, 87.04it/s] 27%|██▋       | 118/430 [00:01<00:03, 80.98it/s] 30%|██▉       | 128/430 [00:01<00:03, 84.99it/s] 32%|███▏      | 138/430 [00:02<00:03, 87.52it/s] 34%|███▍      | 148/430 [00:02<00:03, 88.56it/s] 37%|███▋      | 157/430 [00:02<00:03, 87.72it/s] 39%|███▊      | 166/430 [00:02<00:03, 84.29it/s] 41%|████      | 176/430 [00:02<00:02, 87.02it/s] 43%|████▎     | 185/430 [00:02<00:02, 86.85it/s] 45%|████▌     | 194/430 [00:02<00:02, 86.33it/s] 47%|████▋     | 204/430 [00:02<00:02, 88.75it/s] 50%|████▉     | 214/430 [00:02<00:02, 90.50it/s] 52%|█████▏    | 224/430 [00:03<00:02, 91.09it/s] 54%|█████▍    | 234/430 [00:03<00:02, 92.18it/s] 57%|█████▋    | 244/430 [00:03<00:01, 93.15it/s] 59%|█████▉    | 254/430 [00:03<00:01, 92.10it/s] 61%|██████▏   | 264/430 [00:03<00:01, 86.13it/s] 63%|██████▎   | 273/430 [00:03<00:01, 85.75it/s] 66%|██████▌   | 283/430 [00:03<00:01, 88.45it/s] 68%|██████▊   | 293/430 [00:03<00:01, 90.40it/s] 70%|███████   | 303/430 [00:03<00:01, 89.97it/s] 73%|███████▎  | 313/430 [00:04<00:01, 91.35it/s] 75%|███████▌  | 323/430 [00:04<00:01, 91.58it/s] 77%|███████▋  | 333/430 [00:04<00:01, 92.34it/s] 80%|███████▉  | 343/430 [00:04<00:00, 93.32it/s] 82%|████████▏ | 353/430 [00:04<00:00, 94.10it/s] 84%|████████▍ | 363/430 [00:04<00:00, 93.01it/s] 87%|████████▋ | 373/430 [00:04<00:00, 92.47it/s] 89%|████████▉ | 383/430 [00:04<00:00, 87.91it/s] 91%|█████████ | 392/430 [00:04<00:00, 80.77it/s] 93%|█████████▎| 401/430 [00:05<00:00, 74.89it/s] 95%|█████████▌| 409/430 [00:05<00:00, 72.46it/s] 97%|█████████▋| 418/430 [00:05<00:00, 74.86it/s] 99%|█████████▉| 426/430 [00:05<00:00, 73.97it/s]100%|██████████| 430/430 [00:05<00:00, 78.36it/s]
55000 images processed, 5.6418068408966064 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:48,  1.76it/s] 12%|█▏        | 10/86 [00:00<00:03, 19.29it/s] 23%|██▎       | 20/86 [00:00<00:01, 36.58it/s] 35%|███▍      | 30/86 [00:00<00:01, 50.95it/s] 47%|████▋     | 40/86 [00:00<00:00, 62.34it/s] 58%|█████▊    | 50/86 [00:01<00:00, 70.68it/s] 70%|██████▉   | 60/86 [00:01<00:00, 77.73it/s] 81%|████████▏ | 70/86 [00:01<00:00, 83.15it/s] 93%|█████████▎| 80/86 [00:01<00:00, 87.28it/s]100%|██████████| 86/86 [00:01<00:00, 58.28it/s]
11000 images processed, 1.5107314586639404 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:56,  1.75it/s]  5%|▌         | 11/204 [00:00<00:09, 21.04it/s] 10%|▉         | 20/204 [00:00<00:05, 36.11it/s] 15%|█▍        | 30/204 [00:00<00:03, 50.39it/s] 20%|█▉        | 40/204 [00:00<00:02, 61.97it/s] 25%|██▍       | 50/204 [00:01<00:02, 69.92it/s] 29%|██▉       | 59/204 [00:01<00:01, 73.44it/s] 33%|███▎      | 68/204 [00:01<00:02, 65.40it/s] 37%|███▋      | 76/204 [00:01<00:01, 68.69it/s] 42%|████▏     | 86/204 [00:01<00:01, 74.92it/s] 47%|████▋     | 95/204 [00:01<00:01, 64.27it/s] 50%|█████     | 103/204 [00:01<00:01, 64.14it/s] 54%|█████▍    | 111/204 [00:02<00:01, 64.82it/s] 58%|█████▊    | 118/204 [00:02<00:01, 58.79it/s] 61%|██████▏   | 125/204 [00:02<00:01, 59.23it/s] 65%|██████▍   | 132/204 [00:02<00:01, 60.22it/s] 70%|██████▉   | 142/204 [00:02<00:00, 69.39it/s] 75%|███████▍  | 152/204 [00:02<00:00, 76.34it/s] 79%|███████▉  | 162/204 [00:02<00:00, 81.24it/s] 84%|████████▍ | 172/204 [00:02<00:00, 84.89it/s] 89%|████████▉ | 182/204 [00:02<00:00, 88.29it/s] 94%|█████████▍| 192/204 [00:03<00:00, 90.86it/s] 99%|█████████▉| 202/204 [00:03<00:00, 92.47it/s]100%|██████████| 204/204 [00:03<00:00, 64.44it/s]
26032 images processed, 3.206949472427368 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:13,  1.06it/s]  9%|▉         | 7/79 [00:01<00:08,  8.75it/s] 20%|██        | 16/79 [00:01<00:02, 21.34it/s] 30%|███       | 24/79 [00:01<00:01, 31.87it/s] 42%|████▏     | 33/79 [00:01<00:01, 43.49it/s] 54%|█████▍    | 43/79 [00:01<00:00, 55.50it/s] 66%|██████▌   | 52/79 [00:01<00:00, 56.51it/s] 78%|███████▊  | 62/79 [00:01<00:00, 65.76it/s] 91%|█████████ | 72/79 [00:01<00:00, 72.37it/s]100%|██████████| 79/79 [00:03<00:00, 23.49it/s]
10000 images processed, 3.4585087299346924 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:43,  1.79it/s] 10%|█         | 8/79 [00:00<00:04, 15.59it/s] 23%|██▎       | 18/79 [00:00<00:01, 33.96it/s] 35%|███▌      | 28/79 [00:00<00:01, 48.76it/s] 48%|████▊     | 38/79 [00:00<00:00, 60.37it/s] 61%|██████    | 48/79 [00:01<00:00, 69.04it/s] 73%|███████▎  | 58/79 [00:01<00:00, 76.44it/s] 86%|████████▌ | 68/79 [00:01<00:00, 82.23it/s] 99%|█████████▊| 78/79 [00:01<00:00, 86.51it/s]100%|██████████| 79/79 [00:01<00:00, 56.09it/s]
10000 images processed, 1.4293570518493652 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:47,  1.44it/s]  9%|▊         | 6/70 [00:00<00:06,  9.45it/s] 20%|██        | 14/70 [00:00<00:02, 22.81it/s] 34%|███▍      | 24/70 [00:01<00:01, 38.55it/s] 49%|████▊     | 34/70 [00:01<00:00, 51.98it/s] 63%|██████▎   | 44/70 [00:01<00:00, 62.89it/s] 77%|███████▋  | 54/70 [00:01<00:00, 71.80it/s] 91%|█████████▏| 64/70 [00:01<00:00, 78.68it/s]100%|██████████| 70/70 [00:01<00:00, 46.39it/s]
8925 images processed, 1.5407953262329102 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:48,  1.09s/it]  7%|▋         | 3/45 [00:01<00:13,  3.11it/s] 27%|██▋       | 12/45 [00:01<00:02, 15.31it/s] 38%|███▊      | 17/45 [00:01<00:01, 16.51it/s] 47%|████▋     | 21/45 [00:01<00:01, 18.13it/s] 62%|██████▏   | 28/45 [00:01<00:00, 26.68it/s] 73%|███████▎  | 33/45 [00:02<00:00, 16.91it/s] 93%|█████████▎| 42/45 [00:02<00:00, 26.39it/s]100%|██████████| 45/45 [00:02<00:00, 17.88it/s]
5640 images processed, 2.541088581085205 seconds used

21.479174613952637
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.55  99.04  73.57
places365     49.39  89.47  51.31
LSUN          25.67  95.16  75.96
iSUN          25.79  95.00  73.64
dtd           24.45  94.42  71.69
AVG           25.77  94.62  69.23
[incremental] Overall: 0.8500 New: 0.8500 Old: nan
[incremental] Final(Top-1): 0.8500  Average: 0.8500
2.0618767738342285
==== Stage 2: inc={66,67,88,94,57}; seen={0,8,11,40,51}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='retain_only', palm_retain_only=False, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage1', lora_stack=False, lora_orth_enable=True, lora_orth_lambda=1.0, lora_orth_ref_paths='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage1', forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='66,67,88,94,57', forget_classes_seen='0,8,11,40,51', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage1
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/shaokun/PALM/trainer.py:271: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
  2%|▏         | 1/50 [00:07<06:08,  7.51s/it]  4%|▍         | 2/50 [00:08<02:45,  3.46s/it]  6%|▌         | 3/50 [00:08<01:44,  2.23s/it]  8%|▊         | 4/50 [00:09<01:15,  1.63s/it] 10%|█         | 5/50 [00:10<00:58,  1.29s/it] 12%|█▏        | 6/50 [00:10<00:46,  1.05s/it] 14%|█▍        | 7/50 [00:11<00:40,  1.06it/s] 16%|█▌        | 8/50 [00:12<00:35,  1.18it/s] 18%|█▊        | 9/50 [00:12<00:30,  1.34it/s] 20%|██        | 10/50 [00:13<00:28,  1.42it/s] 22%|██▏       | 11/50 [00:13<00:26,  1.49it/s] 24%|██▍       | 12/50 [00:14<00:25,  1.50it/s] 26%|██▌       | 13/50 [00:15<00:24,  1.54it/s] 28%|██▊       | 14/50 [00:15<00:22,  1.57it/s] 30%|███       | 15/50 [00:16<00:21,  1.59it/s] 32%|███▏      | 16/50 [00:17<00:22,  1.54it/s] 34%|███▍      | 17/50 [00:17<00:21,  1.57it/s] 36%|███▌      | 18/50 [00:18<00:20,  1.55it/s] 38%|███▊      | 19/50 [00:19<00:19,  1.58it/s] 40%|████      | 20/50 [00:19<00:18,  1.59it/s] 42%|████▏     | 21/50 [00:20<00:18,  1.59it/s] 44%|████▍     | 22/50 [00:20<00:17,  1.59it/s] 46%|████▌     | 23/50 [00:21<00:16,  1.63it/s] 48%|████▊     | 24/50 [00:22<00:16,  1.54it/s] 50%|█████     | 25/50 [00:22<00:15,  1.58it/s] 52%|█████▏    | 26/50 [00:23<00:15,  1.59it/s] 54%|█████▍    | 27/50 [00:24<00:14,  1.58it/s][loss] ep 0 it 0 total=1.0056 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0056 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 2 it 10 total=1.0056 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0056 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 5 it 0 total=1.0054 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0054 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 7 it 10 total=1.0050 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0050 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 10 it 0 total=1.0046 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0046 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 12 it 10 total=1.0041 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0041 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 15 it 0 total=1.0035 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0035 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 17 it 10 total=1.0031 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0031 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 20 it 0 total=1.0026 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0026 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 22 it 10 total=1.0022 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0022 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 25 it 0 total=1.0018 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0018 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 27 it 10 total=1.0014 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0014 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
 56%|█████▌    | 28/50 [00:24<00:13,  1.60it/s] 58%|█████▊    | 29/50 [00:25<00:13,  1.57it/s] 60%|██████    | 30/50 [00:25<00:12,  1.58it/s] 62%|██████▏   | 31/50 [00:26<00:12,  1.52it/s] 64%|██████▍   | 32/50 [00:27<00:12,  1.47it/s] 66%|██████▌   | 33/50 [00:28<00:11,  1.44it/s] 68%|██████▊   | 34/50 [00:28<00:11,  1.38it/s] 70%|███████   | 35/50 [00:29<00:10,  1.43it/s] 72%|███████▏  | 36/50 [00:30<00:09,  1.46it/s] 74%|███████▍  | 37/50 [00:31<00:09,  1.36it/s] 76%|███████▌  | 38/50 [00:31<00:08,  1.39it/s] 78%|███████▊  | 39/50 [00:32<00:07,  1.44it/s] 80%|████████  | 40/50 [00:33<00:07,  1.42it/s] 82%|████████▏ | 41/50 [00:33<00:06,  1.42it/s] 84%|████████▍ | 42/50 [00:34<00:05,  1.47it/s] 86%|████████▌ | 43/50 [00:35<00:04,  1.52it/s] 88%|████████▊ | 44/50 [00:35<00:04,  1.50it/s] 90%|█████████ | 45/50 [00:36<00:03,  1.48it/s] 92%|█████████▏| 46/50 [00:37<00:02,  1.49it/s] 94%|█████████▍| 47/50 [00:37<00:02,  1.49it/s] 96%|█████████▌| 48/50 [00:38<00:01,  1.52it/s] 98%|█████████▊| 49/50 [00:39<00:00,  1.56it/s]100%|██████████| 50/50 [00:39<00:00,  1.54it/s]100%|██████████| 50/50 [00:39<00:00,  1.26it/s]
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 30 it 0 total=1.0011 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0011 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 32 it 10 total=1.0009 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0009 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 35 it 0 total=1.0007 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0007 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 37 it 10 total=1.0006 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0006 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 40 it 0 total=1.0005 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0005 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 42 it 10 total=1.0004 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0004 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 45 it 0 total=1.0004 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0004 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[loss] ep 47 it 10 total=1.0004 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0004 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stage2-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<03:32,  2.02it/s]  3%|▎         | 11/430 [00:00<00:17, 23.78it/s]  5%|▍         | 21/430 [00:00<00:09, 41.55it/s]  7%|▋         | 31/430 [00:00<00:07, 55.84it/s] 10%|▉         | 41/430 [00:00<00:05, 66.90it/s] 12%|█▏        | 51/430 [00:01<00:05, 75.01it/s] 14%|█▍        | 61/430 [00:01<00:04, 79.12it/s] 17%|█▋        | 71/430 [00:01<00:04, 83.48it/s] 19%|█▉        | 81/430 [00:01<00:04, 86.87it/s] 21%|██        | 91/430 [00:01<00:03, 89.06it/s] 23%|██▎       | 101/430 [00:01<00:03, 90.65it/s] 26%|██▌       | 111/430 [00:01<00:03, 91.45it/s] 28%|██▊       | 121/430 [00:01<00:03, 92.78it/s] 30%|███       | 131/430 [00:01<00:03, 93.42it/s] 33%|███▎      | 141/430 [00:01<00:03, 94.15it/s] 35%|███▌      | 151/430 [00:02<00:03, 92.69it/s] 37%|███▋      | 161/430 [00:02<00:02, 93.35it/s] 40%|███▉      | 171/430 [00:02<00:02, 93.98it/s] 42%|████▏     | 181/430 [00:02<00:02, 94.78it/s] 44%|████▍     | 191/430 [00:02<00:02, 95.03it/s] 47%|████▋     | 201/430 [00:02<00:02, 95.35it/s] 49%|████▉     | 211/430 [00:02<00:02, 95.69it/s] 51%|█████▏    | 221/430 [00:02<00:02, 95.86it/s] 54%|█████▎    | 231/430 [00:02<00:02, 96.00it/s] 56%|█████▌    | 241/430 [00:03<00:01, 95.94it/s] 58%|█████▊    | 251/430 [00:03<00:01, 93.37it/s] 61%|██████    | 261/430 [00:03<00:01, 93.40it/s] 63%|██████▎   | 271/430 [00:03<00:01, 88.43it/s] 65%|██████▌   | 281/430 [00:03<00:01, 90.59it/s] 68%|██████▊   | 291/430 [00:03<00:01, 92.35it/s] 70%|███████   | 301/430 [00:03<00:01, 92.46it/s] 72%|███████▏  | 311/430 [00:03<00:01, 93.60it/s] 75%|███████▍  | 321/430 [00:03<00:01, 93.62it/s] 77%|███████▋  | 331/430 [00:03<00:01, 94.50it/s] 79%|███████▉  | 341/430 [00:04<00:00, 93.86it/s] 82%|████████▏ | 351/430 [00:04<00:00, 94.62it/s] 84%|████████▍ | 361/430 [00:04<00:00, 94.92it/s] 86%|████████▋ | 371/430 [00:04<00:00, 94.95it/s] 89%|████████▊ | 381/430 [00:04<00:00, 95.36it/s] 91%|█████████ | 391/430 [00:04<00:00, 94.04it/s] 93%|█████████▎| 401/430 [00:04<00:00, 89.13it/s] 96%|█████████▌| 411/430 [00:04<00:00, 91.53it/s] 98%|█████████▊| 421/430 [00:04<00:00, 93.41it/s]100%|██████████| 430/430 [00:05<00:00, 84.87it/s]
55000 images processed, 5.10892653465271 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:46,  1.84it/s] 13%|█▎        | 11/86 [00:00<00:03, 21.95it/s] 24%|██▍       | 21/86 [00:00<00:01, 39.12it/s] 36%|███▌      | 31/86 [00:00<00:01, 52.74it/s] 48%|████▊     | 41/86 [00:00<00:00, 63.74it/s] 59%|█████▉    | 51/86 [00:01<00:00, 71.69it/s] 71%|███████   | 61/86 [00:01<00:00, 78.52it/s] 83%|████████▎ | 71/86 [00:01<00:00, 83.73it/s] 94%|█████████▍| 81/86 [00:01<00:00, 87.48it/s]100%|██████████| 86/86 [00:01<00:00, 59.52it/s]
11000 images processed, 1.4606671333312988 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:54,  1.78it/s]  3%|▎         | 7/204 [00:00<00:14, 13.54it/s]  8%|▊         | 17/204 [00:00<00:05, 32.35it/s] 13%|█▎        | 27/204 [00:00<00:03, 48.00it/s] 18%|█▊        | 37/204 [00:00<00:02, 60.05it/s] 23%|██▎       | 46/204 [00:01<00:02, 67.86it/s] 27%|██▋       | 56/204 [00:01<00:01, 75.25it/s] 32%|███▏      | 66/204 [00:01<00:01, 81.13it/s] 37%|███▋      | 76/204 [00:01<00:01, 84.26it/s] 42%|████▏     | 86/204 [00:01<00:01, 87.34it/s] 47%|████▋     | 96/204 [00:01<00:01, 89.05it/s] 52%|█████▏    | 106/204 [00:01<00:01, 90.70it/s] 57%|█████▋    | 116/204 [00:01<00:00, 89.94it/s] 62%|██████▏   | 126/204 [00:01<00:00, 91.31it/s] 67%|██████▋   | 136/204 [00:02<00:00, 92.02it/s] 72%|███████▏  | 146/204 [00:02<00:00, 92.70it/s] 76%|███████▋  | 156/204 [00:02<00:00, 93.47it/s] 81%|████████▏ | 166/204 [00:02<00:00, 93.68it/s] 86%|████████▋ | 176/204 [00:02<00:00, 94.28it/s] 91%|█████████ | 186/204 [00:02<00:00, 95.12it/s] 96%|█████████▌| 196/204 [00:02<00:00, 95.74it/s]100%|██████████| 204/204 [00:02<00:00, 73.88it/s]
26032 images processed, 2.801238775253296 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:53,  1.45it/s]  9%|▉         | 7/79 [00:00<00:06, 11.39it/s] 20%|██        | 16/79 [00:00<00:02, 26.52it/s] 33%|███▎      | 26/79 [00:01<00:01, 41.90it/s] 44%|████▍     | 35/79 [00:01<00:00, 53.07it/s] 57%|█████▋    | 45/79 [00:01<00:00, 63.66it/s] 70%|██████▉   | 55/79 [00:01<00:00, 72.10it/s] 82%|████████▏ | 65/79 [00:01<00:00, 78.75it/s] 95%|█████████▍| 75/79 [00:01<00:00, 83.92it/s]100%|██████████| 79/79 [00:01<00:00, 50.18it/s]
10000 images processed, 1.5980184078216553 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:44,  1.76it/s] 14%|█▍        | 11/79 [00:00<00:03, 21.10it/s] 27%|██▋       | 21/79 [00:00<00:01, 38.01it/s] 39%|███▉      | 31/79 [00:00<00:00, 51.51it/s] 52%|█████▏    | 41/79 [00:00<00:00, 62.61it/s] 65%|██████▍   | 51/79 [00:01<00:00, 71.17it/s] 77%|███████▋  | 61/79 [00:01<00:00, 78.07it/s] 90%|████████▉ | 71/79 [00:01<00:00, 83.17it/s]100%|██████████| 79/79 [00:01<00:00, 56.86it/s]
10000 images processed, 1.413724422454834 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:43,  1.57it/s] 16%|█▌        | 11/70 [00:00<00:03, 19.28it/s] 30%|███       | 21/70 [00:00<00:01, 35.33it/s] 44%|████▍     | 31/70 [00:00<00:00, 49.06it/s] 59%|█████▊    | 41/70 [00:01<00:00, 60.48it/s] 73%|███████▎  | 51/70 [00:01<00:00, 69.93it/s] 87%|████████▋ | 61/70 [00:01<00:00, 77.26it/s]100%|██████████| 70/70 [00:01<00:00, 51.14it/s]
8925 images processed, 1.397829532623291 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:46,  1.06s/it]  7%|▋         | 3/45 [00:01<00:13,  3.16it/s] 16%|█▌        | 7/45 [00:01<00:05,  7.50it/s] 38%|███▊      | 17/45 [00:01<00:01, 19.08it/s] 51%|█████     | 23/45 [00:01<00:01, 17.85it/s] 73%|███████▎  | 33/45 [00:02<00:00, 24.09it/s] 87%|████████▋ | 39/45 [00:02<00:00, 25.81it/s]100%|██████████| 45/45 [00:02<00:00, 18.47it/s]
5640 images processed, 2.464268445968628 seconds used

17.893195867538452
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.79  98.86  78.72
places365     52.73  85.08  47.05
LSUN          44.68  89.26  61.89
iSUN          35.83  91.28  65.27
dtd           32.80  92.10  73.35
AVG           33.97  91.32  65.25
[incremental] Overall: 0.7410 New: 0.7220 Old: 0.7600
[incremental] Final(Top-1): 0.7410  Average: 0.7955
2.3719146251678467
==== Stage 3: inc={59,58,44,93,10}; seen={0,8,11,40,51,66,67,88,94,57}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='retain_only', palm_retain_only=False, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2', lora_stack=False, lora_orth_enable=True, lora_orth_lambda=1.0, lora_orth_ref_paths='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage1,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2', forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='59,58,44,93,10', forget_classes_seen='0,8,11,40,51,66,67,88,94,57', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/shaokun/PALM/trainer.py:271: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
  2%|▏         | 1/50 [00:10<08:56, 10.95s/it]  4%|▍         | 2/50 [00:11<04:05,  5.11s/it]  6%|▌         | 3/50 [00:13<02:35,  3.30s/it]  8%|▊         | 4/50 [00:14<01:51,  2.42s/it] 10%|█         | 5/50 [00:15<01:27,  1.94s/it] 12%|█▏        | 6/50 [00:16<01:10,  1.60s/it] 14%|█▍        | 7/50 [00:17<01:01,  1.43s/it] 16%|█▌        | 8/50 [00:18<00:58,  1.38s/it] 18%|█▊        | 9/50 [00:19<00:53,  1.31s/it] 20%|██        | 10/50 [00:20<00:49,  1.24s/it] 22%|██▏       | 11/50 [00:21<00:46,  1.19s/it] 24%|██▍       | 12/50 [00:22<00:42,  1.13s/it] 26%|██▌       | 13/50 [00:23<00:40,  1.10s/it] 28%|██▊       | 14/50 [00:25<00:40,  1.12s/it] 30%|███       | 15/50 [00:26<00:38,  1.09s/it] 32%|███▏      | 16/50 [00:27<00:36,  1.08s/it] 34%|███▍      | 17/50 [00:28<00:34,  1.04s/it] 36%|███▌      | 18/50 [00:29<00:33,  1.05s/it] 38%|███▊      | 19/50 [00:30<00:32,  1.06s/it] 40%|████      | 20/50 [00:31<00:30,  1.03s/it] 42%|████▏     | 21/50 [00:32<00:29,  1.01s/it] 44%|████▍     | 22/50 [00:33<00:28,  1.03s/it] 46%|████▌     | 23/50 [00:34<00:28,  1.04s/it] 48%|████▊     | 24/50 [00:35<00:26,  1.02s/it] 50%|█████     | 25/50 [00:36<00:25,  1.04s/it] 52%|█████▏    | 26/50 [00:37<00:24,  1.03s/it] 54%|█████▍    | 27/50 [00:38<00:24,  1.08s/it][loss] ep 0 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 2 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 5 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 7 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 10 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 12 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 15 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 17 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 20 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 22 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 25 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 27 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
 56%|█████▌    | 28/50 [00:39<00:23,  1.06s/it] 58%|█████▊    | 29/50 [00:40<00:21,  1.04s/it] 60%|██████    | 30/50 [00:41<00:20,  1.04s/it] 62%|██████▏   | 31/50 [00:42<00:19,  1.03s/it] 64%|██████▍   | 32/50 [00:43<00:18,  1.05s/it] 66%|██████▌   | 33/50 [00:44<00:17,  1.05s/it] 68%|██████▊   | 34/50 [00:45<00:17,  1.07s/it] 70%|███████   | 35/50 [00:46<00:16,  1.07s/it] 72%|███████▏  | 36/50 [00:48<00:14,  1.07s/it] 74%|███████▍  | 37/50 [00:49<00:13,  1.08s/it] 76%|███████▌  | 38/50 [00:50<00:12,  1.03s/it] 78%|███████▊  | 39/50 [00:51<00:11,  1.05s/it] 80%|████████  | 40/50 [00:52<00:10,  1.03s/it] 82%|████████▏ | 41/50 [00:53<00:09,  1.04s/it] 84%|████████▍ | 42/50 [00:54<00:08,  1.06s/it] 86%|████████▌ | 43/50 [00:55<00:07,  1.11s/it] 88%|████████▊ | 44/50 [00:56<00:07,  1.18s/it] 90%|█████████ | 45/50 [00:58<00:05,  1.18s/it] 92%|█████████▏| 46/50 [00:59<00:04,  1.14s/it] 94%|█████████▍| 47/50 [01:00<00:03,  1.17s/it] 96%|█████████▌| 48/50 [01:01<00:02,  1.13s/it] 98%|█████████▊| 49/50 [01:02<00:01,  1.14s/it]100%|██████████| 50/50 [01:03<00:00,  1.12s/it]100%|██████████| 50/50 [01:03<00:00,  1.27s/it]
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 30 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 32 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 35 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 37 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 40 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 42 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 45 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[loss] ep 47 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stage3-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<05:34,  1.28it/s]  1%|          | 5/430 [00:00<00:58,  7.25it/s]  2%|▏         | 9/430 [00:00<00:32, 13.12it/s]  3%|▎         | 15/430 [00:01<00:18, 22.38it/s]  5%|▍         | 21/430 [00:01<00:13, 30.40it/s]  7%|▋         | 28/430 [00:01<00:10, 38.35it/s]  8%|▊         | 34/430 [00:01<00:09, 43.34it/s]  9%|▉         | 40/430 [00:01<00:08, 47.32it/s] 11%|█         | 46/430 [00:01<00:08, 45.81it/s] 12%|█▏        | 52/430 [00:01<00:07, 48.29it/s] 14%|█▎        | 59/430 [00:01<00:06, 53.07it/s] 15%|█▌        | 65/430 [00:01<00:07, 51.82it/s] 17%|█▋        | 72/430 [00:02<00:06, 55.59it/s] 18%|█▊        | 79/430 [00:02<00:06, 54.80it/s] 20%|██        | 86/430 [00:02<00:06, 56.84it/s] 21%|██▏       | 92/430 [00:02<00:06, 56.16it/s] 23%|██▎       | 99/430 [00:02<00:05, 58.29it/s] 24%|██▍       | 105/430 [00:02<00:05, 54.97it/s] 26%|██▌       | 111/430 [00:02<00:06, 48.83it/s] 27%|██▋       | 117/430 [00:02<00:06, 48.13it/s] 28%|██▊       | 122/430 [00:03<00:06, 48.16it/s] 30%|██▉       | 127/430 [00:03<00:06, 47.56it/s] 31%|███       | 134/430 [00:03<00:05, 51.97it/s] 33%|███▎      | 140/430 [00:03<00:05, 53.51it/s] 34%|███▍      | 147/430 [00:03<00:05, 56.39it/s] 36%|███▌      | 153/430 [00:03<00:05, 54.04it/s] 37%|███▋      | 159/430 [00:03<00:05, 47.36it/s] 38%|███▊      | 164/430 [00:03<00:05, 46.76it/s] 39%|███▉      | 169/430 [00:04<00:05, 46.40it/s] 41%|████      | 176/430 [00:04<00:04, 51.36it/s] 42%|████▏     | 182/430 [00:04<00:04, 53.50it/s] 44%|████▎     | 188/430 [00:04<00:04, 50.06it/s] 45%|████▌     | 195/430 [00:04<00:04, 54.16it/s] 47%|████▋     | 201/430 [00:04<00:04, 54.70it/s] 48%|████▊     | 207/430 [00:04<00:04, 55.42it/s] 50%|████▉     | 214/430 [00:04<00:03, 57.85it/s] 51%|█████     | 220/430 [00:04<00:03, 57.89it/s] 53%|█████▎    | 227/430 [00:05<00:03, 58.38it/s] 54%|█████▍    | 234/430 [00:05<00:03, 59.89it/s] 56%|█████▌    | 241/430 [00:05<00:03, 58.11it/s] 58%|█████▊    | 248/430 [00:05<00:03, 59.16it/s] 59%|█████▉    | 254/430 [00:05<00:03, 53.55it/s] 60%|██████    | 260/430 [00:05<00:03, 53.06it/s] 62%|██████▏   | 266/430 [00:05<00:03, 53.04it/s] 63%|██████▎   | 273/430 [00:05<00:02, 54.87it/s] 65%|██████▍   | 279/430 [00:05<00:02, 53.99it/s] 66%|██████▋   | 285/430 [00:06<00:02, 53.80it/s] 68%|██████▊   | 291/430 [00:06<00:02, 52.95it/s] 69%|██████▉   | 297/430 [00:06<00:02, 54.14it/s] 71%|███████   | 304/430 [00:06<00:02, 55.30it/s] 72%|███████▏  | 311/430 [00:06<00:02, 57.26it/s] 74%|███████▎  | 317/430 [00:06<00:01, 57.16it/s] 75%|███████▌  | 324/430 [00:06<00:01, 57.94it/s] 77%|███████▋  | 331/430 [00:06<00:01, 59.96it/s] 79%|███████▊  | 338/430 [00:07<00:01, 59.36it/s] 80%|████████  | 345/430 [00:07<00:01, 57.80it/s] 82%|████████▏ | 351/430 [00:07<00:01, 53.71it/s] 83%|████████▎ | 357/430 [00:07<00:01, 52.08it/s] 85%|████████▍ | 364/430 [00:07<00:01, 55.03it/s] 86%|████████▌ | 370/430 [00:07<00:01, 54.65it/s] 88%|████████▊ | 377/430 [00:07<00:00, 56.41it/s] 89%|████████▉ | 383/430 [00:07<00:00, 55.44it/s] 91%|█████████ | 390/430 [00:07<00:00, 58.14it/s] 92%|█████████▏| 396/430 [00:08<00:00, 55.49it/s] 93%|█████████▎| 402/430 [00:08<00:00, 55.11it/s] 95%|█████████▍| 408/430 [00:08<00:00, 53.90it/s] 96%|█████████▋| 414/430 [00:08<00:00, 53.86it/s] 98%|█████████▊| 421/430 [00:08<00:00, 57.07it/s]100%|█████████▉| 428/430 [00:08<00:00, 59.22it/s]100%|██████████| 430/430 [00:08<00:00, 49.59it/s]
55000 images processed, 8.751641273498535 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<01:18,  1.08it/s]  8%|▊         | 7/86 [00:01<00:08,  8.89it/s] 14%|█▍        | 12/86 [00:01<00:04, 15.20it/s] 21%|██        | 18/86 [00:01<00:03, 22.45it/s] 29%|██▉       | 25/86 [00:01<00:01, 31.45it/s] 36%|███▌      | 31/86 [00:01<00:01, 36.54it/s] 43%|████▎     | 37/86 [00:01<00:01, 41.41it/s] 50%|█████     | 43/86 [00:01<00:00, 45.33it/s] 57%|█████▋    | 49/86 [00:01<00:00, 48.84it/s] 64%|██████▍   | 55/86 [00:01<00:00, 49.81it/s] 72%|███████▏  | 62/86 [00:02<00:00, 53.59it/s] 79%|███████▉  | 68/86 [00:02<00:00, 53.33it/s] 87%|████████▋ | 75/86 [00:02<00:00, 56.85it/s] 95%|█████████▌| 82/86 [00:02<00:00, 56.30it/s]100%|██████████| 86/86 [00:02<00:00, 35.37it/s]
11000 images processed, 2.4476563930511475 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:03,  1.64it/s]  4%|▍         | 8/204 [00:00<00:13, 14.27it/s]  6%|▋         | 13/204 [00:00<00:08, 21.63it/s] 10%|▉         | 20/204 [00:00<00:05, 31.83it/s] 13%|█▎        | 26/204 [00:01<00:04, 37.06it/s] 16%|█▌        | 33/204 [00:01<00:03, 42.76it/s] 19%|█▉        | 39/204 [00:01<00:03, 45.87it/s] 22%|██▏       | 45/204 [00:01<00:04, 39.19it/s] 25%|██▍       | 50/204 [00:01<00:03, 40.12it/s] 28%|██▊       | 58/204 [00:01<00:02, 48.98it/s] 32%|███▏      | 65/204 [00:01<00:02, 53.38it/s] 35%|███▍      | 71/204 [00:01<00:02, 50.28it/s] 38%|███▊      | 77/204 [00:02<00:02, 49.06it/s] 41%|████      | 83/204 [00:02<00:02, 50.70it/s] 44%|████▍     | 90/204 [00:02<00:02, 52.76it/s] 47%|████▋     | 96/204 [00:02<00:02, 53.92it/s] 50%|█████     | 103/204 [00:02<00:01, 55.77it/s] 53%|█████▎    | 109/204 [00:02<00:01, 54.89it/s] 57%|█████▋    | 116/204 [00:02<00:01, 56.09it/s] 60%|██████    | 123/204 [00:02<00:01, 56.84it/s] 63%|██████▎   | 129/204 [00:02<00:01, 56.42it/s] 67%|██████▋   | 136/204 [00:03<00:01, 57.98it/s] 70%|██████▉   | 142/204 [00:03<00:01, 56.29it/s] 73%|███████▎  | 149/204 [00:03<00:00, 56.85it/s] 76%|███████▋  | 156/204 [00:03<00:00, 57.69it/s] 79%|███████▉  | 162/204 [00:03<00:00, 55.42it/s] 82%|████████▏ | 168/204 [00:03<00:00, 51.49it/s] 85%|████████▌ | 174/204 [00:03<00:00, 52.06it/s] 89%|████████▊ | 181/204 [00:03<00:00, 54.68it/s] 92%|█████████▏| 187/204 [00:04<00:00, 55.71it/s] 95%|█████████▍| 193/204 [00:04<00:00, 56.18it/s] 98%|█████████▊| 199/204 [00:04<00:00, 55.88it/s]100%|██████████| 204/204 [00:04<00:00, 46.89it/s]
26032 images processed, 4.393320798873901 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:57,  1.35it/s]  9%|▉         | 7/79 [00:00<00:06, 10.68it/s] 15%|█▌        | 12/79 [00:00<00:03, 17.36it/s] 20%|██        | 16/79 [00:01<00:02, 21.72it/s] 28%|██▊       | 22/79 [00:01<00:01, 28.95it/s] 34%|███▍      | 27/79 [00:01<00:01, 33.75it/s] 41%|████      | 32/79 [00:01<00:01, 37.53it/s] 48%|████▊     | 38/79 [00:01<00:00, 43.01it/s] 56%|█████▌    | 44/79 [00:01<00:00, 47.14it/s] 63%|██████▎   | 50/79 [00:01<00:00, 48.17it/s] 72%|███████▏  | 57/79 [00:01<00:00, 52.68it/s] 80%|███████▉  | 63/79 [00:01<00:00, 54.11it/s] 89%|████████▊ | 70/79 [00:02<00:00, 56.97it/s] 96%|█████████▌| 76/79 [00:02<00:00, 55.37it/s]100%|██████████| 79/79 [00:02<00:00, 31.68it/s]
10000 images processed, 2.5256636142730713 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:02,  1.25it/s]  9%|▉         | 7/79 [00:00<00:07,  9.96it/s] 16%|█▋        | 13/79 [00:01<00:03, 18.50it/s] 24%|██▍       | 19/79 [00:01<00:02, 26.62it/s] 30%|███       | 24/79 [00:01<00:01, 30.51it/s] 37%|███▋      | 29/79 [00:01<00:01, 34.72it/s] 46%|████▌     | 36/79 [00:01<00:01, 42.32it/s] 53%|█████▎    | 42/79 [00:01<00:00, 44.88it/s] 62%|██████▏   | 49/79 [00:01<00:00, 49.64it/s] 70%|██████▉   | 55/79 [00:01<00:00, 50.42it/s] 78%|███████▊  | 62/79 [00:01<00:00, 53.70it/s] 86%|████████▌ | 68/79 [00:02<00:00, 53.09it/s] 94%|█████████▎| 74/79 [00:02<00:00, 53.92it/s]100%|██████████| 79/79 [00:02<00:00, 35.71it/s]
10000 images processed, 2.241786241531372 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:45,  1.51it/s] 10%|█         | 7/70 [00:00<00:05, 11.55it/s] 19%|█▊        | 13/70 [00:00<00:02, 20.63it/s] 27%|██▋       | 19/70 [00:01<00:01, 28.53it/s] 34%|███▍      | 24/70 [00:01<00:01, 28.33it/s] 40%|████      | 28/70 [00:01<00:01, 29.75it/s] 46%|████▌     | 32/70 [00:01<00:01, 31.63it/s] 53%|█████▎    | 37/70 [00:01<00:00, 35.24it/s] 61%|██████▏   | 43/70 [00:01<00:00, 40.06it/s] 71%|███████▏  | 50/70 [00:01<00:00, 46.85it/s] 80%|████████  | 56/70 [00:01<00:00, 47.84it/s] 90%|█████████ | 63/70 [00:01<00:00, 52.56it/s] 99%|█████████▊| 69/70 [00:02<00:00, 51.62it/s]100%|██████████| 70/70 [00:02<00:00, 33.26it/s]
8925 images processed, 2.1419620513916016 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:54,  1.23s/it]  4%|▍         | 2/45 [00:01<00:25,  1.70it/s] 18%|█▊        | 8/45 [00:01<00:04,  8.95it/s] 31%|███       | 14/45 [00:01<00:01, 16.39it/s] 40%|████      | 18/45 [00:01<00:01, 14.58it/s] 56%|█████▌    | 25/45 [00:02<00:00, 22.44it/s] 69%|██████▉   | 31/45 [00:02<00:00, 28.32it/s] 80%|████████  | 36/45 [00:02<00:00, 18.00it/s] 96%|█████████▌| 43/45 [00:02<00:00, 24.74it/s]100%|██████████| 45/45 [00:02<00:00, 15.82it/s]
5640 images processed, 2.8942699432373047 seconds used

27.131486654281616
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.89  98.75  81.54
places365     61.06  83.04  47.69
LSUN          45.97  89.12  67.91
iSUN          40.83  90.51  71.01
dtd           35.60  90.86  75.62
AVG           37.47  90.46  68.75
[incremental] Overall: 0.6980 New: 0.7040 Old: 0.6950
[incremental] Final(Top-1): 0.6980  Average: 0.7630
3.454977512359619
==== Stage 4: inc={64,22,42,9,90}; seen={0,8,11,40,51,66,67,88,94,57,59,58,44,93,10}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='retain_only', palm_retain_only=False, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage4', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3', lora_stack=False, lora_orth_enable=True, lora_orth_lambda=1.0, lora_orth_ref_paths='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage1,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3', forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='64,22,42,9,90', forget_classes_seen='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/shaokun/PALM/trainer.py:271: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
  2%|▏         | 1/50 [00:11<09:08, 11.19s/it]  4%|▍         | 2/50 [00:12<04:12,  5.26s/it]  6%|▌         | 3/50 [00:13<02:37,  3.36s/it]  8%|▊         | 4/50 [00:14<01:52,  2.44s/it] 10%|█         | 5/50 [00:15<01:27,  1.95s/it] 12%|█▏        | 6/50 [00:16<01:13,  1.66s/it] 14%|█▍        | 7/50 [00:17<01:03,  1.48s/it] 16%|█▌        | 8/50 [00:18<00:56,  1.35s/it] 18%|█▊        | 9/50 [00:19<00:52,  1.28s/it] 20%|██        | 10/50 [00:21<00:49,  1.23s/it] 22%|██▏       | 11/50 [00:22<00:45,  1.17s/it] 24%|██▍       | 12/50 [00:23<00:43,  1.16s/it] 26%|██▌       | 13/50 [00:24<00:41,  1.13s/it] 28%|██▊       | 14/50 [00:25<00:40,  1.12s/it] 30%|███       | 15/50 [00:26<00:39,  1.13s/it] 32%|███▏      | 16/50 [00:27<00:37,  1.09s/it] 34%|███▍      | 17/50 [00:28<00:34,  1.03s/it] 36%|███▌      | 18/50 [00:29<00:33,  1.06s/it] 38%|███▊      | 19/50 [00:30<00:32,  1.05s/it] 40%|████      | 20/50 [00:31<00:31,  1.05s/it] 42%|████▏     | 21/50 [00:32<00:30,  1.04s/it] 44%|████▍     | 22/50 [00:33<00:29,  1.05s/it] 46%|████▌     | 23/50 [00:34<00:29,  1.08s/it] 48%|████▊     | 24/50 [00:35<00:28,  1.10s/it] 50%|█████     | 25/50 [00:37<00:27,  1.09s/it] 52%|█████▏    | 26/50 [00:38<00:26,  1.11s/it] 54%|█████▍    | 27/50 [00:39<00:26,  1.14s/it] 56%|█████▌    | 28/50 [00:40<00:24,  1.11s/it] 58%|█████▊    | 29/50 [00:41<00:23,  1.10s/it] 60%|██████    | 30/50 [00:42<00:21,  1.09s/it] 62%|██████▏   | 31/50 [00:43<00:20,  1.06s/it] 64%|██████▍   | 32/50 [00:44<00:19,  1.07s/it] 66%|██████▌   | 33/50 [00:45<00:18,  1.08s/it] 68%|██████▊   | 34/50 [00:46<00:16,  1.05s/it] 70%|███████   | 35/50 [00:47<00:15,  1.06s/it] 72%|███████▏  | 36/50 [00:48<00:14,  1.07s/it] 74%|███████▍  | 37/50 [00:50<00:14,  1.10s/it] 76%|███████▌  | 38/50 [00:51<00:13,  1.09s/it] 78%|███████▊  | 39/50 [00:52<00:12,  1.13s/it] 80%|████████  | 40/50 [00:53<00:10,  1.10s/it] 82%|████████▏ | 41/50 [00:54<00:09,  1.10s/it] 84%|████████▍ | 42/50 [00:55<00:08,  1.08s/it] 86%|████████▌ | 43/50 [00:56<00:07,  1.07s/it] 88%|████████▊ | 44/50 [00:57<00:06,  1.06s/it] 90%|█████████ | 45/50 [00:58<00:05,  1.09s/it] 92%|█████████▏| 46/50 [00:59<00:04,  1.09s/it] 94%|█████████▍| 47/50 [01:00<00:03,  1.07s/it] 96%|█████████▌| 48/50 [01:01<00:02,  1.06s/it] 98%|█████████▊| 49/50 [01:03<00:01,  1.08s/it]100%|██████████| 50/50 [01:04<00:00,  1.06s/it]100%|██████████| 50/50 [01:04<00:00,  1.28s/it]
[loss] ep 0 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage4
[loss] ep 2 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 5 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 7 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 10 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage4
[loss] ep 12 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage4
[loss] ep 15 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage4
[loss] ep 17 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 20 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage4
[loss] ep 22 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage4
[loss] ep 25 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage4
[loss] ep 27 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 30 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 32 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 35 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 37 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 40 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 42 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 45 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 47 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage4
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stage4-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<04:38,  1.54it/s]  2%|▏         | 7/430 [00:00<00:36, 11.59it/s]  3%|▎         | 14/430 [00:00<00:18, 22.79it/s]  5%|▍         | 20/430 [00:00<00:13, 30.85it/s]  6%|▌         | 26/430 [00:01<00:11, 36.72it/s]  8%|▊         | 33/430 [00:01<00:09, 44.04it/s]  9%|▉         | 39/430 [00:01<00:08, 45.80it/s] 11%|█         | 46/430 [00:01<00:07, 51.02it/s] 12%|█▏        | 53/430 [00:01<00:06, 54.51it/s] 14%|█▎        | 59/430 [00:01<00:06, 54.91it/s] 15%|█▌        | 65/430 [00:01<00:06, 56.18it/s] 17%|█▋        | 72/430 [00:01<00:06, 58.95it/s] 18%|█▊        | 79/430 [00:01<00:06, 57.34it/s] 20%|██        | 86/430 [00:02<00:05, 58.70it/s] 21%|██▏       | 92/430 [00:02<00:05, 56.77it/s] 23%|██▎       | 99/430 [00:02<00:05, 58.86it/s] 25%|██▍       | 106/430 [00:02<00:05, 57.48it/s] 26%|██▋       | 113/430 [00:02<00:05, 59.47it/s] 28%|██▊       | 119/430 [00:02<00:05, 59.54it/s] 29%|██▉       | 125/430 [00:02<00:05, 57.78it/s] 31%|███       | 132/430 [00:02<00:05, 59.57it/s] 32%|███▏      | 139/430 [00:02<00:04, 61.83it/s] 34%|███▍      | 146/430 [00:03<00:04, 62.10it/s] 36%|███▌      | 153/430 [00:03<00:04, 59.60it/s] 37%|███▋      | 160/430 [00:03<00:04, 59.09it/s] 39%|███▊      | 166/430 [00:03<00:04, 58.46it/s] 40%|████      | 172/430 [00:03<00:04, 58.22it/s] 41%|████▏     | 178/430 [00:03<00:04, 51.97it/s] 43%|████▎     | 185/430 [00:03<00:04, 55.69it/s] 45%|████▍     | 192/430 [00:03<00:04, 56.56it/s] 46%|████▋     | 199/430 [00:04<00:03, 58.66it/s] 48%|████▊     | 205/430 [00:04<00:03, 57.88it/s] 49%|████▉     | 212/430 [00:04<00:03, 58.04it/s] 51%|█████     | 218/430 [00:04<00:03, 57.76it/s] 52%|█████▏    | 225/430 [00:04<00:03, 58.25it/s] 54%|█████▎    | 231/430 [00:04<00:03, 58.39it/s] 55%|█████▌    | 237/430 [00:04<00:03, 58.77it/s] 57%|█████▋    | 243/430 [00:04<00:03, 54.74it/s] 58%|█████▊    | 250/430 [00:04<00:03, 55.69it/s] 60%|█████▉    | 256/430 [00:05<00:03, 56.24it/s] 61%|██████    | 263/430 [00:05<00:02, 58.81it/s] 63%|██████▎   | 270/430 [00:05<00:02, 58.62it/s] 64%|██████▍   | 276/430 [00:05<00:02, 58.10it/s] 66%|██████▌   | 282/430 [00:05<00:02, 57.10it/s] 67%|██████▋   | 288/430 [00:05<00:02, 56.43it/s] 69%|██████▊   | 295/430 [00:05<00:02, 58.18it/s] 70%|███████   | 301/430 [00:05<00:02, 57.67it/s] 72%|███████▏  | 308/430 [00:05<00:02, 57.77it/s] 73%|███████▎  | 314/430 [00:06<00:02, 57.03it/s] 74%|███████▍  | 320/430 [00:06<00:01, 57.14it/s] 76%|███████▌  | 327/430 [00:06<00:01, 58.00it/s] 78%|███████▊  | 335/430 [00:06<00:01, 62.67it/s] 80%|███████▉  | 342/430 [00:06<00:01, 64.04it/s] 81%|████████  | 349/430 [00:06<00:01, 64.26it/s] 83%|████████▎ | 356/430 [00:06<00:01, 61.63it/s] 84%|████████▍ | 363/430 [00:06<00:01, 61.88it/s] 86%|████████▌ | 370/430 [00:06<00:01, 59.81it/s] 88%|████████▊ | 377/430 [00:07<00:00, 61.09it/s] 89%|████████▉ | 384/430 [00:07<00:00, 60.96it/s] 91%|█████████ | 391/430 [00:07<00:00, 61.11it/s] 93%|█████████▎| 398/430 [00:07<00:00, 59.32it/s] 94%|█████████▍| 405/430 [00:07<00:00, 60.48it/s] 96%|█████████▌| 412/430 [00:07<00:00, 59.08it/s] 97%|█████████▋| 419/430 [00:07<00:00, 60.98it/s] 99%|█████████▉| 426/430 [00:07<00:00, 60.36it/s]100%|██████████| 430/430 [00:07<00:00, 54.15it/s]
55000 images processed, 8.001126289367676 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:51,  1.65it/s]  9%|▉         | 8/86 [00:00<00:05, 14.32it/s] 16%|█▋        | 14/86 [00:00<00:03, 23.21it/s] 23%|██▎       | 20/86 [00:00<00:02, 31.30it/s] 30%|███       | 26/86 [00:01<00:01, 36.77it/s] 38%|███▊      | 33/86 [00:01<00:01, 43.61it/s] 45%|████▌     | 39/86 [00:01<00:01, 45.57it/s] 52%|█████▏    | 45/86 [00:01<00:00, 43.34it/s] 59%|█████▉    | 51/86 [00:01<00:00, 45.42it/s] 66%|██████▋   | 57/86 [00:01<00:00, 47.88it/s] 74%|███████▍  | 64/86 [00:01<00:00, 52.67it/s] 83%|████████▎ | 71/86 [00:01<00:00, 53.26it/s] 91%|█████████ | 78/86 [00:02<00:00, 56.41it/s] 98%|█████████▊| 84/86 [00:02<00:00, 57.35it/s]100%|██████████| 86/86 [00:02<00:00, 39.88it/s]
11000 images processed, 2.1737709045410156 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:40,  1.26it/s]  2%|▏         | 5/204 [00:00<00:27,  7.12it/s]  5%|▌         | 11/204 [00:01<00:11, 16.45it/s]  8%|▊         | 16/204 [00:01<00:08, 22.96it/s] 11%|█▏        | 23/204 [00:01<00:05, 32.13it/s] 14%|█▎        | 28/204 [00:01<00:04, 35.85it/s] 17%|█▋        | 34/204 [00:01<00:04, 40.90it/s] 20%|█▉        | 40/204 [00:01<00:03, 45.27it/s] 23%|██▎       | 46/204 [00:01<00:03, 49.06it/s] 25%|██▌       | 52/204 [00:01<00:02, 51.30it/s] 28%|██▊       | 58/204 [00:01<00:02, 50.06it/s] 31%|███▏      | 64/204 [00:01<00:02, 50.43it/s] 34%|███▍      | 70/204 [00:02<00:02, 52.78it/s] 37%|███▋      | 76/204 [00:02<00:02, 50.91it/s] 40%|████      | 82/204 [00:02<00:02, 48.86it/s] 43%|████▎     | 88/204 [00:02<00:02, 50.83it/s] 46%|████▌     | 94/204 [00:02<00:02, 51.61it/s] 49%|████▉     | 100/204 [00:02<00:02, 46.85it/s] 52%|█████▏    | 106/204 [00:02<00:01, 49.05it/s] 55%|█████▍    | 112/204 [00:02<00:01, 50.84it/s] 58%|█████▊    | 118/204 [00:03<00:01, 53.20it/s] 61%|██████    | 124/204 [00:03<00:01, 51.73it/s] 64%|██████▎   | 130/204 [00:03<00:01, 50.57it/s] 67%|██████▋   | 136/204 [00:03<00:01, 46.02it/s] 70%|███████   | 143/204 [00:03<00:01, 49.45it/s] 74%|███████▎  | 150/204 [00:03<00:01, 52.62it/s] 76%|███████▋  | 156/204 [00:03<00:00, 54.29it/s] 79%|███████▉  | 162/204 [00:03<00:00, 54.21it/s] 82%|████████▏ | 168/204 [00:04<00:00, 54.81it/s] 85%|████████▌ | 174/204 [00:04<00:00, 53.89it/s] 88%|████████▊ | 180/204 [00:04<00:00, 55.30it/s] 91%|█████████ | 186/204 [00:04<00:00, 55.78it/s] 95%|█████████▍| 193/204 [00:04<00:00, 57.67it/s] 98%|█████████▊| 199/204 [00:04<00:00, 55.68it/s]100%|██████████| 204/204 [00:04<00:00, 43.83it/s]
26032 images processed, 4.708636283874512 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:01,  1.26it/s]  9%|▉         | 7/79 [00:00<00:07,  9.88it/s] 15%|█▌        | 12/79 [00:01<00:04, 16.56it/s] 22%|██▏       | 17/79 [00:01<00:02, 23.00it/s] 29%|██▉       | 23/79 [00:01<00:01, 30.09it/s] 38%|███▊      | 30/79 [00:01<00:01, 38.27it/s] 46%|████▌     | 36/79 [00:01<00:00, 43.34it/s] 53%|█████▎    | 42/79 [00:01<00:00, 46.76it/s] 61%|██████    | 48/79 [00:01<00:00, 50.27it/s] 68%|██████▊   | 54/79 [00:01<00:00, 51.98it/s] 76%|███████▌  | 60/79 [00:01<00:00, 53.33it/s] 85%|████████▍ | 67/79 [00:01<00:00, 56.47it/s] 92%|█████████▏| 73/79 [00:02<00:00, 55.69it/s]100%|██████████| 79/79 [00:02<00:00, 20.21it/s]100%|██████████| 79/79 [00:02<00:00, 27.73it/s]
10000 images processed, 2.9076921939849854 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:48,  1.60it/s]  8%|▊         | 6/79 [00:00<00:06, 10.61it/s] 16%|█▋        | 13/79 [00:00<00:02, 22.40it/s] 24%|██▍       | 19/79 [00:00<00:01, 30.76it/s] 30%|███       | 24/79 [00:01<00:01, 34.30it/s] 38%|███▊      | 30/79 [00:01<00:01, 38.44it/s] 47%|████▋     | 37/79 [00:01<00:00, 45.49it/s] 54%|█████▍    | 43/79 [00:01<00:00, 49.11it/s] 63%|██████▎   | 50/79 [00:01<00:00, 52.48it/s] 71%|███████   | 56/79 [00:01<00:00, 53.66it/s] 80%|███████▉  | 63/79 [00:01<00:00, 56.10it/s] 87%|████████▋ | 69/79 [00:01<00:00, 54.96it/s] 96%|█████████▌| 76/79 [00:01<00:00, 57.60it/s]100%|██████████| 79/79 [00:01<00:00, 39.65it/s]
10000 images processed, 2.0135159492492676 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:48,  1.43it/s]  7%|▋         | 5/70 [00:00<00:08,  7.87it/s] 13%|█▎        | 9/70 [00:00<00:04, 13.52it/s] 20%|██        | 14/70 [00:01<00:02, 21.03it/s] 29%|██▊       | 20/70 [00:01<00:01, 29.71it/s] 36%|███▌      | 25/70 [00:01<00:01, 34.51it/s] 44%|████▍     | 31/70 [00:01<00:01, 38.92it/s] 51%|█████▏    | 36/70 [00:01<00:00, 41.71it/s] 60%|██████    | 42/70 [00:01<00:00, 44.54it/s] 69%|██████▊   | 48/70 [00:01<00:00, 46.48it/s] 76%|███████▌  | 53/70 [00:01<00:00, 45.03it/s] 83%|████████▎ | 58/70 [00:01<00:00, 46.28it/s] 90%|█████████ | 63/70 [00:02<00:00, 46.42it/s] 99%|█████████▊| 69/70 [00:02<00:00, 47.56it/s]100%|██████████| 70/70 [00:02<00:00, 32.20it/s]
8925 images processed, 2.225691556930542 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:48,  1.10s/it]  4%|▍         | 2/45 [00:01<00:29,  1.48it/s] 13%|█▎        | 6/45 [00:01<00:06,  5.70it/s] 27%|██▋       | 12/45 [00:01<00:02, 12.77it/s] 40%|████      | 18/45 [00:01<00:01, 16.71it/s] 53%|█████▎    | 24/45 [00:02<00:00, 23.43it/s] 67%|██████▋   | 30/45 [00:02<00:00, 29.17it/s] 78%|███████▊  | 35/45 [00:02<00:00, 19.54it/s] 91%|█████████ | 41/45 [00:02<00:00, 25.04it/s]100%|██████████| 45/45 [00:02<00:00, 16.10it/s]
5640 images processed, 2.8189151287078857 seconds used

27.021801233291626
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.18  98.70  84.51
places365     73.29  79.67  48.99
LSUN          49.17  88.47  71.15
iSUN          50.80  88.26  71.62
dtd           41.06  90.05  77.99
AVG           43.70  89.03  70.85
[incremental] Overall: 0.6220 New: 0.6220 Old: 0.6220
[incremental] Final(Top-1): 0.6220  Average: 0.7277
2.8321869373321533
==== Stage 5: inc={100,101,102,103,104}; seen={0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='retain_only', palm_retain_only=False, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage5', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage4', lora_stack=False, lora_orth_enable=True, lora_orth_lambda=1.0, lora_orth_ref_paths='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage1,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage4', forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='100,101,102,103,104', forget_classes_seen='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage4
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/shaokun/PALM/trainer.py:271: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
  2%|▏         | 1/50 [00:07<06:15,  7.67s/it]  4%|▍         | 2/50 [00:08<02:50,  3.55s/it]  6%|▌         | 3/50 [00:08<01:42,  2.19s/it]  8%|▊         | 4/50 [00:09<01:12,  1.57s/it] 10%|█         | 5/50 [00:10<00:54,  1.22s/it] 12%|█▏        | 6/50 [00:10<00:44,  1.02s/it] 14%|█▍        | 7/50 [00:11<00:37,  1.14it/s] 16%|█▌        | 8/50 [00:12<00:34,  1.22it/s] 18%|█▊        | 9/50 [00:12<00:30,  1.34it/s] 20%|██        | 10/50 [00:13<00:28,  1.42it/s] 22%|██▏       | 11/50 [00:13<00:25,  1.53it/s] 24%|██▍       | 12/50 [00:14<00:24,  1.53it/s] 26%|██▌       | 13/50 [00:15<00:24,  1.52it/s] 28%|██▊       | 14/50 [00:15<00:23,  1.55it/s] 30%|███       | 15/50 [00:16<00:22,  1.58it/s] 32%|███▏      | 16/50 [00:16<00:21,  1.57it/s] 34%|███▍      | 17/50 [00:17<00:20,  1.61it/s] 36%|███▌      | 18/50 [00:18<00:19,  1.65it/s] 38%|███▊      | 19/50 [00:18<00:20,  1.48it/s] 40%|████      | 20/50 [00:19<00:19,  1.51it/s] 42%|████▏     | 21/50 [00:20<00:19,  1.53it/s] 44%|████▍     | 22/50 [00:20<00:18,  1.53it/s] 46%|████▌     | 23/50 [00:21<00:18,  1.50it/s] 48%|████▊     | 24/50 [00:22<00:16,  1.55it/s] 50%|█████     | 25/50 [00:22<00:16,  1.56it/s] 52%|█████▏    | 26/50 [00:23<00:15,  1.60it/s] 54%|█████▍    | 27/50 [00:24<00:14,  1.54it/s] 56%|█████▌    | 28/50 [00:24<00:13,  1.62it/s] 58%|█████▊    | 29/50 [00:25<00:12,  1.68it/s] 60%|██████    | 30/50 [00:25<00:12,  1.64it/s] 62%|██████▏   | 31/50 [00:26<00:11,  1.59it/s] 64%|██████▍   | 32/50 [00:27<00:11,  1.55it/s] 66%|██████▌   | 33/50 [00:27<00:10,  1.57it/s] 68%|██████▊   | 34/50 [00:28<00:10,  1.55it/s] 70%|███████   | 35/50 [00:29<00:09,  1.56it/s] 72%|███████▏  | 36/50 [00:29<00:09,  1.55it/s] 74%|███████▍  | 37/50 [00:30<00:08,  1.49it/s] 76%|███████▌  | 38/50 [00:31<00:07,  1.55it/s] 78%|███████▊  | 39/50 [00:31<00:07,  1.45it/s] 80%|████████  | 40/50 [00:32<00:06,  1.44it/s] 82%|████████▏ | 41/50 [00:33<00:05,  1.52it/s] 84%|████████▍ | 42/50 [00:33<00:05,  1.52it/s] 86%|████████▌ | 43/50 [00:34<00:05,  1.39it/s] 88%|████████▊ | 44/50 [00:35<00:04,  1.46it/s] 90%|█████████ | 45/50 [00:35<00:03,  1.52it/s] 92%|█████████▏| 46/50 [00:36<00:02,  1.59it/s] 94%|█████████▍| 47/50 [00:37<00:01,  1.55it/s] 96%|█████████▌| 48/50 [00:37<00:01,  1.50it/s] 98%|█████████▊| 49/50 [00:38<00:00,  1.40it/s]100%|██████████| 50/50 [00:39<00:00,  1.38it/s]100%|██████████| 50/50 [00:39<00:00,  1.27it/s]
[loss] ep 0 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage5
[loss] ep 2 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage5
[loss] ep 5 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 7 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 10 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 12 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 15 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 17 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 20 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 22 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 25 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 27 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 30 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 32 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 35 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 37 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 40 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 42 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 45 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage5
[loss] ep 47 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage5
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stage5-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<04:25,  1.62it/s]  2%|▏         | 7/430 [00:00<00:33, 12.50it/s]  3%|▎         | 15/430 [00:00<00:15, 26.29it/s]  5%|▌         | 22/430 [00:00<00:11, 35.41it/s]  7%|▋         | 29/430 [00:01<00:09, 42.54it/s]  9%|▊         | 37/430 [00:01<00:07, 51.27it/s] 10%|█         | 45/430 [00:01<00:06, 58.24it/s] 12%|█▏        | 53/430 [00:01<00:06, 62.80it/s] 14%|█▍        | 61/430 [00:01<00:05, 65.50it/s] 16%|█▌        | 69/430 [00:01<00:05, 68.98it/s] 18%|█▊        | 77/430 [00:01<00:04, 71.25it/s] 20%|█▉        | 85/430 [00:01<00:04, 72.63it/s] 22%|██▏       | 93/430 [00:01<00:04, 72.39it/s] 23%|██▎       | 101/430 [00:02<00:04, 73.52it/s] 25%|██▌       | 109/430 [00:02<00:04, 74.84it/s] 27%|██▋       | 117/430 [00:02<00:04, 75.75it/s] 29%|██▉       | 125/430 [00:02<00:04, 76.25it/s] 31%|███       | 133/430 [00:02<00:04, 74.15it/s] 33%|███▎      | 141/430 [00:02<00:03, 75.26it/s] 35%|███▍      | 149/430 [00:02<00:03, 75.86it/s] 37%|███▋      | 157/430 [00:02<00:03, 75.39it/s] 38%|███▊      | 165/430 [00:02<00:03, 69.92it/s] 40%|████      | 173/430 [00:02<00:03, 71.74it/s] 42%|████▏     | 181/430 [00:03<00:03, 71.92it/s] 44%|████▍     | 189/430 [00:03<00:03, 72.36it/s] 46%|████▌     | 198/430 [00:03<00:03, 74.38it/s] 48%|████▊     | 206/430 [00:03<00:03, 67.56it/s] 50%|████▉     | 213/430 [00:03<00:03, 64.32it/s] 51%|█████▏    | 221/430 [00:03<00:03, 67.34it/s] 53%|█████▎    | 229/430 [00:03<00:02, 70.13it/s] 55%|█████▌    | 237/430 [00:03<00:02, 72.21it/s] 57%|█████▋    | 246/430 [00:04<00:02, 74.45it/s] 59%|█████▉    | 254/430 [00:04<00:02, 73.40it/s] 61%|██████    | 262/430 [00:04<00:02, 74.60it/s] 63%|██████▎   | 270/430 [00:04<00:02, 75.59it/s] 65%|██████▍   | 278/430 [00:04<00:01, 76.01it/s] 67%|██████▋   | 286/430 [00:04<00:02, 71.40it/s] 68%|██████▊   | 294/430 [00:04<00:01, 72.72it/s] 70%|███████   | 302/430 [00:04<00:01, 69.45it/s] 72%|███████▏  | 310/430 [00:04<00:01, 71.59it/s] 74%|███████▍  | 318/430 [00:04<00:01, 73.34it/s] 76%|███████▌  | 326/430 [00:05<00:01, 72.23it/s] 78%|███████▊  | 334/430 [00:05<00:01, 73.60it/s] 80%|███████▉  | 342/430 [00:05<00:01, 74.78it/s] 81%|████████▏ | 350/430 [00:05<00:01, 72.49it/s] 83%|████████▎ | 358/430 [00:05<00:01, 68.18it/s] 85%|████████▍ | 365/430 [00:05<00:01, 59.26it/s] 87%|████████▋ | 373/430 [00:05<00:00, 63.82it/s] 88%|████████▊ | 380/430 [00:05<00:00, 63.39it/s] 90%|█████████ | 388/430 [00:06<00:00, 67.35it/s] 92%|█████████▏| 396/430 [00:06<00:00, 70.06it/s] 94%|█████████▍| 404/430 [00:06<00:00, 71.52it/s] 96%|█████████▌| 412/430 [00:06<00:00, 72.33it/s] 98%|█████████▊| 421/430 [00:06<00:00, 76.36it/s]100%|██████████| 430/430 [00:06<00:00, 76.35it/s]100%|██████████| 430/430 [00:06<00:00, 65.23it/s]
55000 images processed, 6.671222686767578 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:41,  2.05it/s]  9%|▉         | 8/86 [00:00<00:04, 16.88it/s] 19%|█▊        | 16/86 [00:00<00:02, 31.68it/s] 27%|██▋       | 23/86 [00:00<00:01, 39.80it/s] 36%|███▌      | 31/86 [00:00<00:01, 49.69it/s] 45%|████▌     | 39/86 [00:01<00:00, 56.92it/s] 55%|█████▍    | 47/86 [00:01<00:00, 62.14it/s] 64%|██████▍   | 55/86 [00:01<00:00, 64.01it/s] 72%|███████▏  | 62/86 [00:01<00:00, 64.73it/s] 81%|████████▏ | 70/86 [00:01<00:00, 67.13it/s] 91%|█████████ | 78/86 [00:01<00:00, 69.82it/s]100%|██████████| 86/86 [00:01<00:00, 70.01it/s]100%|██████████| 86/86 [00:01<00:00, 50.94it/s]
11000 images processed, 1.7057836055755615 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:06,  1.61it/s]  4%|▍         | 8/204 [00:00<00:13, 14.23it/s]  8%|▊         | 16/204 [00:00<00:06, 27.66it/s] 12%|█▏        | 24/204 [00:00<00:04, 38.70it/s] 15%|█▌        | 31/204 [00:01<00:03, 46.25it/s] 19%|█▉        | 39/204 [00:01<00:03, 53.43it/s] 23%|██▎       | 46/204 [00:01<00:02, 57.05it/s] 26%|██▋       | 54/204 [00:01<00:02, 60.33it/s] 30%|███       | 62/204 [00:01<00:02, 64.56it/s] 34%|███▍      | 69/204 [00:01<00:02, 63.35it/s] 38%|███▊      | 77/204 [00:01<00:01, 66.54it/s] 41%|████      | 84/204 [00:01<00:02, 58.33it/s] 45%|████▍     | 91/204 [00:01<00:01, 58.71it/s] 48%|████▊     | 98/204 [00:02<00:01, 61.31it/s] 52%|█████▏    | 106/204 [00:02<00:01, 64.26it/s] 56%|█████▌    | 114/204 [00:02<00:01, 67.01it/s] 60%|█████▉    | 122/204 [00:02<00:01, 67.90it/s] 64%|██████▎   | 130/204 [00:02<00:01, 69.54it/s] 68%|██████▊   | 138/204 [00:02<00:00, 71.14it/s] 72%|███████▏  | 146/204 [00:02<00:00, 70.22it/s] 75%|███████▌  | 154/204 [00:02<00:00, 70.19it/s] 79%|███████▉  | 162/204 [00:02<00:00, 71.59it/s] 83%|████████▎ | 170/204 [00:03<00:00, 72.96it/s] 87%|████████▋ | 178/204 [00:03<00:00, 74.08it/s] 91%|█████████ | 186/204 [00:03<00:00, 75.23it/s] 95%|█████████▌| 194/204 [00:03<00:00, 75.99it/s] 99%|█████████▉| 202/204 [00:03<00:00, 75.75it/s]100%|██████████| 204/204 [00:03<00:00, 57.91it/s]
26032 images processed, 3.5637683868408203 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:01<01:30,  1.15s/it]  9%|▉         | 7/79 [00:01<00:09,  7.37it/s] 19%|█▉        | 15/79 [00:01<00:03, 17.12it/s] 29%|██▉       | 23/79 [00:01<00:02, 27.05it/s] 39%|███▉      | 31/79 [00:01<00:01, 36.70it/s] 48%|████▊     | 38/79 [00:01<00:00, 41.76it/s] 58%|█████▊    | 46/79 [00:01<00:00, 49.50it/s] 67%|██████▋   | 53/79 [00:01<00:00, 50.15it/s] 76%|███████▌  | 60/79 [00:02<00:00, 54.79it/s] 87%|████████▋ | 69/79 [00:02<00:00, 62.30it/s] 97%|█████████▋| 77/79 [00:02<00:00, 66.48it/s]100%|██████████| 79/79 [00:02<00:00, 34.01it/s]
10000 images processed, 2.360415458679199 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:01<01:28,  1.13s/it] 11%|█▏        | 9/79 [00:01<00:07,  9.56it/s] 18%|█▊        | 14/79 [00:01<00:04, 14.85it/s] 28%|██▊       | 22/79 [00:01<00:02, 24.92it/s] 35%|███▌      | 28/79 [00:01<00:01, 30.89it/s] 43%|████▎     | 34/79 [00:01<00:01, 32.53it/s] 52%|█████▏    | 41/79 [00:01<00:00, 39.86it/s] 62%|██████▏   | 49/79 [00:01<00:00, 47.30it/s] 72%|███████▏  | 57/79 [00:02<00:00, 54.68it/s] 82%|████████▏ | 65/79 [00:02<00:00, 60.38it/s] 91%|█████████ | 72/79 [00:02<00:00, 61.70it/s]100%|██████████| 79/79 [00:02<00:00, 33.29it/s]
10000 images processed, 2.421741485595703 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:39,  1.73it/s] 11%|█▏        | 8/70 [00:00<00:04, 14.81it/s] 23%|██▎       | 16/70 [00:00<00:01, 28.49it/s] 34%|███▍      | 24/70 [00:00<00:01, 39.60it/s] 46%|████▌     | 32/70 [00:01<00:00, 48.09it/s] 57%|█████▋    | 40/70 [00:01<00:00, 55.11it/s] 69%|██████▊   | 48/70 [00:01<00:00, 59.82it/s] 80%|████████  | 56/70 [00:01<00:00, 64.70it/s] 91%|█████████▏| 64/70 [00:01<00:00, 68.42it/s]100%|██████████| 70/70 [00:01<00:00, 45.74it/s]
8925 images processed, 1.5612537860870361 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:45,  1.04s/it]  4%|▍         | 2/45 [00:01<00:24,  1.77it/s] 22%|██▏       | 10/45 [00:01<00:02, 11.68it/s] 38%|███▊      | 17/45 [00:01<00:01, 16.17it/s] 47%|████▋     | 21/45 [00:01<00:01, 19.42it/s] 56%|█████▌    | 25/45 [00:01<00:00, 21.81it/s] 67%|██████▋   | 30/45 [00:02<00:00, 25.96it/s] 76%|███████▌  | 34/45 [00:02<00:00, 18.07it/s] 93%|█████████▎| 42/45 [00:02<00:00, 27.31it/s]100%|██████████| 45/45 [00:02<00:00, 17.31it/s]
5640 images processed, 2.649461269378662 seconds used

22.61152410507202
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.29  98.70  86.89
places365     77.41  78.95  52.97
LSUN          49.54  88.75  74.86
iSUN          53.34  88.12  74.83
dtd           41.17  90.30  81.56
AVG           45.15  88.96  74.22
[incremental] Overall: 0.5572 New: 0.4620 Old: 0.5810
[incremental] Final(Top-1): 0.5572  Average: 0.6936
3.8463058471679688
==== Stage 6: inc={105,106,107,108,109}; seen={0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90,100,101,102,103,104}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='retain_only', palm_retain_only=False, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage6', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage5', lora_stack=False, lora_orth_enable=True, lora_orth_lambda=1.0, lora_orth_ref_paths='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage1,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage2,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage3,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage4,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage5', forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='105,106,107,108,109', forget_classes_seen='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90,100,101,102,103,104', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage5
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/shaokun/PALM/trainer.py:271: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
  2%|▏         | 1/50 [00:09<07:51,  9.62s/it]  4%|▍         | 2/50 [00:10<03:39,  4.57s/it]  6%|▌         | 3/50 [00:11<02:17,  2.93s/it]  8%|▊         | 4/50 [00:12<01:39,  2.16s/it] 10%|█         | 5/50 [00:13<01:18,  1.74s/it] 12%|█▏        | 6/50 [00:14<01:05,  1.50s/it] 14%|█▍        | 7/50 [00:15<00:57,  1.34s/it] 16%|█▌        | 8/50 [00:16<00:51,  1.23s/it] 18%|█▊        | 9/50 [00:17<00:48,  1.18s/it] 20%|██        | 10/50 [00:18<00:46,  1.16s/it] 22%|██▏       | 11/50 [00:19<00:43,  1.11s/it] 24%|██▍       | 12/50 [00:20<00:41,  1.09s/it] 26%|██▌       | 13/50 [00:21<00:39,  1.08s/it] 28%|██▊       | 14/50 [00:22<00:37,  1.05s/it] 30%|███       | 15/50 [00:23<00:36,  1.05s/it] 32%|███▏      | 16/50 [00:25<00:35,  1.06s/it] 34%|███▍      | 17/50 [00:26<00:34,  1.06s/it] 36%|███▌      | 18/50 [00:27<00:33,  1.04s/it] 38%|███▊      | 19/50 [00:28<00:31,  1.00s/it] 40%|████      | 20/50 [00:29<00:30,  1.03s/it] 42%|████▏     | 21/50 [00:30<00:30,  1.05s/it] 44%|████▍     | 22/50 [00:31<00:29,  1.07s/it] 46%|████▌     | 23/50 [00:32<00:28,  1.06s/it] 48%|████▊     | 24/50 [00:33<00:27,  1.05s/it] 50%|█████     | 25/50 [00:34<00:26,  1.05s/it] 52%|█████▏    | 26/50 [00:35<00:24,  1.04s/it] 54%|█████▍    | 27/50 [00:36<00:24,  1.08s/it] 56%|█████▌    | 28/50 [00:37<00:25,  1.14s/it] 58%|█████▊    | 29/50 [00:39<00:24,  1.17s/it] 60%|██████    | 30/50 [00:40<00:23,  1.18s/it] 62%|██████▏   | 31/50 [00:41<00:22,  1.19s/it] 64%|██████▍   | 32/50 [00:42<00:21,  1.21s/it] 66%|██████▌   | 33/50 [00:44<00:20,  1.22s/it] 68%|██████▊   | 34/50 [00:45<00:18,  1.15s/it] 70%|███████   | 35/50 [00:46<00:17,  1.17s/it] 72%|███████▏  | 36/50 [00:47<00:16,  1.18s/it] 74%|███████▍  | 37/50 [00:48<00:14,  1.13s/it] 76%|███████▌  | 38/50 [00:49<00:13,  1.09s/it] 78%|███████▊  | 39/50 [00:50<00:11,  1.09s/it] 80%|████████  | 40/50 [00:51<00:10,  1.09s/it] 82%|████████▏ | 41/50 [00:52<00:09,  1.09s/it] 84%|████████▍ | 42/50 [00:53<00:08,  1.10s/it] 86%|████████▌ | 43/50 [00:54<00:07,  1.07s/it] 88%|████████▊ | 44/50 [00:56<00:06,  1.14s/it] 90%|█████████ | 45/50 [00:57<00:05,  1.12s/it] 92%|█████████▏| 46/50 [00:58<00:04,  1.13s/it] 94%|█████████▍| 47/50 [00:59<00:03,  1.12s/it] 96%|█████████▌| 48/50 [01:00<00:02,  1.10s/it] 98%|█████████▊| 49/50 [01:01<00:01,  1.06s/it]100%|██████████| 50/50 [01:02<00:00,  1.07s/it]100%|██████████| 50/50 [01:02<00:00,  1.25s/it]
[loss] ep 0 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage6
[loss] ep 2 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage6
[loss] ep 5 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 7 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 10 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 12 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 15 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage6
[loss] ep 17 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 20 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 22 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 25 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 27 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 30 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 32 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 35 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 37 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 40 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 42 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 45 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 47 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stack/stage6
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-palm_mle_retain_only-pcon_split-stage6-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<04:30,  1.59it/s]  2%|▏         | 8/430 [00:00<00:30, 13.94it/s]  3%|▎         | 15/430 [00:00<00:16, 25.03it/s]  5%|▌         | 22/430 [00:00<00:12, 33.93it/s]  7%|▋         | 29/430 [00:01<00:09, 41.54it/s]  8%|▊         | 36/430 [00:01<00:08, 47.33it/s] 10%|█         | 43/430 [00:01<00:07, 52.54it/s] 12%|█▏        | 50/430 [00:01<00:07, 54.04it/s] 13%|█▎        | 57/430 [00:01<00:06, 56.88it/s] 15%|█▍        | 64/430 [00:01<00:06, 57.64it/s] 17%|█▋        | 71/430 [00:01<00:06, 59.59it/s] 18%|█▊        | 78/430 [00:01<00:05, 60.08it/s] 20%|██        | 86/430 [00:01<00:05, 63.21it/s] 22%|██▏       | 93/430 [00:02<00:05, 62.88it/s] 23%|██▎       | 100/430 [00:02<00:05, 64.02it/s] 25%|██▍       | 107/430 [00:02<00:05, 64.53it/s] 27%|██▋       | 114/430 [00:02<00:04, 64.32it/s] 28%|██▊       | 121/430 [00:02<00:04, 62.26it/s] 30%|██▉       | 128/430 [00:02<00:04, 61.78it/s] 31%|███▏      | 135/430 [00:02<00:04, 62.29it/s] 33%|███▎      | 142/430 [00:02<00:04, 62.00it/s] 35%|███▍      | 149/430 [00:02<00:04, 61.46it/s] 36%|███▋      | 156/430 [00:03<00:04, 62.74it/s] 38%|███▊      | 163/430 [00:03<00:04, 63.22it/s] 40%|███▉      | 170/430 [00:03<00:04, 59.36it/s] 41%|████      | 177/430 [00:03<00:04, 60.64it/s] 43%|████▎     | 184/430 [00:03<00:04, 60.79it/s] 44%|████▍     | 191/430 [00:03<00:03, 61.30it/s] 46%|████▌     | 198/430 [00:03<00:03, 62.74it/s] 48%|████▊     | 205/430 [00:03<00:03, 61.77it/s] 49%|████▉     | 212/430 [00:03<00:03, 63.07it/s] 51%|█████     | 219/430 [00:04<00:03, 62.83it/s] 53%|█████▎    | 226/430 [00:04<00:03, 63.46it/s] 54%|█████▍    | 233/430 [00:04<00:03, 64.52it/s] 56%|█████▌    | 240/430 [00:04<00:02, 64.63it/s] 57%|█████▋    | 247/430 [00:04<00:02, 63.37it/s] 59%|█████▉    | 254/430 [00:04<00:02, 64.02it/s] 61%|██████    | 261/430 [00:04<00:02, 62.78it/s] 62%|██████▏   | 268/430 [00:04<00:02, 63.19it/s] 64%|██████▍   | 275/430 [00:04<00:02, 63.44it/s] 66%|██████▌   | 282/430 [00:05<00:02, 62.68it/s] 67%|██████▋   | 289/430 [00:05<00:02, 63.53it/s] 69%|██████▉   | 296/430 [00:05<00:02, 61.89it/s] 70%|███████   | 303/430 [00:05<00:02, 62.55it/s] 72%|███████▏  | 310/430 [00:05<00:01, 63.64it/s] 74%|███████▎  | 317/430 [00:05<00:01, 62.59it/s] 75%|███████▌  | 324/430 [00:05<00:01, 63.68it/s] 77%|███████▋  | 331/430 [00:05<00:01, 64.06it/s] 79%|███████▊  | 338/430 [00:05<00:01, 64.62it/s] 80%|████████  | 345/430 [00:06<00:01, 64.98it/s] 82%|████████▏ | 352/430 [00:06<00:01, 63.60it/s] 83%|████████▎ | 359/430 [00:06<00:01, 64.14it/s] 85%|████████▌ | 366/430 [00:06<00:01, 63.50it/s] 87%|████████▋ | 373/430 [00:06<00:00, 63.18it/s] 88%|████████▊ | 380/430 [00:06<00:00, 64.03it/s] 90%|█████████ | 387/430 [00:06<00:00, 64.13it/s] 92%|█████████▏| 394/430 [00:06<00:00, 63.09it/s] 93%|█████████▎| 401/430 [00:06<00:00, 63.66it/s] 95%|█████████▍| 408/430 [00:07<00:00, 63.11it/s] 97%|█████████▋| 415/430 [00:07<00:00, 63.07it/s] 98%|█████████▊| 422/430 [00:07<00:00, 64.02it/s]100%|█████████▉| 429/430 [00:07<00:00, 64.39it/s]100%|██████████| 430/430 [00:07<00:00, 57.88it/s]
55000 images processed, 7.515690326690674 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:55,  1.52it/s]  9%|▉         | 8/86 [00:00<00:05, 13.48it/s] 16%|█▋        | 14/86 [00:00<00:03, 22.68it/s] 24%|██▍       | 21/86 [00:00<00:02, 31.81it/s] 33%|███▎      | 28/86 [00:01<00:01, 39.96it/s] 40%|███▉      | 34/86 [00:01<00:01, 43.97it/s] 48%|████▊     | 41/86 [00:01<00:00, 49.40it/s] 56%|█████▌    | 48/86 [00:01<00:00, 53.22it/s] 64%|██████▍   | 55/86 [00:01<00:00, 55.21it/s] 72%|███████▏  | 62/86 [00:01<00:00, 58.06it/s] 80%|████████  | 69/86 [00:01<00:00, 59.26it/s] 88%|████████▊ | 76/86 [00:01<00:00, 60.32it/s] 97%|█████████▋| 83/86 [00:01<00:00, 61.47it/s]100%|██████████| 86/86 [00:02<00:00, 42.06it/s]
11000 images processed, 2.0661044120788574 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:04,  1.63it/s]  4%|▍         | 8/204 [00:00<00:13, 14.09it/s]  7%|▋         | 14/204 [00:00<00:08, 23.20it/s] 10%|█         | 21/204 [00:00<00:05, 32.52it/s] 13%|█▎        | 27/204 [00:01<00:04, 38.45it/s] 17%|█▋        | 34/204 [00:01<00:03, 45.31it/s] 20%|█▉        | 40/204 [00:01<00:03, 47.99it/s] 23%|██▎       | 47/204 [00:01<00:02, 52.58it/s] 26%|██▋       | 54/204 [00:01<00:02, 55.65it/s] 30%|██▉       | 61/204 [00:01<00:02, 58.39it/s] 33%|███▎      | 68/204 [00:01<00:02, 61.04it/s] 37%|███▋      | 75/204 [00:01<00:02, 60.45it/s] 40%|████      | 82/204 [00:01<00:02, 60.88it/s] 44%|████▎     | 89/204 [00:02<00:01, 60.94it/s] 47%|████▋     | 96/204 [00:02<00:01, 61.69it/s] 50%|█████     | 103/204 [00:02<00:01, 59.72it/s] 54%|█████▍    | 110/204 [00:02<00:01, 60.59it/s] 57%|█████▋    | 117/204 [00:02<00:01, 61.69it/s] 61%|██████    | 124/204 [00:02<00:01, 62.60it/s] 64%|██████▍   | 131/204 [00:02<00:01, 60.02it/s] 68%|██████▊   | 138/204 [00:02<00:01, 60.52it/s] 71%|███████   | 145/204 [00:02<00:00, 59.81it/s] 75%|███████▍  | 152/204 [00:03<00:00, 60.91it/s] 78%|███████▊  | 159/204 [00:03<00:00, 58.19it/s] 81%|████████  | 165/204 [00:03<00:00, 57.86it/s] 84%|████████▍ | 172/204 [00:03<00:00, 59.83it/s] 88%|████████▊ | 179/204 [00:03<00:00, 59.56it/s] 91%|█████████ | 186/204 [00:03<00:00, 60.92it/s] 95%|█████████▍| 193/204 [00:03<00:00, 61.04it/s] 98%|█████████▊| 200/204 [00:03<00:00, 61.28it/s]100%|██████████| 204/204 [00:03<00:00, 51.45it/s]
26032 images processed, 4.00327467918396 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:57,  1.36it/s] 10%|█         | 8/79 [00:00<00:05, 12.20it/s] 16%|█▋        | 13/79 [00:00<00:03, 19.15it/s] 24%|██▍       | 19/79 [00:01<00:02, 27.14it/s] 33%|███▎      | 26/79 [00:01<00:01, 35.38it/s] 41%|████      | 32/79 [00:01<00:01, 41.17it/s] 49%|████▉     | 39/79 [00:01<00:00, 46.32it/s] 57%|█████▋    | 45/79 [00:01<00:00, 49.27it/s] 66%|██████▌   | 52/79 [00:01<00:00, 52.97it/s] 75%|███████▍  | 59/79 [00:01<00:00, 57.19it/s] 84%|████████▎ | 66/79 [00:01<00:00, 57.94it/s] 92%|█████████▏| 73/79 [00:01<00:00, 59.92it/s]100%|██████████| 79/79 [00:02<00:00, 38.54it/s]
10000 images processed, 2.084937334060669 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:51,  1.52it/s] 10%|█         | 8/79 [00:00<00:05, 13.35it/s] 16%|█▋        | 13/79 [00:00<00:03, 20.53it/s] 25%|██▌       | 20/79 [00:00<00:01, 30.70it/s] 34%|███▍      | 27/79 [00:01<00:01, 37.91it/s] 43%|████▎     | 34/79 [00:01<00:01, 44.63it/s] 51%|█████     | 40/79 [00:01<00:00, 47.62it/s] 59%|█████▉    | 47/79 [00:01<00:00, 52.10it/s] 68%|██████▊   | 54/79 [00:01<00:00, 55.47it/s] 77%|███████▋  | 61/79 [00:01<00:00, 56.85it/s] 86%|████████▌ | 68/79 [00:01<00:00, 59.14it/s] 95%|█████████▍| 75/79 [00:01<00:00, 59.13it/s]100%|██████████| 79/79 [00:01<00:00, 40.81it/s]
10000 images processed, 1.9594006538391113 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:46,  1.49it/s] 11%|█▏        | 8/70 [00:00<00:04, 13.22it/s] 21%|██▏       | 15/70 [00:00<00:02, 23.87it/s] 31%|███▏      | 22/70 [00:01<00:01, 32.20it/s] 41%|████▏     | 29/70 [00:01<00:01, 40.49it/s] 51%|█████▏    | 36/70 [00:01<00:00, 45.65it/s] 61%|██████▏   | 43/70 [00:01<00:00, 50.57it/s] 71%|███████▏  | 50/70 [00:01<00:00, 53.02it/s] 81%|████████▏ | 57/70 [00:01<00:00, 56.25it/s] 91%|█████████▏| 64/70 [00:01<00:00, 58.32it/s]100%|██████████| 70/70 [00:01<00:00, 39.11it/s]
8925 images processed, 1.8235142230987549 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:57,  1.30s/it]  4%|▍         | 2/45 [00:01<00:26,  1.65it/s] 20%|██        | 9/45 [00:01<00:03,  9.87it/s] 36%|███▌      | 16/45 [00:01<00:01, 18.31it/s] 47%|████▋     | 21/45 [00:02<00:01, 16.16it/s] 62%|██████▏   | 28/45 [00:02<00:00, 23.68it/s] 73%|███████▎  | 33/45 [00:02<00:00, 18.04it/s] 84%|████████▍ | 38/45 [00:02<00:00, 21.87it/s]100%|██████████| 45/45 [00:02<00:00, 28.99it/s]100%|██████████| 45/45 [00:02<00:00, 16.18it/s]
5640 images processed, 2.8015222549438477 seconds used

23.90454649925232
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.18  98.77  89.25
places365     78.80  78.26  55.95
LSUN          47.78  89.42  78.34
iSUN          53.60  87.92  77.15
dtd           39.31  90.83  84.56
AVG           44.73  89.04  77.05
[incremental] Overall: 0.5093 New: 0.4880 Old: 0.5136
[incremental] Final(Top-1): 0.5093  Average: 0.6629
3.9289584159851074
runner-incremental-continual.sh: line 258: syntax error near unexpected token `done'
