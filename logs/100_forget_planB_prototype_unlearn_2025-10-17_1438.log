nohup: ignoring input
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.1, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both_all', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_center_set='all', forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 22187968
[debug] trainable_count = 70
[debug] trainable: base_model.model.encoder.layer1.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer1.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer1.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer1.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer1.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer1.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer1.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer1.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer1.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer1.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer1.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer1.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer2.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer2.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer2.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer2.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer2.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer2.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer2.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer2.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer2.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer2.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer2.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer2.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer2.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer2.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer2.3.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer2.3.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer2.3.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer2.3.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer3.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer3.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer3.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer3.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer3.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer3.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer3.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer3.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer3.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer3.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer3.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer3.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer3.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer3.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer3.3.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer3.3.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer3.3.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer3.3.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer3.4.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer3.4.conv1.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer1.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer1.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer1.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer1.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer1.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer1.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer1.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer1.1.conv2.lora_B.default.weight', 'base_model.model.encoder.layer1.2.conv1.lora_A.default.weight', 'base_model.model.encoder.layer1.2.conv1.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:107: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:36<29:27, 36.08s/it]  4%|▍         | 2/50 [01:00<23:26, 29.30s/it]  6%|▌         | 3/50 [01:24<21:08, 26.99s/it]  8%|▊         | 4/50 [01:49<19:53, 25.95s/it] 10%|█         | 5/50 [02:13<19:02, 25.38s/it] 12%|█▏        | 6/50 [02:37<18:17, 24.94s/it] 14%|█▍        | 7/50 [03:01<17:40, 24.66s/it] 16%|█▌        | 8/50 [03:25<17:09, 24.52s/it][loss] ep 0 it 0 total=13.6015 mle=1.7264 pcon=5.2950 forget=6.5800 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 50 total=13.5264 mle=1.5777 pcon=5.2880 forget=6.6607 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 100 total=13.5086 mle=1.6367 pcon=5.2812 forget=6.5907 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 150 total=13.5913 mle=1.6660 pcon=5.2744 forget=6.6508 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 200 total=13.4568 mle=1.5192 pcon=5.2675 forget=6.6702 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 250 total=13.3114 mle=1.4450 pcon=5.2605 forget=6.6059 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 300 total=13.4341 mle=1.5282 pcon=5.2543 forget=6.6516 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 350 total=13.6588 mle=1.6468 pcon=5.2481 forget=6.7639 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 1 it 10 total=13.4219 mle=1.6175 pcon=5.2420 forget=6.5624 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 60 total=13.4645 mle=1.5842 pcon=5.2355 forget=6.6448 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 110 total=13.4324 mle=1.5628 pcon=5.2294 forget=6.6403 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 160 total=13.5373 mle=1.7031 pcon=5.2234 forget=6.6108 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 210 total=13.4862 mle=1.6113 pcon=5.2177 forget=6.6572 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 260 total=13.4348 mle=1.5719 pcon=5.2120 forget=6.6509 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 310 total=13.4387 mle=1.5968 pcon=5.2063 forget=6.6356 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 360 total=13.3572 mle=1.5837 pcon=5.2008 forget=6.5727 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 2 it 20 total=13.3951 mle=1.5506 pcon=5.1956 forget=6.6488 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 70 total=13.5518 mle=1.7865 pcon=5.1900 forget=6.5754 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 120 total=13.2795 mle=1.5511 pcon=5.1844 forget=6.5440 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 170 total=13.2977 mle=1.4830 pcon=5.1793 forget=6.6354 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 220 total=13.5117 mle=1.7527 pcon=5.1742 forget=6.5848 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 270 total=13.4502 mle=1.6649 pcon=5.1696 forget=6.6157 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 320 total=13.3968 mle=1.5598 pcon=5.1647 forget=6.6722 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 370 total=13.3676 mle=1.6030 pcon=5.1603 forget=6.6043 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 3 it 30 total=13.2938 mle=1.5146 pcon=5.1556 forget=6.6236 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 80 total=13.3010 mle=1.5421 pcon=5.1510 forget=6.6079 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 130 total=13.2154 mle=1.4733 pcon=5.1465 forget=6.5955 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 180 total=13.4025 mle=1.6649 pcon=5.1424 forget=6.5952 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 230 total=13.3442 mle=1.6094 pcon=5.1383 forget=6.5965 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 280 total=13.2289 mle=1.5627 pcon=5.1342 forget=6.5319 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 330 total=13.1789 mle=1.4823 pcon=5.1300 forget=6.5666 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 380 total=13.2273 mle=1.5591 pcon=5.1260 forget=6.5421 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 4 it 40 total=13.4466 mle=1.8017 pcon=5.1222 forget=6.5228 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 90 total=13.4409 mle=1.7396 pcon=5.1186 forget=6.5827 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 140 total=13.4484 mle=1.7238 pcon=5.1153 forget=6.6093 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 190 total=13.3440 mle=1.6906 pcon=5.1119 forget=6.5416 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 240 total=13.2596 mle=1.5656 pcon=5.1084 forget=6.5856 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 290 total=12.9994 mle=1.3336 pcon=5.1049 forget=6.5610 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 340 total=13.3320 mle=1.6141 pcon=5.1017 forget=6.6163 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 5 it 0 total=13.3393 mle=1.6329 pcon=5.0980 forget=6.6085 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 50 total=13.2347 mle=1.6136 pcon=5.0950 forget=6.5261 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 100 total=13.3188 mle=1.7101 pcon=5.0921 forget=6.5167 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 150 total=13.4547 mle=1.7727 pcon=5.0888 forget=6.5931 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 200 total=13.2114 mle=1.5923 pcon=5.0860 forget=6.5331 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 250 total=13.4248 mle=1.7918 pcon=5.0827 forget=6.5503 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 300 total=13.3337 mle=1.6415 pcon=5.0796 forget=6.6126 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 350 total=13.2198 mle=1.6416 pcon=5.0767 forget=6.5015 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 6 it 10 total=13.1234 mle=1.5291 pcon=5.0740 forget=6.5203 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 60 total=13.1312 mle=1.5154 pcon=5.0715 forget=6.5443 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 110 total=13.0913 mle=1.4672 pcon=5.0688 forget=6.5552 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 160 total=13.3146 mle=1.6634 pcon=5.0665 forget=6.5847 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 210 total=13.1241 mle=1.5475 pcon=5.0636 forget=6.5131 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 260 total=13.2829 mle=1.7347 pcon=5.0608 forget=6.4873 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 310 total=13.1154 mle=1.5145 pcon=5.0585 forget=6.5425 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 360 total=13.0311 mle=1.5157 pcon=5.0556 forget=6.4598 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 7 it 20 total=13.1768 mle=1.5905 pcon=5.0530 forget=6.5333 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 70 total=13.1741 mle=1.6059 pcon=5.0508 forget=6.5173 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 120 total=13.1089 mle=1.5156 pcon=5.0484 forget=6.5449 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 170 total=13.1387 mle=1.5456 pcon=5.0461 forget=6.5470 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 220 total=13.2794 mle=1.6413 pcon=5.0439 forget=6.5942 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 270 total=13.2638 mle=1.7164 pcon=5.0419 forget=6.5055 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 320 total=13.1656 mle=1.5767 pcon=5.0393 forget=6.5496 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 370 total=13.2878 mle=1.7560 pcon=5.0370 forget=6.4948 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 8 it 30 total=13.4125 mle=1.7944 pcon=5.0350 forget=6.5831 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 80 total=13.2390 mle=1.7045 pcon=5.0331 forget=6.5014 nr=64 nf=64 protos=540 dmin_norm=NA
 18%|█▊        | 9/50 [03:50<16:44, 24.49s/it] 20%|██        | 10/50 [04:14<16:13, 24.34s/it] 22%|██▏       | 11/50 [04:38<15:44, 24.23s/it] 24%|██▍       | 12/50 [05:02<15:17, 24.15s/it] 26%|██▌       | 13/50 [05:26<14:51, 24.10s/it] 28%|██▊       | 14/50 [05:50<14:26, 24.08s/it] 30%|███       | 15/50 [06:14<14:03, 24.09s/it] 32%|███▏      | 16/50 [06:38<13:38, 24.08s/it][loss] ep 8 it 130 total=13.2487 mle=1.7016 pcon=5.0309 forget=6.5162 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 180 total=13.1705 mle=1.5923 pcon=5.0290 forget=6.5492 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 230 total=12.8842 mle=1.3812 pcon=5.0272 forget=6.4758 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 280 total=12.9977 mle=1.4906 pcon=5.0256 forget=6.4815 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 330 total=13.2164 mle=1.6273 pcon=5.0237 forget=6.5654 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 380 total=13.1759 mle=1.6210 pcon=5.0219 forget=6.5330 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 9 it 40 total=13.1834 mle=1.6661 pcon=5.0198 forget=6.4975 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 90 total=13.4461 mle=1.8869 pcon=5.0181 forget=6.5411 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 140 total=13.0423 mle=1.5525 pcon=5.0165 forget=6.4732 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 190 total=13.0150 mle=1.5387 pcon=5.0149 forget=6.4615 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 240 total=13.3359 mle=1.8225 pcon=5.0132 forget=6.5002 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 290 total=13.0662 mle=1.5632 pcon=5.0117 forget=6.4912 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 340 total=12.9251 mle=1.4320 pcon=5.0101 forget=6.4831 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 10 it 0 total=13.1926 mle=1.6665 pcon=5.0081 forget=6.5181 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 50 total=12.9818 mle=1.4585 pcon=5.0066 forget=6.5167 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 100 total=13.2291 mle=1.6678 pcon=5.0050 forget=6.5562 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 150 total=13.0327 mle=1.5462 pcon=5.0036 forget=6.4829 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 200 total=13.0714 mle=1.5185 pcon=5.0022 forget=6.5508 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 250 total=13.0977 mle=1.6029 pcon=5.0008 forget=6.4941 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 300 total=13.1381 mle=1.6126 pcon=4.9996 forget=6.5259 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 350 total=13.1163 mle=1.5695 pcon=4.9981 forget=6.5487 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 11 it 10 total=12.8962 mle=1.4509 pcon=4.9964 forget=6.4488 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 60 total=12.9411 mle=1.4481 pcon=4.9950 forget=6.4979 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 110 total=13.1201 mle=1.6860 pcon=4.9937 forget=6.4404 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 160 total=13.0966 mle=1.6156 pcon=4.9924 forget=6.4886 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 210 total=13.1152 mle=1.6650 pcon=4.9914 forget=6.4588 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 260 total=12.9790 mle=1.5245 pcon=4.9899 forget=6.4646 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 310 total=13.4586 mle=1.9227 pcon=4.9884 forget=6.5475 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 360 total=13.1947 mle=1.7213 pcon=4.9869 forget=6.4865 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 12 it 20 total=12.9649 mle=1.5461 pcon=4.9858 forget=6.4331 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 70 total=13.0064 mle=1.5484 pcon=4.9845 forget=6.4735 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 120 total=13.1481 mle=1.6641 pcon=4.9834 forget=6.5006 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 170 total=13.1323 mle=1.6578 pcon=4.9821 forget=6.4924 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 220 total=12.9082 mle=1.4583 pcon=4.9809 forget=6.4690 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 270 total=12.9463 mle=1.4980 pcon=4.9798 forget=6.4686 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 320 total=12.9573 mle=1.5515 pcon=4.9785 forget=6.4273 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 370 total=12.9953 mle=1.5550 pcon=4.9773 forget=6.4630 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 13 it 30 total=13.1338 mle=1.6981 pcon=4.9765 forget=6.4591 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 80 total=13.1360 mle=1.5977 pcon=4.9754 forget=6.5629 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 130 total=13.1465 mle=1.7321 pcon=4.9742 forget=6.4403 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 180 total=13.0187 mle=1.6363 pcon=4.9727 forget=6.4097 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 230 total=13.0138 mle=1.5925 pcon=4.9715 forget=6.4498 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 280 total=12.9815 mle=1.6067 pcon=4.9706 forget=6.4041 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 330 total=13.0579 mle=1.6539 pcon=4.9696 forget=6.4345 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 380 total=13.1189 mle=1.7320 pcon=4.9681 forget=6.4188 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 14 it 40 total=13.0305 mle=1.6679 pcon=4.9668 forget=6.3958 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 90 total=13.1651 mle=1.7712 pcon=4.9657 forget=6.4282 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 140 total=13.1185 mle=1.6655 pcon=4.9647 forget=6.4883 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 190 total=13.1645 mle=1.7382 pcon=4.9638 forget=6.4626 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 240 total=12.8702 mle=1.5366 pcon=4.9625 forget=6.3711 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 290 total=12.9925 mle=1.6375 pcon=4.9614 forget=6.3936 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 340 total=12.8337 mle=1.4832 pcon=4.9600 forget=6.3906 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 15 it 0 total=13.0037 mle=1.6422 pcon=4.9588 forget=6.4027 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 50 total=12.8352 mle=1.4855 pcon=4.9575 forget=6.3922 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 100 total=12.8741 mle=1.5080 pcon=4.9563 forget=6.4098 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 150 total=13.0914 mle=1.6443 pcon=4.9548 forget=6.4924 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 200 total=13.0204 mle=1.6326 pcon=4.9532 forget=6.4347 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 250 total=12.9252 mle=1.5670 pcon=4.9517 forget=6.4065 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 300 total=12.9521 mle=1.6237 pcon=4.9505 forget=6.3780 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 350 total=12.9213 mle=1.6068 pcon=4.9490 forget=6.3654 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 16 it 10 total=12.7773 mle=1.4455 pcon=4.9476 forget=6.3842 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 60 total=12.8257 mle=1.5293 pcon=4.9463 forget=6.3501 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 110 total=13.1241 mle=1.7776 pcon=4.9447 forget=6.4019 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 160 total=12.6702 mle=1.3765 pcon=4.9433 forget=6.3504 nr=64 nf=64 protos=540 dmin_norm=NA
 34%|███▍      | 17/50 [07:02<13:14, 24.08s/it] 36%|███▌      | 18/50 [07:26<12:50, 24.07s/it] 38%|███▊      | 19/50 [07:50<12:25, 24.06s/it] 40%|████      | 20/50 [08:14<11:59, 23.99s/it] 42%|████▏     | 21/50 [08:38<11:32, 23.88s/it] 44%|████▍     | 22/50 [09:01<11:07, 23.82s/it] 46%|████▌     | 23/50 [09:25<10:45, 23.91s/it] 48%|████▊     | 24/50 [09:49<10:20, 23.87s/it][loss] ep 16 it 210 total=12.7464 mle=1.4220 pcon=4.9417 forget=6.3826 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 260 total=13.1347 mle=1.8013 pcon=4.9398 forget=6.3936 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 310 total=13.0108 mle=1.6917 pcon=4.9382 forget=6.3809 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 360 total=12.7961 mle=1.4843 pcon=4.9365 forget=6.3753 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 17 it 20 total=13.0286 mle=1.7397 pcon=4.9346 forget=6.3542 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 70 total=13.0673 mle=1.7251 pcon=4.9330 forget=6.4092 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 120 total=12.9068 mle=1.6161 pcon=4.9312 forget=6.3596 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 170 total=12.8665 mle=1.5832 pcon=4.9290 forget=6.3542 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 220 total=12.8773 mle=1.5858 pcon=4.9272 forget=6.3644 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 270 total=12.9359 mle=1.6048 pcon=4.9255 forget=6.4056 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 320 total=12.7855 mle=1.5070 pcon=4.9235 forget=6.3550 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 370 total=12.8236 mle=1.5363 pcon=4.9215 forget=6.3658 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 18 it 30 total=12.8485 mle=1.6032 pcon=4.9194 forget=6.3259 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 80 total=12.8795 mle=1.6230 pcon=4.9175 forget=6.3390 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 130 total=12.7895 mle=1.5147 pcon=4.9152 forget=6.3595 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 180 total=12.9683 mle=1.6536 pcon=4.9131 forget=6.4016 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 230 total=12.6666 mle=1.4357 pcon=4.9110 forget=6.3199 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 280 total=12.9044 mle=1.6387 pcon=4.9088 forget=6.3569 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 330 total=12.7679 mle=1.5353 pcon=4.9063 forget=6.3263 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 380 total=12.9977 mle=1.7519 pcon=4.9041 forget=6.3417 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 19 it 40 total=12.7432 mle=1.5144 pcon=4.9020 forget=6.3268 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 90 total=12.8574 mle=1.5857 pcon=4.8999 forget=6.3719 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 140 total=12.9416 mle=1.6640 pcon=4.8974 forget=6.3802 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 190 total=12.7108 mle=1.4938 pcon=4.8953 forget=6.3216 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 240 total=12.8294 mle=1.5994 pcon=4.8932 forget=6.3368 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 290 total=12.9229 mle=1.6400 pcon=4.8911 forget=6.3917 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 340 total=12.9312 mle=1.7078 pcon=4.8888 forget=6.3347 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 20 it 0 total=12.8245 mle=1.6038 pcon=4.8865 forget=6.3341 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 20 it 50 total=12.7019 mle=1.5266 pcon=4.8841 forget=6.2911 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 20 it 100 total=12.9210 mle=1.6918 pcon=4.8820 forget=6.3472 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 20 it 150 total=12.7166 mle=1.5064 pcon=4.8797 forget=6.3305 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 20 it 200 total=12.8217 mle=1.6565 pcon=4.8773 forget=6.2880 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 20 it 250 total=12.8962 mle=1.7138 pcon=4.8749 forget=6.3075 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 20 it 300 total=12.8691 mle=1.6826 pcon=4.8725 forget=6.3140 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 20 it 350 total=12.8844 mle=1.6927 pcon=4.8700 forget=6.3217 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 21 it 10 total=12.6455 mle=1.4843 pcon=4.8675 forget=6.2938 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 21 it 60 total=12.6812 mle=1.5069 pcon=4.8651 forget=6.3092 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 21 it 110 total=12.7053 mle=1.5232 pcon=4.8630 forget=6.3191 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 21 it 160 total=12.7985 mle=1.6447 pcon=4.8607 forget=6.2930 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 21 it 210 total=12.7305 mle=1.5858 pcon=4.8585 forget=6.2862 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 21 it 260 total=12.8119 mle=1.6133 pcon=4.8561 forget=6.3425 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 21 it 310 total=12.7418 mle=1.5742 pcon=4.8536 forget=6.3139 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 21 it 360 total=12.7235 mle=1.5551 pcon=4.8511 forget=6.3173 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 22 it 20 total=12.7987 mle=1.6194 pcon=4.8488 forget=6.3305 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 22 it 70 total=12.8185 mle=1.6807 pcon=4.8464 forget=6.2914 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 22 it 120 total=12.6595 mle=1.5234 pcon=4.8441 forget=6.2920 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 22 it 170 total=12.8009 mle=1.6522 pcon=4.8419 forget=6.3068 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 22 it 220 total=12.5773 mle=1.4690 pcon=4.8395 forget=6.2689 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 22 it 270 total=12.5065 mle=1.3630 pcon=4.8373 forget=6.3062 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 22 it 320 total=12.7531 mle=1.6041 pcon=4.8350 forget=6.3139 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 22 it 370 total=12.8083 mle=1.6863 pcon=4.8325 forget=6.2896 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 23 it 30 total=12.6886 mle=1.5708 pcon=4.8301 forget=6.2877 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 23 it 80 total=12.5689 mle=1.4647 pcon=4.8279 forget=6.2763 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 23 it 130 total=12.6954 mle=1.5683 pcon=4.8257 forget=6.3015 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 23 it 180 total=12.6395 mle=1.5631 pcon=4.8234 forget=6.2530 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 23 it 230 total=12.5831 mle=1.4903 pcon=4.8212 forget=6.2716 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 23 it 280 total=12.6789 mle=1.5986 pcon=4.8189 forget=6.2613 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 23 it 330 total=12.6397 mle=1.5790 pcon=4.8168 forget=6.2440 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 23 it 380 total=12.6738 mle=1.5910 pcon=4.8147 forget=6.2681 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 24 it 40 total=12.7385 mle=1.6572 pcon=4.8126 forget=6.2687 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 24 it 90 total=12.9129 mle=1.8294 pcon=4.8107 forget=6.2728 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 24 it 140 total=12.5686 mle=1.4756 pcon=4.8088 forget=6.2842 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 24 it 190 total=12.7043 mle=1.6284 pcon=4.8068 forget=6.2691 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 24 it 240 total=12.8710 mle=1.8052 pcon=4.8047 forget=6.2611 nr=64 nf=64 protos=540 dmin_norm=NA
 50%|█████     | 25/50 [10:13<09:58, 23.94s/it] 52%|█████▏    | 26/50 [10:37<09:34, 23.94s/it] 54%|█████▍    | 27/50 [11:01<09:10, 23.95s/it] 56%|█████▌    | 28/50 [11:25<08:45, 23.90s/it] 58%|█████▊    | 29/50 [11:50<08:25, 24.09s/it] 60%|██████    | 30/50 [12:13<08:00, 24.03s/it] 62%|██████▏   | 31/50 [12:38<07:36, 24.04s/it] 64%|██████▍   | 32/50 [13:02<07:12, 24.04s/it][loss] ep 24 it 290 total=12.7089 mle=1.6310 pcon=4.8028 forget=6.2750 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 24 it 340 total=12.6709 mle=1.6229 pcon=4.8009 forget=6.2471 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 25 it 0 total=12.6064 mle=1.5277 pcon=4.7990 forget=6.2797 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 25 it 50 total=12.7604 mle=1.7083 pcon=4.7973 forget=6.2548 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 25 it 100 total=12.6556 mle=1.6277 pcon=4.7958 forget=6.2321 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 25 it 150 total=12.8367 mle=1.7608 pcon=4.7943 forget=6.2816 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 25 it 200 total=12.7253 mle=1.6702 pcon=4.7922 forget=6.2629 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 25 it 250 total=12.7695 mle=1.7306 pcon=4.7905 forget=6.2484 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 25 it 300 total=12.6599 mle=1.6217 pcon=4.7887 forget=6.2495 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 25 it 350 total=12.6105 mle=1.5319 pcon=4.7873 forget=6.2913 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 26 it 10 total=12.5853 mle=1.5351 pcon=4.7857 forget=6.2646 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 26 it 60 total=12.5550 mle=1.4981 pcon=4.7840 forget=6.2728 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 26 it 110 total=12.6900 mle=1.6584 pcon=4.7823 forget=6.2492 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 26 it 160 total=12.7287 mle=1.6618 pcon=4.7806 forget=6.2863 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 26 it 210 total=12.5354 mle=1.5379 pcon=4.7789 forget=6.2186 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 26 it 260 total=12.8827 mle=1.8237 pcon=4.7775 forget=6.2816 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 26 it 310 total=12.7358 mle=1.6905 pcon=4.7759 forget=6.2695 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 26 it 360 total=12.6093 mle=1.5885 pcon=4.7742 forget=6.2465 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 27 it 20 total=12.7262 mle=1.6974 pcon=4.7727 forget=6.2561 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 27 it 70 total=12.5073 mle=1.5125 pcon=4.7712 forget=6.2235 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 27 it 120 total=12.5340 mle=1.4972 pcon=4.7697 forget=6.2671 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 27 it 170 total=12.5948 mle=1.5629 pcon=4.7683 forget=6.2636 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 27 it 220 total=12.6313 mle=1.5724 pcon=4.7670 forget=6.2919 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 27 it 270 total=12.5752 mle=1.5561 pcon=4.7655 forget=6.2536 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 27 it 320 total=12.6012 mle=1.5750 pcon=4.7641 forget=6.2620 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 27 it 370 total=12.6401 mle=1.6003 pcon=4.7626 forget=6.2772 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 28 it 30 total=12.6669 mle=1.6561 pcon=4.7613 forget=6.2496 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 28 it 80 total=12.5731 mle=1.5578 pcon=4.7600 forget=6.2553 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 28 it 130 total=12.4437 mle=1.4450 pcon=4.7587 forget=6.2399 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 28 it 180 total=12.5788 mle=1.5721 pcon=4.7575 forget=6.2492 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 28 it 230 total=12.5442 mle=1.5503 pcon=4.7561 forget=6.2378 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 28 it 280 total=12.6883 mle=1.6768 pcon=4.7550 forget=6.2565 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 28 it 330 total=12.6607 mle=1.6522 pcon=4.7537 forget=6.2548 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 28 it 380 total=12.5854 mle=1.5999 pcon=4.7523 forget=6.2332 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 29 it 40 total=12.8091 mle=1.8158 pcon=4.7510 forget=6.2422 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 29 it 90 total=12.4258 mle=1.4274 pcon=4.7499 forget=6.2484 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 29 it 140 total=12.5902 mle=1.6045 pcon=4.7487 forget=6.2370 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 29 it 190 total=12.5673 mle=1.5832 pcon=4.7475 forget=6.2367 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 29 it 240 total=12.4658 mle=1.4753 pcon=4.7464 forget=6.2440 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 29 it 290 total=12.7602 mle=1.7159 pcon=4.7454 forget=6.2989 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 29 it 340 total=12.5224 mle=1.4935 pcon=4.7441 forget=6.2848 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 30 it 0 total=12.5456 mle=1.5695 pcon=4.7431 forget=6.2330 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 30 it 50 total=12.4635 mle=1.4904 pcon=4.7419 forget=6.2312 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 30 it 100 total=12.5658 mle=1.5782 pcon=4.7409 forget=6.2468 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 30 it 150 total=12.5826 mle=1.6019 pcon=4.7400 forget=6.2408 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 30 it 200 total=12.6879 mle=1.6713 pcon=4.7388 forget=6.2778 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 30 it 250 total=12.5525 mle=1.5605 pcon=4.7377 forget=6.2542 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 30 it 300 total=12.5908 mle=1.5881 pcon=4.7368 forget=6.2659 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 30 it 350 total=12.5852 mle=1.6019 pcon=4.7357 forget=6.2476 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 31 it 10 total=12.5571 mle=1.5683 pcon=4.7347 forget=6.2541 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 31 it 60 total=12.5654 mle=1.5968 pcon=4.7339 forget=6.2347 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 31 it 110 total=12.5414 mle=1.5558 pcon=4.7329 forget=6.2526 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 31 it 160 total=12.7108 mle=1.7062 pcon=4.7322 forget=6.2724 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 31 it 210 total=12.5742 mle=1.5870 pcon=4.7311 forget=6.2560 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 31 it 260 total=12.5229 mle=1.5298 pcon=4.7301 forget=6.2629 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 31 it 310 total=12.4889 mle=1.5192 pcon=4.7290 forget=6.2407 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 31 it 360 total=12.6173 mle=1.6268 pcon=4.7282 forget=6.2623 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 32 it 20 total=12.4458 mle=1.4799 pcon=4.7273 forget=6.2386 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 32 it 70 total=12.7536 mle=1.7807 pcon=4.7264 forget=6.2465 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 32 it 120 total=12.6020 mle=1.6374 pcon=4.7254 forget=6.2392 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 32 it 170 total=12.5467 mle=1.5603 pcon=4.7246 forget=6.2618 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 32 it 220 total=12.5827 mle=1.6284 pcon=4.7238 forget=6.2304 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 32 it 270 total=12.5619 mle=1.5931 pcon=4.7230 forget=6.2458 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 32 it 320 total=12.6522 mle=1.6821 pcon=4.7221 forget=6.2480 nr=64 nf=64 protos=540 dmin_norm=NA
 66%|██████▌   | 33/50 [13:26<06:48, 24.05s/it] 68%|██████▊   | 34/50 [13:50<06:24, 24.02s/it] 70%|███████   | 35/50 [14:13<05:58, 23.92s/it] 72%|███████▏  | 36/50 [14:37<05:34, 23.90s/it] 74%|███████▍  | 37/50 [15:01<05:10, 23.88s/it] 76%|███████▌  | 38/50 [15:25<04:47, 23.94s/it] 78%|███████▊  | 39/50 [15:49<04:22, 23.90s/it] 80%|████████  | 40/50 [16:13<03:59, 23.94s/it] 82%|████████▏ | 41/50 [16:37<03:35, 23.95s/it][loss] ep 32 it 370 total=12.5299 mle=1.5572 pcon=4.7213 forget=6.2514 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 33 it 30 total=12.4511 mle=1.4997 pcon=4.7206 forget=6.2309 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 33 it 80 total=12.4265 mle=1.4762 pcon=4.7198 forget=6.2305 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 33 it 130 total=12.5557 mle=1.5909 pcon=4.7191 forget=6.2458 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 33 it 180 total=12.6484 mle=1.7003 pcon=4.7183 forget=6.2299 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 33 it 230 total=12.4643 mle=1.4983 pcon=4.7174 forget=6.2486 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 33 it 280 total=12.4761 mle=1.4742 pcon=4.7167 forget=6.2852 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 33 it 330 total=12.7887 mle=1.8149 pcon=4.7158 forget=6.2579 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 33 it 380 total=12.5100 mle=1.5635 pcon=4.7151 forget=6.2314 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 34 it 40 total=12.5081 mle=1.5473 pcon=4.7144 forget=6.2464 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 34 it 90 total=12.5132 mle=1.5396 pcon=4.7136 forget=6.2600 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 34 it 140 total=12.6158 mle=1.6783 pcon=4.7129 forget=6.2246 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 34 it 190 total=12.5363 mle=1.5756 pcon=4.7122 forget=6.2485 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 34 it 240 total=12.5994 mle=1.6451 pcon=4.7116 forget=6.2427 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 34 it 290 total=12.6403 mle=1.6738 pcon=4.7108 forget=6.2557 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 34 it 340 total=12.5148 mle=1.5550 pcon=4.7100 forget=6.2497 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 35 it 0 total=12.6287 mle=1.6601 pcon=4.7095 forget=6.2591 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 35 it 50 total=12.5665 mle=1.6120 pcon=4.7089 forget=6.2457 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 35 it 100 total=12.5993 mle=1.6371 pcon=4.7082 forget=6.2540 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 35 it 150 total=12.5820 mle=1.6257 pcon=4.7076 forget=6.2487 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 35 it 200 total=12.7074 mle=1.6826 pcon=4.7072 forget=6.3177 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 35 it 250 total=12.5876 mle=1.6478 pcon=4.7064 forget=6.2334 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 35 it 300 total=12.6594 mle=1.7125 pcon=4.7059 forget=6.2410 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 35 it 350 total=12.6487 mle=1.6922 pcon=4.7053 forget=6.2513 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 36 it 10 total=12.5160 mle=1.5653 pcon=4.7047 forget=6.2460 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 36 it 60 total=12.6116 mle=1.6714 pcon=4.7041 forget=6.2362 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 36 it 110 total=12.5211 mle=1.5829 pcon=4.7036 forget=6.2346 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 36 it 160 total=12.6348 mle=1.6528 pcon=4.7029 forget=6.2791 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 36 it 210 total=12.3967 mle=1.4287 pcon=4.7024 forget=6.2656 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 36 it 260 total=12.6103 mle=1.6287 pcon=4.7019 forget=6.2797 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 36 it 310 total=12.6302 mle=1.6916 pcon=4.7013 forget=6.2373 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 36 it 360 total=12.6207 mle=1.6475 pcon=4.7008 forget=6.2724 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 37 it 20 total=12.4781 mle=1.5113 pcon=4.7002 forget=6.2666 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 37 it 70 total=12.5879 mle=1.6143 pcon=4.6999 forget=6.2738 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 37 it 120 total=12.5365 mle=1.5916 pcon=4.6991 forget=6.2458 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 37 it 170 total=12.5459 mle=1.5979 pcon=4.6986 forget=6.2494 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 37 it 220 total=12.5391 mle=1.5895 pcon=4.6981 forget=6.2515 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 37 it 270 total=12.4100 mle=1.4482 pcon=4.6978 forget=6.2640 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 37 it 320 total=12.7272 mle=1.7636 pcon=4.6974 forget=6.2662 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 37 it 370 total=12.6832 mle=1.7281 pcon=4.6969 forget=6.2582 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 38 it 30 total=12.6966 mle=1.7559 pcon=4.6965 forget=6.2441 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 38 it 80 total=12.6658 mle=1.7176 pcon=4.6961 forget=6.2520 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 38 it 130 total=12.6591 mle=1.7146 pcon=4.6956 forget=6.2489 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 38 it 180 total=12.4650 mle=1.5084 pcon=4.6952 forget=6.2614 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 38 it 230 total=12.4915 mle=1.5294 pcon=4.6947 forget=6.2674 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 38 it 280 total=12.6166 mle=1.6647 pcon=4.6941 forget=6.2578 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 38 it 330 total=12.6655 mle=1.7076 pcon=4.6936 forget=6.2643 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 38 it 380 total=12.5675 mle=1.6187 pcon=4.6931 forget=6.2556 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 39 it 40 total=12.7576 mle=1.7958 pcon=4.6928 forget=6.2691 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 39 it 90 total=12.5595 mle=1.6041 pcon=4.6923 forget=6.2630 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 39 it 140 total=12.4732 mle=1.5230 pcon=4.6921 forget=6.2581 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 39 it 190 total=12.5550 mle=1.5860 pcon=4.6916 forget=6.2774 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 39 it 240 total=12.5538 mle=1.6124 pcon=4.6912 forget=6.2503 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 39 it 290 total=12.6929 mle=1.7529 pcon=4.6907 forget=6.2493 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 39 it 340 total=12.5849 mle=1.6418 pcon=4.6904 forget=6.2528 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 40 it 0 total=12.4497 mle=1.4940 pcon=4.6901 forget=6.2656 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 40 it 50 total=12.4493 mle=1.4899 pcon=4.6897 forget=6.2697 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 40 it 100 total=12.6834 mle=1.6804 pcon=4.6894 forget=6.3136 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 40 it 150 total=12.4487 mle=1.5169 pcon=4.6891 forget=6.2427 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 40 it 200 total=12.6034 mle=1.6611 pcon=4.6889 forget=6.2534 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 40 it 250 total=12.5463 mle=1.5415 pcon=4.6884 forget=6.3164 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 40 it 300 total=12.6504 mle=1.6840 pcon=4.6881 forget=6.2784 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 40 it 350 total=12.4989 mle=1.5486 pcon=4.6877 forget=6.2627 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
[loss] ep 41 it 10 total=12.6140 mle=1.6312 pcon=4.6874 forget=6.2954 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 41 it 60 total=12.6603 mle=1.7011 pcon=4.6871 forget=6.2721 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 41 it 110 total=12.5194 mle=1.5682 pcon=4.6868 forget=6.2645 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 41 it 160 total=12.5066 mle=1.5221 pcon=4.6863 forget=6.2982 nr=64 nf=64 protos=540 dmin_norm=NA
 84%|████████▍ | 42/50 [17:01<03:11, 23.98s/it] 86%|████████▌ | 43/50 [17:25<02:47, 23.99s/it] 88%|████████▊ | 44/50 [17:49<02:23, 23.96s/it] 90%|█████████ | 45/50 [18:13<01:59, 23.95s/it] 92%|█████████▏| 46/50 [18:37<01:35, 23.95s/it] 94%|█████████▍| 47/50 [19:00<01:11, 23.90s/it] 96%|█████████▌| 48/50 [19:24<00:47, 23.90s/it] 98%|█████████▊| 49/50 [19:48<00:23, 23.86s/it]100%|██████████| 50/50 [20:12<00:00, 23.94s/it]100%|██████████| 50/50 [20:12<00:00, 24.26s/it]
[loss] ep 41 it 210 total=12.4754 mle=1.5045 pcon=4.6861 forget=6.2848 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 41 it 260 total=12.6164 mle=1.6684 pcon=4.6857 forget=6.2623 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 41 it 310 total=12.5558 mle=1.5955 pcon=4.6855 forget=6.2749 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 41 it 360 total=12.3886 mle=1.4257 pcon=4.6851 forget=6.2778 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 42 it 20 total=12.5429 mle=1.6065 pcon=4.6851 forget=6.2513 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 42 it 70 total=12.6168 mle=1.6624 pcon=4.6848 forget=6.2696 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 42 it 120 total=12.6299 mle=1.6840 pcon=4.6845 forget=6.2614 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 42 it 170 total=12.6400 mle=1.6998 pcon=4.6842 forget=6.2560 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 42 it 220 total=12.6144 mle=1.6625 pcon=4.6839 forget=6.2680 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 42 it 270 total=12.5486 mle=1.5868 pcon=4.6836 forget=6.2782 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 42 it 320 total=12.6068 mle=1.6595 pcon=4.6832 forget=6.2641 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 42 it 370 total=12.5637 mle=1.6058 pcon=4.6828 forget=6.2751 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 43 it 30 total=12.5325 mle=1.5524 pcon=4.6825 forget=6.2976 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 43 it 80 total=12.7707 mle=1.7761 pcon=4.6823 forget=6.3124 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 43 it 130 total=12.5396 mle=1.5971 pcon=4.6819 forget=6.2605 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 43 it 180 total=12.6019 mle=1.6277 pcon=4.6818 forget=6.2924 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 43 it 230 total=12.5698 mle=1.5993 pcon=4.6817 forget=6.2889 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 43 it 280 total=12.6206 mle=1.6568 pcon=4.6815 forget=6.2823 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 43 it 330 total=12.5628 mle=1.5730 pcon=4.6811 forget=6.3087 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 43 it 380 total=12.3332 mle=1.3868 pcon=4.6809 forget=6.2656 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 44 it 40 total=12.6328 mle=1.6900 pcon=4.6806 forget=6.2622 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 44 it 90 total=12.8267 mle=1.8632 pcon=4.6804 forget=6.2831 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 44 it 140 total=12.5308 mle=1.5920 pcon=4.6803 forget=6.2585 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 44 it 190 total=12.7462 mle=1.7817 pcon=4.6800 forget=6.2846 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 44 it 240 total=12.5404 mle=1.5802 pcon=4.6798 forget=6.2804 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 44 it 290 total=12.6006 mle=1.6595 pcon=4.6797 forget=6.2614 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 44 it 340 total=12.6101 mle=1.6497 pcon=4.6795 forget=6.2809 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 45 it 0 total=12.4698 mle=1.5187 pcon=4.6793 forget=6.2717 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 45 it 50 total=12.4557 mle=1.5229 pcon=4.6791 forget=6.2537 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 45 it 100 total=12.5737 mle=1.6275 pcon=4.6789 forget=6.2673 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 45 it 150 total=12.6131 mle=1.6607 pcon=4.6787 forget=6.2737 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 45 it 200 total=12.5413 mle=1.5931 pcon=4.6784 forget=6.2699 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 45 it 250 total=12.5819 mle=1.6259 pcon=4.6781 forget=6.2779 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 45 it 300 total=12.5554 mle=1.5937 pcon=4.6780 forget=6.2838 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 45 it 350 total=12.5570 mle=1.5748 pcon=4.6778 forget=6.3044 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 46 it 10 total=12.4728 mle=1.5241 pcon=4.6775 forget=6.2711 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 46 it 60 total=12.5114 mle=1.5522 pcon=4.6774 forget=6.2818 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 46 it 110 total=12.5759 mle=1.6336 pcon=4.6772 forget=6.2651 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 46 it 160 total=12.6215 mle=1.6803 pcon=4.6769 forget=6.2643 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 46 it 210 total=12.4769 mle=1.5516 pcon=4.6768 forget=6.2485 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 46 it 260 total=12.5108 mle=1.5720 pcon=4.6765 forget=6.2622 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 46 it 310 total=12.6180 mle=1.6105 pcon=4.6765 forget=6.3311 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 46 it 360 total=12.7078 mle=1.7356 pcon=4.6764 forget=6.2959 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 47 it 20 total=12.5845 mle=1.6263 pcon=4.6761 forget=6.2820 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 47 it 70 total=12.5249 mle=1.5638 pcon=4.6759 forget=6.2851 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 47 it 120 total=12.5075 mle=1.5464 pcon=4.6758 forget=6.2852 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 47 it 170 total=12.6403 mle=1.7019 pcon=4.6758 forget=6.2626 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 47 it 220 total=12.5856 mle=1.6360 pcon=4.6757 forget=6.2739 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 47 it 270 total=12.5867 mle=1.6620 pcon=4.6755 forget=6.2492 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 47 it 320 total=12.5473 mle=1.5906 pcon=4.6753 forget=6.2814 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 47 it 370 total=12.6520 mle=1.6572 pcon=4.6752 forget=6.3196 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 48 it 30 total=12.6254 mle=1.6699 pcon=4.6749 forget=6.2807 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 48 it 80 total=12.6049 mle=1.6285 pcon=4.6746 forget=6.3018 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 48 it 130 total=12.5334 mle=1.5938 pcon=4.6745 forget=6.2651 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 48 it 180 total=12.7916 mle=1.7485 pcon=4.6744 forget=6.3687 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 48 it 230 total=12.7286 mle=1.7767 pcon=4.6742 forget=6.2777 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 48 it 280 total=12.6976 mle=1.7151 pcon=4.6741 forget=6.3084 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 48 it 330 total=12.5908 mle=1.6586 pcon=4.6739 forget=6.2583 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 48 it 380 total=12.6406 mle=1.6729 pcon=4.6737 forget=6.2941 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 49 it 40 total=12.5923 mle=1.6261 pcon=4.6736 forget=6.2927 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 49 it 90 total=12.6003 mle=1.6497 pcon=4.6735 forget=6.2771 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 49 it 140 total=12.7530 mle=1.7977 pcon=4.6734 forget=6.2819 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 49 it 190 total=12.6217 mle=1.6459 pcon=4.6732 forget=6.3026 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 49 it 240 total=12.4582 mle=1.5307 pcon=4.6732 forget=6.2543 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 49 it 290 total=12.6608 mle=1.7103 pcon=4.6729 forget=6.2776 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 49 it 340 total=12.6305 mle=1.6321 pcon=4.6728 forget=6.3256 nr=64 nf=64 protos=540 dmin_norm=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05: Number of model parameters: 22187968
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:51,  3.51it/s]  2%|▏         | 9/391 [00:00<00:13, 27.83it/s]  4%|▍         | 17/391 [00:00<00:08, 42.52it/s]  6%|▋         | 25/391 [00:00<00:07, 51.86it/s]  8%|▊         | 33/391 [00:00<00:06, 57.89it/s] 10%|█         | 41/391 [00:00<00:05, 62.02it/s] 13%|█▎        | 49/391 [00:00<00:05, 64.75it/s] 15%|█▍        | 57/391 [00:01<00:05, 66.49it/s] 17%|█▋        | 65/391 [00:01<00:04, 67.78it/s] 19%|█▊        | 73/391 [00:01<00:04, 68.73it/s] 21%|██        | 81/391 [00:01<00:04, 69.33it/s] 23%|██▎       | 89/391 [00:01<00:04, 69.84it/s] 25%|██▍       | 97/391 [00:01<00:04, 70.12it/s] 27%|██▋       | 105/391 [00:01<00:04, 70.30it/s] 29%|██▉       | 113/391 [00:01<00:03, 70.51it/s] 31%|███       | 121/391 [00:01<00:03, 70.61it/s] 33%|███▎      | 129/391 [00:02<00:03, 70.67it/s] 35%|███▌      | 137/391 [00:02<00:03, 70.70it/s] 37%|███▋      | 145/391 [00:02<00:03, 70.75it/s] 39%|███▉      | 153/391 [00:02<00:03, 70.75it/s] 41%|████      | 161/391 [00:02<00:03, 70.77it/s] 43%|████▎     | 169/391 [00:02<00:03, 70.77it/s] 45%|████▌     | 177/391 [00:02<00:03, 70.82it/s] 47%|████▋     | 185/391 [00:02<00:02, 70.28it/s] 49%|████▉     | 193/391 [00:02<00:02, 70.44it/s] 51%|█████▏    | 201/391 [00:03<00:02, 70.56it/s] 53%|█████▎    | 209/391 [00:03<00:02, 70.73it/s] 55%|█████▌    | 217/391 [00:03<00:02, 70.78it/s] 58%|█████▊    | 225/391 [00:03<00:02, 70.79it/s] 60%|█████▉    | 233/391 [00:03<00:02, 70.80it/s] 62%|██████▏   | 241/391 [00:03<00:02, 70.34it/s] 64%|██████▎   | 249/391 [00:03<00:02, 70.40it/s] 66%|██████▌   | 257/391 [00:03<00:01, 70.53it/s] 68%|██████▊   | 265/391 [00:04<00:01, 70.62it/s] 70%|██████▉   | 273/391 [00:04<00:01, 70.66it/s] 72%|███████▏  | 281/391 [00:04<00:01, 70.67it/s] 74%|███████▍  | 289/391 [00:04<00:01, 70.68it/s] 76%|███████▌  | 297/391 [00:04<00:01, 70.78it/s] 78%|███████▊  | 305/391 [00:04<00:01, 70.70it/s] 80%|████████  | 313/391 [00:04<00:01, 70.73it/s] 82%|████████▏ | 321/391 [00:04<00:00, 70.71it/s] 84%|████████▍ | 329/391 [00:04<00:00, 70.69it/s] 86%|████████▌ | 337/391 [00:05<00:00, 70.72it/s] 88%|████████▊ | 345/391 [00:05<00:00, 70.73it/s] 90%|█████████ | 353/391 [00:05<00:00, 70.75it/s] 92%|█████████▏| 361/391 [00:05<00:00, 70.74it/s] 94%|█████████▍| 369/391 [00:05<00:00, 70.79it/s] 96%|█████████▋| 377/391 [00:05<00:00, 70.63it/s] 98%|█████████▊| 385/391 [00:05<00:00, 70.82it/s]100%|██████████| 391/391 [00:05<00:00, 67.33it/s]
50000 images processed, 5.912813901901245 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:18,  4.12it/s] 11%|█▏        | 9/79 [00:00<00:02, 30.73it/s] 22%|██▏       | 17/79 [00:00<00:01, 45.20it/s] 32%|███▏      | 25/79 [00:00<00:01, 53.91it/s] 42%|████▏     | 33/79 [00:00<00:00, 59.33it/s] 52%|█████▏    | 41/79 [00:00<00:00, 62.94it/s] 62%|██████▏   | 49/79 [00:00<00:00, 65.32it/s] 72%|███████▏  | 57/79 [00:01<00:00, 66.96it/s] 82%|████████▏ | 65/79 [00:01<00:00, 68.02it/s] 92%|█████████▏| 73/79 [00:01<00:00, 68.99it/s]100%|██████████| 79/79 [00:01<00:00, 57.89it/s]
10000 images processed, 1.384371280670166 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<00:45,  4.42it/s]  4%|▍         | 9/204 [00:00<00:06, 32.05it/s]  8%|▊         | 17/204 [00:00<00:04, 46.40it/s] 12%|█▏        | 25/204 [00:00<00:03, 54.78it/s] 16%|█▌        | 32/204 [00:00<00:02, 59.40it/s] 20%|█▉        | 40/204 [00:00<00:02, 62.98it/s] 23%|██▎       | 47/204 [00:00<00:02, 65.01it/s] 27%|██▋       | 55/204 [00:00<00:02, 66.78it/s] 31%|███       | 63/204 [00:01<00:02, 67.84it/s] 34%|███▍      | 70/204 [00:01<00:01, 68.44it/s] 38%|███▊      | 77/204 [00:01<00:01, 68.68it/s] 42%|████▏     | 85/204 [00:01<00:01, 69.17it/s] 46%|████▌     | 93/204 [00:01<00:01, 69.53it/s] 50%|████▉     | 101/204 [00:01<00:01, 69.74it/s] 53%|█████▎    | 108/204 [00:01<00:01, 69.73it/s] 57%|█████▋    | 116/204 [00:01<00:01, 69.79it/s] 60%|██████    | 123/204 [00:01<00:01, 69.83it/s] 64%|██████▎   | 130/204 [00:02<00:01, 69.86it/s] 68%|██████▊   | 138/204 [00:02<00:00, 70.05it/s] 72%|███████▏  | 146/204 [00:02<00:00, 70.12it/s] 75%|███████▌  | 154/204 [00:02<00:00, 70.04it/s] 79%|███████▉  | 162/204 [00:02<00:00, 70.07it/s] 83%|████████▎ | 170/204 [00:02<00:00, 69.79it/s] 87%|████████▋ | 178/204 [00:02<00:00, 69.95it/s] 91%|█████████ | 185/204 [00:02<00:00, 69.84it/s] 94%|█████████▍| 192/204 [00:02<00:00, 69.84it/s] 98%|█████████▊| 200/204 [00:03<00:00, 70.12it/s]100%|██████████| 204/204 [00:03<00:00, 65.12it/s]
26032 images processed, 3.171544075012207 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:35,  2.22it/s] 11%|█▏        | 9/79 [00:00<00:03, 17.95it/s] 20%|██        | 16/79 [00:00<00:02, 29.70it/s] 28%|██▊       | 22/79 [00:00<00:01, 35.98it/s] 35%|███▌      | 28/79 [00:00<00:01, 41.20it/s] 43%|████▎     | 34/79 [00:01<00:00, 45.23it/s] 52%|█████▏    | 41/79 [00:01<00:00, 49.20it/s] 61%|██████    | 48/79 [00:01<00:00, 54.46it/s] 68%|██████▊   | 54/79 [00:01<00:00, 54.85it/s] 76%|███████▌  | 60/79 [00:01<00:00, 55.60it/s] 84%|████████▎ | 66/79 [00:01<00:00, 55.93it/s] 92%|█████████▏| 73/79 [00:01<00:00, 57.03it/s]100%|██████████| 79/79 [00:01<00:00, 44.28it/s]
10000 images processed, 1.819408655166626 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:21,  3.55it/s] 11%|█▏        | 9/79 [00:00<00:02, 28.01it/s] 22%|██▏       | 17/79 [00:00<00:01, 42.63it/s] 32%|███▏      | 25/79 [00:00<00:01, 51.75it/s] 42%|████▏     | 33/79 [00:00<00:00, 57.70it/s] 52%|█████▏    | 41/79 [00:00<00:00, 61.60it/s] 62%|██████▏   | 49/79 [00:00<00:00, 64.26it/s] 72%|███████▏  | 57/79 [00:01<00:00, 66.16it/s] 82%|████████▏ | 65/79 [00:01<00:00, 67.57it/s] 92%|█████████▏| 73/79 [00:01<00:00, 68.66it/s]100%|██████████| 79/79 [00:01<00:00, 57.29it/s]
10000 images processed, 1.4062886238098145 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:16,  4.11it/s] 11%|█▏        | 8/70 [00:00<00:02, 28.26it/s] 23%|██▎       | 16/70 [00:00<00:01, 44.02it/s] 34%|███▍      | 24/70 [00:00<00:00, 53.29it/s] 46%|████▌     | 32/70 [00:00<00:00, 59.06it/s] 57%|█████▋    | 40/70 [00:00<00:00, 62.70it/s] 67%|██████▋   | 47/70 [00:00<00:00, 64.64it/s] 79%|███████▊  | 55/70 [00:01<00:00, 66.58it/s] 89%|████████▊ | 62/70 [00:01<00:00, 67.17it/s]100%|██████████| 70/70 [00:01<00:00, 66.73it/s]100%|██████████| 70/70 [00:01<00:00, 56.61it/s]
8925 images processed, 1.2661526203155518 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:30,  1.45it/s]  4%|▍         | 2/45 [00:00<00:15,  2.81it/s] 20%|██        | 9/45 [00:01<00:02, 12.13it/s] 24%|██▍       | 11/45 [00:01<00:02, 11.42it/s] 38%|███▊      | 17/45 [00:01<00:01, 15.24it/s] 42%|████▏     | 19/45 [00:01<00:01, 15.20it/s] 49%|████▉     | 22/45 [00:01<00:01, 17.02it/s] 56%|█████▌    | 25/45 [00:01<00:01, 16.82it/s] 60%|██████    | 27/45 [00:02<00:01, 16.76it/s] 67%|██████▋   | 30/45 [00:02<00:00, 15.92it/s] 73%|███████▎  | 33/45 [00:02<00:00, 13.61it/s] 80%|████████  | 36/45 [00:02<00:00, 16.29it/s] 91%|█████████ | 41/45 [00:03<00:00, 16.03it/s] 96%|█████████▌| 43/45 [00:03<00:00, 12.86it/s]100%|██████████| 45/45 [00:03<00:00, 13.35it/s]
5640 images processed, 3.3930113315582275 seconds used

19.77729296684265
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.53  99.41
places365     75.68  79.51
LSUN          19.85  96.16
iSUN          82.41  78.59
dtd           31.52  92.68
forget        44.70  91.29
AVG           42.78  89.61
Forget-Acc: 0.8000 | Retain-Acc: 0.7127
Forget-as-OOD (retain known vs forget novel):
  FPR: 44.70 AUROC: 91.29 AUIN: 98.89
11.503686428070068
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth_all-bfmbalanced-fl1-lora_r8a32d0.05_rf.png
