nohup: ignoring input
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=25, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/25 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  4%|▍         | 1/25 [00:27<10:54, 27.26s/it]  8%|▊         | 2/25 [00:46<08:37, 22.49s/it] 12%|█▏        | 3/25 [01:06<07:48, 21.31s/it] 16%|█▌        | 4/25 [01:23<06:57, 19.86s/it] 20%|██        | 5/25 [01:43<06:34, 19.71s/it] 24%|██▍       | 6/25 [02:04<06:20, 20.03s/it][loss] ep 0 it 0 total=9.2464 mle=1.5919 pcon=5.2950 forget=2.3595 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.2173 mle=1.5428 pcon=5.2893 forget=2.3852 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.1525 mle=1.4804 pcon=5.2835 forget=2.3887 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.1413 mle=1.5239 pcon=5.2780 forget=2.3395 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.1268 mle=1.5435 pcon=5.2723 forget=2.3110 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.2533 mle=1.5947 pcon=5.2669 forget=2.3917 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.1129 mle=1.4612 pcon=5.2615 forget=2.3901 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.0807 mle=1.4115 pcon=5.2563 forget=2.4128 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 1 it 10 total=9.1709 mle=1.5103 pcon=5.2514 forget=2.4092 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0498 mle=1.3920 pcon=5.2463 forget=2.4115 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=9.1727 mle=1.5416 pcon=5.2413 forget=2.3898 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=9.0243 mle=1.3751 pcon=5.2365 forget=2.4126 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.1646 mle=1.5295 pcon=5.2317 forget=2.4034 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=9.1583 mle=1.5851 pcon=5.2272 forget=2.3461 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=9.1618 mle=1.5566 pcon=5.2231 forget=2.3821 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0182 mle=1.4155 pcon=5.2187 forget=2.3840 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 2 it 20 total=8.9863 mle=1.4031 pcon=5.2143 forget=2.3689 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0151 mle=1.3856 pcon=5.2098 forget=2.4197 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=9.0635 mle=1.4539 pcon=5.2054 forget=2.4042 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9453 mle=1.3730 pcon=5.2014 forget=2.3708 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=9.0903 mle=1.5262 pcon=5.1973 forget=2.3668 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0293 mle=1.4580 pcon=5.1933 forget=2.3779 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=9.0758 mle=1.4983 pcon=5.1895 forget=2.3880 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=8.9669 mle=1.3827 pcon=5.1856 forget=2.3987 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 3 it 30 total=9.0742 mle=1.4669 pcon=5.1820 forget=2.4253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=9.0811 mle=1.4759 pcon=5.1779 forget=2.4273 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.9772 mle=1.3958 pcon=5.1743 forget=2.4070 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=8.9707 mle=1.3941 pcon=5.1704 forget=2.4062 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9434 mle=1.3594 pcon=5.1668 forget=2.4172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0453 mle=1.4911 pcon=5.1636 forget=2.3906 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=8.9637 mle=1.4166 pcon=5.1608 forget=2.3863 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9645 mle=1.4196 pcon=5.1573 forget=2.3875 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 4 it 40 total=8.8703 mle=1.3392 pcon=5.1543 forget=2.3768 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9022 mle=1.3715 pcon=5.1513 forget=2.3794 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=8.9063 mle=1.3422 pcon=5.1481 forget=2.4159 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.0598 mle=1.5139 pcon=5.1451 forget=2.4009 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.1052 mle=1.5810 pcon=5.1420 forget=2.3821 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.0218 mle=1.4746 pcon=5.1390 forget=2.4081 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=8.8534 mle=1.3719 pcon=5.1361 forget=2.3454 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 5 it 0 total=8.9740 mle=1.4602 pcon=5.1332 forget=2.3806 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=8.9348 mle=1.3975 pcon=5.1306 forget=2.4067 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.0291 mle=1.5439 pcon=5.1278 forget=2.3574 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.0232 mle=1.5443 pcon=5.1254 forget=2.3536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.0748 mle=1.5901 pcon=5.1228 forget=2.3619 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=8.8939 mle=1.4260 pcon=5.1201 forget=2.3478 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=8.8281 mle=1.3561 pcon=5.1178 forget=2.3543 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=8.9904 mle=1.4913 pcon=5.1152 forget=2.3840 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 6 it 10 total=9.0041 mle=1.5243 pcon=5.1124 forget=2.3675 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=8.8049 mle=1.3002 pcon=5.1101 forget=2.3946 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=8.8599 mle=1.3677 pcon=5.1080 forget=2.3842 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.8701 mle=1.4230 pcon=5.1059 forget=2.3412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9503 mle=1.4804 pcon=5.1033 forget=2.3666 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=8.9028 mle=1.4306 pcon=5.1014 forget=2.3708 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=8.9396 mle=1.5030 pcon=5.0992 forget=2.3374 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 28%|██▊       | 7/25 [02:20<05:41, 18.97s/it] 32%|███▏      | 8/25 [02:36<05:05, 17.99s/it] 36%|███▌      | 9/25 [02:56<04:59, 18.70s/it] 40%|████      | 10/25 [03:17<04:49, 19.28s/it] 44%|████▍     | 11/25 [03:35<04:22, 18.77s/it] 48%|████▊     | 12/25 [03:56<04:13, 19.49s/it] 52%|█████▏    | 13/25 [04:19<04:06, 20.50s/it][loss] ep 6 it 360 total=8.9177 mle=1.4553 pcon=5.0974 forget=2.3651 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 7 it 20 total=8.8179 mle=1.3108 pcon=5.0952 forget=2.4118 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=8.8762 mle=1.4492 pcon=5.0936 forget=2.3334 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.7396 mle=1.2789 pcon=5.0914 forget=2.3693 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.8244 mle=1.3771 pcon=5.0895 forget=2.3578 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=8.9283 mle=1.4411 pcon=5.0877 forget=2.3995 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=8.9028 mle=1.4628 pcon=5.0860 forget=2.3540 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=8.9546 mle=1.4981 pcon=5.0843 forget=2.3722 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=8.7866 mle=1.2791 pcon=5.0827 forget=2.4248 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 8 it 30 total=8.8966 mle=1.4739 pcon=5.0812 forget=2.3415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=8.7773 mle=1.3069 pcon=5.0794 forget=2.3910 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=8.8397 mle=1.3975 pcon=5.0780 forget=2.3642 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=8.8546 mle=1.4281 pcon=5.0762 forget=2.3503 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=8.7712 mle=1.3161 pcon=5.0748 forget=2.3803 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.8350 mle=1.4375 pcon=5.0730 forget=2.3245 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=8.6754 mle=1.3047 pcon=5.0715 forget=2.2993 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.9320 mle=1.5152 pcon=5.0702 forget=2.3466 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 9 it 40 total=8.7883 mle=1.3333 pcon=5.0688 forget=2.3862 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=8.7405 mle=1.3422 pcon=5.0674 forget=2.3308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=8.7196 mle=1.3373 pcon=5.0660 forget=2.3163 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.6648 mle=1.2756 pcon=5.0647 forget=2.3245 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=8.8266 mle=1.5485 pcon=5.0633 forget=2.2148 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=8.4702 mle=1.2235 pcon=5.0621 forget=2.1847 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.6058 mle=1.3626 pcon=5.0607 forget=2.1825 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 10 it 0 total=8.4137 mle=1.3304 pcon=5.0592 forget=2.0241 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.4603 mle=1.3792 pcon=5.0584 forget=2.0227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=8.4502 mle=1.4022 pcon=5.0574 forget=1.9906 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=8.4600 mle=1.4111 pcon=5.0564 forget=1.9924 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=8.3367 mle=1.4013 pcon=5.0556 forget=1.8798 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=8.4040 mle=1.4409 pcon=5.0551 forget=1.9080 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=8.3334 mle=1.3804 pcon=5.0547 forget=1.8983 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.3295 mle=1.3560 pcon=5.0546 forget=1.9189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 11 it 10 total=8.3934 mle=1.4295 pcon=5.0544 forget=1.9096 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.3748 mle=1.4300 pcon=5.0546 forget=1.8902 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.3937 mle=1.4383 pcon=5.0550 forget=1.9003 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.3721 mle=1.3964 pcon=5.0553 forget=1.9204 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.3868 mle=1.4554 pcon=5.0558 forget=1.8756 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.4063 mle=1.4244 pcon=5.0568 forget=1.9251 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.4481 mle=1.4803 pcon=5.0575 forget=1.9103 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.3835 mle=1.4034 pcon=5.0584 forget=1.9218 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 12 it 20 total=8.4218 mle=1.4525 pcon=5.0595 forget=1.9098 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.4135 mle=1.4119 pcon=5.0607 forget=1.9410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.4233 mle=1.4492 pcon=5.0619 forget=1.9122 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.5170 mle=1.4881 pcon=5.0632 forget=1.9657 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.4914 mle=1.5074 pcon=5.0645 forget=1.9195 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.4790 mle=1.4707 pcon=5.0660 forget=1.9423 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.5645 mle=1.5574 pcon=5.0673 forget=1.9397 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.5859 mle=1.5706 pcon=5.0684 forget=1.9470 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 30 total=8.4073 mle=1.3978 pcon=5.0699 forget=1.9396 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.4793 mle=1.4663 pcon=5.0712 forget=1.9418 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.3934 mle=1.3698 pcon=5.0722 forget=1.9513 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=8.4457 mle=1.4538 pcon=5.0733 forget=1.9187 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 230 total=8.4339 mle=1.4529 pcon=5.0740 forget=1.9070 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 56%|█████▌    | 14/25 [04:36<03:36, 19.64s/it] 60%|██████    | 15/25 [04:58<03:22, 20.22s/it] 64%|██████▍   | 16/25 [05:19<03:05, 20.58s/it] 68%|██████▊   | 17/25 [05:37<02:38, 19.77s/it] 72%|███████▏  | 18/25 [05:56<02:16, 19.54s/it] 76%|███████▌  | 19/25 [06:14<01:54, 19.13s/it] 80%|████████  | 20/25 [06:33<01:35, 19.12s/it][loss] ep 13 it 280 total=8.4376 mle=1.4224 pcon=5.0747 forget=1.9405 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.4103 mle=1.4453 pcon=5.0754 forget=1.8895 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.4122 mle=1.4609 pcon=5.0760 forget=1.8752 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 40 total=8.3794 mle=1.4278 pcon=5.0763 forget=1.8753 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.3778 mle=1.4300 pcon=5.0767 forget=1.8710 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.3579 mle=1.4392 pcon=5.0767 forget=1.8420 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.2078 mle=1.3078 pcon=5.0767 forget=1.8234 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.2909 mle=1.3848 pcon=5.0767 forget=1.8293 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.2820 mle=1.4167 pcon=5.0765 forget=1.7888 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.2954 mle=1.4426 pcon=5.0764 forget=1.7765 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 15 it 0 total=8.2445 mle=1.4129 pcon=5.0755 forget=1.7561 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.1859 mle=1.3794 pcon=5.0746 forget=1.7318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.2338 mle=1.4254 pcon=5.0740 forget=1.7344 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.2267 mle=1.4389 pcon=5.0730 forget=1.7149 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.1949 mle=1.4313 pcon=5.0721 forget=1.6915 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.1317 mle=1.4033 pcon=5.0714 forget=1.6569 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.1221 mle=1.4251 pcon=5.0705 forget=1.6265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.1559 mle=1.4550 pcon=5.0692 forget=1.6317 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 16 it 10 total=8.1440 mle=1.4271 pcon=5.0679 forget=1.6490 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.0672 mle=1.3979 pcon=5.0662 forget=1.6031 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.0702 mle=1.4268 pcon=5.0651 forget=1.5783 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.0485 mle=1.3775 pcon=5.0635 forget=1.6075 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.0466 mle=1.4122 pcon=5.0615 forget=1.5729 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=7.9925 mle=1.3375 pcon=5.0597 forget=1.5952 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.0237 mle=1.4209 pcon=5.0579 forget=1.5449 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.0154 mle=1.4070 pcon=5.0563 forget=1.5521 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 17 it 20 total=8.0096 mle=1.4310 pcon=5.0544 forget=1.5241 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=7.9027 mle=1.3082 pcon=5.0525 forget=1.5420 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=7.8752 mle=1.3044 pcon=5.0500 forget=1.5208 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.0488 mle=1.4652 pcon=5.0480 forget=1.5355 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.0112 mle=1.4297 pcon=5.0459 forget=1.5355 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=7.9683 mle=1.3804 pcon=5.0437 forget=1.5443 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=7.9094 mle=1.3379 pcon=5.0413 forget=1.5301 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=7.9044 mle=1.3446 pcon=5.0391 forget=1.5206 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 18 it 30 total=7.9044 mle=1.3017 pcon=5.0369 forget=1.5658 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.0539 mle=1.4120 pcon=5.0348 forget=1.6071 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=7.9397 mle=1.3150 pcon=5.0324 forget=1.5924 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.0105 mle=1.3603 pcon=5.0303 forget=1.6199 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=7.9962 mle=1.3423 pcon=5.0281 forget=1.6259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.0637 mle=1.3808 pcon=5.0259 forget=1.6570 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.1185 mle=1.4204 pcon=5.0238 forget=1.6743 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.1984 mle=1.4546 pcon=5.0216 forget=1.7223 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.0825 mle=1.3654 pcon=5.0195 forget=1.6976 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.1168 mle=1.3425 pcon=5.0176 forget=1.7567 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.2454 mle=1.4579 pcon=5.0157 forget=1.7718 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.2262 mle=1.4265 pcon=5.0139 forget=1.7858 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.3065 mle=1.4562 pcon=5.0121 forget=1.8383 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.1760 mle=1.3441 pcon=5.0102 forget=1.8216 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.2756 mle=1.4137 pcon=5.0087 forget=1.8532 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 0 total=8.3966 mle=1.4543 pcon=5.0072 forget=1.9351 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.3056 mle=1.3893 pcon=5.0058 forget=1.9104 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.2715 mle=1.3294 pcon=5.0046 forget=1.9374 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.5572 mle=1.5880 pcon=5.0035 forget=1.9657 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.3949 mle=1.4076 pcon=5.0026 forget=1.9847 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=8.4354 mle=1.4266 pcon=5.0015 forget=2.0073 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=8.5328 mle=1.4892 pcon=5.0005 forget=2.0431 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 84%|████████▍ | 21/25 [06:56<01:20, 20.15s/it] 88%|████████▊ | 22/25 [07:15<00:59, 19.70s/it] 92%|█████████▏| 23/25 [07:35<00:39, 19.91s/it] 96%|█████████▌| 24/25 [07:56<00:20, 20.19s/it]100%|██████████| 25/25 [08:14<00:00, 19.50s/it]100%|██████████| 25/25 [08:14<00:00, 19.77s/it]
[loss] ep 20 it 350 total=8.4984 mle=1.4270 pcon=4.9995 forget=2.0719 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 10 total=8.3941 mle=1.3430 pcon=4.9985 forget=2.0526 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=8.5046 mle=1.4255 pcon=4.9976 forget=2.0814 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.5005 mle=1.3938 pcon=4.9967 forget=2.1100 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.5114 mle=1.3759 pcon=4.9959 forget=2.1395 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=8.6382 mle=1.4998 pcon=4.9954 forget=2.1430 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.6018 mle=1.4373 pcon=4.9950 forget=2.1695 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.6521 mle=1.4549 pcon=4.9944 forget=2.2029 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.6711 mle=1.4815 pcon=4.9940 forget=2.1956 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 20 total=8.6737 mle=1.4314 pcon=4.9936 forget=2.2488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.5757 mle=1.3228 pcon=4.9931 forget=2.2598 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.6342 mle=1.3618 pcon=4.9931 forget=2.2794 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.7477 mle=1.4284 pcon=4.9929 forget=2.3264 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.7596 mle=1.4590 pcon=4.9928 forget=2.3078 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.9361 mle=1.6083 pcon=4.9926 forget=2.3352 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.7715 mle=1.4174 pcon=4.9926 forget=2.3615 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=8.8663 mle=1.5050 pcon=4.9925 forget=2.3688 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 30 total=8.9101 mle=1.5380 pcon=4.9923 forget=2.3798 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.8371 mle=1.4438 pcon=4.9920 forget=2.4014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=8.9844 mle=1.5529 pcon=4.9916 forget=2.4399 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.9414 mle=1.4998 pcon=4.9915 forget=2.4501 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.9328 mle=1.5087 pcon=4.9914 forget=2.4327 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=8.9161 mle=1.4356 pcon=4.9915 forget=2.4891 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=9.0029 mle=1.5228 pcon=4.9915 forget=2.4886 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.9046 mle=1.3940 pcon=4.9916 forget=2.5190 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 40 total=8.9955 mle=1.4832 pcon=4.9915 forget=2.5209 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=8.9676 mle=1.4491 pcon=4.9911 forget=2.5274 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=9.0444 mle=1.5206 pcon=4.9910 forget=2.5328 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=9.0353 mle=1.5029 pcon=4.9910 forget=2.5413 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=9.0578 mle=1.5100 pcon=4.9912 forget=2.5566 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.8953 mle=1.3561 pcon=4.9913 forget=2.5479 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.9863 mle=1.4236 pcon=4.9916 forget=2.5711 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:36,  1.80it/s]  2%|▏         | 6/391 [00:00<00:34, 11.16it/s]  3%|▎         | 11/391 [00:00<00:20, 18.84it/s]  4%|▍         | 16/391 [00:00<00:14, 25.54it/s]  6%|▌         | 22/391 [00:01<00:11, 32.88it/s]  7%|▋         | 28/391 [00:01<00:09, 38.37it/s]  8%|▊         | 33/391 [00:01<00:08, 40.65it/s] 10%|▉         | 39/391 [00:01<00:08, 43.85it/s] 12%|█▏        | 45/391 [00:01<00:07, 45.93it/s] 13%|█▎        | 50/391 [00:01<00:07, 46.97it/s] 14%|█▍        | 55/391 [00:01<00:07, 46.94it/s] 15%|█▌        | 60/391 [00:01<00:06, 47.78it/s] 17%|█▋        | 65/391 [00:01<00:06, 47.72it/s] 18%|█▊        | 71/391 [00:02<00:06, 48.18it/s] 19%|█▉        | 76/391 [00:02<00:06, 47.95it/s] 21%|██        | 82/391 [00:02<00:06, 49.08it/s] 22%|██▏       | 87/391 [00:02<00:06, 47.90it/s] 24%|██▎       | 92/391 [00:02<00:06, 48.36it/s] 25%|██▌       | 98/391 [00:02<00:05, 49.71it/s] 26%|██▋       | 103/391 [00:02<00:05, 48.68it/s] 28%|██▊       | 108/391 [00:02<00:05, 48.96it/s] 29%|██▉       | 114/391 [00:02<00:05, 50.24it/s] 31%|███       | 120/391 [00:03<00:05, 49.04it/s] 32%|███▏      | 125/391 [00:03<00:05, 48.98it/s] 34%|███▎      | 131/391 [00:03<00:05, 50.12it/s] 35%|███▌      | 137/391 [00:03<00:05, 48.56it/s] 36%|███▋      | 142/391 [00:03<00:05, 45.32it/s] 38%|███▊      | 147/391 [00:03<00:05, 45.54it/s] 39%|███▉      | 152/391 [00:03<00:05, 46.61it/s] 40%|████      | 157/391 [00:03<00:05, 46.72it/s] 41%|████▏     | 162/391 [00:03<00:05, 45.69it/s] 43%|████▎     | 167/391 [00:04<00:05, 44.71it/s] 44%|████▍     | 172/391 [00:04<00:04, 44.42it/s] 45%|████▌     | 177/391 [00:04<00:04, 44.57it/s] 47%|████▋     | 182/391 [00:04<00:04, 45.65it/s] 48%|████▊     | 187/391 [00:04<00:04, 44.31it/s] 49%|████▉     | 192/391 [00:04<00:04, 43.15it/s] 50%|█████     | 197/391 [00:04<00:04, 42.06it/s] 52%|█████▏    | 202/391 [00:04<00:04, 43.26it/s] 53%|█████▎    | 208/391 [00:04<00:03, 45.81it/s] 54%|█████▍    | 213/391 [00:05<00:03, 45.92it/s] 56%|█████▌    | 218/391 [00:05<00:03, 46.65it/s] 57%|█████▋    | 224/391 [00:05<00:03, 48.07it/s] 59%|█████▊    | 229/391 [00:05<00:03, 47.53it/s] 60%|█████▉    | 234/391 [00:05<00:03, 48.17it/s] 61%|██████▏   | 240/391 [00:05<00:03, 49.80it/s] 63%|██████▎   | 245/391 [00:05<00:02, 48.77it/s] 64%|██████▍   | 251/391 [00:05<00:02, 50.24it/s] 66%|██████▌   | 257/391 [00:05<00:02, 51.08it/s] 67%|██████▋   | 263/391 [00:06<00:02, 50.27it/s] 69%|██████▉   | 269/391 [00:06<00:02, 50.22it/s] 70%|███████   | 275/391 [00:06<00:02, 45.22it/s] 72%|███████▏  | 281/391 [00:06<00:02, 46.65it/s] 73%|███████▎  | 287/391 [00:06<00:02, 48.41it/s] 75%|███████▍  | 292/391 [00:06<00:02, 48.48it/s] 76%|███████▌  | 297/391 [00:06<00:01, 47.47it/s] 77%|███████▋  | 303/391 [00:06<00:01, 48.84it/s] 79%|███████▉  | 308/391 [00:07<00:01, 44.71it/s] 80%|████████  | 313/391 [00:07<00:01, 42.48it/s] 81%|████████▏ | 318/391 [00:07<00:01, 40.34it/s] 83%|████████▎ | 323/391 [00:07<00:01, 41.97it/s] 84%|████████▍ | 328/391 [00:07<00:01, 43.30it/s] 85%|████████▌ | 334/391 [00:07<00:01, 45.33it/s] 87%|████████▋ | 340/391 [00:07<00:01, 47.43it/s] 88%|████████▊ | 345/391 [00:07<00:00, 48.10it/s] 90%|████████▉ | 350/391 [00:07<00:00, 47.38it/s] 91%|█████████ | 356/391 [00:08<00:00, 48.92it/s] 92%|█████████▏| 361/391 [00:08<00:00, 49.19it/s] 94%|█████████▎| 366/391 [00:08<00:00, 46.20it/s] 95%|█████████▍| 371/391 [00:08<00:00, 44.05it/s] 96%|█████████▌| 376/391 [00:08<00:00, 41.03it/s] 97%|█████████▋| 381/391 [00:08<00:00, 42.33it/s] 99%|█████████▊| 386/391 [00:08<00:00, 42.45it/s]100%|██████████| 391/391 [00:08<00:00, 42.73it/s]100%|██████████| 391/391 [00:08<00:00, 43.82it/s]
50000 images processed, 9.050300121307373 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:34,  2.29it/s]  6%|▋         | 5/79 [00:00<00:06, 11.33it/s] 13%|█▎        | 10/79 [00:00<00:03, 21.16it/s] 20%|██        | 16/79 [00:00<00:02, 29.93it/s] 27%|██▋       | 21/79 [00:00<00:01, 34.57it/s] 34%|███▍      | 27/79 [00:00<00:01, 39.40it/s] 43%|████▎     | 34/79 [00:01<00:00, 46.69it/s] 54%|█████▍    | 43/79 [00:01<00:00, 58.08it/s] 67%|██████▋   | 53/79 [00:01<00:00, 67.95it/s] 80%|███████▉  | 63/79 [00:01<00:00, 75.25it/s] 92%|█████████▏| 73/79 [00:01<00:00, 80.60it/s]100%|██████████| 79/79 [00:01<00:00, 48.40it/s]
10000 images processed, 1.6566016674041748 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:41,  2.00it/s]  3%|▎         | 6/204 [00:00<00:16, 12.15it/s]  5%|▌         | 11/204 [00:00<00:09, 20.66it/s]  8%|▊         | 16/204 [00:00<00:07, 26.38it/s] 10%|█         | 21/204 [00:00<00:05, 30.59it/s] 12%|█▏        | 25/204 [00:01<00:05, 32.60it/s] 15%|█▍        | 30/204 [00:01<00:04, 35.33it/s] 17%|█▋        | 35/204 [00:01<00:04, 37.94it/s] 20%|█▉        | 40/204 [00:01<00:04, 39.38it/s] 22%|██▏       | 45/204 [00:01<00:03, 41.10it/s] 25%|██▍       | 50/204 [00:01<00:03, 42.13it/s] 27%|██▋       | 55/204 [00:01<00:03, 42.02it/s] 29%|██▉       | 60/204 [00:01<00:03, 43.72it/s] 32%|███▏      | 65/204 [00:01<00:03, 43.74it/s] 34%|███▍      | 70/204 [00:02<00:03, 43.78it/s] 37%|███▋      | 75/204 [00:02<00:02, 43.26it/s] 39%|███▉      | 80/204 [00:02<00:02, 43.19it/s] 42%|████▏     | 85/204 [00:02<00:02, 44.33it/s] 44%|████▍     | 90/204 [00:02<00:02, 44.37it/s] 47%|████▋     | 95/204 [00:02<00:02, 44.81it/s] 49%|████▉     | 100/204 [00:02<00:02, 44.49it/s] 51%|█████▏    | 105/204 [00:02<00:02, 45.35it/s] 54%|█████▍    | 110/204 [00:02<00:02, 45.42it/s] 56%|█████▋    | 115/204 [00:03<00:01, 44.56it/s] 59%|█████▉    | 120/204 [00:03<00:01, 45.12it/s] 61%|██████▏   | 125/204 [00:03<00:01, 44.86it/s] 64%|██████▎   | 130/204 [00:03<00:01, 45.90it/s] 66%|██████▌   | 135/204 [00:03<00:01, 42.25it/s] 69%|██████▊   | 140/204 [00:03<00:01, 43.08it/s] 71%|███████   | 145/204 [00:03<00:01, 42.82it/s] 74%|███████▎  | 150/204 [00:03<00:01, 42.75it/s] 76%|███████▌  | 155/204 [00:04<00:01, 44.43it/s] 78%|███████▊  | 160/204 [00:04<00:00, 44.84it/s] 81%|████████  | 165/204 [00:04<00:00, 43.91it/s] 83%|████████▎ | 170/204 [00:04<00:00, 45.35it/s] 86%|████████▌ | 175/204 [00:04<00:00, 45.33it/s] 88%|████████▊ | 180/204 [00:04<00:00, 45.48it/s] 91%|█████████ | 185/204 [00:04<00:00, 44.50it/s] 93%|█████████▎| 190/204 [00:04<00:00, 44.53it/s] 96%|█████████▌| 195/204 [00:04<00:00, 45.63it/s] 98%|█████████▊| 200/204 [00:05<00:00, 45.34it/s]100%|██████████| 204/204 [00:05<00:00, 39.97it/s]
26032 images processed, 5.145087718963623 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:49,  1.57it/s]  4%|▍         | 3/79 [00:00<00:15,  4.83it/s]  9%|▉         | 7/79 [00:00<00:06, 11.44it/s] 15%|█▌        | 12/79 [00:00<00:03, 18.86it/s] 22%|██▏       | 17/79 [00:01<00:02, 25.58it/s] 28%|██▊       | 22/79 [00:01<00:01, 30.28it/s] 34%|███▍      | 27/79 [00:01<00:01, 33.82it/s] 41%|████      | 32/79 [00:01<00:01, 37.21it/s] 47%|████▋     | 37/79 [00:01<00:01, 39.34it/s] 53%|█████▎    | 42/79 [00:01<00:00, 41.83it/s] 59%|█████▉    | 47/79 [00:01<00:00, 41.68it/s] 66%|██████▌   | 52/79 [00:01<00:00, 42.56it/s] 72%|███████▏  | 57/79 [00:01<00:00, 43.74it/s] 78%|███████▊  | 62/79 [00:02<00:00, 44.29it/s] 85%|████████▍ | 67/79 [00:02<00:00, 44.15it/s] 91%|█████████ | 72/79 [00:02<00:00, 44.31it/s] 97%|█████████▋| 77/79 [00:02<00:00, 44.47it/s]100%|██████████| 79/79 [00:02<00:00, 31.88it/s]
10000 images processed, 2.5159990787506104 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:48,  1.62it/s]  8%|▊         | 6/79 [00:00<00:07, 10.22it/s] 13%|█▎        | 10/79 [00:00<00:04, 16.02it/s] 19%|█▉        | 15/79 [00:00<00:02, 22.40it/s] 25%|██▌       | 20/79 [00:01<00:02, 27.75it/s] 32%|███▏      | 25/79 [00:01<00:01, 32.54it/s] 38%|███▊      | 30/79 [00:01<00:01, 35.87it/s] 44%|████▍     | 35/79 [00:01<00:01, 39.08it/s] 51%|█████     | 40/79 [00:01<00:00, 39.91it/s] 57%|█████▋    | 45/79 [00:01<00:00, 41.22it/s] 63%|██████▎   | 50/79 [00:01<00:00, 42.26it/s] 70%|██████▉   | 55/79 [00:01<00:00, 43.89it/s] 76%|███████▌  | 60/79 [00:01<00:00, 43.69it/s] 82%|████████▏ | 65/79 [00:02<00:00, 43.24it/s] 89%|████████▊ | 70/79 [00:02<00:00, 42.87it/s] 95%|█████████▍| 75/79 [00:02<00:00, 42.30it/s]100%|██████████| 79/79 [00:02<00:00, 32.81it/s]
10000 images processed, 2.4288105964660645 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:41,  1.66it/s]  9%|▊         | 6/70 [00:00<00:06, 10.59it/s] 16%|█▌        | 11/70 [00:00<00:03, 18.56it/s] 23%|██▎       | 16/70 [00:00<00:02, 24.47it/s] 30%|███       | 21/70 [00:01<00:01, 29.93it/s] 37%|███▋      | 26/70 [00:01<00:01, 34.12it/s] 44%|████▍     | 31/70 [00:01<00:01, 36.23it/s] 51%|█████▏    | 36/70 [00:01<00:00, 39.20it/s] 60%|██████    | 42/70 [00:01<00:00, 42.32it/s] 67%|██████▋   | 47/70 [00:01<00:00, 43.28it/s] 74%|███████▍  | 52/70 [00:01<00:00, 43.46it/s] 81%|████████▏ | 57/70 [00:01<00:00, 44.81it/s] 89%|████████▊ | 62/70 [00:01<00:00, 45.76it/s] 96%|█████████▌| 67/70 [00:02<00:00, 44.50it/s]100%|██████████| 70/70 [00:02<00:00, 33.02it/s]
8925 images processed, 2.1593716144561768 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:43,  1.01it/s]  4%|▍         | 2/45 [00:01<00:20,  2.12it/s] 16%|█▌        | 7/45 [00:01<00:04,  8.36it/s] 27%|██▋       | 12/45 [00:01<00:02, 14.84it/s] 38%|███▊      | 17/45 [00:01<00:01, 20.86it/s] 47%|████▋     | 21/45 [00:01<00:01, 16.37it/s] 53%|█████▎    | 24/45 [00:02<00:01, 14.74it/s] 64%|██████▍   | 29/45 [00:02<00:00, 19.71it/s] 76%|███████▌  | 34/45 [00:02<00:00, 24.76it/s] 87%|████████▋ | 39/45 [00:02<00:00, 17.99it/s] 98%|█████████▊| 44/45 [00:02<00:00, 22.23it/s]100%|██████████| 45/45 [00:02<00:00, 15.54it/s]
5640 images processed, 2.918076276779175 seconds used

27.5920832157135
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.54  99.35  98.05
places365     67.84  80.51  79.13
LSUN          19.05  95.59  95.67
iSUN          69.12  83.03  85.91
dtd           39.84  90.55  94.04
AVG           39.68  89.81  90.56
Retain-Acc: 0.7462
Forget-as-OOD (retain known vs forget novel):
  FPR: 86.60 AUROC: 80.33 AUIN: 98.72
9.156582593917847
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08_rf.png
