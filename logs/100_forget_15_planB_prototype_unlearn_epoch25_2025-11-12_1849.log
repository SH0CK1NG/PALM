nohup: ignoring input
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=25, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/25 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  4%|▍         | 1/25 [00:13<05:30, 13.78s/it]  8%|▊         | 2/25 [00:22<04:11, 10.92s/it] 12%|█▏        | 3/25 [00:32<03:48, 10.37s/it] 16%|█▌        | 4/25 [00:41<03:30, 10.02s/it] 20%|██        | 5/25 [00:52<03:21, 10.09s/it] 24%|██▍       | 6/25 [01:01<03:06,  9.79s/it][loss] ep 0 it 0 total=9.0836 mle=1.3179 pcon=5.2950 forget=2.4707 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1000 mle=1.3725 pcon=5.2881 forget=2.4395 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.1587 mle=1.3780 pcon=5.2816 forget=2.4992 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.0893 mle=1.4003 pcon=5.2756 forget=2.4134 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.1844 mle=1.4630 pcon=5.2690 forget=2.4524 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1362 mle=1.3968 pcon=5.2630 forget=2.4764 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.1142 mle=1.3667 pcon=5.2573 forget=2.4901 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1859 mle=1.4546 pcon=5.2519 forget=2.4794 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 1 it 10 total=9.1765 mle=1.4571 pcon=5.2462 forget=2.4732 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=8.9867 mle=1.2788 pcon=5.2407 forget=2.4672 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=9.0060 mle=1.2836 pcon=5.2355 forget=2.4870 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=9.0565 mle=1.3071 pcon=5.2301 forget=2.5193 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=8.9994 mle=1.3059 pcon=5.2248 forget=2.4687 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=9.0388 mle=1.3812 pcon=5.2198 forget=2.4378 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=9.0516 mle=1.3555 pcon=5.2146 forget=2.4815 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0830 mle=1.3964 pcon=5.2096 forget=2.4770 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 2 it 20 total=9.0868 mle=1.3840 pcon=5.2048 forget=2.4979 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0265 mle=1.3127 pcon=5.2000 forget=2.5138 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=9.1424 mle=1.5010 pcon=5.1957 forget=2.4456 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=9.0241 mle=1.3719 pcon=5.1913 forget=2.4610 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=9.1414 mle=1.4625 pcon=5.1868 forget=2.4921 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0808 mle=1.3907 pcon=5.1828 forget=2.5073 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=9.0610 mle=1.4014 pcon=5.1787 forget=2.4809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=8.9537 mle=1.2892 pcon=5.1745 forget=2.4899 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 3 it 30 total=9.0018 mle=1.3285 pcon=5.1705 forget=2.5028 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9787 mle=1.3199 pcon=5.1667 forget=2.4921 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.9753 mle=1.3299 pcon=5.1628 forget=2.4826 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=8.9524 mle=1.3001 pcon=5.1590 forget=2.4933 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9677 mle=1.3125 pcon=5.1554 forget=2.4998 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=8.9995 mle=1.3841 pcon=5.1521 forget=2.4633 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0260 mle=1.4089 pcon=5.1488 forget=2.4684 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=9.0612 mle=1.4376 pcon=5.1454 forget=2.4782 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 4 it 40 total=8.9267 mle=1.2968 pcon=5.1423 forget=2.4876 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.8406 mle=1.2445 pcon=5.1391 forget=2.4570 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=8.9498 mle=1.3356 pcon=5.1359 forget=2.4782 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.0093 mle=1.4039 pcon=5.1329 forget=2.4726 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0567 mle=1.4692 pcon=5.1299 forget=2.4575 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.0099 mle=1.3963 pcon=5.1272 forget=2.4863 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=8.9324 mle=1.3028 pcon=5.1242 forget=2.5053 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 5 it 0 total=8.9616 mle=1.3574 pcon=5.1215 forget=2.4826 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=8.8848 mle=1.2895 pcon=5.1187 forget=2.4765 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=8.9950 mle=1.4277 pcon=5.1158 forget=2.4514 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=8.8928 mle=1.3031 pcon=5.1135 forget=2.4762 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=8.8206 mle=1.2109 pcon=5.1112 forget=2.4985 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=9.0597 mle=1.4589 pcon=5.1082 forget=2.4925 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=8.9006 mle=1.3177 pcon=5.1066 forget=2.4764 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=8.8697 mle=1.2733 pcon=5.1045 forget=2.4918 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 6 it 10 total=8.9367 mle=1.3220 pcon=5.1022 forget=2.5125 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=9.0459 mle=1.4737 pcon=5.1002 forget=2.4721 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=8.8647 mle=1.2660 pcon=5.0978 forget=2.5008 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.9258 mle=1.3381 pcon=5.0952 forget=2.4925 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9514 mle=1.3947 pcon=5.0932 forget=2.4636 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=8.9817 mle=1.3919 pcon=5.0913 forget=2.4984 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=8.8866 mle=1.3204 pcon=5.0892 forget=2.4770 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 28%|██▊       | 7/25 [01:09<02:46,  9.27s/it] 32%|███▏      | 8/25 [01:18<02:36,  9.20s/it] 36%|███▌      | 9/25 [01:29<02:34,  9.67s/it] 40%|████      | 10/25 [01:39<02:25,  9.71s/it] 44%|████▍     | 11/25 [01:49<02:17,  9.81s/it] 48%|████▊     | 12/25 [01:57<02:03,  9.49s/it] 52%|█████▏    | 13/25 [02:07<01:53,  9.49s/it][loss] ep 6 it 360 total=8.8594 mle=1.3188 pcon=5.0872 forget=2.4533 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 7 it 20 total=8.8433 mle=1.2810 pcon=5.0853 forget=2.4770 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=8.8908 mle=1.2945 pcon=5.0834 forget=2.5129 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.8865 mle=1.2875 pcon=5.0814 forget=2.5176 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.7736 mle=1.2600 pcon=5.0792 forget=2.4343 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=8.8733 mle=1.3284 pcon=5.0774 forget=2.4675 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=8.9421 mle=1.3864 pcon=5.0758 forget=2.4799 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=8.8707 mle=1.2895 pcon=5.0741 forget=2.5072 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=8.8096 mle=1.2554 pcon=5.0722 forget=2.4819 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 8 it 30 total=8.8858 mle=1.3367 pcon=5.0704 forget=2.4787 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=8.9087 mle=1.3903 pcon=5.0687 forget=2.4497 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=9.0131 mle=1.4665 pcon=5.0669 forget=2.4797 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=8.9176 mle=1.3495 pcon=5.0654 forget=2.5027 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=8.9088 mle=1.3574 pcon=5.0631 forget=2.4882 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.9602 mle=1.4130 pcon=5.0616 forget=2.4855 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=8.9465 mle=1.4369 pcon=5.0599 forget=2.4497 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.9171 mle=1.4146 pcon=5.0585 forget=2.4440 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 9 it 40 total=8.8665 mle=1.3282 pcon=5.0572 forget=2.4811 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=8.9162 mle=1.3930 pcon=5.0557 forget=2.4675 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=8.8518 mle=1.3314 pcon=5.0544 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.8718 mle=1.3299 pcon=5.0530 forget=2.4889 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=8.9379 mle=1.4345 pcon=5.0515 forget=2.4518 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=8.8545 mle=1.3166 pcon=5.0502 forget=2.4877 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.7474 mle=1.2456 pcon=5.0494 forget=2.4524 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 10 it 0 total=9.0067 mle=1.4748 pcon=5.0482 forget=2.4837 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.9581 mle=1.4100 pcon=5.0473 forget=2.5009 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=8.8300 mle=1.3079 pcon=5.0460 forget=2.4760 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=8.8312 mle=1.2886 pcon=5.0449 forget=2.4977 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=8.9713 mle=1.4280 pcon=5.0437 forget=2.4996 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=8.9259 mle=1.4212 pcon=5.0425 forget=2.4622 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=8.9196 mle=1.4035 pcon=5.0413 forget=2.4748 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.8242 mle=1.3103 pcon=5.0404 forget=2.4735 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 11 it 10 total=8.8056 mle=1.2405 pcon=5.0397 forget=2.5254 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.8149 mle=1.2928 pcon=5.0388 forget=2.4833 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.8064 mle=1.3093 pcon=5.0378 forget=2.4593 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.7694 mle=1.2462 pcon=5.0368 forget=2.4864 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.8028 mle=1.2992 pcon=5.0358 forget=2.4679 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.8727 mle=1.3698 pcon=5.0352 forget=2.4677 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.8464 mle=1.3470 pcon=5.0341 forget=2.4653 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.8908 mle=1.3421 pcon=5.0334 forget=2.5153 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 12 it 20 total=8.9809 mle=1.4670 pcon=5.0324 forget=2.4815 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.8810 mle=1.3212 pcon=5.0316 forget=2.5281 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.8137 mle=1.3137 pcon=5.0306 forget=2.4695 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.9431 mle=1.4105 pcon=5.0296 forget=2.5030 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.9102 mle=1.3891 pcon=5.0288 forget=2.4923 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.7823 mle=1.3315 pcon=5.0281 forget=2.4228 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.9052 mle=1.4319 pcon=5.0273 forget=2.4459 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.9154 mle=1.3869 pcon=5.0265 forget=2.5019 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 13 it 30 total=8.8586 mle=1.4009 pcon=5.0260 forget=2.4316 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.9988 mle=1.4730 pcon=5.0254 forget=2.5003 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.8569 mle=1.3305 pcon=5.0248 forget=2.5015 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=8.8811 mle=1.4242 pcon=5.0240 forget=2.4329 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 56%|█████▌    | 14/25 [02:17<01:44,  9.54s/it] 60%|██████    | 15/25 [02:27<01:37,  9.72s/it] 64%|██████▍   | 16/25 [02:35<01:24,  9.35s/it] 68%|██████▊   | 17/25 [02:44<01:13,  9.15s/it] 72%|███████▏  | 18/25 [02:53<01:03,  9.03s/it] 76%|███████▌  | 19/25 [03:01<00:53,  8.98s/it] 80%|████████  | 20/25 [03:11<00:46,  9.23s/it][loss] ep 13 it 230 total=8.8498 mle=1.3446 pcon=5.0232 forget=2.4820 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.8018 mle=1.2804 pcon=5.0228 forget=2.4986 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.9059 mle=1.3988 pcon=5.0223 forget=2.4847 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.7695 mle=1.2797 pcon=5.0216 forget=2.4682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 14 it 40 total=8.9563 mle=1.4831 pcon=5.0209 forget=2.4523 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.8697 mle=1.3654 pcon=5.0205 forget=2.4838 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.8126 mle=1.3141 pcon=5.0199 forget=2.4786 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.8156 mle=1.3031 pcon=5.0194 forget=2.4931 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.8815 mle=1.3459 pcon=5.0187 forget=2.5169 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.7378 mle=1.2362 pcon=5.0181 forget=2.4836 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.9548 mle=1.4892 pcon=5.0175 forget=2.4481 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 15 it 0 total=8.8376 mle=1.3431 pcon=5.0170 forget=2.4775 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.7457 mle=1.2414 pcon=5.0167 forget=2.4876 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.8306 mle=1.3511 pcon=5.0162 forget=2.4634 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.8512 mle=1.3645 pcon=5.0157 forget=2.4710 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.9398 mle=1.4111 pcon=5.0157 forget=2.5130 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.7506 mle=1.2753 pcon=5.0153 forget=2.4600 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.7693 mle=1.2753 pcon=5.0147 forget=2.4792 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.9882 mle=1.5011 pcon=5.0144 forget=2.4726 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 16 it 10 total=8.7673 mle=1.3008 pcon=5.0143 forget=2.4522 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.8970 mle=1.4250 pcon=5.0139 forget=2.4581 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.7615 mle=1.2976 pcon=5.0132 forget=2.4507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.7668 mle=1.2497 pcon=5.0131 forget=2.5040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.8354 mle=1.3460 pcon=5.0123 forget=2.4771 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.8453 mle=1.3685 pcon=5.0118 forget=2.4650 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.8066 mle=1.3711 pcon=5.0114 forget=2.4241 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.7751 mle=1.3035 pcon=5.0111 forget=2.4605 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 17 it 20 total=8.9087 mle=1.4282 pcon=5.0105 forget=2.4701 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.8407 mle=1.3677 pcon=5.0099 forget=2.4631 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.8461 mle=1.3877 pcon=5.0093 forget=2.4490 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.8933 mle=1.3956 pcon=5.0089 forget=2.4888 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.7614 mle=1.3054 pcon=5.0086 forget=2.4474 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.7825 mle=1.3083 pcon=5.0082 forget=2.4660 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.9195 mle=1.4776 pcon=5.0077 forget=2.4343 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.6798 mle=1.2262 pcon=5.0074 forget=2.4462 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.7414 mle=1.2610 pcon=5.0069 forget=2.4735 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.8818 mle=1.4128 pcon=5.0066 forget=2.4624 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.7936 mle=1.3332 pcon=5.0062 forget=2.4542 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.9353 mle=1.5101 pcon=5.0057 forget=2.4195 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.7923 mle=1.3314 pcon=5.0054 forget=2.4556 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.8516 mle=1.4099 pcon=5.0050 forget=2.4367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.8124 mle=1.3369 pcon=5.0046 forget=2.4709 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.9422 mle=1.4722 pcon=5.0038 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 19 it 40 total=8.8005 mle=1.3058 pcon=5.0035 forget=2.4912 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.8573 mle=1.4074 pcon=5.0027 forget=2.4471 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.8413 mle=1.4050 pcon=5.0022 forget=2.4341 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.7455 mle=1.3123 pcon=5.0016 forget=2.4315 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.7721 mle=1.2924 pcon=5.0014 forget=2.4783 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.7851 mle=1.3294 pcon=5.0011 forget=2.4547 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.7596 mle=1.2797 pcon=5.0010 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 20 it 0 total=8.8616 mle=1.3898 pcon=5.0005 forget=2.4713 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.9437 mle=1.4982 pcon=5.0005 forget=2.4450 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.8275 mle=1.3515 pcon=5.0004 forget=2.4755 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 84%|████████▍ | 21/25 [03:21<00:37,  9.49s/it] 88%|████████▊ | 22/25 [03:31<00:28,  9.44s/it] 92%|█████████▏| 23/25 [03:40<00:18,  9.28s/it] 96%|█████████▌| 24/25 [03:49<00:09,  9.22s/it]100%|██████████| 25/25 [03:59<00:00,  9.55s/it]100%|██████████| 25/25 [03:59<00:00,  9.58s/it]
[loss] ep 20 it 150 total=8.7760 mle=1.3284 pcon=5.0000 forget=2.4477 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.8401 mle=1.3577 pcon=4.9994 forget=2.4830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=8.8212 mle=1.3337 pcon=4.9991 forget=2.4883 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=8.8088 mle=1.3557 pcon=4.9988 forget=2.4543 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=8.8725 mle=1.4188 pcon=4.9986 forget=2.4551 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 21 it 10 total=8.7709 mle=1.3396 pcon=4.9980 forget=2.4333 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=8.8501 mle=1.3624 pcon=4.9978 forget=2.4899 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.8294 mle=1.3857 pcon=4.9976 forget=2.4461 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.8003 mle=1.3543 pcon=4.9972 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=8.8832 mle=1.4225 pcon=4.9971 forget=2.4636 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.8492 mle=1.4192 pcon=4.9968 forget=2.4332 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.7223 mle=1.2852 pcon=4.9965 forget=2.4407 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.7473 mle=1.2947 pcon=4.9961 forget=2.4565 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 20 total=8.8134 mle=1.3684 pcon=4.9957 forget=2.4493 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.8191 mle=1.3504 pcon=4.9952 forget=2.4735 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.7463 mle=1.2967 pcon=4.9949 forget=2.4546 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.8244 mle=1.3592 pcon=4.9945 forget=2.4707 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.7859 mle=1.3295 pcon=4.9943 forget=2.4621 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.9322 mle=1.4900 pcon=4.9941 forget=2.4481 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.7063 mle=1.3042 pcon=4.9940 forget=2.4081 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=8.7715 mle=1.3316 pcon=4.9937 forget=2.4462 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[loss] ep 23 it 30 total=8.8398 mle=1.4188 pcon=4.9935 forget=2.4275 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.7137 mle=1.2374 pcon=4.9934 forget=2.4829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=8.7948 mle=1.3425 pcon=4.9933 forget=2.4591 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.8070 mle=1.3369 pcon=4.9930 forget=2.4771 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.8782 mle=1.4436 pcon=4.9929 forget=2.4417 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=8.8062 mle=1.3466 pcon=4.9925 forget=2.4670 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=8.8278 mle=1.3653 pcon=4.9921 forget=2.4703 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.7407 mle=1.3035 pcon=4.9922 forget=2.4450 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 40 total=8.7233 mle=1.2978 pcon=4.9921 forget=2.4333 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=8.7462 mle=1.2796 pcon=4.9917 forget=2.4749 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.9436 mle=1.4963 pcon=4.9917 forget=2.4555 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.7008 mle=1.2572 pcon=4.9914 forget=2.4523 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=8.6444 mle=1.2135 pcon=4.9916 forget=2.4393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.8065 mle=1.3481 pcon=4.9916 forget=2.4668 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.6969 mle=1.2193 pcon=4.9912 forget=2.4865 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:44,  2.37it/s]  2%|▏         | 8/391 [00:00<00:19, 19.42it/s]  4%|▍         | 17/391 [00:00<00:09, 37.86it/s]  7%|▋         | 27/391 [00:00<00:06, 54.18it/s]  9%|▉         | 36/391 [00:00<00:05, 62.71it/s] 12%|█▏        | 45/391 [00:00<00:05, 67.83it/s] 14%|█▎        | 53/391 [00:01<00:04, 70.40it/s] 16%|█▌        | 61/391 [00:01<00:04, 67.47it/s] 18%|█▊        | 69/391 [00:01<00:04, 68.91it/s] 20%|█▉        | 77/391 [00:01<00:04, 69.41it/s] 22%|██▏       | 85/391 [00:01<00:04, 65.24it/s] 24%|██▎       | 92/391 [00:01<00:04, 60.43it/s] 26%|██▌       | 101/391 [00:01<00:04, 66.91it/s] 28%|██▊       | 111/391 [00:01<00:03, 74.89it/s] 31%|███       | 121/391 [00:02<00:03, 81.00it/s] 34%|███▎      | 131/391 [00:02<00:03, 85.52it/s] 36%|███▌      | 141/391 [00:02<00:02, 88.64it/s] 39%|███▊      | 151/391 [00:02<00:02, 90.97it/s] 41%|████      | 161/391 [00:02<00:02, 92.63it/s] 44%|████▎     | 171/391 [00:02<00:02, 93.71it/s] 46%|████▋     | 181/391 [00:02<00:02, 94.65it/s] 49%|████▉     | 191/391 [00:02<00:02, 88.55it/s] 51%|█████     | 200/391 [00:02<00:02, 71.56it/s] 54%|█████▎    | 210/391 [00:03<00:02, 77.11it/s] 56%|█████▋    | 220/391 [00:03<00:02, 82.17it/s] 59%|█████▉    | 230/391 [00:03<00:01, 85.80it/s] 61%|██████▏   | 240/391 [00:03<00:01, 88.84it/s] 64%|██████▍   | 250/391 [00:03<00:01, 90.55it/s] 66%|██████▋   | 260/391 [00:03<00:01, 92.28it/s] 69%|██████▉   | 270/391 [00:03<00:01, 93.42it/s] 72%|███████▏  | 280/391 [00:03<00:01, 94.17it/s] 74%|███████▍  | 290/391 [00:03<00:01, 94.62it/s] 77%|███████▋  | 300/391 [00:03<00:00, 95.08it/s] 79%|███████▉  | 310/391 [00:04<00:00, 95.30it/s] 82%|████████▏ | 320/391 [00:04<00:00, 95.49it/s] 84%|████████▍ | 330/391 [00:04<00:00, 95.76it/s] 87%|████████▋ | 340/391 [00:04<00:00, 95.67it/s] 90%|████████▉ | 350/391 [00:04<00:00, 96.01it/s] 92%|█████████▏| 360/391 [00:04<00:00, 95.25it/s] 95%|█████████▍| 370/391 [00:04<00:00, 96.00it/s] 97%|█████████▋| 380/391 [00:04<00:00, 96.44it/s]100%|█████████▉| 390/391 [00:04<00:00, 96.76it/s]100%|██████████| 391/391 [00:04<00:00, 79.20it/s]
50000 images processed, 5.0326313972473145 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:37,  2.08it/s] 14%|█▍        | 11/79 [00:00<00:02, 24.09it/s] 27%|██▋       | 21/79 [00:00<00:01, 41.73it/s] 39%|███▉      | 31/79 [00:00<00:00, 55.27it/s] 52%|█████▏    | 41/79 [00:00<00:00, 65.48it/s] 65%|██████▍   | 51/79 [00:01<00:00, 72.93it/s] 77%|███████▋  | 61/79 [00:01<00:00, 78.98it/s] 90%|████████▉ | 71/79 [00:01<00:00, 83.44it/s]100%|██████████| 79/79 [00:02<00:00, 39.03it/s]
10000 images processed, 2.0650033950805664 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:45,  1.92it/s]  4%|▍         | 8/204 [00:00<00:11, 16.48it/s]  9%|▉         | 18/204 [00:00<00:05, 35.56it/s] 14%|█▎        | 28/204 [00:00<00:03, 50.54it/s] 19%|█▊        | 38/204 [00:00<00:02, 62.25it/s] 24%|██▎       | 48/204 [00:01<00:02, 71.07it/s] 28%|██▊       | 57/204 [00:01<00:01, 74.48it/s] 32%|███▏      | 66/204 [00:01<00:01, 74.42it/s] 37%|███▋      | 75/204 [00:01<00:01, 75.73it/s] 41%|████      | 84/204 [00:01<00:01, 75.37it/s] 46%|████▌     | 94/204 [00:01<00:01, 80.13it/s] 51%|█████     | 104/204 [00:01<00:01, 84.36it/s] 56%|█████▌    | 114/204 [00:01<00:01, 87.34it/s] 61%|██████    | 124/204 [00:01<00:00, 89.53it/s] 66%|██████▌   | 134/204 [00:02<00:00, 91.38it/s] 71%|███████   | 144/204 [00:02<00:00, 91.93it/s] 75%|███████▌  | 154/204 [00:02<00:00, 92.89it/s] 80%|████████  | 164/204 [00:02<00:00, 93.30it/s] 85%|████████▌ | 174/204 [00:02<00:00, 94.09it/s] 90%|█████████ | 184/204 [00:02<00:00, 95.04it/s] 95%|█████████▌| 194/204 [00:02<00:00, 95.78it/s]100%|██████████| 204/204 [00:02<00:00, 95.14it/s]100%|██████████| 204/204 [00:02<00:00, 73.40it/s]
26032 images processed, 2.8222835063934326 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:11,  1.09it/s]  6%|▋         | 5/79 [00:01<00:11,  6.19it/s] 19%|█▉        | 15/79 [00:01<00:03, 20.70it/s] 32%|███▏      | 25/79 [00:01<00:01, 34.47it/s] 43%|████▎     | 34/79 [00:01<00:00, 45.37it/s] 54%|█████▍    | 43/79 [00:01<00:00, 54.83it/s] 66%|██████▌   | 52/79 [00:01<00:00, 62.99it/s] 77%|███████▋  | 61/79 [00:01<00:00, 69.83it/s] 90%|████████▉ | 71/79 [00:01<00:00, 77.13it/s]100%|██████████| 79/79 [00:01<00:00, 42.79it/s]
10000 images processed, 1.8810858726501465 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:47,  1.63it/s] 13%|█▎        | 10/79 [00:00<00:03, 18.21it/s] 23%|██▎       | 18/79 [00:00<00:01, 30.66it/s] 35%|███▌      | 28/79 [00:00<00:01, 46.03it/s] 47%|████▋     | 37/79 [00:01<00:00, 56.14it/s] 57%|█████▋    | 45/79 [00:01<00:00, 61.53it/s] 68%|██████▊   | 54/79 [00:01<00:00, 67.20it/s] 78%|███████▊  | 62/79 [00:01<00:00, 66.73it/s] 91%|█████████ | 72/79 [00:01<00:00, 74.15it/s]100%|██████████| 79/79 [00:01<00:00, 51.11it/s]
10000 images processed, 1.5706653594970703 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:37,  1.83it/s]  9%|▊         | 6/70 [00:00<00:05, 11.50it/s] 23%|██▎       | 16/70 [00:00<00:01, 30.76it/s] 37%|███▋      | 26/70 [00:00<00:00, 46.24it/s] 50%|█████     | 35/70 [00:00<00:00, 56.25it/s] 64%|██████▍   | 45/70 [00:01<00:00, 66.60it/s] 79%|███████▊  | 55/70 [00:01<00:00, 74.85it/s] 93%|█████████▎| 65/70 [00:01<00:00, 80.72it/s]100%|██████████| 70/70 [00:01<00:00, 51.71it/s]
8925 images processed, 1.3889572620391846 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:41,  1.05it/s]  4%|▍         | 2/45 [00:01<00:19,  2.17it/s] 27%|██▋       | 12/45 [00:01<00:01, 17.18it/s] 38%|███▊      | 17/45 [00:01<00:01, 18.08it/s] 49%|████▉     | 22/45 [00:01<00:00, 23.13it/s] 58%|█████▊    | 26/45 [00:01<00:00, 24.65it/s] 73%|███████▎  | 33/45 [00:02<00:00, 19.78it/s] 96%|█████████▌| 43/45 [00:02<00:00, 31.11it/s]100%|██████████| 45/45 [00:02<00:00, 19.99it/s]
5640 images processed, 2.274440050125122 seconds used

18.815284967422485
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.62  99.34  97.85
places365     68.58  81.36  78.36
LSUN          18.27  95.97  95.79
iSUN          73.39  81.55  83.51
dtd           38.95  91.28  93.99
AVG           40.36  89.90  89.90
Retain-Acc: 0.7571
Forget-as-OOD (retain known vs forget novel):
  FPR: 58.80 AUROC: 87.72 AUIN: 97.54
7.166309118270874
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget15-temp0.08_rf.png
