nohup: ignoring input
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
[DEBUG] forget=2500, retain=47500, half_f=64, half_r=64
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:14<00:59, 14.94s/it] 40%|████      | 2/5 [00:24<00:34, 11.52s/it] 60%|██████    | 3/5 [00:35<00:22, 11.47s/it] 80%|████████  | 4/5 [00:44<00:10, 10.37s/it]100%|██████████| 5/5 [00:53<00:00,  9.95s/it]100%|██████████| 5/5 [00:53<00:00, 10.67s/it]
[loss] ep 0 it 0 total=9.2464 mle=1.5918 pcon=5.2950 forget=2.3595 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.2172 mle=1.5427 pcon=5.2893 forget=2.3852 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.1527 mle=1.4806 pcon=5.2835 forget=2.3886 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.1410 mle=1.5236 pcon=5.2780 forget=2.3395 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.1267 mle=1.5435 pcon=5.2723 forget=2.3109 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.2535 mle=1.5948 pcon=5.2669 forget=2.3918 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.1124 mle=1.4608 pcon=5.2616 forget=2.3901 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.0805 mle=1.4113 pcon=5.2563 forget=2.4128 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[BATCH] f_take=64, r_take=64, need=0
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 1 it 10 total=9.1706 mle=1.5100 pcon=5.2514 forget=2.4093 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0501 mle=1.3923 pcon=5.2463 forget=2.4116 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=9.1730 mle=1.5419 pcon=5.2413 forget=2.3897 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=9.0243 mle=1.3752 pcon=5.2365 forget=2.4126 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.1647 mle=1.5297 pcon=5.2316 forget=2.4034 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=9.1578 mle=1.5846 pcon=5.2272 forget=2.3460 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=9.1619 mle=1.5567 pcon=5.2231 forget=2.3821 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[BATCH] f_take=64, r_take=64, need=0
[loss] ep 1 it 360 total=9.0180 mle=1.4152 pcon=5.2187 forget=2.3841 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 2 it 20 total=8.9866 mle=1.4033 pcon=5.2143 forget=2.3690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0150 mle=1.3855 pcon=5.2098 forget=2.4196 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=9.0632 mle=1.4537 pcon=5.2054 forget=2.4041 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9453 mle=1.3731 pcon=5.2014 forget=2.3708 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=9.0902 mle=1.5262 pcon=5.1973 forget=2.3668 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0292 mle=1.4579 pcon=5.1933 forget=2.3779 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=9.0760 mle=1.4985 pcon=5.1895 forget=2.3880 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[BATCH] f_take=64, r_take=64, need=0
[loss] ep 2 it 370 total=8.9672 mle=1.3829 pcon=5.1856 forget=2.3986 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 3 it 30 total=9.0741 mle=1.4669 pcon=5.1820 forget=2.4253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=9.0810 mle=1.4757 pcon=5.1779 forget=2.4274 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.9775 mle=1.3962 pcon=5.1743 forget=2.4070 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=8.9707 mle=1.3942 pcon=5.1704 forget=2.4062 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9436 mle=1.3596 pcon=5.1669 forget=2.4172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0452 mle=1.4910 pcon=5.1636 forget=2.3906 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=8.9637 mle=1.4165 pcon=5.1608 forget=2.3863 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[BATCH] f_take=64, r_take=64, need=0
[loss] ep 3 it 380 total=8.9645 mle=1.4197 pcon=5.1574 forget=2.3875 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 4 it 40 total=8.8706 mle=1.3395 pcon=5.1543 forget=2.3768 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9021 mle=1.3715 pcon=5.1513 forget=2.3793 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=8.9064 mle=1.3423 pcon=5.1482 forget=2.4160 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.0598 mle=1.5137 pcon=5.1451 forget=2.4010 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.1051 mle=1.5810 pcon=5.1420 forget=2.3821 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.0220 mle=1.4748 pcon=5.1391 forget=2.4082 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=8.8533 mle=1.3718 pcon=5.1361 forget=2.3454 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[BATCH] f_take=64, r_take=64, need=0
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:47,  2.32it/s]  2%|▏         | 8/391 [00:00<00:20, 18.95it/s]  4%|▍         | 17/391 [00:00<00:10, 36.39it/s]  7%|▋         | 27/391 [00:00<00:07, 51.92it/s]  9%|▉         | 37/391 [00:00<00:05, 63.37it/s] 12%|█▏        | 46/391 [00:00<00:04, 70.19it/s] 14%|█▍        | 56/391 [00:01<00:04, 77.54it/s] 17%|█▋        | 66/391 [00:01<00:03, 82.62it/s] 19%|█▉        | 76/391 [00:01<00:03, 85.16it/s] 22%|██▏       | 86/391 [00:01<00:03, 87.29it/s] 25%|██▍       | 96/391 [00:01<00:03, 89.64it/s] 27%|██▋       | 106/391 [00:01<00:03, 90.61it/s] 30%|██▉       | 116/391 [00:01<00:02, 92.25it/s] 32%|███▏      | 126/391 [00:01<00:02, 92.78it/s] 35%|███▍      | 136/391 [00:01<00:02, 93.20it/s] 37%|███▋      | 146/391 [00:02<00:02, 93.38it/s] 40%|███▉      | 156/391 [00:02<00:02, 93.85it/s] 42%|████▏     | 166/391 [00:02<00:02, 93.99it/s] 45%|████▌     | 176/391 [00:02<00:02, 94.14it/s] 48%|████▊     | 186/391 [00:02<00:02, 94.98it/s] 50%|█████     | 196/391 [00:02<00:02, 95.17it/s] 53%|█████▎    | 206/391 [00:02<00:01, 95.37it/s] 55%|█████▌    | 216/391 [00:02<00:01, 95.19it/s] 58%|█████▊    | 226/391 [00:02<00:01, 95.21it/s] 60%|██████    | 236/391 [00:02<00:01, 95.21it/s] 63%|██████▎   | 246/391 [00:03<00:01, 95.66it/s] 65%|██████▌   | 256/391 [00:03<00:01, 91.76it/s] 68%|██████▊   | 266/391 [00:03<00:01, 90.65it/s] 71%|███████   | 276/391 [00:03<00:01, 82.02it/s] 73%|███████▎  | 285/391 [00:03<00:01, 82.67it/s] 75%|███████▌  | 295/391 [00:03<00:01, 86.12it/s] 78%|███████▊  | 305/391 [00:03<00:00, 88.59it/s] 81%|████████  | 315/391 [00:03<00:00, 91.06it/s] 83%|████████▎ | 325/391 [00:03<00:00, 92.50it/s] 86%|████████▌ | 335/391 [00:04<00:00, 93.74it/s] 88%|████████▊ | 345/391 [00:04<00:00, 94.44it/s] 91%|█████████ | 355/391 [00:04<00:00, 95.20it/s] 93%|█████████▎| 365/391 [00:04<00:00, 95.14it/s] 96%|█████████▌| 375/391 [00:04<00:00, 95.98it/s] 98%|█████████▊| 385/391 [00:04<00:00, 96.61it/s]100%|██████████| 391/391 [00:04<00:00, 83.56it/s]
50000 images processed, 4.831608057022095 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.94it/s]  9%|▉         | 7/79 [00:00<00:04, 14.57it/s] 19%|█▉        | 15/79 [00:00<00:02, 29.98it/s] 30%|███       | 24/79 [00:00<00:01, 44.33it/s] 43%|████▎     | 34/79 [00:00<00:00, 57.30it/s] 54%|█████▍    | 43/79 [00:01<00:00, 65.61it/s] 66%|██████▌   | 52/79 [00:01<00:00, 72.07it/s] 78%|███████▊  | 62/79 [00:01<00:00, 78.38it/s] 90%|████████▉ | 71/79 [00:01<00:00, 81.60it/s]100%|██████████| 79/79 [00:01<00:00, 40.66it/s]
10000 images processed, 2.0270891189575195 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:04,  1.63it/s]  5%|▌         | 11/204 [00:00<00:09, 19.94it/s] 10%|█         | 21/204 [00:00<00:05, 36.14it/s] 15%|█▌        | 31/204 [00:00<00:03, 49.98it/s] 20%|██        | 41/204 [00:01<00:02, 61.22it/s] 25%|██▌       | 51/204 [00:01<00:02, 70.10it/s] 30%|██▉       | 61/204 [00:01<00:01, 75.76it/s] 35%|███▍      | 71/204 [00:01<00:01, 80.27it/s] 40%|███▉      | 81/204 [00:01<00:01, 84.11it/s] 45%|████▍     | 91/204 [00:01<00:01, 86.93it/s] 50%|████▉     | 101/204 [00:01<00:01, 88.85it/s] 54%|█████▍    | 111/204 [00:01<00:01, 89.13it/s] 59%|█████▉    | 121/204 [00:01<00:00, 90.64it/s] 64%|██████▍   | 131/204 [00:02<00:00, 91.49it/s] 69%|██████▉   | 141/204 [00:02<00:00, 92.35it/s] 74%|███████▍  | 151/204 [00:02<00:00, 92.80it/s] 79%|███████▉  | 161/204 [00:02<00:00, 93.44it/s] 84%|████████▍ | 171/204 [00:02<00:00, 93.65it/s] 89%|████████▊ | 181/204 [00:02<00:00, 94.13it/s] 94%|█████████▎| 191/204 [00:02<00:00, 94.28it/s] 99%|█████████▊| 201/204 [00:02<00:00, 94.79it/s]100%|██████████| 204/204 [00:02<00:00, 73.21it/s]
26032 images processed, 2.841778516769409 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:53,  1.45it/s] 11%|█▏        | 9/79 [00:00<00:05, 13.08it/s] 23%|██▎       | 18/79 [00:00<00:02, 26.64it/s] 34%|███▍      | 27/79 [00:01<00:01, 39.02it/s] 47%|████▋     | 37/79 [00:01<00:00, 51.86it/s] 59%|█████▉    | 47/79 [00:01<00:00, 62.14it/s] 72%|███████▏  | 57/79 [00:01<00:00, 70.07it/s] 85%|████████▍ | 67/79 [00:01<00:00, 76.76it/s] 97%|█████████▋| 77/79 [00:01<00:00, 82.32it/s]100%|██████████| 79/79 [00:01<00:00, 48.69it/s]
10000 images processed, 1.6551322937011719 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:44,  1.76it/s] 11%|█▏        | 9/79 [00:00<00:04, 17.22it/s] 23%|██▎       | 18/79 [00:00<00:01, 33.06it/s] 35%|███▌      | 28/79 [00:00<00:01, 48.17it/s] 47%|████▋     | 37/79 [00:00<00:00, 58.52it/s] 59%|█████▉    | 47/79 [00:01<00:00, 68.27it/s] 72%|███████▏  | 57/79 [00:01<00:00, 76.05it/s] 85%|████████▍ | 67/79 [00:01<00:00, 81.86it/s] 97%|█████████▋| 77/79 [00:01<00:00, 86.11it/s]100%|██████████| 79/79 [00:01<00:00, 55.68it/s]
10000 images processed, 1.4383745193481445 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:36,  1.91it/s]  7%|▋         | 5/70 [00:00<00:06,  9.90it/s] 21%|██▏       | 15/70 [00:00<00:01, 29.94it/s] 36%|███▌      | 25/70 [00:00<00:00, 45.95it/s] 50%|█████     | 35/70 [00:00<00:00, 58.60it/s] 64%|██████▍   | 45/70 [00:01<00:00, 68.50it/s] 79%|███████▊  | 55/70 [00:01<00:00, 76.27it/s] 93%|█████████▎| 65/70 [00:01<00:00, 82.05it/s]100%|██████████| 70/70 [00:01<00:00, 52.71it/s]
8925 images processed, 1.357628583908081 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:43,  1.01it/s]  4%|▍         | 2/45 [00:01<00:20,  2.12it/s] 24%|██▍       | 11/45 [00:01<00:02, 15.27it/s] 38%|███▊      | 17/45 [00:01<00:01, 18.51it/s] 47%|████▋     | 21/45 [00:01<00:01, 20.06it/s] 62%|██████▏   | 28/45 [00:01<00:00, 28.55it/s] 73%|███████▎  | 33/45 [00:02<00:00, 22.66it/s] 82%|████████▏ | 37/45 [00:02<00:00, 21.81it/s]100%|██████████| 45/45 [00:02<00:00, 19.31it/s]
5640 images processed, 2.3527848720550537 seconds used

18.097285509109497
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.53  99.36  98.11
places365     68.31  80.53  78.96
LSUN          21.69  95.04  95.10
iSUN          72.36  81.48  84.57
dtd           38.32  91.26  94.50
AVG           40.64  89.54  90.25
Retain-Acc: 0.7486
Forget-as-OOD (retain known vs forget novel):
  FPR: 55.40 AUROC: 88.43 AUIN: 99.25
9.714046716690063
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08_rf.png
