nohup: ignoring input
==== Stage 1: inc={0,8,11,40,51}; seen={}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='0,8,11,40,51', forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:08<06:54,  8.46s/it]  4%|▍         | 2/50 [00:09<03:17,  4.11s/it]  6%|▌         | 3/50 [00:10<01:57,  2.50s/it]  8%|▊         | 4/50 [00:10<01:24,  1.84s/it] 10%|█         | 5/50 [00:11<01:05,  1.46s/it] 12%|█▏        | 6/50 [00:12<00:49,  1.12s/it] 14%|█▍        | 7/50 [00:12<00:40,  1.07it/s] 16%|█▌        | 8/50 [00:13<00:32,  1.29it/s] 18%|█▊        | 9/50 [00:14<00:33,  1.23it/s] 20%|██        | 10/50 [00:14<00:31,  1.26it/s] 22%|██▏       | 11/50 [00:15<00:31,  1.22it/s] 24%|██▍       | 12/50 [00:16<00:30,  1.25it/s] 26%|██▌       | 13/50 [00:16<00:26,  1.41it/s] 28%|██▊       | 14/50 [00:17<00:24,  1.49it/s] 30%|███       | 15/50 [00:17<00:21,  1.64it/s] 32%|███▏      | 16/50 [00:18<00:21,  1.61it/s] 34%|███▍      | 17/50 [00:19<00:23,  1.39it/s] 36%|███▌      | 18/50 [00:20<00:22,  1.39it/s] 38%|███▊      | 19/50 [00:20<00:20,  1.49it/s] 40%|████      | 20/50 [00:21<00:22,  1.31it/s] 42%|████▏     | 21/50 [00:22<00:22,  1.29it/s] 44%|████▍     | 22/50 [00:23<00:19,  1.44it/s] 46%|████▌     | 23/50 [00:24<00:21,  1.28it/s] 48%|████▊     | 24/50 [00:24<00:20,  1.28it/s] 50%|█████     | 25/50 [00:25<00:18,  1.35it/s] 52%|█████▏    | 26/50 [00:26<00:17,  1.39it/s] 54%|█████▍    | 27/50 [00:27<00:16,  1.36it/s] 56%|█████▌    | 28/50 [00:27<00:15,  1.45it/s] 58%|█████▊    | 29/50 [00:28<00:13,  1.51it/s] 60%|██████    | 30/50 [00:28<00:12,  1.63it/s] 62%|██████▏   | 31/50 [00:29<00:11,  1.68it/s] 64%|██████▍   | 32/50 [00:29<00:10,  1.73it/s] 66%|██████▌   | 33/50 [00:30<00:09,  1.77it/s] 68%|██████▊   | 34/50 [00:30<00:09,  1.75it/s] 70%|███████   | 35/50 [00:31<00:08,  1.78it/s] 72%|███████▏  | 36/50 [00:31<00:07,  1.84it/s] 74%|███████▍  | 37/50 [00:32<00:06,  1.89it/s] 76%|███████▌  | 38/50 [00:32<00:06,  1.98it/s] 78%|███████▊  | 39/50 [00:33<00:05,  1.91it/s] 80%|████████  | 40/50 [00:34<00:05,  1.82it/s] 82%|████████▏ | 41/50 [00:34<00:04,  1.82it/s] 84%|████████▍ | 42/50 [00:35<00:04,  1.84it/s] 86%|████████▌ | 43/50 [00:35<00:03,  1.83it/s] 88%|████████▊ | 44/50 [00:36<00:03,  1.75it/s] 90%|█████████ | 45/50 [00:36<00:02,  1.75it/s] 92%|█████████▏| 46/50 [00:37<00:02,  1.80it/s] 94%|█████████▍| 47/50 [00:37<00:01,  1.83it/s] 96%|█████████▌| 48/50 [00:38<00:01,  1.89it/s] 98%|█████████▊| 49/50 [00:38<00:00,  1.86it/s]100%|██████████| 50/50 [00:39<00:00,  1.88it/s]100%|██████████| 50/50 [00:39<00:00,  1.27it/s]
[loss] ep 0 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 2 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 5 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 7 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 10 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 12 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 15 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 17 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 20 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 22 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 25 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 27 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 30 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 32 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 35 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 37 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 40 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 42 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 45 it 0 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 47 it 10 total=-0.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stage1-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<04:31,  1.58it/s]  2%|▏         | 9/430 [00:00<00:26, 15.81it/s]  4%|▍         | 19/430 [00:00<00:12, 32.53it/s]  7%|▋         | 29/430 [00:00<00:08, 46.96it/s]  9%|▉         | 39/430 [00:01<00:06, 58.39it/s] 11%|█▏        | 49/430 [00:01<00:05, 66.68it/s] 14%|█▎        | 59/430 [00:01<00:04, 74.52it/s] 16%|█▌        | 68/430 [00:01<00:04, 75.47it/s] 18%|█▊        | 77/430 [00:01<00:04, 71.53it/s] 20%|██        | 87/430 [00:01<00:04, 77.12it/s] 23%|██▎       | 97/430 [00:01<00:04, 81.15it/s] 25%|██▍       | 107/430 [00:01<00:03, 85.21it/s] 27%|██▋       | 117/430 [00:01<00:03, 87.86it/s] 30%|██▉       | 127/430 [00:02<00:03, 89.85it/s] 32%|███▏      | 137/430 [00:02<00:03, 91.15it/s] 34%|███▍      | 147/430 [00:02<00:03, 92.50it/s] 37%|███▋      | 157/430 [00:02<00:02, 92.22it/s] 39%|███▉      | 167/430 [00:02<00:02, 92.74it/s] 41%|████      | 177/430 [00:02<00:02, 91.92it/s] 43%|████▎     | 187/430 [00:02<00:02, 92.15it/s] 46%|████▌     | 197/430 [00:02<00:02, 83.46it/s] 48%|████▊     | 206/430 [00:02<00:02, 79.44it/s] 50%|█████     | 216/430 [00:03<00:02, 83.54it/s] 52%|█████▏    | 225/430 [00:03<00:02, 84.22it/s] 55%|█████▍    | 235/430 [00:03<00:02, 87.67it/s] 57%|█████▋    | 245/430 [00:03<00:02, 89.74it/s] 59%|█████▉    | 255/430 [00:03<00:01, 91.56it/s] 62%|██████▏   | 265/430 [00:03<00:01, 92.03it/s] 64%|██████▍   | 275/430 [00:03<00:01, 93.35it/s] 66%|██████▋   | 285/430 [00:03<00:01, 87.64it/s] 68%|██████▊   | 294/430 [00:03<00:01, 84.24it/s] 70%|███████   | 303/430 [00:04<00:01, 79.76it/s] 73%|███████▎  | 313/430 [00:04<00:01, 83.46it/s] 75%|███████▌  | 323/430 [00:04<00:01, 86.68it/s] 77%|███████▋  | 333/430 [00:04<00:01, 88.27it/s] 80%|███████▉  | 343/430 [00:04<00:00, 90.41it/s] 82%|████████▏ | 353/430 [00:04<00:00, 92.07it/s] 84%|████████▍ | 363/430 [00:04<00:00, 93.11it/s] 87%|████████▋ | 373/430 [00:04<00:00, 91.10it/s] 89%|████████▉ | 383/430 [00:04<00:00, 92.02it/s] 91%|█████████▏| 393/430 [00:05<00:00, 92.88it/s] 94%|█████████▎| 403/430 [00:05<00:00, 93.25it/s] 96%|█████████▌| 413/430 [00:05<00:00, 94.64it/s] 98%|█████████▊| 423/430 [00:05<00:00, 95.58it/s]100%|██████████| 430/430 [00:05<00:00, 78.84it/s]
55000 images processed, 5.490555763244629 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<01:05,  1.30it/s] 13%|█▎        | 11/86 [00:00<00:04, 16.68it/s] 24%|██▍       | 21/86 [00:00<00:02, 31.61it/s] 36%|███▌      | 31/86 [00:01<00:01, 44.94it/s] 48%|████▊     | 41/86 [00:01<00:00, 56.41it/s] 59%|█████▉    | 51/86 [00:01<00:00, 65.88it/s] 71%|███████   | 61/86 [00:01<00:00, 73.78it/s] 83%|████████▎ | 71/86 [00:01<00:00, 70.98it/s] 93%|█████████▎| 80/86 [00:01<00:00, 69.21it/s]100%|██████████| 86/86 [00:01<00:00, 48.06it/s]
11000 images processed, 1.8233442306518555 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:20,  1.45it/s]  5%|▌         | 11/204 [00:00<00:10, 18.07it/s] 10%|█         | 21/204 [00:00<00:05, 33.43it/s] 15%|█▌        | 31/204 [00:01<00:03, 46.94it/s] 20%|██        | 41/204 [00:01<00:02, 58.26it/s] 25%|██▌       | 51/204 [00:01<00:02, 67.69it/s] 30%|██▉       | 61/204 [00:01<00:01, 74.71it/s] 35%|███▍      | 71/204 [00:01<00:01, 75.81it/s] 40%|███▉      | 81/204 [00:01<00:01, 80.64it/s] 45%|████▍     | 91/204 [00:01<00:01, 84.34it/s] 50%|████▉     | 101/204 [00:01<00:01, 86.14it/s] 54%|█████▍    | 111/204 [00:01<00:01, 88.22it/s] 59%|█████▉    | 121/204 [00:02<00:00, 89.61it/s] 64%|██████▍   | 131/204 [00:02<00:00, 90.99it/s] 69%|██████▉   | 141/204 [00:02<00:00, 89.97it/s] 74%|███████▍  | 151/204 [00:02<00:00, 90.93it/s] 79%|███████▉  | 161/204 [00:02<00:00, 91.83it/s] 84%|████████▍ | 171/204 [00:02<00:00, 92.19it/s] 89%|████████▊ | 181/204 [00:02<00:00, 93.21it/s] 94%|█████████▎| 191/204 [00:02<00:00, 94.06it/s] 99%|█████████▊| 201/204 [00:02<00:00, 94.73it/s]100%|██████████| 204/204 [00:02<00:00, 70.59it/s]
26032 images processed, 2.9308583736419678 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:01,  1.26it/s] 14%|█▍        | 11/79 [00:00<00:04, 16.13it/s] 25%|██▌       | 20/79 [00:01<00:02, 28.96it/s] 38%|███▊      | 30/79 [00:01<00:01, 42.64it/s] 49%|████▉     | 39/79 [00:01<00:00, 52.87it/s] 62%|██████▏   | 49/79 [00:01<00:00, 58.13it/s] 75%|███████▍  | 59/79 [00:01<00:00, 67.39it/s] 86%|████████▌ | 68/79 [00:01<00:00, 69.53it/s] 97%|█████████▋| 77/79 [00:01<00:00, 70.63it/s]100%|██████████| 79/79 [00:02<00:00, 27.86it/s]
10000 images processed, 2.870776414871216 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:55,  1.41it/s] 13%|█▎        | 10/79 [00:00<00:04, 16.04it/s] 24%|██▍       | 19/79 [00:00<00:02, 30.00it/s] 34%|███▍      | 27/79 [00:01<00:01, 40.32it/s] 46%|████▌     | 36/79 [00:01<00:00, 51.52it/s] 57%|█████▋    | 45/79 [00:01<00:00, 60.89it/s] 70%|██████▉   | 55/79 [00:01<00:00, 70.14it/s] 82%|████████▏ | 65/79 [00:01<00:00, 77.29it/s] 95%|█████████▍| 75/79 [00:01<00:00, 82.61it/s]100%|██████████| 79/79 [00:01<00:00, 50.08it/s]
10000 images processed, 1.6014330387115479 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:44,  1.56it/s] 16%|█▌        | 11/70 [00:00<00:03, 19.19it/s] 30%|███       | 21/70 [00:00<00:01, 35.20it/s] 44%|████▍     | 31/70 [00:00<00:00, 48.87it/s] 59%|█████▊    | 41/70 [00:01<00:00, 59.75it/s] 73%|███████▎  | 51/70 [00:01<00:00, 69.21it/s] 87%|████████▋ | 61/70 [00:01<00:00, 76.56it/s]100%|██████████| 70/70 [00:01<00:00, 50.79it/s]
8925 images processed, 1.4101402759552002 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:54,  1.23s/it] 18%|█▊        | 8/45 [00:01<00:04,  7.93it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.44it/s] 53%|█████▎    | 24/45 [00:01<00:01, 18.56it/s] 67%|██████▋   | 30/45 [00:02<00:00, 23.68it/s] 78%|███████▊  | 35/45 [00:02<00:00, 17.53it/s]100%|██████████| 45/45 [00:02<00:00, 27.36it/s]100%|██████████| 45/45 [00:02<00:00, 17.13it/s]
5640 images processed, 2.651026725769043 seconds used

20.601710081100464
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.55  99.04  73.57
places365     49.39  89.47  51.31
LSUN          25.67  95.16  75.96
iSUN          25.79  95.00  73.64
dtd           24.45  94.42  71.69
AVG           25.77  94.62  69.23
[incremental] Overall: 0.8500 New: 0.8500 Old: nan
[incremental] Final(Top-1): 0.5097  Average: 0.6630
2.9389004707336426
==== Stage 2: inc={66,67,88,94,57}; seen={0,8,11,40,51}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1', lora_stack=False, lora_orth_enable=True, lora_orth_lambda=1.0, lora_orth_ref_paths='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1', forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='66,67,88,94,57', forget_classes_seen='0,8,11,40,51', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/shaokun/PALM/trainer.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
  2%|▏         | 1/50 [00:12<09:59, 12.24s/it]  4%|▍         | 2/50 [00:13<04:34,  5.72s/it]  6%|▌         | 3/50 [00:14<02:49,  3.60s/it]  8%|▊         | 4/50 [00:15<01:52,  2.44s/it] 10%|█         | 5/50 [00:16<01:34,  2.09s/it] 12%|█▏        | 6/50 [00:18<01:24,  1.92s/it] 14%|█▍        | 7/50 [00:19<01:08,  1.59s/it] 16%|█▌        | 8/50 [00:19<00:57,  1.37s/it] 18%|█▊        | 9/50 [00:20<00:49,  1.22s/it] 20%|██        | 10/50 [00:21<00:44,  1.12s/it] 22%|██▏       | 11/50 [00:22<00:37,  1.04it/s] 24%|██▍       | 12/50 [00:23<00:34,  1.10it/s] 26%|██▌       | 13/50 [00:23<00:31,  1.19it/s] 28%|██▊       | 14/50 [00:24<00:32,  1.09it/s] 30%|███       | 15/50 [00:25<00:29,  1.17it/s] 32%|███▏      | 16/50 [00:26<00:26,  1.28it/s] 34%|███▍      | 17/50 [00:27<00:28,  1.15it/s] 36%|███▌      | 18/50 [00:27<00:25,  1.25it/s] 38%|███▊      | 19/50 [00:28<00:24,  1.27it/s] 40%|████      | 20/50 [00:29<00:23,  1.30it/s] 42%|████▏     | 21/50 [00:30<00:21,  1.36it/s] 44%|████▍     | 22/50 [00:30<00:19,  1.44it/s] 46%|████▌     | 23/50 [00:31<00:19,  1.40it/s] 48%|████▊     | 24/50 [00:32<00:18,  1.39it/s] 50%|█████     | 25/50 [00:32<00:18,  1.37it/s] 52%|█████▏    | 26/50 [00:33<00:17,  1.35it/s] 54%|█████▍    | 27/50 [00:34<00:16,  1.38it/s] 56%|█████▌    | 28/50 [00:35<00:15,  1.41it/s] 58%|█████▊    | 29/50 [00:35<00:14,  1.47it/s] 60%|██████    | 30/50 [00:36<00:13,  1.49it/s][loss] ep 0 it 0 total=1.0056 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0056 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 2 it 10 total=1.0056 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0056 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 5 it 0 total=1.0054 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0054 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 7 it 10 total=1.0050 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0050 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 10 it 0 total=1.0046 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0046 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 12 it 10 total=1.0041 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0041 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 15 it 0 total=1.0035 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0035 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 17 it 10 total=1.0031 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0031 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 20 it 0 total=1.0026 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0026 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 22 it 10 total=1.0022 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0022 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 25 it 0 total=1.0018 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0018 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 27 it 10 total=1.0014 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0014 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
 62%|██████▏   | 31/50 [00:36<00:12,  1.57it/s] 64%|██████▍   | 32/50 [00:37<00:11,  1.51it/s] 66%|██████▌   | 33/50 [00:38<00:11,  1.53it/s] 68%|██████▊   | 34/50 [00:38<00:09,  1.61it/s] 70%|███████   | 35/50 [00:39<00:09,  1.66it/s] 72%|███████▏  | 36/50 [00:40<00:08,  1.60it/s] 74%|███████▍  | 37/50 [00:40<00:07,  1.64it/s] 76%|███████▌  | 38/50 [00:41<00:07,  1.57it/s] 78%|███████▊  | 39/50 [00:42<00:07,  1.54it/s] 80%|████████  | 40/50 [00:42<00:06,  1.53it/s] 82%|████████▏ | 41/50 [00:43<00:06,  1.37it/s] 84%|████████▍ | 42/50 [00:44<00:05,  1.35it/s] 86%|████████▌ | 43/50 [00:45<00:05,  1.37it/s] 88%|████████▊ | 44/50 [00:45<00:04,  1.41it/s] 90%|█████████ | 45/50 [00:46<00:03,  1.52it/s] 92%|█████████▏| 46/50 [00:46<00:02,  1.57it/s] 94%|█████████▍| 47/50 [00:47<00:01,  1.55it/s] 96%|█████████▌| 48/50 [00:48<00:01,  1.48it/s] 98%|█████████▊| 49/50 [00:48<00:00,  1.43it/s]100%|██████████| 50/50 [00:49<00:00,  1.47it/s]100%|██████████| 50/50 [00:49<00:00,  1.01it/s]
[loss] ep 30 it 0 total=1.0011 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0011 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 32 it 10 total=1.0009 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0009 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 35 it 0 total=1.0007 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0007 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 37 it 10 total=1.0006 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0006 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 40 it 0 total=1.0005 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0005 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 42 it 10 total=1.0004 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0004 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 45 it 0 total=1.0004 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0004 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 47 it 10 total=1.0004 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0004 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stage2-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<03:58,  1.80it/s]  2%|▏         | 9/430 [00:00<00:23, 17.71it/s]  4%|▍         | 17/430 [00:00<00:13, 31.17it/s]  6%|▌         | 25/430 [00:00<00:09, 42.28it/s]  7%|▋         | 32/430 [00:00<00:08, 47.69it/s]  9%|▉         | 40/430 [00:01<00:07, 53.51it/s] 11%|█▏        | 49/430 [00:01<00:06, 61.91it/s] 13%|█▎        | 58/430 [00:01<00:05, 66.89it/s] 15%|█▌        | 66/430 [00:01<00:05, 69.16it/s] 17%|█▋        | 74/430 [00:01<00:05, 70.98it/s] 19%|█▉        | 82/430 [00:01<00:04, 72.88it/s] 21%|██        | 91/430 [00:01<00:04, 75.16it/s] 23%|██▎       | 99/430 [00:01<00:04, 74.74it/s] 25%|██▍       | 107/430 [00:01<00:04, 73.89it/s] 27%|██▋       | 115/430 [00:02<00:04, 75.60it/s] 29%|██▊       | 123/430 [00:02<00:04, 74.39it/s] 30%|███       | 131/430 [00:02<00:03, 75.15it/s] 32%|███▏      | 139/430 [00:02<00:03, 75.13it/s] 34%|███▍      | 148/430 [00:02<00:03, 77.66it/s] 36%|███▋      | 156/430 [00:02<00:03, 77.79it/s] 38%|███▊      | 164/430 [00:02<00:03, 77.84it/s] 40%|████      | 172/430 [00:02<00:04, 53.83it/s] 42%|████▏     | 180/430 [00:03<00:04, 59.33it/s] 44%|████▎     | 188/430 [00:03<00:03, 63.69it/s] 46%|████▌     | 196/430 [00:03<00:03, 67.37it/s] 48%|████▊     | 205/430 [00:03<00:03, 73.03it/s] 50%|████▉     | 213/430 [00:03<00:02, 74.29it/s] 52%|█████▏    | 222/430 [00:03<00:02, 77.37it/s] 54%|█████▎    | 231/430 [00:03<00:02, 79.27it/s] 56%|█████▌    | 240/430 [00:03<00:02, 81.01it/s] 58%|█████▊    | 249/430 [00:03<00:02, 81.62it/s] 60%|██████    | 258/430 [00:04<00:02, 80.08it/s] 62%|██████▏   | 267/430 [00:04<00:01, 81.64it/s] 64%|██████▍   | 276/430 [00:04<00:02, 75.58it/s] 66%|██████▌   | 284/430 [00:04<00:01, 73.74it/s] 68%|██████▊   | 292/430 [00:04<00:01, 74.12it/s] 70%|██████▉   | 300/430 [00:04<00:01, 75.02it/s] 72%|███████▏  | 308/430 [00:04<00:01, 74.37it/s] 73%|███████▎  | 316/430 [00:04<00:01, 75.20it/s] 76%|███████▌  | 325/430 [00:04<00:01, 77.51it/s] 77%|███████▋  | 333/430 [00:05<00:01, 71.36it/s] 79%|███████▉  | 341/430 [00:05<00:01, 70.08it/s] 81%|████████  | 349/430 [00:05<00:01, 69.49it/s] 83%|████████▎ | 358/430 [00:05<00:00, 72.98it/s] 85%|████████▌ | 366/430 [00:05<00:00, 72.39it/s] 87%|████████▋ | 374/430 [00:05<00:00, 68.04it/s] 89%|████████▉ | 383/430 [00:05<00:00, 72.05it/s] 91%|█████████ | 391/430 [00:05<00:00, 73.68it/s] 93%|█████████▎| 400/430 [00:05<00:00, 76.49it/s] 95%|█████████▌| 409/430 [00:06<00:00, 77.89it/s] 97%|█████████▋| 418/430 [00:06<00:00, 78.80it/s] 99%|█████████▉| 426/430 [00:06<00:00, 77.60it/s]100%|██████████| 430/430 [00:06<00:00, 67.60it/s]
55000 images processed, 6.439270973205566 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:49,  1.70it/s] 10%|█         | 9/86 [00:00<00:04, 16.95it/s] 20%|█▉        | 17/86 [00:00<00:02, 29.88it/s] 28%|██▊       | 24/86 [00:00<00:01, 39.20it/s] 37%|███▋      | 32/86 [00:01<00:01, 48.87it/s] 47%|████▋     | 40/86 [00:01<00:00, 56.87it/s] 56%|█████▌    | 48/86 [00:01<00:00, 62.56it/s] 65%|██████▌   | 56/86 [00:01<00:00, 66.26it/s] 74%|███████▍  | 64/86 [00:01<00:00, 69.89it/s] 84%|████████▎ | 72/86 [00:01<00:00, 69.80it/s] 93%|█████████▎| 80/86 [00:01<00:00, 71.50it/s]100%|██████████| 86/86 [00:01<00:00, 49.42it/s]
11000 images processed, 1.7548010349273682 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:52,  1.81it/s]  4%|▍         | 8/204 [00:00<00:12, 15.55it/s]  8%|▊         | 17/204 [00:00<00:05, 31.40it/s] 13%|█▎        | 26/204 [00:00<00:04, 44.12it/s] 17%|█▋        | 34/204 [00:00<00:03, 52.02it/s] 21%|██        | 42/204 [00:01<00:02, 58.32it/s] 25%|██▍       | 50/204 [00:01<00:02, 63.88it/s] 28%|██▊       | 58/204 [00:01<00:02, 67.56it/s] 32%|███▏      | 66/204 [00:01<00:02, 68.90it/s] 36%|███▋      | 74/204 [00:01<00:01, 70.67it/s] 40%|████      | 82/204 [00:01<00:01, 65.41it/s] 44%|████▎     | 89/204 [00:01<00:01, 66.48it/s] 47%|████▋     | 96/204 [00:01<00:01, 66.80it/s] 51%|█████▏    | 105/204 [00:01<00:01, 71.08it/s] 55%|█████▌    | 113/204 [00:02<00:01, 72.95it/s] 59%|█████▉    | 121/204 [00:02<00:01, 74.80it/s] 63%|██████▎   | 129/204 [00:02<00:01, 74.72it/s] 67%|██████▋   | 137/204 [00:02<00:00, 74.58it/s] 71%|███████   | 145/204 [00:02<00:00, 74.07it/s] 75%|███████▌  | 153/204 [00:02<00:00, 72.78it/s] 79%|███████▉  | 161/204 [00:02<00:00, 69.31it/s] 82%|████████▏ | 168/204 [00:02<00:00, 68.80it/s] 86%|████████▌ | 175/204 [00:02<00:00, 68.31it/s] 90%|████████▉ | 183/204 [00:03<00:00, 69.47it/s] 94%|█████████▎| 191/204 [00:03<00:00, 70.42it/s] 98%|█████████▊| 199/204 [00:03<00:00, 72.58it/s]100%|██████████| 204/204 [00:03<00:00, 60.93it/s]
26032 images processed, 3.3889145851135254 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:04,  1.21it/s] 11%|█▏        | 9/79 [00:00<00:05, 12.65it/s] 22%|██▏       | 17/79 [00:01<00:02, 24.14it/s] 32%|███▏      | 25/79 [00:01<00:01, 34.48it/s] 41%|████      | 32/79 [00:01<00:01, 42.13it/s] 49%|████▉     | 39/79 [00:01<00:00, 48.60it/s] 58%|█████▊    | 46/79 [00:01<00:00, 53.60it/s] 68%|██████▊   | 54/79 [00:01<00:00, 59.36it/s] 78%|███████▊  | 62/79 [00:01<00:00, 63.23it/s] 89%|████████▊ | 70/79 [00:01<00:00, 65.18it/s] 99%|█████████▊| 78/79 [00:01<00:00, 69.21it/s]100%|██████████| 79/79 [00:02<00:00, 35.56it/s]
10000 images processed, 2.2493062019348145 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:47,  1.63it/s] 10%|█         | 8/79 [00:00<00:04, 14.49it/s] 20%|██        | 16/79 [00:00<00:02, 28.01it/s] 30%|███       | 24/79 [00:00<00:01, 39.28it/s] 39%|███▉      | 31/79 [00:01<00:01, 46.23it/s] 51%|█████     | 40/79 [00:01<00:00, 55.96it/s] 61%|██████    | 48/79 [00:01<00:00, 59.95it/s] 71%|███████   | 56/79 [00:01<00:00, 63.56it/s] 82%|████████▏ | 65/79 [00:01<00:00, 69.28it/s] 94%|█████████▎| 74/79 [00:01<00:00, 72.07it/s]100%|██████████| 79/79 [00:01<00:00, 47.83it/s]
10000 images processed, 1.6717979907989502 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:48,  1.44it/s] 13%|█▎        | 9/70 [00:00<00:04, 14.52it/s] 23%|██▎       | 16/70 [00:00<00:02, 24.78it/s] 33%|███▎      | 23/70 [00:01<00:01, 33.18it/s] 44%|████▍     | 31/70 [00:01<00:00, 43.18it/s] 54%|█████▍    | 38/70 [00:01<00:00, 49.49it/s] 66%|██████▌   | 46/70 [00:01<00:00, 56.54it/s] 76%|███████▌  | 53/70 [00:01<00:00, 59.63it/s] 87%|████████▋ | 61/70 [00:01<00:00, 63.27it/s] 99%|█████████▊| 69/70 [00:01<00:00, 65.82it/s]100%|██████████| 70/70 [00:01<00:00, 41.34it/s]
8925 images processed, 1.7244765758514404 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:54,  1.25s/it]  4%|▍         | 2/45 [00:01<00:28,  1.53it/s] 22%|██▏       | 10/45 [00:01<00:03, 10.47it/s] 38%|███▊      | 17/45 [00:01<00:01, 15.19it/s] 47%|████▋     | 21/45 [00:02<00:01, 15.01it/s] 69%|██████▉   | 31/45 [00:02<00:00, 26.57it/s] 80%|████████  | 36/45 [00:02<00:00, 16.94it/s]100%|██████████| 45/45 [00:02<00:00, 15.43it/s]
5640 images processed, 2.9385974407196045 seconds used

21.91590166091919
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.79  98.86  78.72
places365     52.73  85.08  47.05
LSUN          44.68  89.26  61.89
iSUN          35.83  91.28  65.27
dtd           32.80  92.10  73.35
AVG           33.97  91.32  65.25
[incremental] Overall: 0.7410 New: 0.7220 Old: 0.7600
[incremental] Final(Top-1): 0.5097  Average: 0.6630
2.450516939163208
==== Stage 3: inc={59,58,44,93,10}; seen={0,8,11,40,51,66,67,88,94,57}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2', lora_stack=False, lora_orth_enable=True, lora_orth_lambda=1.0, lora_orth_ref_paths='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2', forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='59,58,44,93,10', forget_classes_seen='0,8,11,40,51,66,67,88,94,57', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/shaokun/PALM/trainer.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
  2%|▏         | 1/50 [00:11<09:12, 11.28s/it]  4%|▍         | 2/50 [00:12<04:08,  5.17s/it]  6%|▌         | 3/50 [00:13<02:35,  3.31s/it]  8%|▊         | 4/50 [00:13<01:43,  2.25s/it] 10%|█         | 5/50 [00:14<01:17,  1.73s/it] 12%|█▏        | 6/50 [00:15<01:03,  1.44s/it] 14%|█▍        | 7/50 [00:16<00:53,  1.24s/it] 16%|█▌        | 8/50 [00:17<00:43,  1.04s/it] 18%|█▊        | 9/50 [00:17<00:38,  1.06it/s] 20%|██        | 10/50 [00:18<00:35,  1.14it/s] 22%|██▏       | 11/50 [00:19<00:31,  1.23it/s] 24%|██▍       | 12/50 [00:19<00:29,  1.29it/s] 26%|██▌       | 13/50 [00:20<00:28,  1.32it/s] 28%|██▊       | 14/50 [00:21<00:27,  1.33it/s] 30%|███       | 15/50 [00:22<00:26,  1.32it/s] 32%|███▏      | 16/50 [00:22<00:26,  1.27it/s] 34%|███▍      | 17/50 [00:23<00:28,  1.15it/s] 36%|███▌      | 18/50 [00:24<00:27,  1.15it/s] 38%|███▊      | 19/50 [00:25<00:25,  1.23it/s] 40%|████      | 20/50 [00:26<00:23,  1.27it/s] 42%|████▏     | 21/50 [00:26<00:21,  1.34it/s] 44%|████▍     | 22/50 [00:27<00:20,  1.37it/s] 46%|████▌     | 23/50 [00:28<00:19,  1.40it/s] 48%|████▊     | 24/50 [00:28<00:18,  1.42it/s] 50%|█████     | 25/50 [00:29<00:17,  1.42it/s] 52%|█████▏    | 26/50 [00:30<00:16,  1.48it/s] 54%|█████▍    | 27/50 [00:30<00:15,  1.50it/s] 56%|█████▌    | 28/50 [00:31<00:14,  1.51it/s] 58%|█████▊    | 29/50 [00:32<00:14,  1.44it/s] 60%|██████    | 30/50 [00:33<00:13,  1.46it/s][loss] ep 0 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 2 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 5 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 7 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 10 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 12 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 15 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 17 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 20 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 22 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 25 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 27 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
 62%|██████▏   | 31/50 [00:33<00:12,  1.46it/s] 64%|██████▍   | 32/50 [00:34<00:12,  1.45it/s] 66%|██████▌   | 33/50 [00:35<00:11,  1.46it/s] 68%|██████▊   | 34/50 [00:35<00:11,  1.40it/s] 70%|███████   | 35/50 [00:36<00:10,  1.42it/s] 72%|███████▏  | 36/50 [00:37<00:09,  1.43it/s] 74%|███████▍  | 37/50 [00:37<00:08,  1.50it/s] 76%|███████▌  | 38/50 [00:38<00:08,  1.45it/s] 78%|███████▊  | 39/50 [00:39<00:07,  1.46it/s] 80%|████████  | 40/50 [00:39<00:06,  1.49it/s] 82%|████████▏ | 41/50 [00:40<00:05,  1.51it/s] 84%|████████▍ | 42/50 [00:41<00:05,  1.56it/s] 86%|████████▌ | 43/50 [00:41<00:04,  1.60it/s] 88%|████████▊ | 44/50 [00:42<00:03,  1.66it/s] 90%|█████████ | 45/50 [00:42<00:02,  1.72it/s] 92%|█████████▏| 46/50 [00:43<00:02,  1.74it/s] 94%|█████████▍| 47/50 [00:43<00:01,  1.73it/s] 96%|█████████▌| 48/50 [00:44<00:01,  1.80it/s] 98%|█████████▊| 49/50 [00:45<00:00,  1.73it/s]100%|██████████| 50/50 [00:45<00:00,  1.77it/s]100%|██████████| 50/50 [00:45<00:00,  1.10it/s]
[loss] ep 30 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 32 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 35 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 37 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 40 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 42 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 45 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 47 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stage3-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<04:06,  1.74it/s]  2%|▏         | 8/430 [00:00<00:27, 15.18it/s]  4%|▍         | 18/430 [00:00<00:12, 33.36it/s]  7%|▋         | 28/430 [00:00<00:08, 48.36it/s]  9%|▉         | 38/430 [00:00<00:06, 60.44it/s] 11%|█         | 48/430 [00:01<00:05, 69.97it/s] 13%|█▎        | 58/430 [00:01<00:04, 76.58it/s] 16%|█▌        | 68/430 [00:01<00:04, 82.12it/s] 18%|█▊        | 78/430 [00:01<00:04, 86.15it/s] 20%|██        | 88/430 [00:01<00:03, 88.75it/s] 23%|██▎       | 98/430 [00:01<00:03, 90.52it/s] 25%|██▌       | 108/430 [00:01<00:03, 90.72it/s] 27%|██▋       | 118/430 [00:01<00:03, 88.85it/s] 30%|██▉       | 128/430 [00:01<00:03, 89.51it/s] 32%|███▏      | 138/430 [00:02<00:03, 91.28it/s] 34%|███▍      | 148/430 [00:02<00:03, 92.26it/s] 37%|███▋      | 158/430 [00:02<00:02, 92.53it/s] 39%|███▉      | 168/430 [00:02<00:02, 93.38it/s] 41%|████▏     | 178/430 [00:02<00:02, 93.64it/s] 44%|████▎     | 188/430 [00:02<00:02, 94.13it/s] 46%|████▌     | 198/430 [00:02<00:03, 76.38it/s] 48%|████▊     | 208/430 [00:02<00:02, 80.57it/s] 51%|█████     | 218/430 [00:03<00:02, 84.50it/s] 53%|█████▎    | 228/430 [00:03<00:02, 87.68it/s] 55%|█████▌    | 238/430 [00:03<00:02, 89.86it/s] 58%|█████▊    | 248/430 [00:03<00:01, 91.38it/s] 60%|██████    | 258/430 [00:03<00:01, 92.81it/s] 62%|██████▏   | 268/430 [00:03<00:01, 92.37it/s] 65%|██████▍   | 278/430 [00:03<00:01, 93.42it/s] 67%|██████▋   | 288/430 [00:03<00:01, 94.16it/s] 69%|██████▉   | 298/430 [00:03<00:01, 94.74it/s] 72%|███████▏  | 308/430 [00:03<00:01, 94.98it/s] 74%|███████▍  | 318/430 [00:04<00:01, 94.77it/s] 76%|███████▋  | 328/430 [00:04<00:01, 94.88it/s] 79%|███████▊  | 338/430 [00:04<00:00, 95.05it/s] 81%|████████  | 348/430 [00:04<00:00, 94.85it/s] 83%|████████▎ | 358/430 [00:04<00:00, 94.66it/s] 86%|████████▌ | 368/430 [00:04<00:00, 94.92it/s] 88%|████████▊ | 378/430 [00:04<00:00, 94.44it/s] 90%|█████████ | 388/430 [00:04<00:00, 94.46it/s] 93%|█████████▎| 398/430 [00:04<00:00, 93.62it/s] 95%|█████████▍| 408/430 [00:05<00:00, 94.77it/s] 97%|█████████▋| 418/430 [00:05<00:00, 95.62it/s]100%|█████████▉| 428/430 [00:05<00:00, 96.13it/s]100%|██████████| 430/430 [00:05<00:00, 82.04it/s]
55000 images processed, 5.313177108764648 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:57,  1.47it/s] 13%|█▎        | 11/86 [00:00<00:04, 18.41it/s] 23%|██▎       | 20/86 [00:00<00:02, 32.32it/s] 35%|███▍      | 30/86 [00:00<00:01, 46.60it/s] 47%|████▋     | 40/86 [00:01<00:00, 57.74it/s] 58%|█████▊    | 50/86 [00:01<00:00, 67.17it/s] 70%|██████▉   | 60/86 [00:01<00:00, 74.68it/s] 81%|████████▏ | 70/86 [00:01<00:00, 80.67it/s] 93%|█████████▎| 80/86 [00:01<00:00, 85.21it/s]100%|██████████| 86/86 [00:01<00:00, 54.07it/s]
11000 images processed, 1.6094143390655518 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:17,  1.47it/s]  3%|▎         | 7/204 [00:00<00:17, 11.53it/s]  8%|▊         | 17/204 [00:00<00:06, 28.61it/s] 13%|█▎        | 27/204 [00:00<00:04, 43.54it/s] 18%|█▊        | 36/204 [00:01<00:03, 54.31it/s] 23%|██▎       | 46/204 [00:01<00:02, 64.60it/s] 27%|██▋       | 56/204 [00:01<00:02, 72.26it/s] 32%|███▏      | 66/204 [00:01<00:01, 78.29it/s] 37%|███▋      | 76/204 [00:01<00:01, 81.60it/s] 42%|████▏     | 85/204 [00:01<00:01, 83.67it/s] 47%|████▋     | 95/204 [00:01<00:01, 85.78it/s] 51%|█████▏    | 105/204 [00:01<00:01, 85.73it/s] 56%|█████▌    | 114/204 [00:01<00:01, 86.43it/s] 61%|██████    | 124/204 [00:02<00:00, 88.28it/s] 66%|██████▌   | 134/204 [00:02<00:00, 89.87it/s] 71%|███████   | 144/204 [00:02<00:00, 91.15it/s] 75%|███████▌  | 154/204 [00:02<00:00, 90.44it/s] 80%|████████  | 164/204 [00:02<00:00, 91.30it/s] 85%|████████▌ | 174/204 [00:02<00:00, 91.07it/s] 90%|█████████ | 184/204 [00:02<00:00, 92.63it/s] 95%|█████████▌| 194/204 [00:02<00:00, 93.77it/s]100%|██████████| 204/204 [00:02<00:00, 93.11it/s]100%|██████████| 204/204 [00:02<00:00, 69.61it/s]
26032 images processed, 2.975616931915283 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:05,  1.19it/s] 14%|█▍        | 11/79 [00:00<00:04, 15.42it/s] 27%|██▋       | 21/79 [00:01<00:01, 29.43it/s] 39%|███▉      | 31/79 [00:01<00:01, 42.52it/s] 52%|█████▏    | 41/79 [00:01<00:00, 54.24it/s] 65%|██████▍   | 51/79 [00:01<00:00, 64.10it/s] 77%|███████▋  | 61/79 [00:01<00:00, 72.16it/s] 90%|████████▉ | 71/79 [00:01<00:00, 78.59it/s]100%|██████████| 79/79 [00:01<00:00, 47.22it/s]
10000 images processed, 1.702892780303955 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:51,  1.51it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.36it/s] 24%|██▍       | 19/79 [00:00<00:01, 31.81it/s] 35%|███▌      | 28/79 [00:00<00:01, 44.44it/s] 48%|████▊     | 38/79 [00:01<00:00, 56.94it/s] 61%|██████    | 48/79 [00:01<00:00, 66.69it/s] 73%|███████▎  | 58/79 [00:01<00:00, 74.55it/s] 86%|████████▌ | 68/79 [00:01<00:00, 80.45it/s] 99%|█████████▊| 78/79 [00:01<00:00, 84.89it/s]100%|██████████| 79/79 [00:01<00:00, 52.48it/s]
10000 images processed, 1.5314855575561523 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:42,  1.62it/s]  9%|▊         | 6/70 [00:00<00:06, 10.59it/s] 17%|█▋        | 12/70 [00:00<00:02, 20.89it/s] 29%|██▊       | 20/70 [00:00<00:01, 33.94it/s] 39%|███▊      | 27/70 [00:01<00:01, 42.09it/s] 47%|████▋     | 33/70 [00:01<00:00, 43.64it/s] 56%|█████▌    | 39/70 [00:01<00:00, 42.05it/s] 69%|██████▊   | 48/70 [00:01<00:00, 52.41it/s] 80%|████████  | 56/70 [00:01<00:00, 57.86it/s] 90%|█████████ | 63/70 [00:01<00:00, 58.69it/s]100%|██████████| 70/70 [00:01<00:00, 40.19it/s]
8925 images processed, 1.7881219387054443 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:42,  1.04it/s]  4%|▍         | 2/45 [00:01<00:22,  1.95it/s] 20%|██        | 9/45 [00:01<00:03, 11.51it/s] 38%|███▊      | 17/45 [00:01<00:01, 17.60it/s] 47%|████▋     | 21/45 [00:01<00:01, 19.32it/s] 53%|█████▎    | 24/45 [00:01<00:01, 19.44it/s] 71%|███████   | 32/45 [00:01<00:00, 30.20it/s] 82%|████████▏ | 37/45 [00:02<00:00, 20.25it/s]100%|██████████| 45/45 [00:02<00:00, 18.04it/s]
5640 images processed, 2.5231971740722656 seconds used

19.55119490623474
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.89  98.75  81.54
places365     61.06  83.04  47.69
LSUN          45.97  89.12  67.91
iSUN          40.83  90.51  71.01
dtd           35.60  90.86  75.62
AVG           37.47  90.46  68.75
[incremental] Overall: 0.6980 New: 0.7040 Old: 0.6950
[incremental] Final(Top-1): 0.5097  Average: 0.6630
4.984665155410767
==== Stage 4: inc={64,22,42,9,90}; seen={0,8,11,40,51,66,67,88,94,57,59,58,44,93,10}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3', lora_stack=False, lora_orth_enable=True, lora_orth_lambda=1.0, lora_orth_ref_paths='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3', forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='64,22,42,9,90', forget_classes_seen='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/shaokun/PALM/trainer.py:241: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
  2%|▏         | 1/50 [00:08<06:37,  8.10s/it]  4%|▍         | 2/50 [00:08<02:55,  3.66s/it]  6%|▌         | 3/50 [00:09<01:45,  2.25s/it]  8%|▊         | 4/50 [00:09<01:13,  1.59s/it] 10%|█         | 5/50 [00:10<00:53,  1.20s/it] 12%|█▏        | 6/50 [00:10<00:43,  1.02it/s] 14%|█▍        | 7/50 [00:11<00:37,  1.16it/s] 16%|█▌        | 8/50 [00:12<00:31,  1.32it/s] 18%|█▊        | 9/50 [00:12<00:28,  1.46it/s] 20%|██        | 10/50 [00:13<00:26,  1.53it/s] 22%|██▏       | 11/50 [00:13<00:24,  1.61it/s] 24%|██▍       | 12/50 [00:14<00:24,  1.58it/s] 26%|██▌       | 13/50 [00:14<00:23,  1.58it/s] 28%|██▊       | 14/50 [00:15<00:23,  1.55it/s] 30%|███       | 15/50 [00:16<00:22,  1.55it/s] 32%|███▏      | 16/50 [00:16<00:21,  1.61it/s] 34%|███▍      | 17/50 [00:17<00:20,  1.61it/s] 36%|███▌      | 18/50 [00:18<00:19,  1.61it/s] 38%|███▊      | 19/50 [00:18<00:18,  1.68it/s] 40%|████      | 20/50 [00:19<00:17,  1.68it/s] 42%|████▏     | 21/50 [00:19<00:17,  1.68it/s] 44%|████▍     | 22/50 [00:20<00:16,  1.72it/s] 46%|████▌     | 23/50 [00:20<00:16,  1.68it/s] 48%|████▊     | 24/50 [00:21<00:15,  1.72it/s] 50%|█████     | 25/50 [00:22<00:14,  1.71it/s] 52%|█████▏    | 26/50 [00:22<00:13,  1.74it/s] 54%|█████▍    | 27/50 [00:23<00:13,  1.74it/s] 56%|█████▌    | 28/50 [00:23<00:12,  1.77it/s] 58%|█████▊    | 29/50 [00:24<00:11,  1.79it/s] 60%|██████    | 30/50 [00:25<00:11,  1.69it/s] 62%|██████▏   | 31/50 [00:25<00:11,  1.72it/s] 64%|██████▍   | 32/50 [00:26<00:10,  1.68it/s] 66%|██████▌   | 33/50 [00:26<00:09,  1.71it/s] 68%|██████▊   | 34/50 [00:27<00:09,  1.65it/s] 70%|███████   | 35/50 [00:28<00:09,  1.57it/s] 72%|███████▏  | 36/50 [00:28<00:08,  1.63it/s] 74%|███████▍  | 37/50 [00:29<00:07,  1.66it/s] 76%|███████▌  | 38/50 [00:29<00:07,  1.71it/s] 78%|███████▊  | 39/50 [00:30<00:06,  1.73it/s] 80%|████████  | 40/50 [00:30<00:05,  1.70it/s] 82%|████████▏ | 41/50 [00:31<00:05,  1.68it/s] 84%|████████▍ | 42/50 [00:32<00:04,  1.72it/s] 86%|████████▌ | 43/50 [00:32<00:04,  1.64it/s] 88%|████████▊ | 44/50 [00:33<00:03,  1.67it/s] 90%|█████████ | 45/50 [00:34<00:03,  1.62it/s] 92%|█████████▏| 46/50 [00:34<00:02,  1.44it/s] 94%|█████████▍| 47/50 [00:35<00:01,  1.54it/s] 96%|█████████▌| 48/50 [00:36<00:01,  1.52it/s] 98%|█████████▊| 49/50 [00:36<00:00,  1.50it/s]100%|██████████| 50/50 [00:37<00:00,  1.51it/s]100%|██████████| 50/50 [00:37<00:00,  1.33it/s]
[loss] ep 0 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 2 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 5 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 7 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 10 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 12 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 15 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 17 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 20 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 22 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 25 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 27 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 30 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 32 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 35 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 37 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 40 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 42 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 45 it 0 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[loss] ep 47 it 10 total=1.0000 mle=0.0000 pcon=0.0000 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stage4-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<04:33,  1.57it/s]  2%|▏         | 9/430 [00:00<00:26, 15.89it/s]  4%|▍         | 17/430 [00:00<00:14, 28.78it/s]  6%|▌         | 25/430 [00:00<00:10, 39.44it/s]  7%|▋         | 32/430 [00:01<00:08, 46.46it/s]  9%|▉         | 40/430 [00:01<00:07, 53.94it/s] 11%|█         | 47/430 [00:01<00:06, 57.93it/s] 13%|█▎        | 54/430 [00:01<00:06, 60.73it/s] 14%|█▍        | 61/430 [00:01<00:06, 57.28it/s] 16%|█▌        | 69/430 [00:01<00:06, 59.81it/s] 18%|█▊        | 77/430 [00:01<00:05, 63.96it/s] 20%|█▉        | 85/430 [00:01<00:05, 66.59it/s] 22%|██▏       | 93/430 [00:01<00:04, 68.87it/s] 23%|██▎       | 101/430 [00:02<00:04, 69.10it/s] 26%|██▌       | 110/430 [00:02<00:04, 71.91it/s] 27%|██▋       | 118/430 [00:02<00:04, 73.15it/s] 29%|██▉       | 126/430 [00:02<00:04, 74.46it/s] 31%|███       | 134/430 [00:02<00:03, 75.21it/s] 33%|███▎      | 142/430 [00:02<00:03, 75.63it/s] 35%|███▍      | 150/430 [00:02<00:03, 74.04it/s] 37%|███▋      | 159/430 [00:02<00:03, 76.60it/s] 39%|███▉      | 167/430 [00:02<00:03, 67.11it/s] 41%|████      | 175/430 [00:03<00:03, 69.50it/s] 43%|████▎     | 184/430 [00:03<00:03, 73.41it/s] 45%|████▍     | 192/430 [00:03<00:03, 71.76it/s] 47%|████▋     | 200/430 [00:03<00:03, 71.95it/s] 48%|████▊     | 208/430 [00:03<00:03, 70.33it/s] 50%|█████     | 216/430 [00:03<00:02, 71.54it/s] 52%|█████▏    | 225/430 [00:03<00:02, 74.61it/s] 54%|█████▍    | 233/430 [00:03<00:02, 73.70it/s] 56%|█████▌    | 241/430 [00:03<00:02, 69.88it/s] 58%|█████▊    | 249/430 [00:04<00:02, 61.76it/s] 60%|█████▉    | 256/430 [00:04<00:02, 61.30it/s] 61%|██████    | 263/430 [00:04<00:02, 60.38it/s] 63%|██████▎   | 270/430 [00:04<00:02, 58.18it/s] 65%|██████▍   | 278/430 [00:04<00:02, 63.17it/s] 67%|██████▋   | 286/430 [00:04<00:02, 67.06it/s] 68%|██████▊   | 293/430 [00:04<00:02, 67.61it/s] 70%|███████   | 301/430 [00:04<00:01, 67.86it/s] 72%|███████▏  | 309/430 [00:05<00:01, 68.62it/s] 74%|███████▍  | 318/430 [00:05<00:01, 72.16it/s] 76%|███████▌  | 326/430 [00:05<00:01, 71.15it/s] 78%|███████▊  | 334/430 [00:05<00:01, 73.10it/s] 80%|███████▉  | 343/430 [00:05<00:01, 75.80it/s] 82%|████████▏ | 351/430 [00:05<00:01, 74.36it/s] 83%|████████▎ | 359/430 [00:05<00:00, 73.43it/s] 85%|████████▌ | 367/430 [00:05<00:00, 73.88it/s] 87%|████████▋ | 375/430 [00:05<00:00, 74.21it/s] 89%|████████▉ | 383/430 [00:06<00:00, 74.65it/s] 91%|█████████ | 391/430 [00:06<00:00, 72.46it/s] 93%|█████████▎| 399/430 [00:06<00:00, 70.44it/s] 95%|█████████▍| 408/430 [00:06<00:00, 73.74it/s] 97%|█████████▋| 416/430 [00:06<00:00, 75.00it/s] 99%|█████████▊| 424/430 [00:06<00:00, 72.58it/s]100%|██████████| 430/430 [00:06<00:00, 64.09it/s]
55000 images processed, 6.840455770492554 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:54,  1.55it/s] 10%|█         | 9/86 [00:00<00:04, 15.40it/s] 19%|█▊        | 16/86 [00:00<00:02, 25.43it/s] 27%|██▋       | 23/86 [00:00<00:01, 34.35it/s] 36%|███▌      | 31/86 [00:01<00:01, 44.97it/s] 45%|████▌     | 39/86 [00:01<00:00, 52.49it/s] 53%|█████▎    | 46/86 [00:01<00:00, 56.99it/s] 62%|██████▏   | 53/86 [00:01<00:00, 59.70it/s] 70%|██████▉   | 60/86 [00:01<00:00, 61.69it/s] 78%|███████▊  | 67/86 [00:01<00:00, 63.22it/s] 88%|████████▊ | 76/86 [00:01<00:00, 67.18it/s] 98%|█████████▊| 84/86 [00:01<00:00, 67.51it/s]100%|██████████| 86/86 [00:01<00:00, 45.85it/s]
11000 images processed, 1.8976552486419678 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:08,  1.59it/s]  4%|▍         | 9/204 [00:00<00:12, 15.87it/s]  8%|▊         | 16/204 [00:00<00:07, 26.42it/s] 11%|█▏        | 23/204 [00:00<00:05, 35.85it/s] 15%|█▌        | 31/204 [00:01<00:03, 45.46it/s] 19%|█▉        | 39/204 [00:01<00:03, 52.93it/s] 23%|██▎       | 47/204 [00:01<00:02, 59.68it/s] 27%|██▋       | 55/204 [00:01<00:02, 63.15it/s] 31%|███       | 63/204 [00:01<00:02, 66.01it/s] 35%|███▍      | 71/204 [00:01<00:02, 65.88it/s] 39%|███▊      | 79/204 [00:01<00:01, 69.18it/s] 43%|████▎     | 88/204 [00:01<00:01, 72.73it/s] 48%|████▊     | 97/204 [00:01<00:01, 74.82it/s] 51%|█████▏    | 105/204 [00:02<00:01, 75.32it/s] 55%|█████▌    | 113/204 [00:02<00:01, 69.91it/s] 59%|█████▉    | 121/204 [00:02<00:01, 68.86it/s] 63%|██████▎   | 128/204 [00:02<00:01, 68.03it/s] 66%|██████▌   | 135/204 [00:02<00:01, 67.35it/s] 70%|██████▉   | 142/204 [00:02<00:00, 67.09it/s] 74%|███████▎  | 150/204 [00:02<00:00, 69.27it/s] 77%|███████▋  | 157/204 [00:02<00:00, 62.34it/s] 80%|████████  | 164/204 [00:03<00:00, 52.60it/s] 83%|████████▎ | 170/204 [00:03<00:00, 52.24it/s] 86%|████████▋ | 176/204 [00:03<00:00, 52.15it/s] 90%|████████▉ | 183/204 [00:03<00:00, 56.42it/s] 94%|█████████▎| 191/204 [00:03<00:00, 61.45it/s] 97%|█████████▋| 198/204 [00:03<00:00, 62.71it/s]100%|██████████| 204/204 [00:03<00:00, 55.39it/s]
26032 images processed, 3.7509377002716064 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:02,  1.25it/s] 10%|█         | 8/79 [00:00<00:06, 11.51it/s] 19%|█▉        | 15/79 [00:01<00:02, 21.56it/s] 29%|██▉       | 23/79 [00:01<00:01, 32.63it/s] 37%|███▋      | 29/79 [00:01<00:01, 38.56it/s] 46%|████▌     | 36/79 [00:01<00:00, 45.77it/s] 54%|█████▍    | 43/79 [00:01<00:00, 50.61it/s] 63%|██████▎   | 50/79 [00:01<00:00, 52.03it/s] 73%|███████▎  | 58/79 [00:01<00:00, 58.63it/s] 84%|████████▎ | 66/79 [00:01<00:00, 61.66it/s] 95%|█████████▍| 75/79 [00:01<00:00, 67.02it/s]100%|██████████| 79/79 [00:01<00:00, 40.25it/s]
10000 images processed, 1.9974136352539062 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:58,  1.34it/s] 13%|█▎        | 10/79 [00:00<00:04, 15.62it/s] 25%|██▌       | 20/79 [00:00<00:01, 30.96it/s] 38%|███▊      | 30/79 [00:01<00:01, 44.82it/s] 51%|█████     | 40/79 [00:01<00:00, 56.56it/s] 63%|██████▎   | 50/79 [00:01<00:00, 66.24it/s] 76%|███████▌  | 60/79 [00:01<00:00, 74.06it/s] 89%|████████▊ | 70/79 [00:01<00:00, 80.08it/s]100%|██████████| 79/79 [00:01<00:00, 49.69it/s]
10000 images processed, 1.6095795631408691 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:54,  1.26it/s] 11%|█▏        | 8/70 [00:00<00:05, 11.76it/s] 26%|██▌       | 18/70 [00:00<00:01, 27.31it/s] 40%|████      | 28/70 [00:01<00:01, 41.54it/s] 54%|█████▍    | 38/70 [00:01<00:00, 54.00it/s] 69%|██████▊   | 48/70 [00:01<00:00, 64.51it/s] 83%|████████▎ | 58/70 [00:01<00:00, 72.94it/s] 97%|█████████▋| 68/70 [00:01<00:00, 79.47it/s]100%|██████████| 70/70 [00:01<00:00, 45.35it/s]
8925 images processed, 1.5759193897247314 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:55,  1.27s/it]  4%|▍         | 2/45 [00:01<00:28,  1.53it/s] 27%|██▋       | 12/45 [00:01<00:02, 12.63it/s] 38%|███▊      | 17/45 [00:01<00:01, 16.02it/s] 47%|████▋     | 21/45 [00:01<00:01, 16.83it/s] 67%|██████▋   | 30/45 [00:02<00:00, 21.81it/s] 76%|███████▌  | 34/45 [00:02<00:00, 14.88it/s] 98%|█████████▊| 44/45 [00:02<00:00, 24.38it/s]100%|██████████| 45/45 [00:02<00:00, 15.30it/s]
5640 images processed, 2.9700605869293213 seconds used

22.388755083084106
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.18  98.70  84.51
places365     73.29  79.67  48.99
LSUN          49.17  88.47  71.15
iSUN          50.80  88.26  71.62
dtd           41.06  90.05  77.99
AVG           43.70  89.03  70.85
[incremental] Overall: 0.6220 New: 0.6220 Old: 0.6220
[incremental] Final(Top-1): 0.5097  Average: 0.6630
5.514648675918579
==== Stage 5: inc={100,101,102,103,104}; seen={0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4', lora_stack=False, lora_orth_enable=True, lora_orth_lambda=1.0, lora_orth_ref_paths='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4', forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='100,101,102,103,104', forget_classes_seen='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/shaokun/PALM/trainer.py:269: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
  2%|▏         | 1/50 [00:07<06:30,  7.96s/it]  4%|▍         | 2/50 [00:08<03:01,  3.79s/it]  6%|▌         | 3/50 [00:09<01:56,  2.47s/it]  8%|▊         | 4/50 [00:10<01:29,  1.94s/it] 10%|█         | 5/50 [00:11<01:12,  1.61s/it] 12%|█▏        | 6/50 [00:12<00:58,  1.32s/it] 14%|█▍        | 7/50 [00:13<00:49,  1.14s/it] 16%|█▌        | 8/50 [00:14<00:43,  1.03s/it] 18%|█▊        | 9/50 [00:14<00:39,  1.05it/s] 20%|██        | 10/50 [00:15<00:37,  1.08it/s] 22%|██▏       | 11/50 [00:16<00:34,  1.15it/s] 24%|██▍       | 12/50 [00:17<00:33,  1.14it/s] 26%|██▌       | 13/50 [00:18<00:31,  1.19it/s] 28%|██▊       | 14/50 [00:19<00:29,  1.23it/s] 30%|███       | 15/50 [00:19<00:29,  1.18it/s] 32%|███▏      | 16/50 [00:20<00:28,  1.20it/s] 34%|███▍      | 17/50 [00:21<00:27,  1.21it/s] 36%|███▌      | 18/50 [00:22<00:25,  1.27it/s] 38%|███▊      | 19/50 [00:22<00:23,  1.30it/s] 40%|████      | 20/50 [00:23<00:22,  1.31it/s] 42%|████▏     | 21/50 [00:24<00:21,  1.36it/s] 44%|████▍     | 22/50 [00:25<00:21,  1.32it/s] 46%|████▌     | 23/50 [00:25<00:20,  1.31it/s] 48%|████▊     | 24/50 [00:26<00:19,  1.33it/s] 50%|█████     | 25/50 [00:27<00:18,  1.32it/s] 52%|█████▏    | 26/50 [00:28<00:18,  1.31it/s] 54%|█████▍    | 27/50 [00:29<00:18,  1.27it/s] 56%|█████▌    | 28/50 [00:29<00:17,  1.24it/s] 58%|█████▊    | 29/50 [00:30<00:16,  1.24it/s] 60%|██████    | 30/50 [00:31<00:15,  1.25it/s][loss] ep 0 it 0 total=22.6948 mle=11.3112 pcon=10.3835 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 2 it 10 total=22.6301 mle=11.2391 pcon=10.3911 forget=0.0000 orth=1.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 5 it 0 total=18.2859 mle=6.9034 pcon=10.3942 forget=0.0000 orth=0.9882 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 7 it 10 total=16.2624 mle=4.9156 pcon=10.3780 forget=0.0000 orth=0.9688 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 10 it 0 total=15.8989 mle=4.5803 pcon=10.3571 forget=0.0000 orth=0.9615 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 12 it 10 total=15.5677 mle=4.2753 pcon=10.3354 forget=0.0000 orth=0.9571 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 15 it 0 total=15.4053 mle=4.1379 pcon=10.3137 forget=0.0000 orth=0.9537 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 17 it 10 total=15.1968 mle=3.9538 pcon=10.2920 forget=0.0000 orth=0.9510 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 20 it 0 total=15.0095 mle=3.7904 pcon=10.2702 forget=0.0000 orth=0.9489 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 22 it 10 total=14.9006 mle=3.7052 pcon=10.2482 forget=0.0000 orth=0.9472 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 25 it 0 total=14.8094 mle=3.6373 pcon=10.2262 forget=0.0000 orth=0.9458 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 27 it 10 total=14.7069 mle=3.5581 pcon=10.2040 forget=0.0000 orth=0.9447 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
 62%|██████▏   | 31/50 [00:32<00:14,  1.28it/s] 64%|██████▍   | 32/50 [00:32<00:13,  1.32it/s] 66%|██████▌   | 33/50 [00:33<00:13,  1.30it/s] 68%|██████▊   | 34/50 [00:34<00:12,  1.30it/s] 70%|███████   | 35/50 [00:35<00:11,  1.27it/s] 72%|███████▏  | 36/50 [00:36<00:10,  1.32it/s] 74%|███████▍  | 37/50 [00:36<00:09,  1.40it/s] 76%|███████▌  | 38/50 [00:37<00:08,  1.43it/s] 78%|███████▊  | 39/50 [00:38<00:07,  1.40it/s] 80%|████████  | 40/50 [00:38<00:06,  1.46it/s] 82%|████████▏ | 41/50 [00:39<00:05,  1.51it/s] 84%|████████▍ | 42/50 [00:39<00:05,  1.52it/s] 86%|████████▌ | 43/50 [00:40<00:04,  1.49it/s] 88%|████████▊ | 44/50 [00:41<00:03,  1.52it/s] 90%|█████████ | 45/50 [00:41<00:03,  1.55it/s] 92%|█████████▏| 46/50 [00:42<00:02,  1.52it/s] 94%|█████████▍| 47/50 [00:43<00:02,  1.38it/s] 96%|█████████▌| 48/50 [00:44<00:01,  1.45it/s] 98%|█████████▊| 49/50 [00:44<00:00,  1.47it/s]100%|██████████| 50/50 [00:45<00:00,  1.51it/s]100%|██████████| 50/50 [00:45<00:00,  1.10it/s]
[loss] ep 30 it 0 total=14.6371 mle=3.5112 pcon=10.1821 forget=0.0000 orth=0.9438 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 32 it 10 total=14.5694 mle=3.4660 pcon=10.1602 forget=0.0000 orth=0.9432 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 35 it 0 total=14.5287 mle=3.4473 pcon=10.1388 forget=0.0000 orth=0.9426 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 37 it 10 total=14.5097 mle=3.4499 pcon=10.1176 forget=0.0000 orth=0.9422 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 40 it 0 total=14.4610 mle=3.4220 pcon=10.0970 forget=0.0000 orth=0.9420 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 42 it 10 total=14.4584 mle=3.4396 pcon=10.0770 forget=0.0000 orth=0.9418 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 45 it 0 total=14.4050 mle=3.4056 pcon=10.0576 forget=0.0000 orth=0.9418 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[loss] ep 47 it 10 total=14.4037 mle=3.4230 pcon=10.0390 forget=0.0000 orth=0.9417 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stage5-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<03:17,  2.18it/s]  2%|▏         | 10/430 [00:00<00:18, 22.88it/s]  5%|▍         | 20/430 [00:00<00:09, 41.64it/s]  7%|▋         | 30/430 [00:00<00:07, 56.26it/s]  9%|▉         | 40/430 [00:00<00:05, 66.69it/s] 12%|█▏        | 50/430 [00:00<00:05, 74.03it/s] 14%|█▍        | 60/430 [00:01<00:04, 79.90it/s] 16%|█▋        | 70/430 [00:01<00:04, 83.51it/s] 19%|█▊        | 80/430 [00:01<00:04, 86.81it/s] 21%|██        | 90/430 [00:01<00:03, 87.94it/s] 23%|██▎       | 100/430 [00:01<00:03, 89.99it/s] 26%|██▌       | 110/430 [00:01<00:03, 91.70it/s] 28%|██▊       | 120/430 [00:01<00:03, 92.31it/s] 30%|███       | 130/430 [00:01<00:03, 92.90it/s] 33%|███▎      | 140/430 [00:01<00:03, 93.52it/s] 35%|███▍      | 150/430 [00:02<00:02, 93.37it/s] 37%|███▋      | 160/430 [00:02<00:02, 93.93it/s] 40%|███▉      | 170/430 [00:02<00:02, 94.20it/s] 42%|████▏     | 180/430 [00:02<00:02, 94.54it/s] 44%|████▍     | 190/430 [00:02<00:02, 94.32it/s] 47%|████▋     | 200/430 [00:02<00:02, 94.11it/s] 49%|████▉     | 210/430 [00:02<00:02, 94.31it/s] 51%|█████     | 220/430 [00:02<00:02, 94.49it/s] 53%|█████▎    | 230/430 [00:02<00:02, 93.91it/s] 56%|█████▌    | 240/430 [00:03<00:02, 92.58it/s] 58%|█████▊    | 250/430 [00:03<00:01, 91.95it/s] 60%|██████    | 260/430 [00:03<00:01, 92.68it/s] 63%|██████▎   | 270/430 [00:03<00:01, 93.50it/s] 65%|██████▌   | 280/430 [00:03<00:01, 93.98it/s] 67%|██████▋   | 290/430 [00:03<00:01, 93.70it/s] 70%|██████▉   | 300/430 [00:03<00:01, 91.64it/s] 72%|███████▏  | 310/430 [00:03<00:01, 92.56it/s] 74%|███████▍  | 320/430 [00:03<00:01, 92.86it/s] 77%|███████▋  | 330/430 [00:03<00:01, 93.58it/s] 79%|███████▉  | 340/430 [00:04<00:01, 88.66it/s] 81%|████████▏ | 350/430 [00:04<00:00, 89.34it/s] 84%|████████▎ | 360/430 [00:04<00:00, 89.80it/s] 86%|████████▌ | 370/430 [00:04<00:00, 90.43it/s] 88%|████████▊ | 380/430 [00:04<00:00, 91.19it/s] 91%|█████████ | 390/430 [00:04<00:00, 92.46it/s] 93%|█████████▎| 400/430 [00:04<00:00, 92.91it/s] 95%|█████████▌| 410/430 [00:04<00:00, 94.19it/s] 98%|█████████▊| 420/430 [00:04<00:00, 95.17it/s]100%|██████████| 430/430 [00:05<00:00, 93.63it/s]100%|██████████| 430/430 [00:05<00:00, 84.81it/s]
55000 images processed, 5.1347222328186035 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:39,  2.13it/s] 13%|█▎        | 11/86 [00:00<00:03, 24.49it/s] 24%|██▍       | 21/86 [00:00<00:01, 42.51it/s] 36%|███▌      | 31/86 [00:00<00:00, 56.52it/s] 48%|████▊     | 41/86 [00:00<00:00, 66.96it/s] 59%|█████▉    | 51/86 [00:01<00:00, 74.91it/s] 71%|███████   | 61/86 [00:01<00:00, 80.68it/s] 83%|████████▎ | 71/86 [00:01<00:00, 85.37it/s] 94%|█████████▍| 81/86 [00:01<00:00, 88.73it/s]100%|██████████| 86/86 [00:01<00:00, 62.85it/s]
11000 images processed, 1.3868310451507568 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:17,  1.48it/s]  4%|▍         | 9/204 [00:00<00:12, 15.12it/s]  8%|▊         | 16/204 [00:00<00:07, 25.87it/s] 13%|█▎        | 26/204 [00:00<00:04, 41.42it/s] 18%|█▊        | 36/204 [00:01<00:03, 54.31it/s] 23%|██▎       | 46/204 [00:01<00:02, 64.64it/s] 27%|██▋       | 56/204 [00:01<00:02, 72.17it/s] 32%|███▏      | 66/204 [00:01<00:01, 77.44it/s] 37%|███▋      | 76/204 [00:01<00:01, 82.09it/s] 42%|████▏     | 85/204 [00:01<00:01, 83.79it/s] 46%|████▌     | 94/204 [00:01<00:01, 85.08it/s] 51%|█████     | 104/204 [00:01<00:01, 87.70it/s] 56%|█████▌    | 114/204 [00:01<00:01, 89.26it/s] 61%|██████    | 124/204 [00:02<00:00, 88.15it/s] 65%|██████▌   | 133/204 [00:02<00:00, 88.45it/s] 70%|███████   | 143/204 [00:02<00:00, 90.21it/s] 75%|███████▌  | 153/204 [00:02<00:00, 90.99it/s] 80%|███████▉  | 163/204 [00:02<00:00, 91.45it/s] 85%|████████▍ | 173/204 [00:02<00:00, 91.79it/s] 90%|████████▉ | 183/204 [00:02<00:00, 93.12it/s] 95%|█████████▍| 193/204 [00:02<00:00, 93.88it/s]100%|█████████▉| 203/204 [00:02<00:00, 94.27it/s]100%|██████████| 204/204 [00:02<00:00, 69.73it/s]
26032 images processed, 2.9742298126220703 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:05,  1.20it/s]  5%|▌         | 4/79 [00:00<00:13,  5.38it/s] 14%|█▍        | 11/79 [00:01<00:04, 16.35it/s] 23%|██▎       | 18/79 [00:01<00:02, 26.23it/s] 33%|███▎      | 26/79 [00:01<00:01, 36.45it/s] 43%|████▎     | 34/79 [00:01<00:00, 45.18it/s] 52%|█████▏    | 41/79 [00:01<00:00, 50.28it/s] 61%|██████    | 48/79 [00:01<00:00, 46.20it/s] 68%|██████▊   | 54/79 [00:01<00:00, 49.33it/s] 76%|███████▌  | 60/79 [00:01<00:00, 47.77it/s] 85%|████████▍ | 67/79 [00:02<00:00, 52.82it/s] 94%|█████████▎| 74/79 [00:02<00:00, 56.60it/s]100%|██████████| 79/79 [00:02<00:00, 36.06it/s]
10000 images processed, 2.226234197616577 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:50,  1.55it/s]  9%|▉         | 7/79 [00:00<00:06, 11.98it/s] 18%|█▊        | 14/79 [00:00<00:02, 23.60it/s] 28%|██▊       | 22/79 [00:00<00:01, 35.34it/s] 37%|███▋      | 29/79 [00:01<00:01, 43.38it/s] 47%|████▋     | 37/79 [00:01<00:00, 52.01it/s] 56%|█████▌    | 44/79 [00:01<00:00, 55.27it/s] 65%|██████▍   | 51/79 [00:01<00:00, 58.40it/s] 76%|███████▌  | 60/79 [00:01<00:00, 65.43it/s] 86%|████████▌ | 68/79 [00:01<00:00, 67.89it/s] 96%|█████████▌| 76/79 [00:01<00:00, 69.63it/s]100%|██████████| 79/79 [00:01<00:00, 45.33it/s]
10000 images processed, 1.7662854194641113 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:40,  1.71it/s] 13%|█▎        | 9/70 [00:00<00:03, 16.70it/s] 23%|██▎       | 16/70 [00:00<00:01, 27.59it/s] 33%|███▎      | 23/70 [00:00<00:01, 36.50it/s] 44%|████▍     | 31/70 [00:01<00:00, 46.45it/s] 56%|█████▌    | 39/70 [00:01<00:00, 54.54it/s] 69%|██████▊   | 48/70 [00:01<00:00, 63.59it/s] 80%|████████  | 56/70 [00:01<00:00, 66.80it/s] 93%|█████████▎| 65/70 [00:01<00:00, 70.97it/s]100%|██████████| 70/70 [00:01<00:00, 46.30it/s]
8925 images processed, 1.5458450317382812 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:55,  1.26s/it]  7%|▋         | 3/45 [00:01<00:15,  2.71it/s] 22%|██▏       | 10/45 [00:01<00:03, 11.01it/s] 36%|███▌      | 16/45 [00:01<00:01, 18.10it/s] 47%|████▋     | 21/45 [00:02<00:01, 14.03it/s] 64%|██████▍   | 29/45 [00:02<00:00, 22.27it/s] 76%|███████▌  | 34/45 [00:02<00:00, 16.15it/s] 91%|█████████ | 41/45 [00:02<00:00, 22.46it/s]100%|██████████| 45/45 [00:02<00:00, 15.46it/s]
5640 images processed, 2.93131947517395 seconds used

19.93694806098938
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.30  98.71  86.93
places365     77.54  78.97  53.02
LSUN          49.58  88.79  74.94
iSUN          53.47  88.14  74.87
dtd           41.40  90.29  81.57
AVG           45.26  88.98  74.27
[incremental] Overall: 0.5564 New: 0.4640 Old: 0.5795
[incremental] Final(Top-1): 0.5097  Average: 0.6628
4.915966033935547
==== Stage 6: inc={105,106,107,108,109}; seen={0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90,100,101,102,103,104}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5', lora_stack=False, lora_orth_enable=True, lora_orth_lambda=1.0, lora_orth_ref_paths='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5', forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='105,106,107,108,109', forget_classes_seen='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90,100,101,102,103,104', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage5
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/shaokun/PALM/trainer.py:244: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
  2%|▏         | 1/50 [00:07<06:15,  7.66s/it]  4%|▍         | 2/50 [00:08<02:50,  3.55s/it]  6%|▌         | 3/50 [00:09<01:45,  2.24s/it]  8%|▊         | 4/50 [00:09<01:13,  1.60s/it] 10%|█         | 5/50 [00:10<00:56,  1.26s/it] 12%|█▏        | 6/50 [00:11<00:47,  1.08s/it] 14%|█▍        | 7/50 [00:11<00:41,  1.02it/s] 16%|█▌        | 8/50 [00:12<00:36,  1.16it/s] 18%|█▊        | 9/50 [00:13<00:34,  1.19it/s] 20%|██        | 10/50 [00:13<00:31,  1.27it/s] 22%|██▏       | 11/50 [00:14<00:29,  1.32it/s] 24%|██▍       | 12/50 [00:15<00:27,  1.39it/s] 26%|██▌       | 13/50 [00:15<00:25,  1.44it/s] 28%|██▊       | 14/50 [00:16<00:24,  1.46it/s] 30%|███       | 15/50 [00:17<00:23,  1.49it/s] 32%|███▏      | 16/50 [00:17<00:23,  1.45it/s] 34%|███▍      | 17/50 [00:18<00:24,  1.37it/s] 36%|███▌      | 18/50 [00:19<00:22,  1.40it/s] 38%|███▊      | 19/50 [00:19<00:21,  1.45it/s] 40%|████      | 20/50 [00:20<00:20,  1.43it/s] 42%|████▏     | 21/50 [00:21<00:20,  1.42it/s] 44%|████▍     | 22/50 [00:22<00:19,  1.45it/s] 46%|████▌     | 23/50 [00:22<00:18,  1.50it/s] 48%|████▊     | 24/50 [00:23<00:17,  1.52it/s] 50%|█████     | 25/50 [00:24<00:16,  1.50it/s] 52%|█████▏    | 26/50 [00:24<00:15,  1.54it/s] 54%|█████▍    | 27/50 [00:25<00:15,  1.47it/s] 56%|█████▌    | 28/50 [00:26<00:14,  1.51it/s] 58%|█████▊    | 29/50 [00:26<00:13,  1.53it/s] 60%|██████    | 30/50 [00:27<00:13,  1.46it/s][loss] ep 0 it 0 total=17.2378 mle=5.4439 pcon=10.6038 forget=0.0000 orth=1.1901 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 2 it 10 total=16.8212 mle=5.0413 pcon=10.5909 forget=0.0000 orth=1.1890 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 5 it 0 total=16.4851 mle=4.7239 pcon=10.5755 forget=0.0000 orth=1.1857 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 7 it 10 total=16.1655 mle=4.4274 pcon=10.5579 forget=0.0000 orth=1.1802 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 10 it 0 total=15.8665 mle=4.1548 pcon=10.5388 forget=0.0000 orth=1.1729 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 12 it 10 total=15.6658 mle=3.9825 pcon=10.5185 forget=0.0000 orth=1.1648 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 15 it 0 total=15.4858 mle=3.8311 pcon=10.4975 forget=0.0000 orth=1.1572 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 17 it 10 total=15.3501 mle=3.7243 pcon=10.4758 forget=0.0000 orth=1.1500 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 20 it 0 total=15.2498 mle=3.6527 pcon=10.4537 forget=0.0000 orth=1.1434 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 22 it 10 total=15.1507 mle=3.5817 pcon=10.4316 forget=0.0000 orth=1.1374 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 25 it 0 total=15.0624 mle=3.5213 pcon=10.4091 forget=0.0000 orth=1.1321 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 27 it 10 total=15.0266 mle=3.5126 pcon=10.3868 forget=0.0000 orth=1.1273 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
 62%|██████▏   | 31/50 [00:28<00:12,  1.49it/s] 64%|██████▍   | 32/50 [00:28<00:12,  1.47it/s] 66%|██████▌   | 33/50 [00:29<00:11,  1.44it/s] 68%|██████▊   | 34/50 [00:30<00:11,  1.40it/s] 70%|███████   | 35/50 [00:31<00:12,  1.25it/s] 72%|███████▏  | 36/50 [00:31<00:10,  1.27it/s] 74%|███████▍  | 37/50 [00:32<00:10,  1.30it/s] 76%|███████▌  | 38/50 [00:33<00:09,  1.28it/s] 78%|███████▊  | 39/50 [00:34<00:08,  1.32it/s] 80%|████████  | 40/50 [00:34<00:07,  1.32it/s] 82%|████████▏ | 41/50 [00:35<00:06,  1.35it/s] 84%|████████▍ | 42/50 [00:36<00:05,  1.35it/s] 86%|████████▌ | 43/50 [00:37<00:05,  1.34it/s] 88%|████████▊ | 44/50 [00:37<00:04,  1.32it/s] 90%|█████████ | 45/50 [00:38<00:03,  1.31it/s] 92%|█████████▏| 46/50 [00:39<00:03,  1.33it/s] 94%|█████████▍| 47/50 [00:40<00:02,  1.33it/s] 96%|█████████▌| 48/50 [00:41<00:01,  1.27it/s] 98%|█████████▊| 49/50 [00:41<00:00,  1.25it/s]100%|██████████| 50/50 [00:42<00:00,  1.20it/s]100%|██████████| 50/50 [00:42<00:00,  1.17it/s]
[loss] ep 30 it 0 total=14.9659 mle=3.4781 pcon=10.3646 forget=0.0000 orth=1.1232 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 32 it 10 total=14.9057 mle=3.4432 pcon=10.3427 forget=0.0000 orth=1.1198 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 35 it 0 total=14.8718 mle=3.4334 pcon=10.3213 forget=0.0000 orth=1.1171 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 37 it 10 total=14.8237 mle=3.4083 pcon=10.3002 forget=0.0000 orth=1.1151 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 40 it 0 total=14.8094 mle=3.4158 pcon=10.2799 forget=0.0000 orth=1.1137 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 42 it 10 total=14.7690 mle=3.3959 pcon=10.2603 forget=0.0000 orth=1.1128 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 45 it 0 total=14.7431 mle=3.3895 pcon=10.2412 forget=0.0000 orth=1.1123 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[loss] ep 47 it 10 total=14.7234 mle=3.3882 pcon=10.2230 forget=0.0000 orth=1.1121 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage6
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stage6-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<05:04,  1.41it/s]  2%|▏         | 10/430 [00:00<00:25, 16.16it/s]  4%|▍         | 18/430 [00:00<00:14, 27.98it/s]  6%|▌         | 26/430 [00:01<00:10, 38.40it/s]  8%|▊         | 33/430 [00:01<00:09, 43.87it/s] 10%|▉         | 41/430 [00:01<00:07, 52.35it/s] 11%|█         | 48/430 [00:01<00:06, 55.83it/s] 13%|█▎        | 56/430 [00:01<00:06, 61.58it/s] 15%|█▍        | 64/430 [00:01<00:05, 66.01it/s] 17%|█▋        | 73/430 [00:01<00:05, 70.73it/s] 19%|█▉        | 81/430 [00:01<00:04, 72.71it/s] 21%|██        | 89/430 [00:01<00:04, 72.65it/s] 23%|██▎       | 97/430 [00:01<00:04, 73.98it/s] 24%|██▍       | 105/430 [00:02<00:04, 71.24it/s] 26%|██▋       | 113/430 [00:02<00:04, 70.64it/s] 28%|██▊       | 121/430 [00:02<00:04, 67.90it/s] 30%|███       | 129/430 [00:02<00:04, 69.51it/s] 32%|███▏      | 138/430 [00:02<00:04, 72.95it/s] 34%|███▍      | 146/430 [00:02<00:03, 71.41it/s] 36%|███▌      | 154/430 [00:02<00:03, 72.13it/s] 38%|███▊      | 163/430 [00:02<00:03, 75.82it/s] 40%|███▉      | 171/430 [00:03<00:03, 76.79it/s] 42%|████▏     | 180/430 [00:03<00:03, 79.66it/s] 44%|████▍     | 189/430 [00:03<00:02, 80.84it/s] 46%|████▌     | 198/430 [00:03<00:02, 80.73it/s] 48%|████▊     | 207/430 [00:03<00:02, 79.74it/s] 50%|█████     | 215/430 [00:03<00:02, 73.61it/s] 52%|█████▏    | 223/430 [00:03<00:02, 73.69it/s] 54%|█████▎    | 231/430 [00:03<00:02, 73.90it/s] 56%|█████▌    | 239/430 [00:03<00:02, 74.01it/s] 57%|█████▋    | 247/430 [00:04<00:02, 73.83it/s] 60%|█████▉    | 256/430 [00:04<00:02, 76.77it/s] 61%|██████▏   | 264/430 [00:04<00:02, 77.53it/s] 63%|██████▎   | 272/430 [00:04<00:02, 75.16it/s] 65%|██████▌   | 280/430 [00:04<00:02, 74.56it/s] 67%|██████▋   | 288/430 [00:04<00:01, 74.92it/s] 69%|██████▉   | 296/430 [00:04<00:01, 73.96it/s] 71%|███████   | 304/430 [00:04<00:01, 73.99it/s] 73%|███████▎  | 312/430 [00:04<00:01, 74.77it/s] 74%|███████▍  | 320/430 [00:04<00:01, 70.79it/s] 76%|███████▋  | 328/430 [00:05<00:01, 72.63it/s] 78%|███████▊  | 336/430 [00:05<00:01, 70.78it/s] 80%|████████  | 345/430 [00:05<00:01, 73.39it/s] 82%|████████▏ | 353/430 [00:05<00:01, 73.34it/s] 84%|████████▍ | 361/430 [00:05<00:00, 73.68it/s] 86%|████████▌ | 369/430 [00:05<00:00, 71.34it/s] 88%|████████▊ | 377/430 [00:05<00:00, 72.76it/s] 90%|████████▉ | 385/430 [00:05<00:00, 72.88it/s] 91%|█████████▏| 393/430 [00:05<00:00, 72.45it/s] 93%|█████████▎| 401/430 [00:06<00:00, 69.37it/s] 95%|█████████▌| 409/430 [00:06<00:00, 70.64it/s] 97%|█████████▋| 419/430 [00:06<00:00, 78.18it/s]100%|█████████▉| 429/430 [00:06<00:00, 83.73it/s]100%|██████████| 430/430 [00:06<00:00, 66.59it/s]
55000 images processed, 6.513190031051636 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:46,  1.85it/s] 12%|█▏        | 10/86 [00:00<00:03, 20.22it/s] 23%|██▎       | 20/86 [00:00<00:01, 38.03it/s] 34%|███▎      | 29/86 [00:00<00:01, 50.55it/s] 45%|████▌     | 39/86 [00:00<00:00, 61.95it/s] 57%|█████▋    | 49/86 [00:01<00:00, 71.02it/s] 69%|██████▊   | 59/86 [00:01<00:00, 77.89it/s] 80%|████████  | 69/86 [00:01<00:00, 83.06it/s] 92%|█████████▏| 79/86 [00:01<00:00, 86.84it/s]100%|██████████| 86/86 [00:01<00:00, 59.14it/s]
11000 images processed, 1.4757475852966309 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:10,  1.56it/s]  5%|▌         | 11/204 [00:00<00:10, 19.13it/s] 10%|█         | 21/204 [00:00<00:05, 35.12it/s] 14%|█▍        | 29/204 [00:00<00:03, 44.83it/s] 19%|█▉        | 39/204 [00:01<00:02, 57.05it/s] 24%|██▍       | 49/204 [00:01<00:02, 66.85it/s] 29%|██▉       | 59/204 [00:01<00:01, 73.86it/s] 34%|███▍      | 69/204 [00:01<00:01, 78.99it/s] 39%|███▊      | 79/204 [00:01<00:01, 82.42it/s] 43%|████▎     | 88/204 [00:01<00:01, 84.30it/s] 48%|████▊     | 98/204 [00:01<00:01, 87.14it/s] 53%|█████▎    | 108/204 [00:01<00:01, 88.79it/s] 58%|█████▊    | 118/204 [00:01<00:00, 89.99it/s] 63%|██████▎   | 128/204 [00:02<00:00, 88.10it/s] 68%|██████▊   | 138/204 [00:02<00:00, 89.71it/s] 73%|███████▎  | 148/204 [00:02<00:00, 90.76it/s] 77%|███████▋  | 158/204 [00:02<00:00, 91.26it/s] 82%|████████▏ | 168/204 [00:02<00:00, 91.85it/s] 87%|████████▋ | 178/204 [00:02<00:00, 92.65it/s] 92%|█████████▏| 188/204 [00:02<00:00, 93.77it/s] 97%|█████████▋| 198/204 [00:02<00:00, 94.44it/s]100%|██████████| 204/204 [00:02<00:00, 71.49it/s]
26032 images processed, 2.8951256275177 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:01<01:18,  1.01s/it]  4%|▍         | 3/79 [00:01<00:22,  3.35it/s] 14%|█▍        | 11/79 [00:01<00:04, 14.69it/s] 27%|██▋       | 21/79 [00:01<00:01, 29.17it/s] 37%|███▋      | 29/79 [00:01<00:01, 38.63it/s] 49%|████▉     | 39/79 [00:01<00:00, 44.69it/s] 62%|██████▏   | 49/79 [00:01<00:00, 55.14it/s] 75%|███████▍  | 59/79 [00:01<00:00, 64.33it/s] 87%|████████▋ | 69/79 [00:01<00:00, 72.05it/s]100%|██████████| 79/79 [00:02<00:00, 23.57it/s]100%|██████████| 79/79 [00:02<00:00, 26.72it/s]
10000 images processed, 2.9944143295288086 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:00,  1.29it/s] 11%|█▏        | 9/79 [00:00<00:05, 13.36it/s] 24%|██▍       | 19/79 [00:00<00:02, 28.52it/s] 37%|███▋      | 29/79 [00:01<00:01, 42.20it/s] 48%|████▊     | 38/79 [00:01<00:00, 52.71it/s] 61%|██████    | 48/79 [00:01<00:00, 62.78it/s] 73%|███████▎  | 58/79 [00:01<00:00, 71.39it/s] 86%|████████▌ | 68/79 [00:01<00:00, 78.04it/s] 99%|█████████▊| 78/79 [00:01<00:00, 83.07it/s]100%|██████████| 79/79 [00:01<00:00, 48.55it/s]
10000 images processed, 1.650428056716919 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:55,  1.25it/s] 13%|█▎        | 9/70 [00:00<00:04, 13.19it/s] 27%|██▋       | 19/70 [00:01<00:01, 28.33it/s] 41%|████▏     | 29/70 [00:01<00:00, 42.20it/s] 56%|█████▌    | 39/70 [00:01<00:00, 54.15it/s] 70%|███████   | 49/70 [00:01<00:00, 64.36it/s] 84%|████████▍ | 59/70 [00:01<00:00, 72.62it/s] 99%|█████████▊| 69/70 [00:01<00:00, 78.91it/s]100%|██████████| 70/70 [00:01<00:00, 45.22it/s]
8925 images processed, 1.5844860076904297 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<01:06,  1.52s/it] 22%|██▏       | 10/45 [00:01<00:04,  8.33it/s] 38%|███▊      | 17/45 [00:02<00:02,  9.83it/s] 60%|██████    | 27/45 [00:02<00:00, 18.11it/s] 73%|███████▎  | 33/45 [00:02<00:00, 14.31it/s] 96%|█████████▌| 43/45 [00:03<00:00, 22.29it/s]100%|██████████| 45/45 [00:03<00:00, 14.70it/s]
5640 images processed, 3.087580442428589 seconds used

21.9943904876709
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.14  98.78  89.32
places365     78.79  78.33  56.10
LSUN          47.33  89.56  78.62
iSUN          53.32  88.00  77.32
dtd           39.43  90.82  84.58
AVG           44.60  89.10  77.19
[incremental] Overall: 0.5087 New: 0.4880 Old: 0.5128
[incremental] Final(Top-1): 0.5087  Average: 0.6627
3.875737190246582
[done] continual incremental run finished. Adapters at: checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack
