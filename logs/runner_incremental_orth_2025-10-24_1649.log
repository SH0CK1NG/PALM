nohup: ignoring input
==== Stage 1: forget_inc={0,8,11,40,51}; forget_seen={}; all={0,8,11,40,51,66,67,88,94,57} ====
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1', adapter_load_path=None, adapter_load_paths=None, lora_new_adapter_name=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc='0,8,11,40,51', forget_classes_seen=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [01:43<1:24:42, 103.72s/it]  4%|▍         | 2/50 [02:24<53:24, 66.75s/it]     6%|▌         | 3/50 [03:03<42:27, 54.19s/it]  8%|▊         | 4/50 [03:43<37:17, 48.64s/it] 10%|█         | 5/50 [04:23<34:05, 45.46s/it] 12%|█▏        | 6/50 [05:01<31:25, 42.86s/it] 14%|█▍        | 7/50 [05:36<28:51, 40.28s/it][loss] ep 0 it 0 total=8.4101 mle=1.6270 pcon=5.2951 forget=1.4880 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 0 it 50 total=8.7561 mle=1.9940 pcon=5.2910 forget=1.4711 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 100 total=8.8803 mle=2.1357 pcon=5.2868 forget=1.4579 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 150 total=9.2352 mle=2.4878 pcon=5.2829 forget=1.4645 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 0 it 200 total=9.2721 mle=2.5438 pcon=5.2787 forget=1.4496 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 250 total=8.6726 mle=1.9316 pcon=5.2747 forget=1.4663 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 0 it 300 total=8.4975 mle=1.7806 pcon=5.2707 forget=1.4462 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 350 total=9.0522 mle=2.3363 pcon=5.2668 forget=1.4491 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 1 it 10 total=8.3920 mle=1.6598 pcon=5.2628 forget=1.4693 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 1 it 60 total=8.3567 mle=1.6490 pcon=5.2590 forget=1.4488 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 1 it 110 total=8.4048 mle=1.6979 pcon=5.2552 forget=1.4517 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 1 it 160 total=8.5316 mle=1.7838 pcon=5.2515 forget=1.4963 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 1 it 210 total=8.2692 mle=1.5551 pcon=5.2477 forget=1.4664 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 1 it 260 total=8.7198 mle=2.0035 pcon=5.2439 forget=1.4725 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 1 it 310 total=8.9396 mle=2.2293 pcon=5.2404 forget=1.4699 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 1 it 360 total=8.5475 mle=1.8686 pcon=5.2366 forget=1.4422 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 2 it 20 total=8.0426 mle=1.3554 pcon=5.2331 forget=1.4542 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 2 it 70 total=8.4407 mle=1.8001 pcon=5.2298 forget=1.4109 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 2 it 120 total=8.3368 mle=1.6571 pcon=5.2262 forget=1.4534 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 2 it 170 total=8.7019 mle=2.0438 pcon=5.2228 forget=1.4353 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 2 it 220 total=9.0506 mle=2.4041 pcon=5.2195 forget=1.4270 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 2 it 270 total=8.4800 mle=1.8172 pcon=5.2162 forget=1.4466 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 2 it 320 total=8.2501 mle=1.5928 pcon=5.2130 forget=1.4443 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 2 it 370 total=8.6811 mle=2.0346 pcon=5.2098 forget=1.4366 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 3 it 30 total=8.2823 mle=1.6329 pcon=5.2065 forget=1.4429 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 3 it 80 total=8.7730 mle=2.0478 pcon=5.2032 forget=1.5221 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 3 it 130 total=8.5102 mle=1.9325 pcon=5.2003 forget=1.3774 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 3 it 180 total=8.7391 mle=2.1012 pcon=5.1969 forget=1.4410 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 3 it 230 total=8.4282 mle=1.8246 pcon=5.1937 forget=1.4099 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 3 it 280 total=8.4480 mle=1.8523 pcon=5.1909 forget=1.4049 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 3 it 330 total=8.7578 mle=2.0952 pcon=5.1880 forget=1.4747 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 3 it 380 total=8.3020 mle=1.6276 pcon=5.1850 forget=1.4895 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 4 it 40 total=8.5239 mle=1.8599 pcon=5.1821 forget=1.4819 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 4 it 90 total=8.2765 mle=1.7143 pcon=5.1790 forget=1.3831 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 4 it 140 total=9.0105 mle=2.3579 pcon=5.1762 forget=1.4764 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 4 it 190 total=8.4977 mle=1.8558 pcon=5.1731 forget=1.4688 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 4 it 240 total=8.4982 mle=1.8217 pcon=5.1701 forget=1.5064 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 4 it 290 total=8.2818 mle=1.7157 pcon=5.1672 forget=1.3989 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 4 it 340 total=8.6391 mle=2.0394 pcon=5.1643 forget=1.4354 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 5 it 0 total=8.5785 mle=2.0072 pcon=5.1619 forget=1.4095 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 5 it 50 total=8.8873 mle=2.3018 pcon=5.1590 forget=1.4265 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 5 it 100 total=8.8508 mle=2.2195 pcon=5.1564 forget=1.4749 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 5 it 150 total=8.4287 mle=1.8597 pcon=5.1539 forget=1.4152 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 5 it 200 total=9.1520 mle=2.5827 pcon=5.1511 forget=1.4183 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 5 it 250 total=8.3929 mle=1.7595 pcon=5.1486 forget=1.4848 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 5 it 300 total=8.7813 mle=2.2384 pcon=5.1460 forget=1.3969 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 5 it 350 total=8.8130 mle=2.2542 pcon=5.1434 forget=1.4154 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 6 it 10 total=8.1659 mle=1.6211 pcon=5.1409 forget=1.4039 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 6 it 60 total=8.4755 mle=1.9203 pcon=5.1384 forget=1.4168 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 6 it 110 total=8.3708 mle=1.8518 pcon=5.1357 forget=1.3833 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 6 it 160 total=8.5824 mle=2.0545 pcon=5.1330 forget=1.3949 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 6 it 210 total=8.4005 mle=1.8511 pcon=5.1299 forget=1.4195 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 6 it 260 total=8.4750 mle=1.9268 pcon=5.1273 forget=1.4209 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 6 it 310 total=8.5553 mle=1.9831 pcon=5.1246 forget=1.4475 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 6 it 360 total=8.5026 mle=1.9076 pcon=5.1225 forget=1.4726 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 7 it 20 total=8.2133 mle=1.6841 pcon=5.1198 forget=1.4094 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 7 it 70 total=8.1565 mle=1.6641 pcon=5.1172 forget=1.3752 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 7 it 120 total=8.6481 mle=2.1503 pcon=5.1149 forget=1.3829 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 7 it 170 total=8.2573 mle=1.6816 pcon=5.1127 forget=1.4630 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
 16%|█▌        | 8/50 [06:14<27:45, 39.65s/it] 18%|█▊        | 9/50 [06:53<26:49, 39.26s/it] 20%|██        | 10/50 [07:31<25:54, 38.86s/it] 22%|██▏       | 11/50 [08:09<25:14, 38.82s/it] 24%|██▍       | 12/50 [08:51<25:04, 39.60s/it] 26%|██▌       | 13/50 [09:20<22:24, 36.33s/it] 28%|██▊       | 14/50 [09:42<19:19, 32.21s/it] 30%|███       | 15/50 [10:06<17:12, 29.51s/it][loss] ep 7 it 220 total=8.0989 mle=1.5845 pcon=5.1101 forget=1.4043 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 7 it 270 total=9.2862 mle=2.8276 pcon=5.1076 forget=1.3510 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 7 it 320 total=8.4349 mle=1.9331 pcon=5.1049 forget=1.3968 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 7 it 370 total=8.3673 mle=1.8711 pcon=5.1024 forget=1.3938 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 8 it 30 total=8.5725 mle=2.0809 pcon=5.1002 forget=1.3914 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 8 it 80 total=8.1369 mle=1.6263 pcon=5.0975 forget=1.4130 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 8 it 130 total=8.5035 mle=1.9970 pcon=5.0950 forget=1.4114 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 8 it 180 total=8.4669 mle=1.9922 pcon=5.0927 forget=1.3820 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 8 it 230 total=8.1784 mle=1.6775 pcon=5.0902 forget=1.4106 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 8 it 280 total=7.8591 mle=1.4214 pcon=5.0877 forget=1.3500 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 8 it 330 total=8.8793 mle=2.3863 pcon=5.0849 forget=1.4080 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 8 it 380 total=8.2309 mle=1.7399 pcon=5.0823 forget=1.4087 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 9 it 40 total=8.3514 mle=1.9132 pcon=5.0797 forget=1.3584 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 9 it 90 total=8.3206 mle=1.8321 pcon=5.0773 forget=1.4113 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 9 it 140 total=8.1276 mle=1.6915 pcon=5.0751 forget=1.3610 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 9 it 190 total=8.0827 mle=1.6185 pcon=5.0730 forget=1.3913 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 9 it 240 total=8.5547 mle=2.1161 pcon=5.0708 forget=1.3679 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 9 it 290 total=8.1486 mle=1.6792 pcon=5.0685 forget=1.4008 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 9 it 340 total=8.0061 mle=1.5736 pcon=5.0663 forget=1.3662 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 10 it 0 total=8.0609 mle=1.5904 pcon=5.0639 forget=1.4066 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 10 it 50 total=8.1402 mle=1.7012 pcon=5.0618 forget=1.3772 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 10 it 100 total=7.9373 mle=1.4903 pcon=5.0593 forget=1.3876 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 10 it 150 total=8.2568 mle=1.8154 pcon=5.0570 forget=1.3844 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 10 it 200 total=8.4026 mle=1.9576 pcon=5.0552 forget=1.3898 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 10 it 250 total=8.9024 mle=2.4604 pcon=5.0530 forget=1.3891 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 10 it 300 total=8.2753 mle=1.8249 pcon=5.0507 forget=1.3996 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 10 it 350 total=8.2062 mle=1.7587 pcon=5.0488 forget=1.3987 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 11 it 10 total=8.1482 mle=1.7122 pcon=5.0468 forget=1.3893 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 11 it 60 total=8.0433 mle=1.6031 pcon=5.0448 forget=1.3954 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 11 it 110 total=7.8338 mle=1.3935 pcon=5.0428 forget=1.3975 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 11 it 160 total=8.2046 mle=1.7614 pcon=5.0409 forget=1.4023 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 11 it 210 total=8.1243 mle=1.6803 pcon=5.0388 forget=1.4052 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 11 it 260 total=7.9286 mle=1.4742 pcon=5.0366 forget=1.4178 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 11 it 310 total=8.4020 mle=1.9599 pcon=5.0344 forget=1.4078 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 11 it 360 total=8.2259 mle=1.7762 pcon=5.0321 forget=1.4177 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 12 it 20 total=7.9591 mle=1.5068 pcon=5.0300 forget=1.4224 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 70 total=8.2089 mle=1.7420 pcon=5.0276 forget=1.4393 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 12 it 120 total=8.1215 mle=1.6433 pcon=5.0254 forget=1.4528 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 12 it 170 total=8.3885 mle=1.9212 pcon=5.0230 forget=1.4443 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 220 total=8.1087 mle=1.6330 pcon=5.0206 forget=1.4552 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 12 it 270 total=8.0877 mle=1.6124 pcon=5.0181 forget=1.4572 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 320 total=8.2823 mle=1.8136 pcon=5.0158 forget=1.4529 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 12 it 370 total=8.4810 mle=2.0048 pcon=5.0134 forget=1.4628 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 13 it 30 total=8.4340 mle=1.9808 pcon=5.0111 forget=1.4422 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 13 it 80 total=8.2360 mle=1.7456 pcon=5.0088 forget=1.4816 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 13 it 130 total=8.2974 mle=1.8093 pcon=5.0065 forget=1.4815 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 13 it 180 total=8.0988 mle=1.6247 pcon=5.0039 forget=1.4701 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 13 it 230 total=8.0363 mle=1.5614 pcon=5.0015 forget=1.4733 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 13 it 280 total=8.0169 mle=1.5695 pcon=4.9992 forget=1.4482 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 13 it 330 total=8.1587 mle=1.6764 pcon=4.9968 forget=1.4855 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 13 it 380 total=8.1333 mle=1.6763 pcon=4.9946 forget=1.4624 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 14 it 40 total=7.9687 mle=1.5053 pcon=4.9923 forget=1.4711 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 90 total=8.2726 mle=1.7827 pcon=4.9903 forget=1.4997 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 140 total=8.0912 mle=1.6238 pcon=4.9880 forget=1.4794 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 14 it 190 total=7.9724 mle=1.5291 pcon=4.9858 forget=1.4574 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 240 total=8.0072 mle=1.5842 pcon=4.9835 forget=1.4395 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 14 it 290 total=7.9031 mle=1.4805 pcon=4.9812 forget=1.4414 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 14 it 340 total=7.9942 mle=1.5970 pcon=4.9793 forget=1.4179 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 15 it 0 total=7.9165 mle=1.5307 pcon=4.9770 forget=1.4088 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
 32%|███▏      | 16/50 [10:31<15:59, 28.22s/it] 34%|███▍      | 17/50 [10:54<14:37, 26.59s/it] 36%|███▌      | 18/50 [11:16<13:32, 25.39s/it] 38%|███▊      | 19/50 [11:39<12:38, 24.46s/it] 40%|████      | 20/50 [12:01<11:55, 23.84s/it] 42%|████▏     | 21/50 [12:24<11:26, 23.67s/it] 44%|████▍     | 22/50 [12:49<11:11, 23.99s/it][loss] ep 15 it 50 total=7.9812 mle=1.5793 pcon=4.9748 forget=1.4271 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 15 it 100 total=8.0132 mle=1.6050 pcon=4.9729 forget=1.4353 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 150 total=8.2908 mle=1.9121 pcon=4.9707 forget=1.4080 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 200 total=7.9317 mle=1.6031 pcon=4.9687 forget=1.3599 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 15 it 250 total=8.0304 mle=1.7052 pcon=4.9669 forget=1.3583 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 15 it 300 total=7.9788 mle=1.7226 pcon=4.9647 forget=1.2915 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 350 total=7.6935 mle=1.5093 pcon=4.9627 forget=1.2215 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 16 it 10 total=7.7671 mle=1.4590 pcon=4.9606 forget=1.3475 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 16 it 60 total=7.7674 mle=1.5520 pcon=4.9586 forget=1.2569 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 16 it 110 total=7.8887 mle=1.6976 pcon=4.9567 forget=1.2344 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 16 it 160 total=7.7837 mle=1.5692 pcon=4.9547 forget=1.2598 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 16 it 210 total=7.9970 mle=1.8030 pcon=4.9527 forget=1.2412 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 16 it 260 total=7.8325 mle=1.6987 pcon=4.9506 forget=1.1832 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 16 it 310 total=7.8264 mle=1.6613 pcon=4.9486 forget=1.2165 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 16 it 360 total=7.8341 mle=1.7054 pcon=4.9466 forget=1.1821 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 17 it 20 total=8.0538 mle=1.9513 pcon=4.9447 forget=1.1577 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 70 total=7.8162 mle=1.6447 pcon=4.9429 forget=1.2286 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 120 total=7.8673 mle=1.8129 pcon=4.9409 forget=1.1135 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 170 total=7.8541 mle=1.8056 pcon=4.9389 forget=1.1096 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 17 it 220 total=7.8701 mle=1.8358 pcon=4.9367 forget=1.0976 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 17 it 270 total=7.6130 mle=1.6100 pcon=4.9344 forget=1.0686 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 17 it 320 total=7.5927 mle=1.6575 pcon=4.9319 forget=1.0033 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 17 it 370 total=7.5895 mle=1.6692 pcon=4.9292 forget=0.9911 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 18 it 30 total=7.4774 mle=1.5503 pcon=4.9266 forget=1.0005 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 18 it 80 total=7.6371 mle=1.7124 pcon=4.9240 forget=1.0007 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 18 it 130 total=7.4299 mle=1.5214 pcon=4.9214 forget=0.9871 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 18 it 180 total=7.5896 mle=1.7034 pcon=4.9186 forget=0.9676 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 18 it 230 total=7.5446 mle=1.6886 pcon=4.9161 forget=0.9400 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 18 it 280 total=7.5396 mle=1.6614 pcon=4.9133 forget=0.9649 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 18 it 330 total=7.8060 mle=1.9084 pcon=4.9104 forget=0.9872 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 18 it 380 total=7.3197 mle=1.4341 pcon=4.9079 forget=0.9776 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 19 it 40 total=7.5948 mle=1.7154 pcon=4.9054 forget=0.9740 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 19 it 90 total=7.4473 mle=1.5586 pcon=4.9027 forget=0.9860 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 19 it 140 total=7.5261 mle=1.6576 pcon=4.9000 forget=0.9684 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 19 it 190 total=7.4413 mle=1.5470 pcon=4.8973 forget=0.9970 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 19 it 240 total=7.5045 mle=1.6203 pcon=4.8947 forget=0.9896 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 19 it 290 total=7.5090 mle=1.6642 pcon=4.8923 forget=0.9525 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 19 it 340 total=7.3041 mle=1.4289 pcon=4.8897 forget=0.9855 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 20 it 0 total=7.3079 mle=1.4355 pcon=4.8873 forget=0.9851 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 20 it 50 total=7.5038 mle=1.6387 pcon=4.8848 forget=0.9802 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 20 it 100 total=7.4666 mle=1.5839 pcon=4.8823 forget=1.0005 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 20 it 150 total=7.7325 mle=1.8651 pcon=4.8798 forget=0.9877 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 20 it 200 total=7.5593 mle=1.6808 pcon=4.8774 forget=1.0011 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 20 it 250 total=7.6080 mle=1.7475 pcon=4.8750 forget=0.9854 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 20 it 300 total=7.4157 mle=1.5459 pcon=4.8726 forget=0.9973 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 20 it 350 total=7.4683 mle=1.5929 pcon=4.8704 forget=1.0051 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 21 it 10 total=7.3538 mle=1.4989 pcon=4.8680 forget=0.9869 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 21 it 60 total=7.4467 mle=1.5723 pcon=4.8656 forget=1.0088 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 21 it 110 total=7.3399 mle=1.4714 pcon=4.8633 forget=1.0052 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 21 it 160 total=7.4048 mle=1.5418 pcon=4.8610 forget=1.0019 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 21 it 210 total=7.3490 mle=1.4669 pcon=4.8587 forget=1.0234 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 21 it 260 total=7.6600 mle=1.7746 pcon=4.8566 forget=1.0288 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 21 it 310 total=7.4910 mle=1.6160 pcon=4.8544 forget=1.0205 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 21 it 360 total=7.5582 mle=1.6703 pcon=4.8525 forget=1.0353 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 22 it 20 total=7.5052 mle=1.6181 pcon=4.8504 forget=1.0366 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 22 it 70 total=7.7826 mle=1.8996 pcon=4.8482 forget=1.0348 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 22 it 120 total=7.6917 mle=1.7877 pcon=4.8461 forget=1.0580 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 22 it 170 total=7.4458 mle=1.5631 pcon=4.8441 forget=1.0386 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 22 it 220 total=7.5920 mle=1.6998 pcon=4.8421 forget=1.0500 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 22 it 270 total=7.5033 mle=1.5961 pcon=4.8402 forget=1.0670 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
 46%|████▌     | 23/50 [13:12<10:38, 23.64s/it] 48%|████▊     | 24/50 [13:35<10:09, 23.45s/it] 50%|█████     | 25/50 [13:57<09:35, 23.04s/it] 52%|█████▏    | 26/50 [14:23<09:37, 24.05s/it] 54%|█████▍    | 27/50 [14:46<09:03, 23.62s/it] 56%|█████▌    | 28/50 [15:09<08:34, 23.37s/it] 58%|█████▊    | 29/50 [15:30<07:58, 22.77s/it] 60%|██████    | 30/50 [15:46<06:56, 20.83s/it] 62%|██████▏   | 31/50 [16:00<05:52, 18.56s/it][loss] ep 22 it 320 total=7.5140 mle=1.6131 pcon=4.8383 forget=1.0626 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 22 it 370 total=7.6667 mle=1.7554 pcon=4.8365 forget=1.0748 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 23 it 30 total=7.6235 mle=1.7331 pcon=4.8346 forget=1.0559 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 23 it 80 total=7.5367 mle=1.6329 pcon=4.8326 forget=1.0712 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 23 it 130 total=7.5917 mle=1.6892 pcon=4.8305 forget=1.0719 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 23 it 180 total=7.5339 mle=1.6274 pcon=4.8286 forget=1.0779 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 23 it 230 total=7.3596 mle=1.4499 pcon=4.8267 forget=1.0830 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 23 it 280 total=7.6244 mle=1.7047 pcon=4.8250 forget=1.0946 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 23 it 330 total=7.6249 mle=1.7105 pcon=4.8232 forget=1.0911 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 23 it 380 total=7.6031 mle=1.6844 pcon=4.8213 forget=1.0973 favg=0.0000 nr=21 nf=21 protos=570 fproto_sim=NA
[loss] ep 24 it 40 total=7.4487 mle=1.5335 pcon=4.8195 forget=1.0956 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 24 it 90 total=7.5816 mle=1.6111 pcon=4.8178 forget=1.1527 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 24 it 140 total=7.3739 mle=1.4403 pcon=4.8162 forget=1.1174 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 24 it 190 total=7.5762 mle=1.6441 pcon=4.8144 forget=1.1176 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 24 it 240 total=7.5133 mle=1.5727 pcon=4.8129 forget=1.1277 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 24 it 290 total=7.8360 mle=1.8974 pcon=4.8110 forget=1.1277 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 24 it 340 total=7.5921 mle=1.6386 pcon=4.8095 forget=1.1441 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 0 total=7.4794 mle=1.5164 pcon=4.8080 forget=1.1550 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 50 total=7.5498 mle=1.5953 pcon=4.8065 forget=1.1480 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 25 it 100 total=7.6989 mle=1.7566 pcon=4.8050 forget=1.1373 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 25 it 150 total=7.5814 mle=1.6226 pcon=4.8034 forget=1.1553 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 25 it 200 total=7.7148 mle=1.7396 pcon=4.8019 forget=1.1734 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 250 total=7.4740 mle=1.5182 pcon=4.8004 forget=1.1554 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 25 it 300 total=7.6720 mle=1.6908 pcon=4.7989 forget=1.1823 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 350 total=7.6583 mle=1.6923 pcon=4.7975 forget=1.1685 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 10 total=7.7060 mle=1.7398 pcon=4.7961 forget=1.1701 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 26 it 60 total=7.6835 mle=1.7124 pcon=4.7947 forget=1.1764 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 110 total=7.5895 mle=1.6094 pcon=4.7934 forget=1.1867 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 26 it 160 total=7.4999 mle=1.5284 pcon=4.7921 forget=1.1794 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 26 it 210 total=7.6864 mle=1.7033 pcon=4.7908 forget=1.1922 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 26 it 260 total=7.5889 mle=1.6166 pcon=4.7894 forget=1.1828 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 26 it 310 total=7.6120 mle=1.6166 pcon=4.7881 forget=1.2073 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 360 total=7.6349 mle=1.6373 pcon=4.7867 forget=1.2109 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 27 it 20 total=7.5724 mle=1.5845 pcon=4.7853 forget=1.2027 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 27 it 70 total=7.4566 mle=1.4574 pcon=4.7840 forget=1.2152 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 27 it 120 total=7.8603 mle=1.8742 pcon=4.7826 forget=1.2035 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 27 it 170 total=7.6231 mle=1.6256 pcon=4.7814 forget=1.2161 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 27 it 220 total=7.4779 mle=1.4775 pcon=4.7801 forget=1.2203 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 27 it 270 total=7.4861 mle=1.4768 pcon=4.7788 forget=1.2305 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 27 it 320 total=7.5813 mle=1.5768 pcon=4.7775 forget=1.2271 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 27 it 370 total=7.5074 mle=1.4876 pcon=4.7762 forget=1.2436 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 28 it 30 total=7.8768 mle=1.8772 pcon=4.7750 forget=1.2245 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 28 it 80 total=7.7329 mle=1.7248 pcon=4.7738 forget=1.2344 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 28 it 130 total=7.5577 mle=1.5407 pcon=4.7727 forget=1.2443 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 28 it 180 total=7.5256 mle=1.5143 pcon=4.7716 forget=1.2397 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 28 it 230 total=7.6837 mle=1.6664 pcon=4.7706 forget=1.2466 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 28 it 280 total=7.5888 mle=1.5781 pcon=4.7695 forget=1.2412 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 28 it 330 total=7.5620 mle=1.5356 pcon=4.7683 forget=1.2580 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 28 it 380 total=7.6379 mle=1.6177 pcon=4.7672 forget=1.2530 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 29 it 40 total=7.6627 mle=1.6227 pcon=4.7660 forget=1.2740 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 29 it 90 total=7.7285 mle=1.7021 pcon=4.7650 forget=1.2613 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 29 it 140 total=7.6593 mle=1.6309 pcon=4.7639 forget=1.2645 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 29 it 190 total=7.7346 mle=1.6867 pcon=4.7628 forget=1.2851 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 29 it 240 total=7.8095 mle=1.7654 pcon=4.7619 forget=1.2822 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 29 it 290 total=7.6542 mle=1.6122 pcon=4.7608 forget=1.2812 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 29 it 340 total=7.7053 mle=1.6632 pcon=4.7599 forget=1.2821 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 30 it 0 total=7.7413 mle=1.6953 pcon=4.7589 forget=1.2871 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 30 it 50 total=7.7208 mle=1.6814 pcon=4.7579 forget=1.2815 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 30 it 100 total=7.6375 mle=1.5928 pcon=4.7572 forget=1.2875 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 30 it 150 total=7.7773 mle=1.7292 pcon=4.7562 forget=1.2919 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 30 it 200 total=7.7444 mle=1.6924 pcon=4.7554 forget=1.2966 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 30 it 250 total=7.5314 mle=1.4710 pcon=4.7545 forget=1.3059 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 30 it 300 total=7.7321 mle=1.6678 pcon=4.7536 forget=1.3106 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 30 it 350 total=7.6488 mle=1.5999 pcon=4.7528 forget=1.2961 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 31 it 10 total=7.6091 mle=1.5585 pcon=4.7519 forget=1.2987 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 31 it 60 total=7.7550 mle=1.6875 pcon=4.7511 forget=1.3164 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 31 it 110 total=7.4757 mle=1.4096 pcon=4.7503 forget=1.3158 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 31 it 160 total=7.5634 mle=1.4832 pcon=4.7495 forget=1.3308 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 31 it 210 total=7.7767 mle=1.7225 pcon=4.7487 forget=1.3056 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
 64%|██████▍   | 32/50 [16:13<05:06, 17.03s/it] 66%|██████▌   | 33/50 [16:26<04:30, 15.92s/it] 68%|██████▊   | 34/50 [16:39<04:00, 15.06s/it] 70%|███████   | 35/50 [16:53<03:38, 14.54s/it] 72%|███████▏  | 36/50 [17:06<03:19, 14.24s/it] 74%|███████▍  | 37/50 [17:20<03:01, 13.95s/it] 76%|███████▌  | 38/50 [17:33<02:45, 13.80s/it] 78%|███████▊  | 39/50 [17:46<02:30, 13.67s/it] 80%|████████  | 40/50 [17:59<02:15, 13.51s/it][loss] ep 31 it 260 total=7.6738 mle=1.6022 pcon=4.7479 forget=1.3237 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 31 it 310 total=7.5894 mle=1.5230 pcon=4.7471 forget=1.3193 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 31 it 360 total=7.6441 mle=1.5715 pcon=4.7464 forget=1.3261 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 32 it 20 total=7.5593 mle=1.4864 pcon=4.7457 forget=1.3272 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 32 it 70 total=7.5817 mle=1.5066 pcon=4.7449 forget=1.3302 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 32 it 120 total=7.6280 mle=1.5366 pcon=4.7441 forget=1.3473 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 32 it 170 total=7.7512 mle=1.6619 pcon=4.7434 forget=1.3460 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 32 it 220 total=7.7668 mle=1.6738 pcon=4.7426 forget=1.3504 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 32 it 270 total=7.6910 mle=1.6054 pcon=4.7417 forget=1.3439 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 32 it 320 total=7.7656 mle=1.6825 pcon=4.7408 forget=1.3423 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 32 it 370 total=7.5647 mle=1.4753 pcon=4.7402 forget=1.3492 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 33 it 30 total=7.9192 mle=1.8300 pcon=4.7396 forget=1.3496 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 33 it 80 total=7.6027 mle=1.5160 pcon=4.7388 forget=1.3480 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 33 it 130 total=7.6372 mle=1.5372 pcon=4.7381 forget=1.3618 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 33 it 180 total=7.7347 mle=1.6409 pcon=4.7375 forget=1.3563 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 33 it 230 total=7.8385 mle=1.7397 pcon=4.7368 forget=1.3619 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 33 it 280 total=8.1916 mle=2.0758 pcon=4.7361 forget=1.3797 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 33 it 330 total=7.5894 mle=1.4918 pcon=4.7354 forget=1.3623 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 33 it 380 total=7.6515 mle=1.5421 pcon=4.7349 forget=1.3745 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 34 it 40 total=7.7602 mle=1.6509 pcon=4.7343 forget=1.3750 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 34 it 90 total=7.7547 mle=1.6415 pcon=4.7337 forget=1.3796 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 34 it 140 total=7.8584 mle=1.7453 pcon=4.7330 forget=1.3800 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 34 it 190 total=7.7058 mle=1.5816 pcon=4.7324 forget=1.3917 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 34 it 240 total=7.7106 mle=1.5965 pcon=4.7318 forget=1.3823 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 34 it 290 total=7.6006 mle=1.4770 pcon=4.7313 forget=1.3923 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 34 it 340 total=7.6396 mle=1.5156 pcon=4.7307 forget=1.3933 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 35 it 0 total=7.6815 mle=1.5540 pcon=4.7301 forget=1.3974 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 35 it 50 total=7.9832 mle=1.8522 pcon=4.7296 forget=1.4015 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 35 it 100 total=7.7243 mle=1.5919 pcon=4.7291 forget=1.4034 favg=0.0000 nr=43 nf=43 protos=570 fproto_sim=NA
[loss] ep 35 it 150 total=7.7146 mle=1.5839 pcon=4.7286 forget=1.4021 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 35 it 200 total=7.6174 mle=1.4791 pcon=4.7281 forget=1.4101 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 35 it 250 total=7.8432 mle=1.7031 pcon=4.7276 forget=1.4124 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 35 it 300 total=7.7601 mle=1.6168 pcon=4.7271 forget=1.4161 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 35 it 350 total=7.6695 mle=1.5263 pcon=4.7266 forget=1.4166 favg=0.0000 nr=21 nf=21 protos=570 fproto_sim=NA
[loss] ep 36 it 10 total=7.7219 mle=1.5806 pcon=4.7261 forget=1.4153 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 36 it 60 total=7.6362 mle=1.4896 pcon=4.7256 forget=1.4211 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 36 it 110 total=7.8420 mle=1.6812 pcon=4.7251 forget=1.4357 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 36 it 160 total=7.5492 mle=1.3865 pcon=4.7246 forget=1.4381 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 36 it 210 total=7.8750 mle=1.6891 pcon=4.7243 forget=1.4617 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 36 it 260 total=7.8411 mle=1.6655 pcon=4.7239 forget=1.4517 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 36 it 310 total=7.7047 mle=1.5394 pcon=4.7235 forget=1.4417 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 36 it 360 total=7.7124 mle=1.5387 pcon=4.7232 forget=1.4504 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 37 it 20 total=7.6595 mle=1.4818 pcon=4.7229 forget=1.4548 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 37 it 70 total=8.0241 mle=1.8507 pcon=4.7225 forget=1.4510 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 37 it 120 total=7.6637 mle=1.4782 pcon=4.7220 forget=1.4634 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 37 it 170 total=7.6202 mle=1.4307 pcon=4.7216 forget=1.4679 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 37 it 220 total=7.9396 mle=1.7506 pcon=4.7212 forget=1.4678 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 37 it 270 total=7.7981 mle=1.6155 pcon=4.7208 forget=1.4618 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 37 it 320 total=7.6954 mle=1.5081 pcon=4.7205 forget=1.4668 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 37 it 370 total=7.7900 mle=1.5897 pcon=4.7201 forget=1.4802 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 38 it 30 total=7.9292 mle=1.7259 pcon=4.7198 forget=1.4834 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 38 it 80 total=7.7952 mle=1.5886 pcon=4.7195 forget=1.4872 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 38 it 130 total=7.8058 mle=1.5921 pcon=4.7192 forget=1.4945 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 38 it 180 total=7.6493 mle=1.4282 pcon=4.7188 forget=1.5022 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 38 it 230 total=7.6683 mle=1.4391 pcon=4.7186 forget=1.5106 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 38 it 280 total=8.2339 mle=2.0117 pcon=4.7182 forget=1.5040 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 38 it 330 total=7.7991 mle=1.5698 pcon=4.7180 forget=1.5113 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 38 it 380 total=7.8310 mle=1.5989 pcon=4.7176 forget=1.5145 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 39 it 40 total=7.8408 mle=1.5998 pcon=4.7173 forget=1.5236 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 39 it 90 total=7.8185 mle=1.5584 pcon=4.7170 forget=1.5431 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 39 it 140 total=8.0726 mle=1.8390 pcon=4.7167 forget=1.5169 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 39 it 190 total=7.9644 mle=1.6974 pcon=4.7165 forget=1.5505 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 39 it 240 total=7.9915 mle=1.7477 pcon=4.7164 forget=1.5274 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 39 it 290 total=8.0901 mle=1.8207 pcon=4.7161 forget=1.5533 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 39 it 340 total=7.8947 mle=1.6417 pcon=4.7159 forget=1.5371 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 40 it 0 total=7.6904 mle=1.4204 pcon=4.7156 forget=1.5545 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 40 it 50 total=7.7311 mle=1.4677 pcon=4.7153 forget=1.5481 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 40 it 100 total=8.0999 mle=1.8282 pcon=4.7150 forget=1.5566 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 40 it 150 total=7.8404 mle=1.5622 pcon=4.7147 forget=1.5635 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
 82%|████████▏ | 41/50 [18:13<02:01, 13.47s/it] 84%|████████▍ | 42/50 [18:27<01:48, 13.57s/it] 86%|████████▌ | 43/50 [18:40<01:34, 13.48s/it] 88%|████████▊ | 44/50 [18:53<01:20, 13.36s/it] 90%|█████████ | 45/50 [19:06<01:06, 13.26s/it] 92%|█████████▏| 46/50 [19:19<00:52, 13.20s/it] 94%|█████████▍| 47/50 [19:33<00:39, 13.27s/it] 96%|█████████▌| 48/50 [19:46<00:26, 13.18s/it] 98%|█████████▊| 49/50 [19:59<00:13, 13.19s/it][loss] ep 40 it 200 total=7.9571 mle=1.6691 pcon=4.7145 forget=1.5735 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 40 it 250 total=7.9697 mle=1.6939 pcon=4.7142 forget=1.5616 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 40 it 300 total=7.9764 mle=1.6696 pcon=4.7140 forget=1.5928 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 40 it 350 total=7.7581 mle=1.4728 pcon=4.7137 forget=1.5716 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 41 it 10 total=7.8648 mle=1.5647 pcon=4.7136 forget=1.5865 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 41 it 60 total=7.9514 mle=1.6595 pcon=4.7134 forget=1.5785 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 41 it 110 total=8.0298 mle=1.7122 pcon=4.7133 forget=1.6043 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 41 it 160 total=7.7144 mle=1.4019 pcon=4.7130 forget=1.5994 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 41 it 210 total=8.1643 mle=1.8498 pcon=4.7127 forget=1.6018 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 41 it 260 total=8.0473 mle=1.7328 pcon=4.7125 forget=1.6019 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 41 it 310 total=7.8132 mle=1.4962 pcon=4.7124 forget=1.6046 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 41 it 360 total=7.9567 mle=1.6366 pcon=4.7122 forget=1.6078 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 42 it 20 total=7.9019 mle=1.5733 pcon=4.7122 forget=1.6164 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 70 total=7.9854 mle=1.6467 pcon=4.7121 forget=1.6266 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 120 total=8.1848 mle=1.8510 pcon=4.7119 forget=1.6220 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 42 it 170 total=8.1420 mle=1.7975 pcon=4.7118 forget=1.6327 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 220 total=7.8813 mle=1.5389 pcon=4.7116 forget=1.6309 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 42 it 270 total=8.0213 mle=1.6676 pcon=4.7114 forget=1.6423 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 42 it 320 total=8.1265 mle=1.7780 pcon=4.7113 forget=1.6372 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 42 it 370 total=7.8873 mle=1.5406 pcon=4.7111 forget=1.6356 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 43 it 30 total=8.3215 mle=1.9789 pcon=4.7110 forget=1.6316 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 43 it 80 total=8.2330 mle=1.8677 pcon=4.7107 forget=1.6546 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 43 it 130 total=8.3604 mle=1.9863 pcon=4.7106 forget=1.6635 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 43 it 180 total=7.9423 mle=1.5643 pcon=4.7105 forget=1.6675 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 43 it 230 total=7.8558 mle=1.4778 pcon=4.7105 forget=1.6676 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 43 it 280 total=7.8028 mle=1.4323 pcon=4.7105 forget=1.6600 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 43 it 330 total=8.0319 mle=1.6482 pcon=4.7104 forget=1.6733 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 43 it 380 total=8.1700 mle=1.7903 pcon=4.7104 forget=1.6693 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 44 it 40 total=8.5225 mle=2.1308 pcon=4.7103 forget=1.6814 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 44 it 90 total=7.8532 mle=1.4447 pcon=4.7101 forget=1.6985 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 44 it 140 total=8.0537 mle=1.6624 pcon=4.7099 forget=1.6814 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 44 it 190 total=8.0843 mle=1.6796 pcon=4.7099 forget=1.6948 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 44 it 240 total=8.1404 mle=1.7245 pcon=4.7098 forget=1.7061 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 44 it 290 total=8.3310 mle=1.9328 pcon=4.7098 forget=1.6883 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 44 it 340 total=7.9466 mle=1.5478 pcon=4.7098 forget=1.6890 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 45 it 0 total=7.7942 mle=1.3907 pcon=4.7097 forget=1.6939 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 50 total=8.0068 mle=1.5901 pcon=4.7096 forget=1.7071 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 100 total=8.1072 mle=1.6804 pcon=4.7095 forget=1.7173 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 45 it 150 total=8.0514 mle=1.6105 pcon=4.7095 forget=1.7314 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 45 it 200 total=8.0281 mle=1.5981 pcon=4.7094 forget=1.7207 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 45 it 250 total=8.1037 mle=1.6674 pcon=4.7092 forget=1.7271 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 300 total=7.8476 mle=1.4224 pcon=4.7092 forget=1.7160 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 45 it 350 total=8.3153 mle=1.8676 pcon=4.7091 forget=1.7386 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 46 it 10 total=8.0386 mle=1.5906 pcon=4.7091 forget=1.7390 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 46 it 60 total=7.9844 mle=1.5297 pcon=4.7091 forget=1.7457 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 46 it 110 total=7.9306 mle=1.4865 pcon=4.7090 forget=1.7351 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 46 it 160 total=8.2838 mle=1.8384 pcon=4.7091 forget=1.7362 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 46 it 210 total=8.2263 mle=1.7790 pcon=4.7089 forget=1.7384 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 46 it 260 total=8.2636 mle=1.7852 pcon=4.7088 forget=1.7696 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 46 it 310 total=8.1572 mle=1.6776 pcon=4.7087 forget=1.7708 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 46 it 360 total=8.2027 mle=1.7544 pcon=4.7088 forget=1.7395 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 47 it 20 total=8.4664 mle=1.9753 pcon=4.7088 forget=1.7824 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 47 it 70 total=7.9387 mle=1.4582 pcon=4.7087 forget=1.7718 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 47 it 120 total=8.0578 mle=1.5819 pcon=4.7087 forget=1.7672 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 47 it 170 total=8.1228 mle=1.6522 pcon=4.7087 forget=1.7620 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 47 it 220 total=8.0772 mle=1.6168 pcon=4.7086 forget=1.7518 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 47 it 270 total=8.1695 mle=1.6842 pcon=4.7086 forget=1.7767 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 47 it 320 total=7.9961 mle=1.5187 pcon=4.7086 forget=1.7688 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 47 it 370 total=8.0449 mle=1.5476 pcon=4.7085 forget=1.7888 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 30 total=8.2217 mle=1.7412 pcon=4.7085 forget=1.7721 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 48 it 80 total=8.1865 mle=1.6791 pcon=4.7085 forget=1.7990 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 48 it 130 total=8.0801 mle=1.5784 pcon=4.7085 forget=1.7932 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 180 total=8.2710 mle=1.7715 pcon=4.7085 forget=1.7910 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 48 it 230 total=8.3370 mle=1.8287 pcon=4.7085 forget=1.7997 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 48 it 280 total=8.1781 mle=1.6699 pcon=4.7086 forget=1.7996 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 48 it 330 total=7.9790 mle=1.4537 pcon=4.7087 forget=1.8167 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 380 total=8.1615 mle=1.6423 pcon=4.7087 forget=1.8105 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 49 it 40 total=7.9684 mle=1.4589 pcon=4.7087 forget=1.8008 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 49 it 90 total=8.1791 mle=1.6575 pcon=4.7087 forget=1.8128 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
100%|██████████| 50/50 [20:14<00:00, 13.70s/it]100%|██████████| 50/50 [20:14<00:00, 24.28s/it]
[loss] ep 49 it 140 total=7.9475 mle=1.4311 pcon=4.7087 forget=1.8077 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 49 it 190 total=8.1253 mle=1.5897 pcon=4.7087 forget=1.8269 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 49 it 240 total=8.3058 mle=1.7617 pcon=4.7087 forget=1.8355 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 49 it 290 total=8.0666 mle=1.5553 pcon=4.7088 forget=1.8025 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 49 it 340 total=8.1638 mle=1.6327 pcon=4.7089 forget=1.8222 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] loaded adapter 'stage1' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] set_adapter(list) failed: unhashable type: 'list'; try adapter fusion
[peft] adapter fusion failed: adapter fusion API not available or single adapter provided; fallback to last adapter
[peft] active adapter set to: stage1
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1: Number of model parameters: 22082496
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:11,  2.03it/s]  3%|▎         | 10/391 [00:00<00:17, 21.71it/s]  5%|▌         | 20/391 [00:00<00:09, 40.27it/s]  8%|▊         | 30/391 [00:00<00:06, 54.95it/s] 10%|█         | 40/391 [00:00<00:05, 66.13it/s] 13%|█▎        | 50/391 [00:01<00:04, 74.56it/s] 15%|█▌        | 60/391 [00:01<00:04, 80.75it/s] 18%|█▊        | 70/391 [00:01<00:03, 85.19it/s] 20%|██        | 80/391 [00:01<00:03, 88.34it/s] 23%|██▎       | 90/391 [00:01<00:03, 90.62it/s] 26%|██▌       | 100/391 [00:01<00:03, 92.07it/s] 28%|██▊       | 110/391 [00:01<00:03, 93.06it/s] 31%|███       | 120/391 [00:01<00:02, 93.92it/s] 33%|███▎      | 130/391 [00:01<00:02, 94.82it/s] 36%|███▌      | 140/391 [00:01<00:02, 94.00it/s] 38%|███▊      | 150/391 [00:02<00:02, 94.45it/s] 41%|████      | 160/391 [00:02<00:02, 95.05it/s] 43%|████▎     | 170/391 [00:02<00:02, 94.33it/s] 46%|████▌     | 180/391 [00:02<00:02, 93.85it/s] 49%|████▊     | 190/391 [00:02<00:02, 94.31it/s] 51%|█████     | 200/391 [00:02<00:02, 93.61it/s] 54%|█████▎    | 210/391 [00:02<00:01, 94.11it/s] 56%|█████▋    | 220/391 [00:02<00:01, 94.50it/s] 59%|█████▉    | 230/391 [00:02<00:01, 94.54it/s] 61%|██████▏   | 240/391 [00:03<00:01, 94.63it/s] 64%|██████▍   | 250/391 [00:03<00:01, 94.88it/s] 66%|██████▋   | 260/391 [00:03<00:01, 95.00it/s] 69%|██████▉   | 270/391 [00:03<00:01, 93.76it/s] 72%|███████▏  | 280/391 [00:03<00:01, 94.34it/s] 74%|███████▍  | 290/391 [00:03<00:01, 94.61it/s] 77%|███████▋  | 300/391 [00:03<00:00, 94.97it/s] 79%|███████▉  | 310/391 [00:03<00:00, 94.13it/s] 82%|████████▏ | 320/391 [00:03<00:00, 94.09it/s] 84%|████████▍ | 330/391 [00:03<00:00, 94.31it/s] 87%|████████▋ | 340/391 [00:04<00:00, 93.54it/s] 90%|████████▉ | 350/391 [00:04<00:00, 94.15it/s] 92%|█████████▏| 360/391 [00:04<00:00, 93.89it/s] 95%|█████████▍| 370/391 [00:04<00:00, 94.84it/s] 97%|█████████▋| 380/391 [00:04<00:00, 95.47it/s]100%|█████████▉| 390/391 [00:04<00:00, 95.98it/s]100%|██████████| 391/391 [00:04<00:00, 84.68it/s]
50000 images processed, 4.722570180892944 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:45,  1.70it/s] 13%|█▎        | 10/79 [00:00<00:03, 18.93it/s] 25%|██▌       | 20/79 [00:00<00:01, 36.06it/s] 38%|███▊      | 30/79 [00:00<00:00, 50.08it/s] 51%|█████     | 40/79 [00:01<00:00, 61.09it/s] 63%|██████▎   | 50/79 [00:01<00:00, 69.96it/s] 76%|███████▌  | 60/79 [00:01<00:00, 76.85it/s] 89%|████████▊ | 70/79 [00:01<00:00, 81.96it/s]100%|██████████| 79/79 [00:01<00:00, 55.10it/s]
10000 images processed, 1.4564547538757324 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:12,  1.53it/s]  5%|▌         | 11/204 [00:00<00:10, 19.04it/s] 10%|█         | 21/204 [00:00<00:05, 35.15it/s] 15%|█▌        | 31/204 [00:00<00:03, 49.09it/s] 20%|██        | 41/204 [00:01<00:02, 60.66it/s] 25%|██▌       | 51/204 [00:01<00:02, 69.84it/s] 30%|██▉       | 61/204 [00:01<00:01, 76.72it/s] 35%|███▍      | 71/204 [00:01<00:01, 81.50it/s] 40%|███▉      | 81/204 [00:01<00:01, 84.83it/s] 45%|████▍     | 91/204 [00:01<00:01, 87.55it/s] 50%|████▉     | 101/204 [00:01<00:01, 88.59it/s] 54%|█████▍    | 111/204 [00:01<00:01, 90.36it/s] 59%|█████▉    | 121/204 [00:01<00:00, 91.53it/s] 64%|██████▍   | 131/204 [00:02<00:00, 92.39it/s] 69%|██████▉   | 141/204 [00:02<00:00, 92.70it/s] 74%|███████▍  | 151/204 [00:02<00:00, 93.31it/s] 79%|███████▉  | 161/204 [00:02<00:00, 93.45it/s] 84%|████████▍ | 171/204 [00:02<00:00, 94.01it/s] 89%|████████▊ | 181/204 [00:02<00:00, 94.53it/s] 94%|█████████▎| 191/204 [00:02<00:00, 95.07it/s] 99%|█████████▊| 201/204 [00:02<00:00, 95.41it/s]100%|██████████| 204/204 [00:02<00:00, 72.77it/s]
26032 images processed, 2.8526110649108887 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:10,  1.11it/s] 14%|█▍        | 11/79 [00:01<00:04, 14.56it/s] 27%|██▋       | 21/79 [00:01<00:02, 28.23it/s] 39%|███▉      | 31/79 [00:01<00:01, 41.28it/s] 51%|█████     | 40/79 [00:01<00:00, 50.99it/s] 63%|██████▎   | 50/79 [00:01<00:00, 61.48it/s] 76%|███████▌  | 60/79 [00:01<00:00, 70.17it/s] 89%|████████▊ | 70/79 [00:01<00:00, 76.75it/s]100%|██████████| 79/79 [00:01<00:00, 45.90it/s]
10000 images processed, 1.7531485557556152 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:59,  1.31it/s] 14%|█▍        | 11/79 [00:00<00:04, 16.72it/s] 27%|██▋       | 21/79 [00:00<00:01, 31.56it/s] 39%|███▉      | 31/79 [00:01<00:01, 44.63it/s] 52%|█████▏    | 41/79 [00:01<00:00, 56.00it/s] 65%|██████▍   | 51/79 [00:01<00:00, 65.69it/s] 77%|███████▋  | 61/79 [00:01<00:00, 73.68it/s] 90%|████████▉ | 71/79 [00:01<00:00, 79.45it/s]100%|██████████| 79/79 [00:01<00:00, 49.87it/s]
10000 images processed, 1.6086559295654297 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:48,  1.43it/s] 16%|█▌        | 11/70 [00:00<00:03, 17.85it/s] 30%|███       | 21/70 [00:00<00:01, 33.34it/s] 44%|████▍     | 31/70 [00:01<00:00, 47.03it/s] 59%|█████▊    | 41/70 [00:01<00:00, 58.65it/s] 73%|███████▎  | 51/70 [00:01<00:00, 67.67it/s] 87%|████████▋ | 61/70 [00:01<00:00, 75.23it/s]100%|██████████| 70/70 [00:01<00:00, 48.73it/s]
8925 images processed, 1.4688475131988525 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:53,  1.22s/it]  4%|▍         | 2/45 [00:01<00:24,  1.76it/s] 22%|██▏       | 10/45 [00:01<00:02, 11.84it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.81it/s] 60%|██████    | 27/45 [00:02<00:00, 23.19it/s] 73%|███████▎  | 33/45 [00:02<00:00, 14.43it/s] 96%|█████████▌| 43/45 [00:02<00:00, 22.50it/s]100%|██████████| 45/45 [00:02<00:00, 15.29it/s]
5640 images processed, 2.9661202430725098 seconds used

18.663837432861328
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           5.52  98.78
places365     77.95  77.59
LSUN          43.45  88.76
iSUN          74.89  80.82
dtd           46.99  89.22
AVG           49.76  87.03
Retain-Acc: 0.7275
Traceback (most recent call last):
  File "/home/shaokun/PALM/eval_cifar.py", line 19, in <module>
    eval_maha(args)
  File "/home/shaokun/PALM/util/evaluations/mahalanobis.py", line 173, in eval_maha
    fr_results = metrics.cal_metric(retain_scores, forget_scores)
  File "/home/shaokun/PALM/util/evaluations/metrics.py", line 16, in cal_metric
    tp, fp, fpr_at_tpr95 = get_curve(known, novel, method)
  File "/home/shaokun/PALM/util/evaluations/metrics.py", line 85, in get_curve
    end = np.max([np.max(known), np.max(novel)])
  File "/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/numpy/core/fromnumeric.py", line 2810, in max
    return _wrapreduction(a, np.maximum, 'max', axis, None, out,
  File "/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/numpy/core/fromnumeric.py", line 88, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
ValueError: zero-size array to reduction operation maximum which has no identity
