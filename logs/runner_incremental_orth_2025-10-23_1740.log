nohup: ignoring input
==== Stage 1: forget_inc={0,8,11,40,51}; forget_seen={}; all={0,8,11,40,51,66,67,88,94,57} ====
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1', adapter_load_path=None, adapter_load_paths=None, lora_new_adapter_name='train_s1', lora_stack=True, lora_orth_enable=True, lora_orth_lambda=0.1, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc='0,8,11,40,51', forget_classes_seen=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] added trainable adapter 'train_s1'
[peft] set_adapter(list) failed: unhashable type: 'list'; fallback to last adapter
[peft] active adapter set to: train_s1
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 22082496
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.train_s1.weight
[debug] trainable: base_model.model.head.2.lora_A.train_s1.weight
[debug] trainable: base_model.model.head.2.lora_B.train_s1.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.train_s1.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.train_s1.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.train_s1.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.train_s1.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.train_s1.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [01:30<1:13:57, 90.55s/it]  4%|▍         | 2/50 [01:51<39:37, 49.52s/it]    6%|▌         | 3/50 [02:08<27:05, 34.59s/it]  8%|▊         | 4/50 [02:31<23:04, 30.10s/it] 10%|█         | 5/50 [02:53<20:26, 27.26s/it] 12%|█▏        | 6/50 [03:15<18:37, 25.40s/it] 14%|█▍        | 7/50 [03:36<17:08, 23.91s/it][loss] ep 0 it 0 total=8.8609 mle=2.1075 pcon=5.2951 forget=1.4584 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 0 it 50 total=8.5073 mle=1.7687 pcon=5.2912 forget=1.4474 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 100 total=8.7543 mle=2.0172 pcon=5.2869 forget=1.4502 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 150 total=9.0629 mle=2.3296 pcon=5.2833 forget=1.4500 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 0 it 200 total=8.7184 mle=1.9566 pcon=5.2792 forget=1.4826 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 250 total=8.2314 mle=1.4705 pcon=5.2752 forget=1.4856 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 0 it 300 total=8.5743 mle=1.8216 pcon=5.2714 forget=1.4813 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 350 total=8.5492 mle=1.8183 pcon=5.2676 forget=1.4634 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 1 it 10 total=8.6587 mle=1.9367 pcon=5.2637 forget=1.4582 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 1 it 60 total=8.2952 mle=1.5961 pcon=5.2601 forget=1.4389 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 1 it 110 total=8.5684 mle=1.8962 pcon=5.2564 forget=1.4159 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 1 it 160 total=8.9049 mle=2.1531 pcon=5.2527 forget=1.4990 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 1 it 210 total=8.9399 mle=2.2520 pcon=5.2489 forget=1.4390 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 1 it 260 total=8.3602 mle=1.6894 pcon=5.2452 forget=1.4256 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 1 it 310 total=8.3142 mle=1.6445 pcon=5.2417 forget=1.4280 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 1 it 360 total=8.4358 mle=1.7437 pcon=5.2381 forget=1.4540 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 2 it 20 total=8.4943 mle=1.8255 pcon=5.2347 forget=1.4341 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 2 it 70 total=8.3885 mle=1.7292 pcon=5.2311 forget=1.4283 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 2 it 120 total=8.6765 mle=2.0298 pcon=5.2276 forget=1.4191 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 2 it 170 total=8.4891 mle=1.7812 pcon=5.2242 forget=1.4837 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 2 it 220 total=8.7015 mle=2.0877 pcon=5.2210 forget=1.3928 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 2 it 270 total=8.4993 mle=1.8193 pcon=5.2177 forget=1.4623 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 2 it 320 total=8.4412 mle=1.7785 pcon=5.2145 forget=1.4482 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 2 it 370 total=8.6231 mle=1.9535 pcon=5.2112 forget=1.4585 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 3 it 30 total=9.1452 mle=2.4586 pcon=5.2081 forget=1.4785 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 3 it 80 total=8.4340 mle=1.7715 pcon=5.2050 forget=1.4575 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 3 it 130 total=8.4290 mle=1.8115 pcon=5.2018 forget=1.4157 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 3 it 180 total=8.2826 mle=1.6189 pcon=5.1989 forget=1.4649 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 3 it 230 total=8.5760 mle=1.9234 pcon=5.1959 forget=1.4567 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 3 it 280 total=8.4160 mle=1.8164 pcon=5.1929 forget=1.4068 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 3 it 330 total=8.6556 mle=2.0159 pcon=5.1897 forget=1.4500 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 3 it 380 total=8.4982 mle=1.8669 pcon=5.1868 forget=1.4445 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 4 it 40 total=8.2955 mle=1.6802 pcon=5.1839 forget=1.4314 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 4 it 90 total=8.3728 mle=1.7412 pcon=5.1809 forget=1.4507 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 4 it 140 total=8.9599 mle=2.2987 pcon=5.1778 forget=1.4834 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 4 it 190 total=8.6957 mle=2.0471 pcon=5.1748 forget=1.4738 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 4 it 240 total=8.5810 mle=2.0075 pcon=5.1718 forget=1.4018 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 4 it 290 total=8.1869 mle=1.5615 pcon=5.1688 forget=1.4567 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 4 it 340 total=8.6952 mle=2.0945 pcon=5.1659 forget=1.4348 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 5 it 0 total=8.3761 mle=1.7651 pcon=5.1633 forget=1.4477 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 5 it 50 total=8.3577 mle=1.7143 pcon=5.1605 forget=1.4829 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 5 it 100 total=8.5173 mle=1.9346 pcon=5.1578 forget=1.4249 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 5 it 150 total=8.2546 mle=1.6725 pcon=5.1549 forget=1.4272 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 5 it 200 total=8.8732 mle=2.2929 pcon=5.1523 forget=1.4279 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 5 it 250 total=8.2812 mle=1.7121 pcon=5.1499 forget=1.4192 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 5 it 300 total=8.2720 mle=1.7014 pcon=5.1471 forget=1.4235 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 5 it 350 total=8.3334 mle=1.7854 pcon=5.1445 forget=1.4035 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 6 it 10 total=8.2106 mle=1.6606 pcon=5.1418 forget=1.4083 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 6 it 60 total=8.5629 mle=1.9937 pcon=5.1393 forget=1.4300 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 6 it 110 total=8.4077 mle=1.8646 pcon=5.1367 forget=1.4064 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 6 it 160 total=8.3694 mle=1.7975 pcon=5.1342 forget=1.4377 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 6 it 210 total=8.5387 mle=1.9520 pcon=5.1314 forget=1.4553 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 6 it 260 total=8.2796 mle=1.7251 pcon=5.1288 forget=1.4257 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 6 it 310 total=8.5216 mle=1.9588 pcon=5.1263 forget=1.4366 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 6 it 360 total=8.3353 mle=1.8099 pcon=5.1235 forget=1.4019 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 7 it 20 total=8.2934 mle=1.7305 pcon=5.1211 forget=1.4418 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 7 it 70 total=8.5684 mle=2.0641 pcon=5.1185 forget=1.3858 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 7 it 120 total=8.2286 mle=1.6787 pcon=5.1159 forget=1.4340 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 7 it 170 total=8.5265 mle=1.9735 pcon=5.1134 forget=1.4395 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
 16%|█▌        | 8/50 [03:58<16:26, 23.48s/it] 18%|█▊        | 9/50 [04:21<15:52, 23.24s/it] 20%|██        | 10/50 [04:43<15:16, 22.91s/it] 22%|██▏       | 11/50 [05:06<14:49, 22.80s/it] 24%|██▍       | 12/50 [05:31<14:54, 23.53s/it] 26%|██▌       | 13/50 [05:53<14:17, 23.17s/it] 28%|██▊       | 14/50 [06:16<13:53, 23.15s/it] 30%|███       | 15/50 [06:39<13:29, 23.14s/it][loss] ep 7 it 220 total=8.3741 mle=1.8717 pcon=5.1113 forget=1.3911 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 7 it 270 total=8.3025 mle=1.7532 pcon=5.1089 forget=1.4405 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 7 it 320 total=8.2096 mle=1.7013 pcon=5.1065 forget=1.4019 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 7 it 370 total=8.6180 mle=2.0996 pcon=5.1041 forget=1.4143 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 8 it 30 total=8.2721 mle=1.7661 pcon=5.1018 forget=1.4041 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 8 it 80 total=8.4871 mle=1.9938 pcon=5.0994 forget=1.3938 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 8 it 130 total=8.1618 mle=1.6817 pcon=5.0973 forget=1.3828 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 8 it 180 total=7.9678 mle=1.5103 pcon=5.0950 forget=1.3625 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 8 it 230 total=8.1762 mle=1.6534 pcon=5.0927 forget=1.4300 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 8 it 280 total=8.1289 mle=1.6962 pcon=5.0902 forget=1.3424 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 8 it 330 total=8.8140 mle=2.3250 pcon=5.0878 forget=1.4011 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 8 it 380 total=8.1422 mle=1.6601 pcon=5.0853 forget=1.3969 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 9 it 40 total=8.2197 mle=1.7312 pcon=5.0826 forget=1.4058 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 9 it 90 total=8.0259 mle=1.5574 pcon=5.0804 forget=1.3881 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 9 it 140 total=7.8619 mle=1.3887 pcon=5.0777 forget=1.3955 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 9 it 190 total=8.2028 mle=1.7768 pcon=5.0751 forget=1.3509 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 9 it 240 total=8.6385 mle=2.1990 pcon=5.0727 forget=1.3667 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 9 it 290 total=8.5449 mle=2.0574 pcon=5.0702 forget=1.4172 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 9 it 340 total=8.1652 mle=1.7379 pcon=5.0678 forget=1.3596 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 10 it 0 total=8.6407 mle=2.1626 pcon=5.0656 forget=1.4125 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 10 it 50 total=7.8196 mle=1.3910 pcon=5.0632 forget=1.3654 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 10 it 100 total=8.3422 mle=1.9123 pcon=5.0607 forget=1.3692 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 10 it 150 total=8.3061 mle=1.8523 pcon=5.0582 forget=1.3956 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 10 it 200 total=8.1295 mle=1.6871 pcon=5.0558 forget=1.3866 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 10 it 250 total=8.2734 mle=1.8486 pcon=5.0535 forget=1.3714 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 10 it 300 total=8.1667 mle=1.7515 pcon=5.0511 forget=1.3641 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 10 it 350 total=8.1187 mle=1.6926 pcon=5.0485 forget=1.3776 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 11 it 10 total=7.9929 mle=1.5446 pcon=5.0461 forget=1.4022 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 11 it 60 total=8.3463 mle=1.9114 pcon=5.0439 forget=1.3909 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 11 it 110 total=7.9874 mle=1.5539 pcon=5.0416 forget=1.3919 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 11 it 160 total=8.4291 mle=1.9651 pcon=5.0394 forget=1.4246 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 11 it 210 total=8.3009 mle=1.8671 pcon=5.0371 forget=1.3967 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 11 it 260 total=7.8760 mle=1.4367 pcon=5.0349 forget=1.4044 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 11 it 310 total=8.0768 mle=1.6379 pcon=5.0323 forget=1.4066 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 11 it 360 total=8.2528 mle=1.8007 pcon=5.0300 forget=1.4221 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 12 it 20 total=8.0304 mle=1.5927 pcon=5.0276 forget=1.4102 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 70 total=8.0320 mle=1.5784 pcon=5.0251 forget=1.4285 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 12 it 120 total=8.2021 mle=1.7387 pcon=5.0226 forget=1.4408 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 12 it 170 total=8.1689 mle=1.7103 pcon=5.0202 forget=1.4384 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 220 total=7.9279 mle=1.4941 pcon=5.0179 forget=1.4160 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 12 it 270 total=8.2386 mle=1.7908 pcon=5.0154 forget=1.4324 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 320 total=8.2143 mle=1.7593 pcon=5.0129 forget=1.4421 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 12 it 370 total=8.2650 mle=1.7922 pcon=5.0106 forget=1.4621 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 13 it 30 total=7.8828 mle=1.4262 pcon=5.0084 forget=1.4483 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 13 it 80 total=8.0865 mle=1.5864 pcon=5.0058 forget=1.4942 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 13 it 130 total=8.3099 mle=1.8481 pcon=5.0033 forget=1.4585 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 13 it 180 total=8.2246 mle=1.7647 pcon=5.0010 forget=1.4588 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 13 it 230 total=8.0829 mle=1.6220 pcon=4.9987 forget=1.4622 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 13 it 280 total=8.1197 mle=1.6473 pcon=4.9963 forget=1.4761 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 13 it 330 total=8.0209 mle=1.5506 pcon=4.9940 forget=1.4763 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 13 it 380 total=8.1521 mle=1.6650 pcon=4.9916 forget=1.4955 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 14 it 40 total=8.2990 mle=1.8062 pcon=4.9892 forget=1.5036 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 90 total=8.0679 mle=1.5462 pcon=4.9868 forget=1.5349 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 140 total=8.1921 mle=1.7341 pcon=4.9843 forget=1.4737 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 14 it 190 total=8.1291 mle=1.6904 pcon=4.9820 forget=1.4567 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 240 total=8.3482 mle=1.8841 pcon=4.9796 forget=1.4845 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 14 it 290 total=8.0043 mle=1.5672 pcon=4.9771 forget=1.4599 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 14 it 340 total=8.1895 mle=1.7621 pcon=4.9747 forget=1.4527 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 15 it 0 total=7.9779 mle=1.5603 pcon=4.9724 forget=1.4452 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
 32%|███▏      | 16/50 [07:04<13:24, 23.65s/it] 34%|███▍      | 17/50 [07:27<12:54, 23.46s/it] 36%|███▌      | 18/50 [07:50<12:25, 23.30s/it] 38%|███▊      | 19/50 [08:13<11:55, 23.07s/it] 40%|████      | 20/50 [08:35<11:26, 22.89s/it] 42%|████▏     | 21/50 [08:59<11:09, 23.09s/it] 44%|████▍     | 22/50 [09:24<11:02, 23.68s/it][loss] ep 15 it 50 total=7.9654 mle=1.5715 pcon=4.9702 forget=1.4237 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 15 it 100 total=8.1566 mle=1.7442 pcon=4.9681 forget=1.4443 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 150 total=7.9808 mle=1.5549 pcon=4.9660 forget=1.4600 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 200 total=7.9545 mle=1.5845 pcon=4.9637 forget=1.4064 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 15 it 250 total=7.9585 mle=1.6550 pcon=4.9615 forget=1.3420 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 15 it 300 total=7.7856 mle=1.4961 pcon=4.9594 forget=1.3301 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 350 total=8.0088 mle=1.7376 pcon=4.9574 forget=1.3138 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 16 it 10 total=7.9127 mle=1.5911 pcon=4.9555 forget=1.3661 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 16 it 60 total=7.8119 mle=1.5892 pcon=4.9535 forget=1.2692 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 16 it 110 total=7.8399 mle=1.6099 pcon=4.9514 forget=1.2785 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 16 it 160 total=8.0114 mle=1.8206 pcon=4.9494 forget=1.2414 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 16 it 210 total=7.9083 mle=1.6819 pcon=4.9474 forget=1.2790 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 16 it 260 total=7.6532 mle=1.4554 pcon=4.9455 forget=1.2523 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 16 it 310 total=7.7165 mle=1.4782 pcon=4.9435 forget=1.2948 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 16 it 360 total=7.8319 mle=1.6633 pcon=4.9414 forget=1.2272 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 17 it 20 total=7.6711 mle=1.5570 pcon=4.9396 forget=1.1745 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 70 total=7.9203 mle=1.6911 pcon=4.9377 forget=1.2915 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 120 total=8.0497 mle=1.9438 pcon=4.9357 forget=1.1702 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 170 total=7.6958 mle=1.5778 pcon=4.9341 forget=1.1839 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 17 it 220 total=7.7099 mle=1.6328 pcon=4.9319 forget=1.1452 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 17 it 270 total=7.8061 mle=1.6973 pcon=4.9300 forget=1.1789 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 17 it 320 total=7.4588 mle=1.4355 pcon=4.9279 forget=1.0954 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 17 it 370 total=7.6542 mle=1.6351 pcon=4.9257 forget=1.0933 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 18 it 30 total=7.6881 mle=1.6838 pcon=4.9236 forget=1.0808 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 18 it 80 total=8.0385 mle=2.0893 pcon=4.9212 forget=1.0280 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 18 it 130 total=7.6191 mle=1.5959 pcon=4.9188 forget=1.1044 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 18 it 180 total=7.5185 mle=1.5896 pcon=4.9163 forget=1.0126 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 18 it 230 total=7.4432 mle=1.5493 pcon=4.9138 forget=0.9801 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 18 it 280 total=7.5721 mle=1.6215 pcon=4.9111 forget=1.0395 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 18 it 330 total=7.4989 mle=1.5724 pcon=4.9082 forget=1.0183 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 18 it 380 total=7.7139 mle=1.7908 pcon=4.9053 forget=1.0179 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 19 it 40 total=7.5488 mle=1.6581 pcon=4.9025 forget=0.9883 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 19 it 90 total=7.4632 mle=1.5613 pcon=4.8996 forget=1.0023 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 19 it 140 total=7.6327 mle=1.7404 pcon=4.8968 forget=0.9956 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 19 it 190 total=7.5504 mle=1.6719 pcon=4.8941 forget=0.9844 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 19 it 240 total=7.6709 mle=1.7757 pcon=4.8914 forget=1.0038 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 19 it 290 total=7.4224 mle=1.5534 pcon=4.8887 forget=0.9803 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 19 it 340 total=7.4911 mle=1.6203 pcon=4.8860 forget=0.9847 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 20 it 0 total=7.5857 mle=1.6949 pcon=4.8834 forget=1.0074 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 20 it 50 total=7.3281 mle=1.4686 pcon=4.8808 forget=0.9787 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 20 it 100 total=7.5979 mle=1.7330 pcon=4.8781 forget=0.9868 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 20 it 150 total=7.4735 mle=1.6077 pcon=4.8754 forget=0.9904 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 20 it 200 total=7.7187 mle=1.8492 pcon=4.8727 forget=0.9968 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 20 it 250 total=7.7052 mle=1.8012 pcon=4.8700 forget=1.0340 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 20 it 300 total=7.4798 mle=1.6047 pcon=4.8673 forget=1.0077 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 20 it 350 total=7.4529 mle=1.5698 pcon=4.8649 forget=1.0183 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 21 it 10 total=7.3774 mle=1.4624 pcon=4.8623 forget=1.0527 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 21 it 60 total=7.6258 mle=1.7449 pcon=4.8598 forget=1.0210 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 21 it 110 total=7.8423 mle=1.9426 pcon=4.8572 forget=1.0425 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 21 it 160 total=7.6248 mle=1.7596 pcon=4.8549 forget=1.0104 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 21 it 210 total=7.4030 mle=1.5259 pcon=4.8523 forget=1.0248 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 21 it 260 total=7.4704 mle=1.5751 pcon=4.8498 forget=1.0456 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 21 it 310 total=7.6022 mle=1.7043 pcon=4.8472 forget=1.0507 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 21 it 360 total=7.6346 mle=1.7462 pcon=4.8448 forget=1.0435 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 22 it 20 total=7.3617 mle=1.4803 pcon=4.8426 forget=1.0388 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 22 it 70 total=7.6522 mle=1.7764 pcon=4.8403 forget=1.0355 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 22 it 120 total=7.5200 mle=1.6198 pcon=4.8380 forget=1.0622 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 22 it 170 total=7.4774 mle=1.5727 pcon=4.8357 forget=1.0689 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 22 it 220 total=7.7592 mle=1.8602 pcon=4.8334 forget=1.0657 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 22 it 270 total=7.4852 mle=1.5751 pcon=4.8312 forget=1.0789 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
 46%|████▌     | 23/50 [09:47<10:34, 23.48s/it] 48%|████▊     | 24/50 [10:10<10:09, 23.44s/it] 50%|█████     | 25/50 [10:33<09:41, 23.25s/it] 52%|█████▏    | 26/50 [10:58<09:29, 23.75s/it] 54%|█████▍    | 27/50 [11:19<08:50, 23.07s/it] 56%|█████▌    | 28/50 [11:43<08:28, 23.10s/it] 58%|█████▊    | 29/50 [12:07<08:15, 23.59s/it] 60%|██████    | 30/50 [12:31<07:49, 23.48s/it] 62%|██████▏   | 31/50 [12:56<07:35, 23.99s/it][loss] ep 22 it 320 total=7.4405 mle=1.5365 pcon=4.8290 forget=1.0750 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 22 it 370 total=7.6936 mle=1.7871 pcon=4.8269 forget=1.0796 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 23 it 30 total=7.5303 mle=1.6308 pcon=4.8247 forget=1.0748 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 23 it 80 total=7.4682 mle=1.5494 pcon=4.8225 forget=1.0963 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 23 it 130 total=7.8316 mle=1.9247 pcon=4.8205 forget=1.0865 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 23 it 180 total=7.7384 mle=1.8261 pcon=4.8183 forget=1.0941 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 23 it 230 total=7.2365 mle=1.3227 pcon=4.8162 forget=1.0975 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 23 it 280 total=7.9319 mle=2.0228 pcon=4.8143 forget=1.0947 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 23 it 330 total=7.7468 mle=1.8127 pcon=4.8122 forget=1.1220 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 23 it 380 total=7.6056 mle=1.6764 pcon=4.8102 forget=1.1190 favg=0.0000 nr=21 nf=21 protos=570 fproto_sim=NA
[loss] ep 24 it 40 total=7.5106 mle=1.5741 pcon=4.8082 forget=1.1282 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 24 it 90 total=7.7027 mle=1.7640 pcon=4.8063 forget=1.1324 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 24 it 140 total=7.5076 mle=1.5710 pcon=4.8043 forget=1.1323 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 24 it 190 total=7.4762 mle=1.5335 pcon=4.8024 forget=1.1403 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 24 it 240 total=7.6128 mle=1.6560 pcon=4.8006 forget=1.1562 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 24 it 290 total=7.6009 mle=1.6577 pcon=4.7986 forget=1.1446 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 24 it 340 total=7.6395 mle=1.6963 pcon=4.7968 forget=1.1464 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 0 total=7.8190 mle=1.8628 pcon=4.7950 forget=1.1612 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 50 total=7.7510 mle=1.7985 pcon=4.7932 forget=1.1593 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 25 it 100 total=7.7558 mle=1.8076 pcon=4.7915 forget=1.1567 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 25 it 150 total=7.6197 mle=1.6538 pcon=4.7898 forget=1.1761 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 25 it 200 total=7.5828 mle=1.6196 pcon=4.7881 forget=1.1751 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 250 total=7.4632 mle=1.4952 pcon=4.7865 forget=1.1815 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 25 it 300 total=7.6663 mle=1.6708 pcon=4.7849 forget=1.2106 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 350 total=7.4288 mle=1.4594 pcon=4.7832 forget=1.1861 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 10 total=7.4814 mle=1.5143 pcon=4.7816 forget=1.1854 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 26 it 60 total=7.5711 mle=1.5887 pcon=4.7801 forget=1.2023 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 110 total=7.4158 mle=1.4397 pcon=4.7785 forget=1.1976 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 26 it 160 total=7.6610 mle=1.6752 pcon=4.7769 forget=1.2090 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 26 it 210 total=7.5980 mle=1.6088 pcon=4.7754 forget=1.2138 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 26 it 260 total=7.8540 mle=1.8508 pcon=4.7739 forget=1.2292 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 26 it 310 total=7.6938 mle=1.7052 pcon=4.7724 forget=1.2162 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 360 total=7.7115 mle=1.7170 pcon=4.7709 forget=1.2237 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 27 it 20 total=7.4551 mle=1.4493 pcon=4.7697 forget=1.2362 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 27 it 70 total=7.6237 mle=1.6293 pcon=4.7683 forget=1.2261 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 27 it 120 total=7.6219 mle=1.6228 pcon=4.7669 forget=1.2322 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 27 it 170 total=7.8131 mle=1.8050 pcon=4.7655 forget=1.2427 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 27 it 220 total=7.5303 mle=1.5218 pcon=4.7641 forget=1.2444 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 27 it 270 total=7.5633 mle=1.5503 pcon=4.7628 forget=1.2502 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 27 it 320 total=7.5381 mle=1.5314 pcon=4.7615 forget=1.2452 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 27 it 370 total=7.5362 mle=1.5188 pcon=4.7601 forget=1.2573 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 28 it 30 total=7.6242 mle=1.5986 pcon=4.7589 forget=1.2667 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 28 it 80 total=7.7383 mle=1.7209 pcon=4.7577 forget=1.2597 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 28 it 130 total=7.5519 mle=1.5340 pcon=4.7565 forget=1.2614 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 28 it 180 total=7.5776 mle=1.5478 pcon=4.7553 forget=1.2745 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 28 it 230 total=7.5427 mle=1.5233 pcon=4.7542 forget=1.2652 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 28 it 280 total=7.4241 mle=1.4018 pcon=4.7530 forget=1.2693 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 28 it 330 total=7.6445 mle=1.6099 pcon=4.7518 forget=1.2828 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 28 it 380 total=7.7664 mle=1.7365 pcon=4.7506 forget=1.2793 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 29 it 40 total=7.6148 mle=1.5738 pcon=4.7497 forget=1.2913 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 29 it 90 total=7.6479 mle=1.6003 pcon=4.7484 forget=1.2991 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 29 it 140 total=7.6560 mle=1.6280 pcon=4.7473 forget=1.2806 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 29 it 190 total=7.6989 mle=1.6488 pcon=4.7462 forget=1.3039 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 29 it 240 total=7.6038 mle=1.5645 pcon=4.7451 forget=1.2942 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 29 it 290 total=7.7739 mle=1.7215 pcon=4.7442 forget=1.3082 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 29 it 340 total=7.5626 mle=1.5209 pcon=4.7430 forget=1.2986 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 30 it 0 total=7.6021 mle=1.5603 pcon=4.7419 forget=1.2999 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 30 it 50 total=7.8570 mle=1.8112 pcon=4.7408 forget=1.3050 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 30 it 100 total=7.6554 mle=1.6082 pcon=4.7398 forget=1.3075 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 30 it 150 total=7.6721 mle=1.6229 pcon=4.7389 forget=1.3103 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 30 it 200 total=7.6910 mle=1.6380 pcon=4.7379 forget=1.3151 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 30 it 250 total=7.6427 mle=1.5866 pcon=4.7367 forget=1.3193 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 30 it 300 total=7.8062 mle=1.7465 pcon=4.7358 forget=1.3239 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 30 it 350 total=7.6012 mle=1.5357 pcon=4.7350 forget=1.3304 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 31 it 10 total=7.6586 mle=1.5981 pcon=4.7342 forget=1.3263 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 31 it 60 total=7.7071 mle=1.6454 pcon=4.7333 forget=1.3284 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 31 it 110 total=7.8593 mle=1.8022 pcon=4.7325 forget=1.3246 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 31 it 160 total=8.0779 mle=2.0088 pcon=4.7316 forget=1.3375 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 31 it 210 total=7.8356 mle=1.7400 pcon=4.7307 forget=1.3649 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
 64%|██████▍   | 32/50 [13:20<07:13, 24.08s/it] 66%|██████▌   | 33/50 [13:45<06:52, 24.24s/it] 68%|██████▊   | 34/50 [14:09<06:29, 24.34s/it] 70%|███████   | 35/50 [14:34<06:08, 24.54s/it] 72%|███████▏  | 36/50 [15:00<05:47, 24.80s/it] 74%|███████▍  | 37/50 [15:24<05:22, 24.77s/it] 76%|███████▌  | 38/50 [15:50<05:01, 25.16s/it] 78%|███████▊  | 39/50 [16:15<04:36, 25.12s/it] 80%|████████  | 40/50 [16:41<04:11, 25.11s/it][loss] ep 31 it 260 total=7.7789 mle=1.7072 pcon=4.7299 forget=1.3419 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 31 it 310 total=7.5363 mle=1.4691 pcon=4.7291 forget=1.3381 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 31 it 360 total=7.5424 mle=1.4584 pcon=4.7282 forget=1.3557 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 32 it 20 total=7.6923 mle=1.6254 pcon=4.7275 forget=1.3394 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 32 it 70 total=7.7850 mle=1.7094 pcon=4.7266 forget=1.3490 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 32 it 120 total=7.6061 mle=1.5242 pcon=4.7260 forget=1.3560 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 32 it 170 total=7.6940 mle=1.6119 pcon=4.7253 forget=1.3569 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 32 it 220 total=7.8927 mle=1.8154 pcon=4.7245 forget=1.3528 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 32 it 270 total=7.6023 mle=1.5217 pcon=4.7239 forget=1.3568 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 32 it 320 total=7.5975 mle=1.5053 pcon=4.7230 forget=1.3692 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 32 it 370 total=7.9353 mle=1.8533 pcon=4.7223 forget=1.3597 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 33 it 30 total=7.9476 mle=1.8660 pcon=4.7216 forget=1.3600 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 33 it 80 total=7.7426 mle=1.6591 pcon=4.7209 forget=1.3627 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 33 it 130 total=7.5652 mle=1.4763 pcon=4.7202 forget=1.3686 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 33 it 180 total=7.4727 mle=1.3888 pcon=4.7195 forget=1.3644 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 33 it 230 total=7.9302 mle=1.8402 pcon=4.7188 forget=1.3711 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 33 it 280 total=7.7320 mle=1.6296 pcon=4.7182 forget=1.3842 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 33 it 330 total=7.8670 mle=1.7866 pcon=4.7178 forget=1.3626 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 33 it 380 total=7.5674 mle=1.4793 pcon=4.7171 forget=1.3710 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 34 it 40 total=7.6022 mle=1.5081 pcon=4.7165 forget=1.3776 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 34 it 90 total=7.9486 mle=1.8309 pcon=4.7159 forget=1.4018 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 34 it 140 total=7.8082 mle=1.7087 pcon=4.7151 forget=1.3843 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 34 it 190 total=7.7225 mle=1.6295 pcon=4.7145 forget=1.3786 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 34 it 240 total=7.9166 mle=1.8171 pcon=4.7139 forget=1.3856 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 34 it 290 total=7.5836 mle=1.4750 pcon=4.7134 forget=1.3952 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 34 it 340 total=7.6138 mle=1.5127 pcon=4.7128 forget=1.3882 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 35 it 0 total=7.6360 mle=1.5266 pcon=4.7123 forget=1.3971 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 35 it 50 total=8.0824 mle=1.9697 pcon=4.7116 forget=1.4010 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 35 it 100 total=7.7675 mle=1.6589 pcon=4.7111 forget=1.3975 favg=0.0000 nr=43 nf=43 protos=570 fproto_sim=NA
[loss] ep 35 it 150 total=7.5922 mle=1.4762 pcon=4.7106 forget=1.4053 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 35 it 200 total=7.7091 mle=1.5933 pcon=4.7100 forget=1.4058 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 35 it 250 total=7.8468 mle=1.7264 pcon=4.7094 forget=1.4110 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 35 it 300 total=7.6796 mle=1.5540 pcon=4.7089 forget=1.4168 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 35 it 350 total=7.7382 mle=1.6228 pcon=4.7085 forget=1.4069 favg=0.0000 nr=21 nf=21 protos=570 fproto_sim=NA
[loss] ep 36 it 10 total=7.7639 mle=1.6348 pcon=4.7080 forget=1.4211 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 36 it 60 total=7.7233 mle=1.6016 pcon=4.7075 forget=1.4142 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 36 it 110 total=7.7220 mle=1.5937 pcon=4.7071 forget=1.4212 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 36 it 160 total=7.9866 mle=1.8536 pcon=4.7067 forget=1.4263 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 36 it 210 total=7.9200 mle=1.7910 pcon=4.7062 forget=1.4228 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 36 it 260 total=7.6092 mle=1.4805 pcon=4.7058 forget=1.4229 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 36 it 310 total=7.6870 mle=1.5508 pcon=4.7053 forget=1.4309 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 36 it 360 total=7.5750 mle=1.4340 pcon=4.7048 forget=1.4362 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 37 it 20 total=7.8570 mle=1.7115 pcon=4.7044 forget=1.4411 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 37 it 70 total=7.7102 mle=1.5703 pcon=4.7039 forget=1.4360 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 37 it 120 total=7.7496 mle=1.6097 pcon=4.7036 forget=1.4363 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 37 it 170 total=7.7336 mle=1.5897 pcon=4.7031 forget=1.4407 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 37 it 220 total=7.6478 mle=1.5049 pcon=4.7028 forget=1.4401 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 37 it 270 total=7.7976 mle=1.6416 pcon=4.7024 forget=1.4536 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 37 it 320 total=7.9319 mle=1.7758 pcon=4.7020 forget=1.4541 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 37 it 370 total=7.7787 mle=1.6164 pcon=4.7016 forget=1.4607 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 38 it 30 total=7.8158 mle=1.6635 pcon=4.7013 forget=1.4510 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 38 it 80 total=7.5654 mle=1.4097 pcon=4.7009 forget=1.4548 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 38 it 130 total=7.9460 mle=1.7748 pcon=4.7005 forget=1.4707 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 38 it 180 total=7.6074 mle=1.4443 pcon=4.7002 forget=1.4630 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 38 it 230 total=7.6292 mle=1.4370 pcon=4.6998 forget=1.4924 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 38 it 280 total=7.7829 mle=1.6220 pcon=4.6994 forget=1.4615 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 38 it 330 total=7.8229 mle=1.6444 pcon=4.6991 forget=1.4794 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 38 it 380 total=7.9270 mle=1.7588 pcon=4.6986 forget=1.4697 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 39 it 40 total=7.7159 mle=1.5325 pcon=4.6984 forget=1.4850 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 39 it 90 total=7.7731 mle=1.5984 pcon=4.6980 forget=1.4767 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 39 it 140 total=7.8229 mle=1.6168 pcon=4.6977 forget=1.5085 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 39 it 190 total=7.9916 mle=1.8049 pcon=4.6974 forget=1.4894 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 39 it 240 total=7.8973 mle=1.7341 pcon=4.6970 forget=1.4661 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 39 it 290 total=7.8377 mle=1.6535 pcon=4.6967 forget=1.4875 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 39 it 340 total=7.8092 mle=1.6240 pcon=4.6965 forget=1.4888 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 40 it 0 total=7.8079 mle=1.6071 pcon=4.6962 forget=1.5046 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 40 it 50 total=7.8536 mle=1.6627 pcon=4.6960 forget=1.4949 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 40 it 100 total=7.7542 mle=1.5591 pcon=4.6957 forget=1.4994 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 40 it 150 total=7.7138 mle=1.5186 pcon=4.6954 forget=1.4998 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
 82%|████████▏ | 41/50 [17:05<03:44, 24.97s/it] 84%|████████▍ | 42/50 [17:30<03:18, 24.80s/it] 86%|████████▌ | 43/50 [17:54<02:53, 24.77s/it] 88%|████████▊ | 44/50 [18:19<02:29, 24.86s/it] 90%|█████████ | 45/50 [18:45<02:05, 25.03s/it] 92%|█████████▏| 46/50 [19:11<01:41, 25.32s/it] 94%|█████████▍| 47/50 [19:32<01:12, 24.04s/it] 96%|█████████▌| 48/50 [19:53<00:46, 23.23s/it] 98%|█████████▊| 49/50 [20:15<00:22, 22.81s/it][loss] ep 40 it 200 total=7.9126 mle=1.6954 pcon=4.6951 forget=1.5221 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 40 it 250 total=7.9587 mle=1.7585 pcon=4.6948 forget=1.5055 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 40 it 300 total=7.8521 mle=1.6397 pcon=4.6945 forget=1.5179 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 40 it 350 total=7.7910 mle=1.5784 pcon=4.6942 forget=1.5184 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 41 it 10 total=8.1383 mle=1.9147 pcon=4.6940 forget=1.5296 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 41 it 60 total=8.1034 mle=1.8757 pcon=4.6937 forget=1.5340 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 41 it 110 total=7.7445 mle=1.5298 pcon=4.6934 forget=1.5213 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 41 it 160 total=7.6474 mle=1.4218 pcon=4.6932 forget=1.5324 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 41 it 210 total=7.6585 mle=1.4277 pcon=4.6930 forget=1.5378 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 41 it 260 total=7.9239 mle=1.6870 pcon=4.6927 forget=1.5441 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 41 it 310 total=8.0925 mle=1.8555 pcon=4.6925 forget=1.5445 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 41 it 360 total=8.0341 mle=1.7850 pcon=4.6922 forget=1.5569 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 42 it 20 total=7.7827 mle=1.5417 pcon=4.6919 forget=1.5490 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 70 total=7.8801 mle=1.6387 pcon=4.6917 forget=1.5497 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 120 total=7.7652 mle=1.5054 pcon=4.6915 forget=1.5683 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 42 it 170 total=7.7214 mle=1.4705 pcon=4.6913 forget=1.5596 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 220 total=7.9265 mle=1.6751 pcon=4.6912 forget=1.5602 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 42 it 270 total=7.9718 mle=1.7209 pcon=4.6910 forget=1.5599 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 42 it 320 total=8.0277 mle=1.7648 pcon=4.6908 forget=1.5721 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 42 it 370 total=7.9440 mle=1.6758 pcon=4.6905 forget=1.5776 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 43 it 30 total=7.8921 mle=1.6340 pcon=4.6904 forget=1.5677 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 43 it 80 total=7.8153 mle=1.5520 pcon=4.6902 forget=1.5730 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 43 it 130 total=7.9342 mle=1.6676 pcon=4.6900 forget=1.5765 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 43 it 180 total=7.9247 mle=1.6478 pcon=4.6898 forget=1.5871 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 43 it 230 total=7.8707 mle=1.6003 pcon=4.6896 forget=1.5808 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 43 it 280 total=7.8365 mle=1.5671 pcon=4.6894 forget=1.5800 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 43 it 330 total=7.7557 mle=1.4802 pcon=4.6892 forget=1.5863 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 43 it 380 total=7.7674 mle=1.4897 pcon=4.6890 forget=1.5886 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 44 it 40 total=7.8783 mle=1.5864 pcon=4.6889 forget=1.6030 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 44 it 90 total=7.8719 mle=1.5870 pcon=4.6887 forget=1.5962 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 44 it 140 total=7.7948 mle=1.5109 pcon=4.6887 forget=1.5953 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 44 it 190 total=7.9695 mle=1.6727 pcon=4.6885 forget=1.6083 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 44 it 240 total=8.0428 mle=1.7502 pcon=4.6883 forget=1.6044 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 44 it 290 total=8.0606 mle=1.7760 pcon=4.6881 forget=1.5965 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 44 it 340 total=7.7900 mle=1.4760 pcon=4.6880 forget=1.6259 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 45 it 0 total=7.8255 mle=1.5231 pcon=4.6878 forget=1.6145 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 50 total=7.9057 mle=1.6116 pcon=4.6878 forget=1.6064 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 100 total=7.8956 mle=1.5843 pcon=4.6877 forget=1.6236 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 45 it 150 total=7.8916 mle=1.5880 pcon=4.6875 forget=1.6160 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 45 it 200 total=7.9214 mle=1.6166 pcon=4.6874 forget=1.6174 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 45 it 250 total=8.1335 mle=1.8061 pcon=4.6873 forget=1.6400 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 300 total=7.7433 mle=1.4205 pcon=4.6873 forget=1.6356 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 45 it 350 total=8.1545 mle=1.8388 pcon=4.6872 forget=1.6285 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 46 it 10 total=7.7700 mle=1.4455 pcon=4.6871 forget=1.6374 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 46 it 60 total=7.8156 mle=1.4955 pcon=4.6869 forget=1.6332 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 46 it 110 total=7.7156 mle=1.3905 pcon=4.6868 forget=1.6384 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 46 it 160 total=7.7849 mle=1.4619 pcon=4.6867 forget=1.6363 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 46 it 210 total=8.1918 mle=1.8700 pcon=4.6866 forget=1.6352 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 46 it 260 total=7.7301 mle=1.3927 pcon=4.6865 forget=1.6509 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 46 it 310 total=7.9246 mle=1.5898 pcon=4.6864 forget=1.6484 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 46 it 360 total=8.0682 mle=1.7428 pcon=4.6863 forget=1.6391 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 47 it 20 total=7.7949 mle=1.4571 pcon=4.6862 forget=1.6516 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 47 it 70 total=7.7445 mle=1.4051 pcon=4.6861 forget=1.6533 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 47 it 120 total=7.9911 mle=1.6493 pcon=4.6859 forget=1.6559 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 47 it 170 total=7.8810 mle=1.5415 pcon=4.6857 forget=1.6538 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 47 it 220 total=7.9777 mle=1.6303 pcon=4.6858 forget=1.6616 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 47 it 270 total=8.1781 mle=1.8303 pcon=4.6857 forget=1.6621 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 47 it 320 total=8.1527 mle=1.7856 pcon=4.6855 forget=1.6816 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 47 it 370 total=7.9659 mle=1.6088 pcon=4.6854 forget=1.6718 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 30 total=8.0452 mle=1.6927 pcon=4.6854 forget=1.6670 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 48 it 80 total=8.0079 mle=1.6395 pcon=4.6852 forget=1.6832 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 48 it 130 total=7.9573 mle=1.5790 pcon=4.6851 forget=1.6932 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 180 total=7.9070 mle=1.5490 pcon=4.6851 forget=1.6730 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 48 it 230 total=7.8721 mle=1.5023 pcon=4.6851 forget=1.6846 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 48 it 280 total=8.1001 mle=1.7130 pcon=4.6850 forget=1.7021 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 48 it 330 total=7.8949 mle=1.5187 pcon=4.6848 forget=1.6915 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 380 total=8.0416 mle=1.6588 pcon=4.6848 forget=1.6981 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 49 it 40 total=8.2236 mle=1.8618 pcon=4.6847 forget=1.6771 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 49 it 90 total=8.0293 mle=1.6493 pcon=4.6846 forget=1.6954 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
100%|██████████| 50/50 [20:40<00:00, 23.47s/it]100%|██████████| 50/50 [20:40<00:00, 24.81s/it]
[loss] ep 49 it 140 total=8.0126 mle=1.6484 pcon=4.6844 forget=1.6798 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 49 it 190 total=7.9419 mle=1.5696 pcon=4.6844 forget=1.6880 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 49 it 240 total=7.9740 mle=1.5935 pcon=4.6843 forget=1.6962 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 49 it 290 total=8.0632 mle=1.6884 pcon=4.6842 forget=1.6906 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 49 it 340 total=8.2404 mle=1.8583 pcon=4.6842 forget=1.6979 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] loaded adapter 'stage1' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] set_adapter(list) failed: unhashable type: 'list'; fallback to last adapter
[peft] active adapter set to: stage1
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1: Number of model parameters: 22082496
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:18,  2.81it/s]  3%|▎         | 10/391 [00:00<00:13, 27.73it/s]  5%|▌         | 20/391 [00:00<00:07, 47.47it/s]  8%|▊         | 30/391 [00:00<00:05, 60.77it/s] 10%|█         | 40/391 [00:00<00:04, 71.07it/s] 13%|█▎        | 49/391 [00:00<00:04, 74.49it/s] 15%|█▍        | 58/391 [00:01<00:04, 76.37it/s] 17%|█▋        | 67/391 [00:01<00:04, 79.73it/s] 20%|█▉        | 77/391 [00:01<00:03, 84.18it/s] 22%|██▏       | 87/391 [00:01<00:03, 85.70it/s] 25%|██▍       | 97/391 [00:01<00:03, 88.45it/s] 27%|██▋       | 107/391 [00:01<00:03, 90.64it/s] 30%|██▉       | 117/391 [00:01<00:03, 89.71it/s] 32%|███▏      | 127/391 [00:01<00:02, 89.86it/s] 35%|███▌      | 137/391 [00:01<00:02, 87.01it/s] 37%|███▋      | 146/391 [00:01<00:02, 87.05it/s] 40%|███▉      | 155/391 [00:02<00:02, 86.39it/s] 42%|████▏     | 164/391 [00:02<00:02, 86.30it/s] 45%|████▍     | 174/391 [00:02<00:02, 89.09it/s] 47%|████▋     | 184/391 [00:02<00:02, 90.66it/s] 50%|████▉     | 194/391 [00:02<00:02, 89.40it/s] 52%|█████▏    | 203/391 [00:02<00:02, 87.68it/s] 54%|█████▍    | 212/391 [00:02<00:02, 84.26it/s] 57%|█████▋    | 221/391 [00:02<00:02, 83.85it/s] 59%|█████▉    | 230/391 [00:02<00:01, 84.30it/s] 61%|██████▏   | 240/391 [00:03<00:01, 87.22it/s] 64%|██████▍   | 250/391 [00:03<00:01, 88.58it/s] 66%|██████▋   | 260/391 [00:03<00:01, 89.27it/s] 69%|██████▉   | 269/391 [00:03<00:01, 87.08it/s] 71%|███████   | 278/391 [00:03<00:01, 84.60it/s] 73%|███████▎  | 287/391 [00:03<00:01, 85.39it/s] 76%|███████▌  | 296/391 [00:03<00:01, 85.75it/s] 78%|███████▊  | 306/391 [00:03<00:00, 87.57it/s] 81%|████████  | 315/391 [00:03<00:00, 87.99it/s] 83%|████████▎ | 325/391 [00:04<00:00, 89.00it/s] 86%|████████▌ | 335/391 [00:04<00:00, 90.10it/s] 88%|████████▊ | 345/391 [00:04<00:00, 91.11it/s] 91%|█████████ | 355/391 [00:04<00:00, 91.82it/s] 93%|█████████▎| 365/391 [00:04<00:00, 90.43it/s] 96%|█████████▌| 375/391 [00:04<00:00, 91.47it/s] 98%|█████████▊| 385/391 [00:04<00:00, 91.96it/s]100%|██████████| 391/391 [00:04<00:00, 82.17it/s]
50000 images processed, 4.838550567626953 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:26,  2.93it/s] 11%|█▏        | 9/79 [00:00<00:02, 25.59it/s] 23%|██▎       | 18/79 [00:00<00:01, 44.58it/s] 35%|███▌      | 28/79 [00:00<00:00, 59.18it/s] 47%|████▋     | 37/79 [00:00<00:00, 66.64it/s] 58%|█████▊    | 46/79 [00:00<00:00, 73.18it/s] 70%|██████▉   | 55/79 [00:00<00:00, 77.34it/s] 81%|████████  | 64/79 [00:01<00:00, 80.02it/s] 94%|█████████▎| 74/79 [00:01<00:00, 84.12it/s]100%|██████████| 79/79 [00:01<00:00, 63.31it/s]
10000 images processed, 1.2769627571105957 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:21,  2.51it/s]  5%|▍         | 10/204 [00:00<00:07, 25.18it/s]  9%|▉         | 19/204 [00:00<00:04, 42.55it/s] 14%|█▎        | 28/204 [00:00<00:03, 54.24it/s] 18%|█▊        | 37/204 [00:00<00:02, 63.49it/s] 23%|██▎       | 47/204 [00:00<00:02, 71.60it/s] 28%|██▊       | 57/204 [00:01<00:01, 77.57it/s] 32%|███▏      | 66/204 [00:01<00:01, 80.87it/s] 37%|███▋      | 76/204 [00:01<00:01, 84.83it/s] 42%|████▏     | 85/204 [00:01<00:01, 82.64it/s] 46%|████▌     | 94/204 [00:01<00:01, 82.89it/s] 50%|█████     | 103/204 [00:01<00:01, 83.88it/s] 55%|█████▌    | 113/204 [00:01<00:01, 86.65it/s] 60%|█████▉    | 122/204 [00:01<00:00, 84.42it/s] 65%|██████▍   | 132/204 [00:01<00:00, 86.63it/s] 69%|██████▉   | 141/204 [00:02<00:00, 85.07it/s] 74%|███████▎  | 150/204 [00:02<00:00, 85.74it/s] 78%|███████▊  | 159/204 [00:02<00:00, 85.47it/s] 82%|████████▏ | 168/204 [00:02<00:00, 85.57it/s] 87%|████████▋ | 177/204 [00:02<00:00, 85.84it/s] 92%|█████████▏| 187/204 [00:02<00:00, 87.95it/s] 97%|█████████▋| 197/204 [00:02<00:00, 90.24it/s]100%|██████████| 204/204 [00:02<00:00, 74.85it/s]
26032 images processed, 2.7740654945373535 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:38,  2.01it/s]  8%|▊         | 6/79 [00:00<00:05, 12.58it/s] 13%|█▎        | 10/79 [00:00<00:03, 18.49it/s] 22%|██▏       | 17/79 [00:00<00:02, 29.58it/s] 32%|███▏      | 25/79 [00:01<00:01, 36.19it/s] 39%|███▉      | 31/79 [00:01<00:01, 40.86it/s] 46%|████▌     | 36/79 [00:01<00:01, 40.94it/s] 52%|█████▏    | 41/79 [00:01<00:00, 42.44it/s] 58%|█████▊    | 46/79 [00:01<00:00, 44.02it/s] 65%|██████▍   | 51/79 [00:01<00:00, 44.31it/s] 72%|███████▏  | 57/79 [00:01<00:00, 45.33it/s] 82%|████████▏ | 65/79 [00:01<00:00, 46.26it/s] 92%|█████████▏| 73/79 [00:02<00:00, 45.28it/s]100%|██████████| 79/79 [00:02<00:00, 37.54it/s]
10000 images processed, 2.1456711292266846 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:28,  2.70it/s] 11%|█▏        | 9/79 [00:00<00:02, 23.65it/s] 24%|██▍       | 19/79 [00:00<00:01, 44.29it/s] 35%|███▌      | 28/79 [00:00<00:00, 56.70it/s] 47%|████▋     | 37/79 [00:00<00:00, 64.22it/s] 58%|█████▊    | 46/79 [00:00<00:00, 69.55it/s] 70%|██████▉   | 55/79 [00:01<00:00, 74.77it/s] 81%|████████  | 64/79 [00:01<00:00, 76.11it/s] 94%|█████████▎| 74/79 [00:01<00:00, 81.99it/s]100%|██████████| 79/79 [00:01<00:00, 62.02it/s]
10000 images processed, 1.2939329147338867 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:25,  2.74it/s] 11%|█▏        | 8/70 [00:00<00:02, 21.62it/s] 24%|██▍       | 17/70 [00:00<00:01, 41.01it/s] 37%|███▋      | 26/70 [00:00<00:00, 53.99it/s] 50%|█████     | 35/70 [00:00<00:00, 63.71it/s] 64%|██████▍   | 45/70 [00:00<00:00, 72.53it/s] 77%|███████▋  | 54/70 [00:00<00:00, 76.24it/s] 91%|█████████▏| 64/70 [00:01<00:00, 82.27it/s]100%|██████████| 70/70 [00:01<00:00, 60.11it/s]
8925 images processed, 1.1977477073669434 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:40,  1.08it/s]  4%|▍         | 2/45 [00:01<00:19,  2.21it/s] 20%|██        | 9/45 [00:01<00:03,  9.67it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.52it/s] 31%|███       | 14/45 [00:01<00:02, 12.36it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.46it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.37it/s] 49%|████▉     | 22/45 [00:02<00:01, 13.07it/s] 56%|█████▌    | 25/45 [00:02<00:01, 15.06it/s] 60%|██████    | 27/45 [00:02<00:01, 11.52it/s] 67%|██████▋   | 30/45 [00:02<00:01, 12.85it/s] 73%|███████▎  | 33/45 [00:03<00:00, 12.69it/s] 78%|███████▊  | 35/45 [00:03<00:01,  9.80it/s] 91%|█████████ | 41/45 [00:03<00:00, 16.30it/s] 98%|█████████▊| 44/45 [00:04<00:00,  9.01it/s]100%|██████████| 45/45 [00:04<00:00, 10.17it/s]
5640 images processed, 4.44389009475708 seconds used

19.75870990753174
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.50  99.36
places365     67.90  80.57
LSUN          21.54  95.06
iSUN          72.28  81.40
dtd           38.17  91.27
AVG           40.48  89.53
Retain-Acc: 0.7433
Forget-as-OOD (retain known vs forget novel):
  FPR: 84.00 AUROC: 85.57 AUIN: 99.08
12.46263837814331
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1_rf.png
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] loaded adapter 'stage1' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] set_adapter(list) failed: unhashable type: 'list'; fallback to last adapter
[peft] active adapter set to: stage1
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-seen: Number of model parameters: 22082496
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:01<09:57,  1.53s/it]  3%|▎         | 10/391 [00:01<00:46,  8.25it/s]  5%|▌         | 20/391 [00:01<00:20, 18.10it/s]  8%|▊         | 30/391 [00:01<00:12, 28.88it/s] 10%|█         | 40/391 [00:01<00:08, 39.66it/s] 13%|█▎        | 49/391 [00:02<00:07, 48.65it/s] 15%|█▌        | 59/391 [00:02<00:05, 58.58it/s] 18%|█▊        | 69/391 [00:02<00:04, 67.09it/s] 20%|██        | 79/391 [00:02<00:05, 57.13it/s] 23%|██▎       | 89/391 [00:02<00:04, 64.81it/s] 25%|██▌       | 99/391 [00:02<00:04, 71.26it/s] 28%|██▊       | 108/391 [00:02<00:04, 66.41it/s] 30%|███       | 118/391 [00:02<00:03, 73.47it/s] 33%|███▎      | 128/391 [00:03<00:03, 79.18it/s] 35%|███▌      | 138/391 [00:03<00:03, 82.90it/s] 38%|███▊      | 148/391 [00:03<00:02, 86.58it/s] 40%|████      | 158/391 [00:03<00:02, 89.17it/s] 43%|████▎     | 168/391 [00:03<00:02, 90.27it/s] 46%|████▌     | 178/391 [00:03<00:02, 91.02it/s] 48%|████▊     | 188/391 [00:03<00:02, 92.61it/s] 51%|█████     | 198/391 [00:03<00:02, 93.25it/s] 53%|█████▎    | 208/391 [00:03<00:01, 93.33it/s] 56%|█████▌    | 218/391 [00:04<00:01, 93.42it/s] 58%|█████▊    | 228/391 [00:04<00:01, 94.19it/s] 61%|██████    | 238/391 [00:04<00:01, 94.73it/s] 63%|██████▎   | 248/391 [00:04<00:01, 93.11it/s] 66%|██████▌   | 258/391 [00:04<00:01, 91.81it/s] 69%|██████▊   | 268/391 [00:04<00:01, 93.20it/s] 71%|███████   | 278/391 [00:04<00:01, 93.92it/s] 74%|███████▎  | 288/391 [00:04<00:01, 94.69it/s] 76%|███████▌  | 298/391 [00:04<00:00, 93.41it/s] 79%|███████▉  | 308/391 [00:04<00:00, 93.86it/s] 81%|████████▏ | 318/391 [00:05<00:00, 94.44it/s] 84%|████████▍ | 328/391 [00:05<00:00, 95.01it/s] 86%|████████▋ | 338/391 [00:05<00:00, 92.37it/s] 89%|████████▉ | 348/391 [00:05<00:00, 92.57it/s] 92%|█████████▏| 358/391 [00:05<00:00, 92.30it/s] 94%|█████████▍| 368/391 [00:05<00:00, 91.82it/s] 97%|█████████▋| 378/391 [00:05<00:00, 92.45it/s] 99%|█████████▉| 388/391 [00:05<00:00, 93.84it/s]100%|██████████| 391/391 [00:05<00:00, 66.33it/s]
50000 images processed, 5.980361223220825 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:23,  3.29it/s] 13%|█▎        | 10/79 [00:00<00:02, 30.59it/s] 24%|██▍       | 19/79 [00:00<00:01, 48.53it/s] 37%|███▋      | 29/79 [00:00<00:00, 62.43it/s] 49%|████▉     | 39/79 [00:00<00:00, 72.54it/s] 62%|██████▏   | 49/79 [00:00<00:00, 78.95it/s] 75%|███████▍  | 59/79 [00:00<00:00, 82.93it/s] 87%|████████▋ | 69/79 [00:01<00:00, 86.79it/s]100%|██████████| 79/79 [00:01<00:00, 87.70it/s]100%|██████████| 79/79 [00:01<00:00, 68.19it/s]
10000 images processed, 1.1809403896331787 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:01,  3.29it/s]  2%|▏         | 5/204 [00:00<00:14, 13.73it/s]  7%|▋         | 15/204 [00:00<00:04, 38.40it/s] 12%|█▏        | 25/204 [00:00<00:03, 55.63it/s] 17%|█▋        | 35/204 [00:00<00:02, 67.55it/s] 22%|██▏       | 45/204 [00:00<00:02, 75.65it/s] 27%|██▋       | 55/204 [00:00<00:01, 81.60it/s] 31%|███▏      | 64/204 [00:01<00:01, 83.92it/s] 36%|███▋      | 74/204 [00:01<00:01, 86.78it/s] 41%|████      | 83/204 [00:01<00:01, 86.96it/s] 46%|████▌     | 93/204 [00:01<00:01, 89.34it/s] 50%|█████     | 103/204 [00:01<00:01, 90.84it/s] 55%|█████▌    | 113/204 [00:01<00:01, 87.03it/s] 60%|██████    | 123/204 [00:01<00:00, 88.69it/s] 65%|██████▍   | 132/204 [00:01<00:00, 88.70it/s] 70%|██████▉   | 142/204 [00:01<00:00, 89.61it/s] 75%|███████▍  | 152/204 [00:02<00:00, 86.96it/s] 79%|███████▉  | 161/204 [00:02<00:00, 87.69it/s] 84%|████████▍ | 171/204 [00:02<00:00, 88.67it/s] 88%|████████▊ | 180/204 [00:02<00:00, 88.56it/s] 93%|█████████▎| 189/204 [00:02<00:00, 88.33it/s] 98%|█████████▊| 199/204 [00:02<00:00, 90.72it/s]100%|██████████| 204/204 [00:02<00:00, 77.74it/s]
26032 images processed, 2.6721506118774414 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:35,  2.20it/s] 11%|█▏        | 9/79 [00:00<00:04, 17.19it/s] 22%|██▏       | 17/79 [00:00<00:02, 28.33it/s] 32%|███▏      | 25/79 [00:00<00:01, 32.69it/s] 42%|████▏     | 33/79 [00:01<00:01, 35.33it/s] 52%|█████▏    | 41/79 [00:01<00:00, 41.19it/s] 62%|██████▏   | 49/79 [00:01<00:00, 43.83it/s] 72%|███████▏  | 57/79 [00:01<00:00, 45.45it/s] 82%|████████▏ | 65/79 [00:01<00:00, 45.74it/s] 92%|█████████▏| 73/79 [00:01<00:00, 47.53it/s]100%|██████████| 79/79 [00:02<00:00, 39.05it/s]
10000 images processed, 2.0601439476013184 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.04it/s] 13%|█▎        | 10/79 [00:00<00:02, 28.63it/s] 25%|██▌       | 20/79 [00:00<00:01, 49.03it/s] 38%|███▊      | 30/79 [00:00<00:00, 61.94it/s] 51%|█████     | 40/79 [00:00<00:00, 71.85it/s] 63%|██████▎   | 50/79 [00:00<00:00, 78.04it/s] 76%|███████▌  | 60/79 [00:00<00:00, 83.27it/s] 89%|████████▊ | 70/79 [00:01<00:00, 86.96it/s]100%|██████████| 79/79 [00:01<00:00, 67.89it/s]
10000 images processed, 1.185227870941162 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:21,  3.17it/s] 13%|█▎        | 9/70 [00:00<00:02, 27.10it/s] 27%|██▋       | 19/70 [00:00<00:01, 48.72it/s] 41%|████▏     | 29/70 [00:00<00:00, 62.73it/s] 54%|█████▍    | 38/70 [00:00<00:00, 69.53it/s] 67%|██████▋   | 47/70 [00:00<00:00, 75.20it/s] 81%|████████▏ | 57/70 [00:00<00:00, 80.61it/s] 96%|█████████▌| 67/70 [00:01<00:00, 85.46it/s]100%|██████████| 70/70 [00:01<00:00, 64.40it/s]
8925 images processed, 1.1255733966827393 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:36,  1.22it/s]  4%|▍         | 2/45 [00:01<00:20,  2.11it/s] 20%|██        | 9/45 [00:01<00:03, 11.47it/s] 27%|██▋       | 12/45 [00:01<00:03,  8.58it/s] 40%|████      | 18/45 [00:02<00:02, 10.46it/s] 53%|█████▎    | 24/45 [00:02<00:01, 14.25it/s] 58%|█████▊    | 26/45 [00:02<00:01, 11.80it/s] 71%|███████   | 32/45 [00:02<00:00, 17.16it/s] 78%|███████▊  | 35/45 [00:03<00:00, 11.38it/s] 93%|█████████▎| 42/45 [00:04<00:00, 10.80it/s]100%|██████████| 45/45 [00:04<00:00, 10.96it/s]
5640 images processed, 4.128337860107422 seconds used

19.990813970565796
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.50  99.36
places365     67.90  80.57
LSUN          21.54  95.06
iSUN          72.28  81.40
dtd           38.17  91.27
AVG           40.48  89.53
Retain-Acc: 0.7433
Forget-as-OOD (retain known vs forget novel):
  FPR: 84.00 AUROC: 85.57 AUIN: 99.08
11.619314193725586
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-seen_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-seen_rf.png
==== Stage 2: forget_inc={66,67,88,94,57}; forget_seen={0,8,11,40,51}; all={0,8,11,40,51,66,67,88,94,57} ====
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2', adapter_load_path=None, adapter_load_paths='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1', lora_new_adapter_name='train_s2', lora_stack=True, lora_orth_enable=True, lora_orth_lambda=0.1, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc='66,67,88,94,57', forget_classes_seen='0,8,11,40,51', forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] loaded adapter 'stage1' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] added trainable adapter 'train_s2'
[peft] set_adapter(list) failed: unhashable type: 'list'; fallback to last adapter
[peft] active adapter set to: train_s2
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 22321088
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.train_s2.weight
[debug] trainable: base_model.model.head.2.lora_A.train_s2.weight
[debug] trainable: base_model.model.head.2.lora_B.train_s2.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.train_s2.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.train_s2.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.train_s2.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.train_s2.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.train_s2.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.train_s2.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.train_s2.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.train_s2.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.train_s2.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.train_s2.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [01:43<1:24:45, 103.79s/it]  4%|▍         | 2/50 [02:17<50:07, 62.65s/it]     6%|▌         | 3/50 [02:50<38:15, 48.84s/it]  8%|▊         | 4/50 [03:24<33:01, 43.08s/it] 10%|█         | 5/50 [03:56<29:23, 39.18s/it] 12%|█▏        | 6/50 [04:31<27:39, 37.72s/it] 14%|█▍        | 7/50 [05:04<25:56, 36.20s/it][loss] ep 0 it 0 total=8523175.0000 mle=2.0445 pcon=5.2951 forget=1.4856 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=8506027.0000 mle=1.7287 pcon=5.2899 forget=1.4724 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=8436203.0000 mle=1.9454 pcon=5.2851 forget=1.4544 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8375109.0000 mle=2.0301 pcon=5.2798 forget=1.4858 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=8342033.0000 mle=2.1181 pcon=5.2745 forget=1.3810 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=8327343.5000 mle=2.0962 pcon=5.2692 forget=1.4102 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=8320638.0000 mle=2.0622 pcon=5.2639 forget=1.4102 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=8317102.5000 mle=2.0566 pcon=5.2586 forget=1.3846 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 1 it 10 total=8314878.5000 mle=2.0375 pcon=5.2533 forget=1.4046 favg=0.0000 nr=23 nf=23 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=8313256.5000 mle=1.6598 pcon=5.2482 forget=1.4196 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=8311960.5000 mle=2.2422 pcon=5.2429 forget=1.3759 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=8310916.5000 mle=1.8689 pcon=5.2377 forget=1.3814 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=8309968.0000 mle=1.8683 pcon=5.2323 forget=1.3932 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=8309153.5000 mle=1.8656 pcon=5.2268 forget=1.3875 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=8308480.0000 mle=1.9684 pcon=5.2210 forget=1.4024 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=8307868.5000 mle=2.2517 pcon=5.2153 forget=1.4531 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 2 it 20 total=8307382.0000 mle=1.5508 pcon=5.2099 forget=1.4049 favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=8306960.5000 mle=2.0724 pcon=5.2045 forget=1.3826 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=8306599.5000 mle=2.1226 pcon=5.1986 forget=1.4248 favg=0.0000 nr=24 nf=24 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=8306300.5000 mle=1.6007 pcon=5.1935 forget=1.3608 favg=0.0000 nr=40 nf=40 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=8306053.5000 mle=1.9072 pcon=5.1881 forget=1.4261 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=8305855.5000 mle=2.4824 pcon=5.1827 forget=1.3630 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=8305718.0000 mle=1.7916 pcon=5.1771 forget=1.3712 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=8305589.5000 mle=1.7731 pcon=5.1717 forget=1.3744 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 3 it 30 total=8305480.5000 mle=1.9260 pcon=5.1663 forget=1.3804 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=8305392.5000 mle=1.7828 pcon=5.1607 forget=1.3337 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=8305313.5000 mle=1.9606 pcon=5.1555 forget=1.3714 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=8305264.0000 mle=2.0522 pcon=5.1505 forget=1.3961 favg=0.0000 nr=23 nf=23 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=8305224.0000 mle=2.6915 pcon=5.1452 forget=1.3818 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=8305200.5000 mle=2.0986 pcon=5.1400 forget=1.3545 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=8305155.0000 mle=1.9882 pcon=5.1345 forget=1.3846 favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=8305148.0000 mle=2.0043 pcon=5.1296 forget=1.3487 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 4 it 40 total=8305130.5000 mle=2.3214 pcon=5.1247 forget=1.3931 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=8305114.5000 mle=2.2296 pcon=5.1201 forget=1.3803 favg=0.0000 nr=23 nf=23 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8305103.5000 mle=2.2765 pcon=5.1154 forget=1.4064 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=8305092.0000 mle=2.1079 pcon=5.1107 forget=1.3768 favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=8305084.0000 mle=1.9904 pcon=5.1064 forget=1.3645 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=8305062.0000 mle=2.3050 pcon=5.1020 forget=1.3576 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=8305072.5000 mle=2.5793 pcon=5.0979 forget=1.4225 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 5 it 0 total=8305058.0000 mle=2.5879 pcon=5.0936 forget=1.4189 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=8305077.0000 mle=2.3400 pcon=5.0899 forget=1.3763 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=8305060.0000 mle=2.7395 pcon=5.0862 forget=1.4148 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=8305078.0000 mle=2.6163 pcon=5.0822 forget=1.4145 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=8305055.0000 mle=2.1553 pcon=5.0787 forget=1.3913 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=8305067.5000 mle=2.3037 pcon=5.0751 forget=1.4001 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=8305058.5000 mle=2.2228 pcon=5.0715 forget=1.4269 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=8305070.0000 mle=2.6700 pcon=5.0678 forget=1.4170 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 6 it 10 total=8305059.0000 mle=2.1789 pcon=5.0647 forget=1.3818 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=8305053.5000 mle=2.9322 pcon=5.0611 forget=1.4073 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=8305046.0000 mle=2.5881 pcon=5.0575 forget=1.4306 favg=0.0000 nr=24 nf=24 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=8305058.0000 mle=2.3862 pcon=5.0538 forget=1.4224 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=8305049.5000 mle=2.8303 pcon=5.0506 forget=1.4189 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=8305049.5000 mle=3.1770 pcon=5.0473 forget=1.4224 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=8305054.5000 mle=2.2223 pcon=5.0441 forget=1.3930 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=8305037.0000 mle=2.5341 pcon=5.0407 forget=1.4415 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 7 it 20 total=8305037.0000 mle=2.2384 pcon=5.0377 forget=1.3935 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
 16%|█▌        | 8/50 [05:38<24:55, 35.62s/it] 18%|█▊        | 9/50 [06:13<24:02, 35.17s/it] 20%|██        | 10/50 [06:45<22:58, 34.46s/it] 22%|██▏       | 11/50 [07:21<22:33, 34.71s/it] 24%|██▍       | 12/50 [07:56<22:05, 34.88s/it] 26%|██▌       | 13/50 [08:28<20:58, 34.02s/it] 28%|██▊       | 14/50 [08:59<19:46, 32.95s/it][loss] ep 7 it 70 total=8305053.5000 mle=3.1769 pcon=5.0347 forget=1.4132 favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=8305057.0000 mle=2.9859 pcon=5.0319 forget=1.4848 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=8305027.5000 mle=2.4404 pcon=5.0291 forget=1.4301 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=8305028.0000 mle=2.2144 pcon=5.0262 forget=1.4250 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=8305033.0000 mle=2.8065 pcon=5.0231 forget=1.4359 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=8305021.0000 mle=3.0180 pcon=5.0200 forget=1.4500 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=8305041.0000 mle=2.7499 pcon=5.0169 forget=1.4100 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 8 it 30 total=8305029.0000 mle=2.7299 pcon=5.0141 forget=1.4209 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=8305024.5000 mle=3.6068 pcon=5.0115 forget=1.4106 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=8305037.0000 mle=2.9429 pcon=5.0088 forget=1.4656 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=8305025.0000 mle=3.0626 pcon=5.0064 forget=1.4304 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=8305017.5000 mle=3.2997 pcon=5.0040 forget=1.4533 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=8305034.0000 mle=3.5661 pcon=5.0015 forget=1.5129 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=8305023.0000 mle=2.9818 pcon=4.9992 forget=1.4252 favg=0.0000 nr=41 nf=41 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=8305014.0000 mle=3.3905 pcon=4.9969 forget=1.4714 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 9 it 40 total=8305020.5000 mle=4.8770 pcon=4.9950 forget=1.5881 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=8305039.0000 mle=3.6952 pcon=4.9934 forget=1.4737 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=8305049.5000 mle=3.4974 pcon=4.9916 forget=1.4589 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=8305020.5000 mle=3.3938 pcon=4.9898 forget=1.5400 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=8305025.0000 mle=3.9405 pcon=4.9878 forget=1.5003 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=8305029.5000 mle=3.4231 pcon=4.9859 forget=1.4454 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=8305023.0000 mle=3.2904 pcon=4.9839 forget=1.5199 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 10 it 0 total=8305015.0000 mle=3.4420 pcon=4.9815 forget=1.4274 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=8305033.0000 mle=2.9119 pcon=4.9791 forget=1.4657 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=8305030.0000 mle=3.7644 pcon=4.9770 forget=1.4115 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=8305026.0000 mle=3.4704 pcon=4.9750 forget=1.4571 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=8305022.0000 mle=3.6398 pcon=4.9733 forget=1.5246 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=8305026.5000 mle=3.1171 pcon=4.9713 forget=1.4962 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=8305027.5000 mle=3.3543 pcon=4.9694 forget=1.5180 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=8305042.0000 mle=3.2866 pcon=4.9672 forget=1.5047 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 11 it 10 total=8305043.0000 mle=2.8938 pcon=4.9649 forget=1.4893 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=8305014.5000 mle=3.0256 pcon=4.9625 forget=1.5124 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=8305015.0000 mle=3.4334 pcon=4.9601 forget=1.5107 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=8305011.0000 mle=3.6747 pcon=4.9576 forget=1.4542 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=8305038.0000 mle=3.9801 pcon=4.9554 forget=1.4310 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=8305038.0000 mle=3.8648 pcon=4.9531 forget=1.4197 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=8305022.0000 mle=3.4365 pcon=4.9506 forget=1.4392 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=8305021.0000 mle=3.1286 pcon=4.9480 forget=1.5346 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 12 it 20 total=8305025.5000 mle=3.0915 pcon=4.9455 forget=1.4915 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=8305027.0000 mle=3.2879 pcon=4.9432 forget=1.5018 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=8305020.0000 mle=3.2421 pcon=4.9408 forget=1.4543 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=8305025.0000 mle=3.1748 pcon=4.9386 forget=1.5036 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=8305022.0000 mle=3.4074 pcon=4.9361 forget=1.4804 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=8305025.0000 mle=2.6439 pcon=4.9337 forget=1.5142 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=8305025.5000 mle=2.9895 pcon=4.9312 forget=1.4725 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=8305025.5000 mle=3.3576 pcon=4.9288 forget=1.4509 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 13 it 30 total=8305039.0000 mle=3.0933 pcon=4.9268 forget=1.4581 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=8305038.5000 mle=3.2100 pcon=4.9251 forget=1.4616 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=8305023.5000 mle=2.7575 pcon=4.9231 forget=1.4944 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=8305029.5000 mle=3.0964 pcon=4.9212 forget=1.4392 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=8305029.5000 mle=3.0333 pcon=4.9192 forget=1.4437 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=8305031.0000 mle=3.4402 pcon=4.9171 forget=1.4837 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=8305016.5000 mle=3.5528 pcon=4.9151 forget=1.4592 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=8305026.5000 mle=2.7139 pcon=4.9129 forget=1.3975 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 14 it 40 total=8305041.0000 mle=3.9520 pcon=4.9108 forget=1.5840 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=8305029.0000 mle=2.7631 pcon=4.9094 forget=1.4937 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=8305024.5000 mle=2.7040 pcon=4.9076 forget=1.4911 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
 30%|███       | 15/50 [09:29<18:42, 32.08s/it] 32%|███▏      | 16/50 [10:07<19:16, 34.01s/it] 34%|███▍      | 17/50 [10:41<18:41, 33.98s/it] 36%|███▌      | 18/50 [11:14<17:59, 33.75s/it] 38%|███▊      | 19/50 [11:48<17:23, 33.66s/it] 40%|████      | 20/50 [12:20<16:40, 33.35s/it] 42%|████▏     | 21/50 [12:54<16:13, 33.56s/it] 44%|████▍     | 22/50 [13:30<15:59, 34.27s/it][loss] ep 14 it 190 total=8305023.0000 mle=3.1779 pcon=4.9056 forget=1.4444 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=8305022.0000 mle=3.5953 pcon=4.9036 forget=1.4521 favg=0.0000 nr=23 nf=23 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=8305027.5000 mle=3.5684 pcon=4.9019 forget=1.5040 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=8305017.5000 mle=3.0389 pcon=4.9001 forget=1.4773 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 15 it 0 total=8305029.5000 mle=3.0152 pcon=4.8983 forget=1.4780 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=8305028.0000 mle=3.0607 pcon=4.8965 forget=1.5076 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=8305031.0000 mle=3.1554 pcon=4.8946 forget=1.4600 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=8305007.0000 mle=2.9587 pcon=4.8928 forget=1.4633 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=8305021.0000 mle=3.0012 pcon=4.8910 forget=1.4474 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=8305012.5000 mle=2.7840 pcon=4.8891 forget=1.4917 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=8305031.0000 mle=2.9517 pcon=4.8872 forget=1.4312 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=8305021.0000 mle=2.8160 pcon=4.8857 forget=1.4610 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 16 it 10 total=8305033.0000 mle=2.9514 pcon=4.8840 forget=1.4979 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=8305027.0000 mle=3.2427 pcon=4.8820 forget=1.4803 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=8305034.5000 mle=2.9355 pcon=4.8802 forget=1.4456 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=8305025.5000 mle=3.2022 pcon=4.8784 forget=1.4561 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=8305014.0000 mle=2.7315 pcon=4.8767 forget=1.5256 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=8305029.5000 mle=3.2572 pcon=4.8752 forget=1.4823 favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=8305029.5000 mle=3.0304 pcon=4.8736 forget=1.4397 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=8305021.5000 mle=3.3390 pcon=4.8722 forget=1.5057 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 17 it 20 total=8305018.5000 mle=2.7659 pcon=4.8707 forget=1.4193 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=8305011.0000 mle=3.1659 pcon=4.8689 forget=1.5175 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=8305037.0000 mle=2.9657 pcon=4.8670 forget=1.4567 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=8305009.0000 mle=2.9081 pcon=4.8651 forget=1.4100 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=8305024.5000 mle=2.8019 pcon=4.8632 forget=1.4316 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=8305024.5000 mle=3.7259 pcon=4.8614 forget=1.4777 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=8305024.5000 mle=2.4990 pcon=4.8596 forget=1.4534 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=8305032.0000 mle=3.1025 pcon=4.8579 forget=1.4338 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=8305019.5000 mle=2.7872 pcon=4.8561 forget=1.4515 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=8305026.5000 mle=2.6137 pcon=4.8542 forget=1.4596 favg=0.0000 nr=41 nf=41 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=8305038.5000 mle=2.6554 pcon=4.8523 forget=1.3795 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=8305022.5000 mle=2.9611 pcon=4.8504 forget=1.5147 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=8305022.0000 mle=3.4015 pcon=4.8486 forget=1.4537 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=8305022.5000 mle=2.8892 pcon=4.8469 forget=1.4686 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=8305025.0000 mle=3.0351 pcon=4.8451 forget=1.4782 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=8305016.5000 mle=2.6152 pcon=4.8432 forget=1.4425 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 19 it 40 total=8305029.0000 mle=2.4889 pcon=4.8412 forget=1.4514 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=8305027.0000 mle=3.0904 pcon=4.8394 forget=1.4197 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=8305014.5000 mle=2.7583 pcon=4.8375 forget=1.4670 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=8305022.0000 mle=2.4634 pcon=4.8357 forget=1.4916 favg=0.0000 nr=24 nf=24 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=8305030.5000 mle=2.5647 pcon=4.8339 forget=1.4138 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=8305026.5000 mle=2.9920 pcon=4.8320 forget=1.4061 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=8305030.5000 mle=2.7943 pcon=4.8300 forget=1.3965 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 20 it 0 total=8305024.0000 mle=2.4120 pcon=4.8281 forget=1.4341 favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=8305031.5000 mle=2.7433 pcon=4.8262 forget=1.4305 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=8305026.0000 mle=2.8789 pcon=4.8243 forget=1.4572 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=8305024.0000 mle=2.9571 pcon=4.8223 forget=1.4438 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=8305014.5000 mle=2.8876 pcon=4.8204 forget=1.4360 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=8305028.0000 mle=2.2624 pcon=4.8183 forget=1.3926 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=8305012.5000 mle=2.7046 pcon=4.8164 forget=1.4209 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=8305028.5000 mle=2.5889 pcon=4.8146 forget=1.3833 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 21 it 10 total=8305042.5000 mle=2.6772 pcon=4.8127 forget=1.4400 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=8305001.0000 mle=2.5300 pcon=4.8109 forget=1.4075 favg=0.0000 nr=40 nf=40 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=8305036.5000 mle=2.4586 pcon=4.8089 forget=1.4187 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=8305026.5000 mle=2.5422 pcon=4.8070 forget=1.3752 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=8305017.0000 mle=2.5831 pcon=4.8054 forget=1.3884 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=8305014.0000 mle=2.6530 pcon=4.8037 forget=1.4466 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=8305021.0000 mle=3.1405 pcon=4.8020 forget=1.3714 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=8305022.5000 mle=2.5627 pcon=4.8003 forget=1.4266 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 22 it 20 total=8305034.5000 mle=2.7350 pcon=4.7987 forget=1.4572 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=8305034.0000 mle=2.9256 pcon=4.7971 forget=1.4678 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=8305006.5000 mle=2.5914 pcon=4.7955 forget=1.4669 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=8305029.5000 mle=3.1722 pcon=4.7939 forget=1.4385 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA 46%|████▌     | 23/50 [14:03<15:11, 33.75s/it] 48%|████▊     | 24/50 [14:35<14:26, 33.32s/it] 50%|█████     | 25/50 [15:07<13:44, 32.98s/it] 52%|█████▏    | 26/50 [15:45<13:43, 34.32s/it] 54%|█████▍    | 27/50 [16:20<13:18, 34.73s/it] 56%|█████▌    | 28/50 [16:56<12:48, 34.93s/it] 58%|█████▊    | 29/50 [17:29<12:00, 34.30s/it]
[loss] ep 22 it 220 total=8305024.5000 mle=2.3558 pcon=4.7922 forget=1.4038 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=8305022.5000 mle=2.5895 pcon=4.7904 forget=1.4092 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=8305017.0000 mle=2.5927 pcon=4.7887 forget=1.3864 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=8305014.0000 mle=2.8433 pcon=4.7872 forget=1.4188 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 23 it 30 total=8305014.0000 mle=2.4947 pcon=4.7856 forget=1.4119 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=8305038.0000 mle=2.8939 pcon=4.7841 forget=1.4566 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=8305033.0000 mle=2.6926 pcon=4.7827 forget=1.3919 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=8305040.5000 mle=2.4217 pcon=4.7811 forget=1.3810 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=8305024.5000 mle=2.4716 pcon=4.7793 forget=1.4319 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=8305018.5000 mle=2.5932 pcon=4.7774 forget=1.3994 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=8305019.0000 mle=2.4743 pcon=4.7757 forget=1.3689 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=8305009.5000 mle=2.4093 pcon=4.7740 forget=1.3900 favg=0.0000 nr=43 nf=43 protos=540 fproto_sim=NA
[loss] ep 24 it 40 total=8305013.5000 mle=2.2331 pcon=4.7723 forget=1.4295 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=8305007.5000 mle=2.4871 pcon=4.7705 forget=1.4078 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=8305024.5000 mle=2.3291 pcon=4.7686 forget=1.3844 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=8305033.0000 mle=2.5239 pcon=4.7668 forget=1.4069 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 24 it 240 total=8305024.5000 mle=2.2817 pcon=4.7650 forget=1.4207 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=8305031.0000 mle=2.4562 pcon=4.7634 forget=1.3914 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=8305027.0000 mle=2.1448 pcon=4.7617 forget=1.4046 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 25 it 0 total=8305025.0000 mle=2.4942 pcon=4.7601 forget=1.3965 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=8305010.0000 mle=2.8218 pcon=4.7584 forget=1.3790 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=8305016.0000 mle=2.0978 pcon=4.7566 forget=1.4008 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 25 it 150 total=8305018.5000 mle=2.7537 pcon=4.7550 forget=1.4107 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 25 it 200 total=8305022.0000 mle=2.1129 pcon=4.7533 forget=1.3913 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=8305019.0000 mle=2.2940 pcon=4.7517 forget=1.3860 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=8305018.0000 mle=2.8190 pcon=4.7501 forget=1.3812 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=8305024.0000 mle=2.1195 pcon=4.7486 forget=1.3797 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 26 it 10 total=8305036.0000 mle=2.1355 pcon=4.7470 forget=1.3700 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=8305027.0000 mle=2.3165 pcon=4.7453 forget=1.3551 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=8305026.0000 mle=2.1727 pcon=4.7438 forget=1.3502 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=8305022.5000 mle=1.9051 pcon=4.7421 forget=1.3714 favg=0.0000 nr=23 nf=23 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=8305024.5000 mle=2.0942 pcon=4.7407 forget=1.3557 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=8305033.5000 mle=2.1653 pcon=4.7392 forget=1.3801 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 26 it 310 total=8305031.5000 mle=1.9442 pcon=4.7379 forget=1.3874 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=8305012.5000 mle=2.1621 pcon=4.7365 forget=1.3772 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 27 it 20 total=8305024.5000 mle=2.2283 pcon=4.7351 forget=1.3694 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=8305016.0000 mle=2.0192 pcon=4.7339 forget=1.3462 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=8305032.0000 mle=1.9997 pcon=4.7326 forget=1.4023 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=8305037.5000 mle=2.2776 pcon=4.7313 forget=1.4236 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=8305020.0000 mle=2.3362 pcon=4.7300 forget=1.3345 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=8305017.5000 mle=1.9937 pcon=4.7287 forget=1.3612 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=8305015.5000 mle=1.9791 pcon=4.7274 forget=1.3826 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=8305028.0000 mle=2.0977 pcon=4.7261 forget=1.3360 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 28 it 30 total=8305021.5000 mle=2.4955 pcon=4.7248 forget=1.3311 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=8305022.0000 mle=2.3567 pcon=4.7235 forget=1.3222 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=8304999.0000 mle=2.2219 pcon=4.7223 forget=1.3390 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=8305015.0000 mle=2.2251 pcon=4.7209 forget=1.3396 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=8305020.5000 mle=2.0826 pcon=4.7198 forget=1.3643 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=8305006.0000 mle=2.4162 pcon=4.7187 forget=1.3937 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=8305015.0000 mle=2.1063 pcon=4.7178 forget=1.3854 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=8305028.5000 mle=2.3542 pcon=4.7167 forget=1.3885 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 29 it 40 total=8305018.0000 mle=2.5024 pcon=4.7156 forget=1.3291 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=8305009.5000 mle=2.1671 pcon=4.7144 forget=1.3411 favg=0.0000 nr=40 nf=40 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=8305018.0000 mle=2.3022 pcon=4.7132 forget=1.3036 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=8305008.5000 mle=2.0472 pcon=4.7120 forget=1.3273 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=8305013.5000 mle=1.7660 pcon=4.7109 forget=1.2781 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=8304994.5000 mle=1.9412 pcon=4.7097 forget=1.2916 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=8305004.0000 mle=2.2589 pcon=4.7087 forget=1.2895 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
 60%|██████    | 30/50 [18:01<11:15, 33.79s/it] 62%|██████▏   | 31/50 [18:33<10:32, 33.31s/it] 64%|██████▍   | 32/50 [19:05<09:48, 32.70s/it] 66%|██████▌   | 33/50 [19:36<09:06, 32.16s/it] 68%|██████▊   | 34/50 [20:08<08:33, 32.12s/it] 70%|███████   | 35/50 [20:41<08:06, 32.45s/it] 72%|███████▏  | 36/50 [21:21<08:07, 34.83s/it][peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 30 it 0 total=8305013.5000 mle=2.3459 pcon=4.7075 forget=1.2835 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=8305003.5000 mle=1.9496 pcon=4.7065 forget=1.2837 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=8305001.5000 mle=2.1745 pcon=4.7054 forget=1.2440 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=8305013.5000 mle=2.4915 pcon=4.7042 forget=1.2584 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=8305008.0000 mle=2.1854 pcon=4.7030 forget=1.3019 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=8304996.0000 mle=1.9552 pcon=4.7018 forget=1.2805 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=8305013.5000 mle=1.8747 pcon=4.7006 forget=1.3095 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=8305001.5000 mle=2.6224 pcon=4.6994 forget=1.2338 favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 31 it 10 total=8305011.0000 mle=2.2183 pcon=4.6983 forget=1.2702 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=8305017.5000 mle=2.3021 pcon=4.6972 forget=1.2640 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=8305022.0000 mle=2.4052 pcon=4.6961 forget=1.3335 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=8305014.0000 mle=2.0973 pcon=4.6951 forget=1.2630 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=8305012.0000 mle=1.9627 pcon=4.6940 forget=1.2796 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 31 it 260 total=8305009.0000 mle=2.0311 pcon=4.6930 forget=1.2286 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=8305013.5000 mle=2.3947 pcon=4.6921 forget=1.2332 favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=8305006.0000 mle=2.1569 pcon=4.6910 forget=1.2919 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 32 it 20 total=8305005.5000 mle=2.0321 pcon=4.6901 forget=1.3041 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=8304993.5000 mle=2.1784 pcon=4.6890 forget=1.2791 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=8305005.0000 mle=1.8656 pcon=4.6880 forget=1.3157 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 32 it 170 total=8305000.0000 mle=2.1706 pcon=4.6870 forget=1.3093 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=8305014.5000 mle=2.0021 pcon=4.6859 forget=1.2522 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=8305012.0000 mle=2.1766 pcon=4.6849 forget=1.2206 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=8305004.0000 mle=2.0953 pcon=4.6839 forget=1.2228 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=8304985.0000 mle=1.9449 pcon=4.6829 forget=1.2820 favg=0.0000 nr=41 nf=41 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 33 it 30 total=8305004.5000 mle=2.0377 pcon=4.6820 forget=1.2363 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=8304989.5000 mle=2.2793 pcon=4.6809 forget=1.2361 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=8305000.0000 mle=1.9535 pcon=4.6800 forget=1.2525 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=8304993.5000 mle=2.2737 pcon=4.6789 forget=1.2710 favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=8305005.0000 mle=1.7921 pcon=4.6781 forget=1.1937 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=8304994.0000 mle=1.9940 pcon=4.6773 forget=1.2496 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=8304987.5000 mle=1.9978 pcon=4.6762 forget=1.2202 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 33 it 380 total=8304997.5000 mle=2.1626 pcon=4.6753 forget=1.2089 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 34 it 40 total=8304977.5000 mle=2.3727 pcon=4.6745 forget=1.2239 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=8304983.0000 mle=2.0502 pcon=4.6735 forget=1.1998 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 34 it 140 total=8304992.5000 mle=1.9215 pcon=4.6725 forget=1.2170 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=8304996.0000 mle=2.2252 pcon=4.6716 forget=1.2442 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 34 it 240 total=8304979.0000 mle=2.1888 pcon=4.6706 forget=1.2507 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=8304984.0000 mle=1.9324 pcon=4.6698 forget=1.2768 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=8304997.0000 mle=1.8907 pcon=4.6690 forget=1.2428 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 35 it 0 total=8304985.5000 mle=2.0042 pcon=4.6682 forget=1.2939 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 35 it 50 total=8304988.0000 mle=1.9643 pcon=4.6674 forget=1.2713 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 35 it 100 total=8304978.0000 mle=2.0049 pcon=4.6666 forget=1.2313 favg=0.0000 nr=21 nf=21 protos=540 fproto_sim=NA
[loss] ep 35 it 150 total=8304988.5000 mle=1.8228 pcon=4.6658 forget=1.2280 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 35 it 200 total=8304991.5000 mle=1.9362 pcon=4.6649 forget=1.2601 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 35 it 250 total=8304963.5000 mle=2.1362 pcon=4.6641 forget=1.2496 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 35 it 300 total=8304969.5000 mle=2.0094 pcon=4.6634 forget=1.2656 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 35 it 350 total=8304970.5000 mle=2.0454 pcon=4.6626 forget=1.2541 favg=0.0000 nr=43 nf=43 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 36 it 10 total=8304976.5000 mle=1.8800 pcon=4.6618 forget=1.2252 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 36 it 60 total=8304974.0000 mle=2.0311 pcon=4.6610 forget=1.2494 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 36 it 110 total=8304969.0000 mle=2.1731 pcon=4.6602 forget=1.1990 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 36 it 160 total=8304965.5000 mle=2.0698 pcon=4.6594 forget=1.2572 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 36 it 210 total=8304955.5000 mle=2.0033 pcon=4.6586 forget=1.2253 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 36 it 260 total=8304949.5000 mle=2.4137 pcon=4.6578 forget=1.2223 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 36 it 310 total=8304962.0000 mle=1.9338 pcon=4.6571 forget=1.2333 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 36 it 360 total=8304955.5000 mle=1.8855 pcon=4.6564 forget=1.1969 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
 74%|███████▍  | 37/50 [22:06<08:12, 37.91s/it] 76%|███████▌  | 38/50 [22:53<08:06, 40.53s/it] 78%|███████▊  | 39/50 [23:41<07:49, 42.72s/it] 80%|████████  | 40/50 [24:45<08:11, 49.15s/it] 82%|████████▏ | 41/50 [26:02<08:37, 57.54s/it] 84%|████████▍ | 42/50 [27:16<08:18, 62.30s/it] 86%|████████▌ | 43/50 [28:25<07:31, 64.50s/it][peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 37 it 20 total=8304962.0000 mle=1.7802 pcon=4.6557 forget=1.2239 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 37 it 70 total=8304945.0000 mle=2.0924 pcon=4.6549 forget=1.2041 favg=0.0000 nr=24 nf=24 protos=540 fproto_sim=NA
[loss] ep 37 it 120 total=8304971.0000 mle=1.9744 pcon=4.6541 forget=1.1818 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 37 it 170 total=8304935.5000 mle=1.8443 pcon=4.6534 forget=1.2807 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 37 it 220 total=8304957.0000 mle=2.0481 pcon=4.6527 forget=1.2699 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 37 it 270 total=8304935.5000 mle=1.9429 pcon=4.6519 forget=1.2527 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 37 it 320 total=8304953.5000 mle=2.0123 pcon=4.6512 forget=1.2734 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 37 it 370 total=8304933.5000 mle=2.0043 pcon=4.6504 forget=1.3245 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 38 it 30 total=8304935.5000 mle=1.9516 pcon=4.6497 forget=1.2532 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 38 it 80 total=8304917.5000 mle=1.9814 pcon=4.6491 forget=1.2803 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 38 it 130 total=8304932.5000 mle=2.1065 pcon=4.6484 forget=1.2900 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 38 it 180 total=8304936.0000 mle=2.0202 pcon=4.6477 forget=1.2600 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 38 it 230 total=8304930.5000 mle=1.8981 pcon=4.6470 forget=1.2601 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 38 it 280 total=8304932.0000 mle=2.1935 pcon=4.6464 forget=1.2762 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 38 it 330 total=8304931.5000 mle=1.9459 pcon=4.6457 forget=1.2673 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 38 it 380 total=8304938.5000 mle=2.5330 pcon=4.6451 forget=1.2269 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 39 it 40 total=8304909.5000 mle=2.0163 pcon=4.6445 forget=1.2531 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 39 it 90 total=8304908.0000 mle=1.9559 pcon=4.6438 forget=1.2895 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 39 it 140 total=8304912.0000 mle=1.8395 pcon=4.6432 forget=1.2796 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 39 it 190 total=8304895.0000 mle=2.1571 pcon=4.6426 forget=1.2700 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 39 it 240 total=8304905.0000 mle=2.0493 pcon=4.6421 forget=1.2936 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 39 it 290 total=8304912.0000 mle=2.1833 pcon=4.6414 forget=1.2946 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 39 it 340 total=8304901.5000 mle=1.9424 pcon=4.6409 forget=1.2615 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 40 it 0 total=8304885.5000 mle=1.9316 pcon=4.6402 forget=1.2524 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 40 it 50 total=8304903.5000 mle=1.8909 pcon=4.6397 forget=1.3141 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 40 it 100 total=8304887.5000 mle=1.9395 pcon=4.6392 forget=1.2600 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 40 it 150 total=8304877.0000 mle=1.9239 pcon=4.6386 forget=1.2687 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 40 it 200 total=8304892.0000 mle=1.9823 pcon=4.6381 forget=1.2715 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 40 it 250 total=8304880.0000 mle=1.7910 pcon=4.6375 forget=1.3134 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 40 it 300 total=8304888.0000 mle=1.8247 pcon=4.6369 forget=1.2852 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 40 it 350 total=8304878.5000 mle=1.8927 pcon=4.6363 forget=1.2775 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 41 it 10 total=8304881.5000 mle=1.9631 pcon=4.6359 forget=1.2948 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 41 it 60 total=8304858.5000 mle=1.9599 pcon=4.6354 forget=1.2756 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 41 it 110 total=8304845.5000 mle=2.0513 pcon=4.6350 forget=1.3166 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 41 it 160 total=8304865.0000 mle=1.9779 pcon=4.6345 forget=1.2892 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 41 it 210 total=8304855.5000 mle=1.8647 pcon=4.6340 forget=1.3127 favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 41 it 260 total=8304868.0000 mle=1.9004 pcon=4.6336 forget=1.3375 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 41 it 310 total=8304853.0000 mle=2.0066 pcon=4.6331 forget=1.3070 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 41 it 360 total=8304837.5000 mle=1.8988 pcon=4.6326 forget=1.3139 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 42 it 20 total=8304837.0000 mle=1.9308 pcon=4.6322 forget=1.3029 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 42 it 70 total=8304821.0000 mle=1.9004 pcon=4.6317 forget=1.2948 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 42 it 120 total=8304829.0000 mle=2.0919 pcon=4.6313 forget=1.3263 favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 42 it 170 total=8304829.0000 mle=2.0666 pcon=4.6310 forget=1.3046 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 42 it 220 total=8304835.5000 mle=1.8723 pcon=4.6305 forget=1.3372 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 42 it 270 total=8304823.5000 mle=2.0028 pcon=4.6301 forget=1.3525 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 42 it 320 total=8304826.5000 mle=1.9236 pcon=4.6296 forget=1.3279 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 42 it 370 total=8304825.5000 mle=1.9886 pcon=4.6292 forget=1.3048 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 43 it 30 total=8304801.5000 mle=1.8601 pcon=4.6288 forget=1.3120 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 43 it 80 total=8304825.5000 mle=2.0544 pcon=4.6284 forget=1.3292 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 43 it 130 total=8304812.0000 mle=1.9431 pcon=4.6280 forget=1.3045 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 43 it 180 total=8304798.5000 mle=1.9456 pcon=4.6276 forget=1.3306 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 43 it 230 total=8304814.5000 mle=2.0750 pcon=4.6273 forget=1.3421 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 43 it 280 total=8304797.5000 mle=1.8488 pcon=4.6269 forget=1.3563 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 43 it 330 total=8304789.5000 mle=2.1849 pcon=4.6265 forget=1.3384 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 43 it 380 total=8304794.5000 mle=1.8650 pcon=4.6261 forget=1.3345 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
 88%|████████▊ | 44/50 [29:34<06:34, 65.74s/it] 90%|█████████ | 45/50 [30:32<05:17, 63.42s/it] 92%|█████████▏| 46/50 [31:28<04:05, 61.30s/it] 94%|█████████▍| 47/50 [32:21<02:56, 58.91s/it] 96%|█████████▌| 48/50 [33:03<01:47, 53.79s/it] 98%|█████████▊| 49/50 [33:38<00:47, 47.95s/it]100%|██████████| 50/50 [34:15<00:00, 44.91s/it]100%|██████████| 50/50 [34:15<00:00, 41.12s/it]
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 44 it 40 total=8304806.0000 mle=2.3885 pcon=4.6257 forget=1.3424 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 44 it 90 total=8304796.0000 mle=2.0899 pcon=4.6253 forget=1.3752 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 44 it 140 total=8304785.0000 mle=1.7894 pcon=4.6249 forget=1.4001 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 44 it 190 total=8304797.5000 mle=2.4436 pcon=4.6245 forget=1.3200 favg=0.0000 nr=24 nf=24 protos=540 fproto_sim=NA
[loss] ep 44 it 240 total=8304801.5000 mle=1.9172 pcon=4.6242 forget=1.4191 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 44 it 290 total=8304797.0000 mle=1.9809 pcon=4.6239 forget=1.3255 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 44 it 340 total=8304806.5000 mle=2.0060 pcon=4.6236 forget=1.4199 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 45 it 0 total=8304795.5000 mle=1.9002 pcon=4.6233 forget=1.3322 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 45 it 50 total=8304786.5000 mle=1.8457 pcon=4.6230 forget=1.3806 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 45 it 100 total=8304808.0000 mle=1.9589 pcon=4.6227 forget=1.3654 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 45 it 150 total=8304812.0000 mle=2.0361 pcon=4.6224 forget=1.3703 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 45 it 200 total=8304818.5000 mle=2.0290 pcon=4.6220 forget=1.3604 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 45 it 250 total=8304820.0000 mle=1.7646 pcon=4.6217 forget=1.3601 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 45 it 300 total=8304822.5000 mle=1.8115 pcon=4.6214 forget=1.3381 favg=0.0000 nr=40 nf=40 protos=540 fproto_sim=NA
[loss] ep 45 it 350 total=8304834.5000 mle=2.0922 pcon=4.6212 forget=1.3658 favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 46 it 10 total=8304821.0000 mle=1.9466 pcon=4.6209 forget=1.3317 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 46 it 60 total=8304838.0000 mle=2.4364 pcon=4.6207 forget=1.3376 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 46 it 110 total=8304832.0000 mle=1.8452 pcon=4.6205 forget=1.3357 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 46 it 160 total=8304845.5000 mle=2.0116 pcon=4.6202 forget=1.3763 favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 46 it 210 total=8304850.0000 mle=2.2387 pcon=4.6199 forget=1.3568 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 46 it 260 total=8304861.0000 mle=1.9292 pcon=4.6197 forget=1.3702 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 46 it 310 total=8304871.5000 mle=2.0949 pcon=4.6195 forget=1.4056 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 46 it 360 total=8304865.5000 mle=1.7028 pcon=4.6193 forget=1.3815 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 47 it 20 total=8304871.5000 mle=2.1107 pcon=4.6191 forget=1.3442 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 47 it 70 total=8304870.5000 mle=1.8150 pcon=4.6188 forget=1.3698 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 47 it 120 total=8304867.5000 mle=2.0199 pcon=4.6185 forget=1.3681 favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 47 it 170 total=8304868.0000 mle=1.9000 pcon=4.6183 forget=1.3699 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 47 it 220 total=8304870.5000 mle=1.8497 pcon=4.6180 forget=1.3842 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 47 it 270 total=8304889.5000 mle=1.8672 pcon=4.6178 forget=1.3734 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 47 it 320 total=8304881.5000 mle=2.1262 pcon=4.6177 forget=1.3864 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 47 it 370 total=8304891.5000 mle=1.9101 pcon=4.6175 forget=1.3877 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 48 it 30 total=8304893.5000 mle=1.9844 pcon=4.6172 forget=1.4471 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 48 it 80 total=8304889.5000 mle=1.9154 pcon=4.6170 forget=1.3983 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 48 it 130 total=8304878.5000 mle=1.9815 pcon=4.6167 forget=1.3541 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 48 it 180 total=8304887.5000 mle=1.8345 pcon=4.6166 forget=1.3754 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 48 it 230 total=8304890.5000 mle=2.0910 pcon=4.6164 forget=1.3947 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 48 it 280 total=8304892.0000 mle=1.8396 pcon=4.6162 forget=1.3824 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 48 it 330 total=8304891.5000 mle=2.1107 pcon=4.6160 forget=1.3831 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 48 it 380 total=8304888.0000 mle=2.0574 pcon=4.6158 forget=1.4072 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 49 it 40 total=8304889.0000 mle=2.0782 pcon=4.6156 forget=1.3911 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 49 it 90 total=8304878.0000 mle=2.2497 pcon=4.6154 forget=1.3704 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 49 it 140 total=8304897.5000 mle=1.8445 pcon=4.6152 forget=1.3857 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 49 it 190 total=8304888.0000 mle=1.9627 pcon=4.6150 forget=1.3751 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 49 it 240 total=8304881.5000 mle=2.0278 pcon=4.6148 forget=1.3759 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 49 it 290 total=8304868.0000 mle=1.9187 pcon=4.6146 forget=1.4157 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 49 it 340 total=8304885.5000 mle=1.8208 pcon=4.6145 forget=1.4000 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] loaded adapter 'stage1' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] loaded adapter 'stage2' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[peft] set_adapter(list) failed: unhashable type: 'list'; fallback to last adapter
[peft] active adapter set to: stage2
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc1: Number of model parameters: 22321088
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:22,  1.92it/s]  3%|▎         | 10/391 [00:00<00:18, 20.47it/s]  5%|▍         | 19/391 [00:00<00:10, 35.76it/s]  7%|▋         | 29/391 [00:00<00:07, 51.00it/s] 10%|▉         | 39/391 [00:00<00:05, 62.26it/s] 13%|█▎        | 49/391 [00:01<00:04, 71.32it/s] 15%|█▌        | 59/391 [00:01<00:04, 78.13it/s] 17%|█▋        | 68/391 [00:01<00:04, 80.58it/s] 20%|█▉        | 78/391 [00:01<00:03, 85.19it/s] 23%|██▎       | 88/391 [00:01<00:03, 88.31it/s] 25%|██▌       | 98/391 [00:01<00:03, 90.21it/s] 28%|██▊       | 108/391 [00:01<00:03, 89.91it/s] 30%|███       | 118/391 [00:01<00:03, 89.21it/s] 33%|███▎      | 128/391 [00:01<00:02, 91.19it/s] 35%|███▌      | 138/391 [00:02<00:02, 91.69it/s] 38%|███▊      | 148/391 [00:02<00:02, 93.06it/s] 40%|████      | 158/391 [00:02<00:02, 93.84it/s] 43%|████▎     | 168/391 [00:02<00:02, 92.95it/s] 46%|████▌     | 178/391 [00:02<00:02, 92.57it/s] 48%|████▊     | 188/391 [00:02<00:02, 92.61it/s] 51%|█████     | 198/391 [00:02<00:02, 92.27it/s] 53%|█████▎    | 208/391 [00:02<00:02, 91.31it/s] 56%|█████▌    | 218/391 [00:02<00:01, 90.50it/s] 58%|█████▊    | 228/391 [00:02<00:01, 91.88it/s] 61%|██████    | 238/391 [00:03<00:01, 92.84it/s] 63%|██████▎   | 248/391 [00:03<00:01, 92.97it/s] 66%|██████▌   | 258/391 [00:03<00:01, 91.83it/s] 69%|██████▊   | 268/391 [00:03<00:01, 90.50it/s] 71%|███████   | 278/391 [00:03<00:01, 92.22it/s] 74%|███████▎  | 288/391 [00:03<00:01, 90.17it/s] 76%|███████▌  | 298/391 [00:03<00:01, 91.91it/s] 79%|███████▉  | 308/391 [00:03<00:00, 92.86it/s] 81%|████████▏ | 318/391 [00:03<00:00, 92.52it/s] 84%|████████▍ | 328/391 [00:04<00:00, 93.17it/s] 86%|████████▋ | 338/391 [00:04<00:00, 93.04it/s] 89%|████████▉ | 348/391 [00:04<00:00, 93.04it/s] 92%|█████████▏| 358/391 [00:04<00:00, 93.53it/s] 94%|█████████▍| 368/391 [00:04<00:00, 94.21it/s] 97%|█████████▋| 378/391 [00:04<00:00, 95.23it/s] 99%|█████████▉| 388/391 [00:04<00:00, 96.04it/s]100%|██████████| 391/391 [00:04<00:00, 82.43it/s]
50000 images processed, 4.8190553188323975 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.91it/s] 11%|█▏        | 9/79 [00:00<00:03, 18.62it/s] 23%|██▎       | 18/79 [00:00<00:01, 34.66it/s] 35%|███▌      | 28/79 [00:00<00:01, 49.93it/s] 48%|████▊     | 38/79 [00:00<00:00, 61.25it/s] 61%|██████    | 48/79 [00:01<00:00, 69.48it/s] 73%|███████▎  | 58/79 [00:01<00:00, 76.62it/s] 86%|████████▌ | 68/79 [00:01<00:00, 81.94it/s] 99%|█████████▊| 78/79 [00:01<00:00, 85.73it/s]100%|██████████| 79/79 [00:01<00:00, 56.21it/s]
10000 images processed, 1.4347331523895264 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:07,  1.59it/s]  5%|▍         | 10/204 [00:00<00:10, 17.92it/s]  9%|▉         | 18/204 [00:00<00:06, 30.55it/s] 13%|█▎        | 27/204 [00:00<00:04, 44.08it/s] 18%|█▊        | 37/204 [00:01<00:02, 56.93it/s] 23%|██▎       | 46/204 [00:01<00:02, 64.59it/s] 27%|██▋       | 56/204 [00:01<00:02, 72.92it/s] 32%|███▏      | 65/204 [00:01<00:01, 76.59it/s] 37%|███▋      | 75/204 [00:01<00:01, 81.70it/s] 42%|████▏     | 85/204 [00:01<00:01, 84.74it/s] 46%|████▌     | 94/204 [00:01<00:01, 85.25it/s] 51%|█████     | 104/204 [00:01<00:01, 87.74it/s] 56%|█████▌    | 114/204 [00:01<00:01, 87.72it/s] 60%|██████    | 123/204 [00:02<00:00, 87.29it/s] 65%|██████▌   | 133/204 [00:02<00:00, 89.35it/s] 70%|███████   | 143/204 [00:02<00:00, 91.06it/s] 75%|███████▌  | 153/204 [00:02<00:00, 92.38it/s] 80%|███████▉  | 163/204 [00:02<00:00, 86.92it/s] 85%|████████▍ | 173/204 [00:02<00:00, 88.49it/s] 90%|████████▉ | 183/204 [00:02<00:00, 90.10it/s] 95%|█████████▍| 193/204 [00:02<00:00, 91.57it/s]100%|█████████▉| 203/204 [00:02<00:00, 92.46it/s]100%|██████████| 204/204 [00:02<00:00, 70.42it/s]
26032 images processed, 2.9757113456726074 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:00,  1.30it/s]  5%|▌         | 4/79 [00:00<00:12,  5.78it/s] 18%|█▊        | 14/79 [00:00<00:02, 22.67it/s] 30%|███       | 24/79 [00:01<00:01, 37.78it/s] 43%|████▎     | 34/79 [00:01<00:00, 50.81it/s] 56%|█████▌    | 44/79 [00:01<00:00, 61.47it/s] 67%|██████▋   | 53/79 [00:01<00:00, 66.83it/s] 80%|███████▉  | 63/79 [00:01<00:00, 74.58it/s] 92%|█████████▏| 73/79 [00:01<00:00, 80.46it/s]100%|██████████| 79/79 [00:01<00:00, 47.09it/s]
10000 images processed, 1.715822696685791 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:53,  1.45it/s] 13%|█▎        | 10/79 [00:00<00:04, 16.56it/s] 25%|██▌       | 20/79 [00:00<00:01, 32.60it/s] 38%|███▊      | 30/79 [00:01<00:01, 46.65it/s] 51%|█████     | 40/79 [00:01<00:00, 58.31it/s] 63%|██████▎   | 50/79 [00:01<00:00, 67.73it/s] 76%|███████▌  | 60/79 [00:01<00:00, 75.38it/s] 89%|████████▊ | 70/79 [00:01<00:00, 81.32it/s]100%|██████████| 79/79 [00:01<00:00, 52.41it/s]
10000 images processed, 1.5271823406219482 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:43,  1.58it/s] 14%|█▍        | 10/70 [00:00<00:03, 17.75it/s] 29%|██▊       | 20/70 [00:00<00:01, 34.29it/s] 41%|████▏     | 29/70 [00:00<00:00, 46.77it/s] 56%|█████▌    | 39/70 [00:01<00:00, 58.48it/s] 70%|███████   | 49/70 [00:01<00:00, 68.47it/s] 84%|████████▍ | 59/70 [00:01<00:00, 76.20it/s] 99%|█████████▊| 69/70 [00:01<00:00, 81.87it/s]100%|██████████| 70/70 [00:01<00:00, 50.74it/s]
8925 images processed, 1.4092812538146973 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:54,  1.23s/it] 20%|██        | 9/45 [00:01<00:03,  9.02it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.87it/s] 47%|████▋     | 21/45 [00:01<00:01, 14.76it/s] 69%|██████▉   | 31/45 [00:02<00:00, 25.81it/s] 82%|████████▏ | 37/45 [00:02<00:00, 19.55it/s]100%|██████████| 45/45 [00:02<00:00, 17.31it/s]
5640 images processed, 2.6219003200531006 seconds used

18.337975025177002
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.50  99.36
places365     67.90  80.57
LSUN          21.54  95.06
iSUN          72.28  81.40
dtd           38.17  91.27
AVG           40.48  89.53
Retain-Acc: 0.7433
Forget-as-OOD (retain known vs forget novel):
  FPR: 84.00 AUROC: 85.57 AUIN: 99.08
12.821937561035156
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc1_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc1_rf.png
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] loaded adapter 'stage1' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] loaded adapter 'stage2' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[peft] set_adapter(list) failed: unhashable type: 'list'; fallback to last adapter
[peft] active adapter set to: stage2
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc2: Number of model parameters: 22321088
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:17,  1.97it/s]  2%|▏         | 8/391 [00:00<00:22, 16.68it/s]  5%|▍         | 18/391 [00:00<00:10, 36.02it/s]  7%|▋         | 28/391 [00:00<00:07, 51.22it/s]  9%|▉         | 37/391 [00:00<00:05, 60.58it/s] 12%|█▏        | 47/391 [00:01<00:04, 69.63it/s] 14%|█▍        | 56/391 [00:01<00:04, 74.43it/s] 17%|█▋        | 66/391 [00:01<00:04, 80.16it/s] 19%|█▉        | 75/391 [00:01<00:03, 81.58it/s] 21%|██▏       | 84/391 [00:01<00:03, 82.80it/s] 24%|██▍       | 93/391 [00:01<00:03, 84.14it/s] 26%|██▋       | 103/391 [00:01<00:03, 87.44it/s] 29%|██▉       | 113/391 [00:01<00:03, 90.03it/s] 31%|███▏      | 123/391 [00:01<00:02, 91.67it/s] 34%|███▍      | 133/391 [00:01<00:02, 92.99it/s] 37%|███▋      | 143/391 [00:02<00:02, 93.30it/s] 39%|███▉      | 153/391 [00:02<00:02, 93.90it/s] 42%|████▏     | 163/391 [00:02<00:02, 94.56it/s] 44%|████▍     | 173/391 [00:02<00:02, 95.04it/s] 47%|████▋     | 183/391 [00:02<00:02, 94.52it/s] 49%|████▉     | 193/391 [00:02<00:02, 92.49it/s] 52%|█████▏    | 203/391 [00:02<00:02, 91.55it/s] 54%|█████▍    | 213/391 [00:02<00:01, 91.93it/s] 57%|█████▋    | 223/391 [00:02<00:01, 91.90it/s] 60%|█████▉    | 233/391 [00:03<00:01, 91.41it/s] 62%|██████▏   | 243/391 [00:03<00:01, 92.57it/s] 65%|██████▍   | 253/391 [00:03<00:01, 93.09it/s] 67%|██████▋   | 263/391 [00:03<00:01, 93.52it/s] 70%|██████▉   | 273/391 [00:03<00:01, 93.12it/s] 72%|███████▏  | 283/391 [00:03<00:01, 91.80it/s] 75%|███████▍  | 293/391 [00:03<00:01, 92.82it/s] 77%|███████▋  | 303/391 [00:03<00:00, 91.37it/s] 80%|████████  | 313/391 [00:03<00:00, 89.26it/s] 83%|████████▎ | 323/391 [00:04<00:00, 90.77it/s] 85%|████████▌ | 333/391 [00:04<00:00, 92.32it/s] 88%|████████▊ | 343/391 [00:04<00:00, 91.15it/s] 90%|█████████ | 353/391 [00:04<00:00, 92.56it/s] 93%|█████████▎| 363/391 [00:04<00:00, 93.00it/s] 95%|█████████▌| 373/391 [00:04<00:00, 94.45it/s] 98%|█████████▊| 383/391 [00:04<00:00, 95.46it/s]100%|██████████| 391/391 [00:04<00:00, 82.13it/s]
50000 images processed, 4.866615056991577 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.92it/s] 11%|█▏        | 9/79 [00:00<00:03, 18.56it/s] 24%|██▍       | 19/79 [00:00<00:01, 37.09it/s] 37%|███▋      | 29/79 [00:00<00:00, 52.05it/s] 48%|████▊     | 38/79 [00:00<00:00, 60.93it/s] 59%|█████▉    | 47/79 [00:01<00:00, 66.96it/s] 72%|███████▏  | 57/79 [00:01<00:00, 75.16it/s] 85%|████████▍ | 67/79 [00:01<00:00, 80.87it/s] 97%|█████████▋| 77/79 [00:01<00:00, 85.37it/s]100%|██████████| 79/79 [00:01<00:00, 56.82it/s]
10000 images processed, 1.4117307662963867 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:11,  1.54it/s]  5%|▌         | 11/204 [00:00<00:10, 19.16it/s] 10%|█         | 21/204 [00:00<00:05, 34.89it/s] 15%|█▍        | 30/204 [00:00<00:03, 46.33it/s] 20%|█▉        | 40/204 [00:01<00:02, 57.02it/s] 24%|██▍       | 49/204 [00:01<00:02, 64.61it/s] 29%|██▉       | 59/204 [00:01<00:02, 71.91it/s] 33%|███▎      | 68/204 [00:01<00:01, 76.48it/s] 38%|███▊      | 77/204 [00:01<00:01, 75.14it/s] 42%|████▏     | 86/204 [00:01<00:01, 75.23it/s] 47%|████▋     | 96/204 [00:01<00:01, 80.33it/s] 52%|█████▏    | 106/204 [00:01<00:01, 83.74it/s] 56%|█████▋    | 115/204 [00:01<00:01, 83.71it/s] 61%|██████▏   | 125/204 [00:02<00:00, 86.58it/s] 66%|██████▌   | 134/204 [00:02<00:00, 86.88it/s] 71%|███████   | 144/204 [00:02<00:00, 89.18it/s] 75%|███████▌  | 153/204 [00:02<00:00, 88.80it/s] 79%|███████▉  | 162/204 [00:02<00:00, 87.62it/s] 84%|████████▍ | 171/204 [00:02<00:00, 86.69it/s] 89%|████████▊ | 181/204 [00:02<00:00, 89.41it/s] 94%|█████████▎| 191/204 [00:02<00:00, 91.64it/s] 99%|█████████▊| 201/204 [00:02<00:00, 93.05it/s]100%|██████████| 204/204 [00:02<00:00, 69.21it/s]
26032 images processed, 2.9958271980285645 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:59,  1.31it/s] 13%|█▎        | 10/79 [00:00<00:04, 15.18it/s] 23%|██▎       | 18/79 [00:00<00:02, 26.65it/s] 35%|███▌      | 28/79 [00:01<00:01, 41.03it/s] 46%|████▌     | 36/79 [00:01<00:00, 47.39it/s] 57%|█████▋    | 45/79 [00:01<00:00, 56.99it/s] 67%|██████▋   | 53/79 [00:01<00:00, 59.99it/s] 80%|███████▉  | 63/79 [00:01<00:00, 68.82it/s] 92%|█████████▏| 73/79 [00:01<00:00, 76.05it/s]100%|██████████| 79/79 [00:01<00:00, 46.62it/s]
10000 images processed, 1.7267425060272217 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:56,  1.37it/s] 13%|█▎        | 10/79 [00:00<00:04, 15.72it/s] 25%|██▌       | 20/79 [00:00<00:01, 31.21it/s] 38%|███▊      | 30/79 [00:01<00:01, 45.15it/s] 51%|█████     | 40/79 [00:01<00:00, 56.86it/s] 62%|██████▏   | 49/79 [00:01<00:00, 64.38it/s] 75%|███████▍  | 59/79 [00:01<00:00, 72.89it/s] 87%|████████▋ | 69/79 [00:01<00:00, 79.20it/s]100%|██████████| 79/79 [00:01<00:00, 50.58it/s]
10000 images processed, 1.5850248336791992 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:40,  1.69it/s] 14%|█▍        | 10/70 [00:00<00:03, 18.53it/s] 29%|██▊       | 20/70 [00:00<00:01, 35.32it/s] 41%|████▏     | 29/70 [00:00<00:00, 47.40it/s] 54%|█████▍    | 38/70 [00:01<00:00, 57.83it/s] 69%|██████▊   | 48/70 [00:01<00:00, 68.08it/s] 83%|████████▎ | 58/70 [00:01<00:00, 75.93it/s] 97%|█████████▋| 68/70 [00:01<00:00, 81.27it/s]100%|██████████| 70/70 [00:01<00:00, 51.48it/s]
8925 images processed, 1.3995423316955566 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:57,  1.30s/it] 11%|█         | 5/45 [00:01<00:08,  4.63it/s] 31%|███       | 14/45 [00:01<00:02, 15.11it/s] 44%|████▍     | 20/45 [00:01<00:01, 15.20it/s] 53%|█████▎    | 24/45 [00:02<00:01, 17.41it/s] 71%|███████   | 32/45 [00:02<00:00, 26.49it/s] 82%|████████▏ | 37/45 [00:02<00:00, 19.15it/s]100%|██████████| 45/45 [00:02<00:00, 16.80it/s]
5640 images processed, 2.7011539936065674 seconds used

18.38544726371765
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.53  99.36
places365     68.01  81.23
LSUN          17.64  96.11
iSUN          72.54  81.90
dtd           37.73  91.47
AVG           39.69  90.01
Retain-Acc: 0.7418
Forget-as-OOD (retain known vs forget novel):
  FPR: 73.20 AUROC: 88.35 AUIN: 99.33
10.291162252426147
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc2_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc2_rf.png
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] loaded adapter 'stage1' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] loaded adapter 'stage2' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[peft] set_adapter(list) failed: unhashable type: 'list'; fallback to last adapter
[peft] active adapter set to: stage2
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-seen: Number of model parameters: 22321088
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:09,  2.06it/s]  2%|▏         | 7/391 [00:00<00:25, 15.01it/s]  4%|▍         | 17/391 [00:00<00:10, 34.77it/s]  7%|▋         | 27/391 [00:00<00:07, 50.27it/s]  9%|▉         | 37/391 [00:00<00:05, 62.40it/s] 12%|█▏        | 47/391 [00:01<00:04, 71.62it/s] 15%|█▍        | 57/391 [00:01<00:04, 78.17it/s] 17%|█▋        | 67/391 [00:01<00:03, 82.22it/s] 19%|█▉        | 76/391 [00:01<00:03, 84.35it/s] 22%|██▏       | 86/391 [00:01<00:03, 86.74it/s] 25%|██▍       | 96/391 [00:01<00:03, 88.97it/s] 27%|██▋       | 106/391 [00:01<00:03, 87.70it/s] 29%|██▉       | 115/391 [00:01<00:03, 87.19it/s] 32%|███▏      | 125/391 [00:01<00:02, 89.04it/s] 35%|███▍      | 135/391 [00:01<00:02, 90.78it/s] 37%|███▋      | 145/391 [00:02<00:02, 91.97it/s] 40%|███▉      | 155/391 [00:02<00:02, 92.71it/s] 42%|████▏     | 165/391 [00:02<00:02, 90.74it/s] 45%|████▍     | 175/391 [00:02<00:02, 92.13it/s] 47%|████▋     | 185/391 [00:02<00:02, 92.91it/s] 50%|████▉     | 195/391 [00:02<00:02, 92.81it/s] 52%|█████▏    | 205/391 [00:02<00:02, 91.42it/s] 55%|█████▍    | 215/391 [00:02<00:01, 92.40it/s] 58%|█████▊    | 225/391 [00:02<00:01, 93.27it/s] 60%|██████    | 235/391 [00:03<00:01, 93.83it/s] 63%|██████▎   | 245/391 [00:03<00:01, 92.95it/s] 65%|██████▌   | 255/391 [00:03<00:01, 93.46it/s] 68%|██████▊   | 265/391 [00:03<00:01, 93.67it/s] 70%|███████   | 275/391 [00:03<00:01, 93.89it/s] 73%|███████▎  | 285/391 [00:03<00:01, 93.54it/s] 75%|███████▌  | 295/391 [00:03<00:01, 92.00it/s] 78%|███████▊  | 305/391 [00:03<00:00, 90.93it/s] 81%|████████  | 315/391 [00:03<00:00, 91.36it/s] 83%|████████▎ | 325/391 [00:04<00:00, 91.28it/s] 86%|████████▌ | 335/391 [00:04<00:00, 90.73it/s] 88%|████████▊ | 345/391 [00:04<00:00, 88.97it/s] 91%|█████████ | 355/391 [00:04<00:00, 90.70it/s] 93%|█████████▎| 365/391 [00:04<00:00, 91.95it/s] 96%|█████████▌| 375/391 [00:04<00:00, 93.57it/s] 98%|█████████▊| 385/391 [00:04<00:00, 94.67it/s]100%|██████████| 391/391 [00:04<00:00, 82.25it/s]
50000 images processed, 4.84138298034668 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.92it/s] 14%|█▍        | 11/79 [00:00<00:02, 22.79it/s] 27%|██▋       | 21/79 [00:00<00:01, 40.23it/s] 39%|███▉      | 31/79 [00:00<00:00, 54.14it/s] 52%|█████▏    | 41/79 [00:00<00:00, 64.77it/s] 65%|██████▍   | 51/79 [00:01<00:00, 72.99it/s] 77%|███████▋  | 61/79 [00:01<00:00, 79.36it/s] 90%|████████▉ | 71/79 [00:01<00:00, 84.09it/s]100%|██████████| 79/79 [00:02<00:00, 39.44it/s]
10000 images processed, 2.0271363258361816 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:01,  1.68it/s]  4%|▍         | 9/204 [00:00<00:11, 16.42it/s]  9%|▉         | 19/204 [00:00<00:05, 33.44it/s] 14%|█▍        | 29/204 [00:00<00:03, 48.13it/s] 19%|█▉        | 39/204 [00:01<00:02, 60.15it/s] 24%|██▎       | 48/204 [00:01<00:02, 67.45it/s] 28%|██▊       | 58/204 [00:01<00:01, 75.20it/s] 33%|███▎      | 68/204 [00:01<00:01, 80.57it/s] 38%|███▊      | 78/204 [00:01<00:01, 84.35it/s] 43%|████▎     | 88/204 [00:01<00:01, 87.08it/s] 48%|████▊     | 98/204 [00:01<00:01, 88.26it/s] 53%|█████▎    | 108/204 [00:01<00:01, 89.87it/s] 58%|█████▊    | 118/204 [00:01<00:00, 91.51it/s] 63%|██████▎   | 128/204 [00:01<00:00, 92.80it/s] 68%|██████▊   | 138/204 [00:02<00:00, 93.36it/s] 73%|███████▎  | 148/204 [00:02<00:00, 91.91it/s] 77%|███████▋  | 158/204 [00:02<00:00, 92.40it/s] 82%|████████▏ | 168/204 [00:02<00:00, 93.16it/s] 87%|████████▋ | 178/204 [00:02<00:00, 92.34it/s] 92%|█████████▏| 188/204 [00:02<00:00, 93.18it/s] 97%|█████████▋| 198/204 [00:02<00:00, 93.67it/s]100%|██████████| 204/204 [00:02<00:00, 72.64it/s]
26032 images processed, 2.8493571281433105 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:59,  1.31it/s] 13%|█▎        | 10/79 [00:00<00:04, 15.19it/s] 24%|██▍       | 19/79 [00:00<00:02, 28.48it/s] 37%|███▋      | 29/79 [00:01<00:01, 42.54it/s] 49%|████▉     | 39/79 [00:01<00:00, 54.64it/s] 61%|██████    | 48/79 [00:01<00:00, 62.25it/s] 73%|███████▎  | 58/79 [00:01<00:00, 71.05it/s] 85%|████████▍ | 67/79 [00:01<00:00, 75.65it/s] 97%|█████████▋| 77/79 [00:01<00:00, 81.47it/s]100%|██████████| 79/79 [00:01<00:00, 48.80it/s]
10000 images processed, 1.6542210578918457 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:45,  1.72it/s] 13%|█▎        | 10/79 [00:00<00:03, 18.99it/s] 25%|██▌       | 20/79 [00:00<00:01, 36.09it/s] 38%|███▊      | 30/79 [00:00<00:00, 50.07it/s] 49%|████▉     | 39/79 [00:01<00:00, 58.54it/s] 62%|██████▏   | 49/79 [00:01<00:00, 67.74it/s] 75%|███████▍  | 59/79 [00:01<00:00, 75.54it/s] 87%|████████▋ | 69/79 [00:01<00:00, 81.33it/s]100%|██████████| 79/79 [00:01<00:00, 55.42it/s]
10000 images processed, 1.4611148834228516 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:43,  1.60it/s] 16%|█▌        | 11/70 [00:00<00:02, 19.71it/s] 27%|██▋       | 19/70 [00:00<00:01, 32.26it/s] 40%|████      | 28/70 [00:00<00:00, 45.43it/s] 53%|█████▎    | 37/70 [00:01<00:00, 56.21it/s] 67%|██████▋   | 47/70 [00:01<00:00, 66.90it/s] 81%|████████▏ | 57/70 [00:01<00:00, 75.13it/s] 96%|█████████▌| 67/70 [00:01<00:00, 81.19it/s]100%|██████████| 70/70 [00:01<00:00, 50.69it/s]
8925 images processed, 1.4132559299468994 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:47,  1.07s/it]  4%|▍         | 2/45 [00:01<00:23,  1.84it/s] 27%|██▋       | 12/45 [00:01<00:02, 14.83it/s] 38%|███▊      | 17/45 [00:01<00:01, 15.56it/s] 47%|████▋     | 21/45 [00:01<00:01, 17.48it/s] 56%|█████▌    | 25/45 [00:02<00:01, 17.22it/s] 73%|███████▎  | 33/45 [00:02<00:00, 19.94it/s] 80%|████████  | 36/45 [00:02<00:00, 21.24it/s]100%|██████████| 45/45 [00:02<00:00, 17.38it/s]
5640 images processed, 2.6091055870056152 seconds used

18.684638738632202
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.53  99.36
places365     67.87  81.18
LSUN          17.60  96.07
iSUN          72.36  81.68
dtd           37.87  91.39
AVG           39.65  89.93
Retain-Acc: 0.7406
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
8.739965915679932
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-seen_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-seen_rf.png
runner-incremental-stack.sh: line 110: one: command not found
