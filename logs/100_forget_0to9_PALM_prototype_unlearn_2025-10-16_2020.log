nohup: ignoring input
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.1, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='/home/shaokun/PALM/checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter', adapter_load_path=None, forget_classes='0,1,2,3,4,5,6,7,8,9', forget_list_path=None, forget_center_set='all', forget_lambda=0.1, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from /home/shaokun/PALM/checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:107: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:15<04:48, 15.16s/it] 10%|█         | 2/20 [00:26<03:55, 13.11s/it] 15%|█▌        | 3/20 [00:38<03:28, 12.26s/it] 20%|██        | 4/20 [00:49<03:10, 11.93s/it] 25%|██▌       | 5/20 [01:00<02:55, 11.72s/it] 30%|███       | 6/20 [01:12<02:45, 11.81s/it] 35%|███▌      | 7/20 [01:24<02:31, 11.67s/it] 40%|████      | 8/20 [01:35<02:19, 11.59s/it][loss] ep 0 it 0 total=7.4110 mle=1.4483 pcon=5.2950 forget=0.6677 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 50 total=7.5281 mle=1.5740 pcon=5.2881 forget=0.6660 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 100 total=7.6162 mle=1.6739 pcon=5.2808 forget=0.6614 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 150 total=7.4140 mle=1.4739 pcon=5.2742 forget=0.6660 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 200 total=7.3381 mle=1.4082 pcon=5.2676 forget=0.6623 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 250 total=7.4603 mle=1.5378 pcon=5.2611 forget=0.6615 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 300 total=7.3884 mle=1.4659 pcon=5.2548 forget=0.6678 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 0 it 350 total=7.5506 mle=1.6429 pcon=5.2485 forget=0.6592 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 1 it 10 total=7.5253 mle=1.6142 pcon=5.2423 forget=0.6687 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 60 total=7.4716 mle=1.5769 pcon=5.2364 forget=0.6583 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 110 total=7.3882 mle=1.4973 pcon=5.2304 forget=0.6605 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 160 total=7.5747 mle=1.6920 pcon=5.2244 forget=0.6583 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 210 total=7.3374 mle=1.4586 pcon=5.2187 forget=0.6602 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 260 total=7.3551 mle=1.4831 pcon=5.2136 forget=0.6583 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 310 total=7.5828 mle=1.7184 pcon=5.2080 forget=0.6564 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 1 it 360 total=7.8809 mle=2.0173 pcon=5.2028 forget=0.6609 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 2 it 20 total=7.5676 mle=1.7052 pcon=5.1973 forget=0.6650 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 70 total=7.5520 mle=1.7035 pcon=5.1923 forget=0.6562 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 120 total=7.4031 mle=1.5519 pcon=5.1873 forget=0.6639 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 170 total=7.4741 mle=1.6332 pcon=5.1823 forget=0.6586 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 220 total=7.3465 mle=1.5047 pcon=5.1778 forget=0.6640 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 270 total=7.5026 mle=1.6704 pcon=5.1731 forget=0.6591 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 320 total=7.5067 mle=1.6809 pcon=5.1686 forget=0.6572 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 2 it 370 total=7.3692 mle=1.5474 pcon=5.1643 forget=0.6575 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 3 it 30 total=7.3490 mle=1.5330 pcon=5.1602 forget=0.6558 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 80 total=7.3726 mle=1.5627 pcon=5.1560 forget=0.6540 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 130 total=7.5402 mle=1.7330 pcon=5.1514 forget=0.6558 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 180 total=7.5845 mle=1.7778 pcon=5.1474 forget=0.6593 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 230 total=7.4357 mle=1.6372 pcon=5.1430 forget=0.6555 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 280 total=7.3576 mle=1.5587 pcon=5.1388 forget=0.6602 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 330 total=7.3398 mle=1.5416 pcon=5.1347 forget=0.6635 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 3 it 380 total=7.3018 mle=1.5109 pcon=5.1305 forget=0.6604 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 4 it 40 total=7.1776 mle=1.3925 pcon=5.1267 forget=0.6584 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 90 total=7.2139 mle=1.4315 pcon=5.1230 forget=0.6594 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 140 total=7.3769 mle=1.5987 pcon=5.1194 forget=0.6588 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 190 total=7.5152 mle=1.7437 pcon=5.1159 forget=0.6556 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 240 total=7.3789 mle=1.6153 pcon=5.1121 forget=0.6514 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 290 total=7.2565 mle=1.4910 pcon=5.1088 forget=0.6567 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 4 it 340 total=7.3858 mle=1.6259 pcon=5.1056 forget=0.6542 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 5 it 0 total=7.3859 mle=1.6155 pcon=5.1021 forget=0.6683 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 50 total=7.3191 mle=1.5634 pcon=5.0989 forget=0.6568 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 100 total=7.4379 mle=1.6888 pcon=5.0959 forget=0.6533 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 150 total=7.4470 mle=1.6993 pcon=5.0928 forget=0.6550 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 200 total=7.2377 mle=1.4944 pcon=5.0899 forget=0.6534 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 250 total=7.3390 mle=1.5926 pcon=5.0870 forget=0.6594 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 300 total=7.2932 mle=1.5539 pcon=5.0842 forget=0.6552 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 5 it 350 total=7.2983 mle=1.5601 pcon=5.0810 forget=0.6572 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 6 it 10 total=7.3095 mle=1.5784 pcon=5.0779 forget=0.6532 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 60 total=7.4175 mle=1.6884 pcon=5.0751 forget=0.6540 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 110 total=7.5707 mle=1.8481 pcon=5.0724 forget=0.6502 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 160 total=7.3182 mle=1.5934 pcon=5.0696 forget=0.6552 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 210 total=7.5946 mle=1.8685 pcon=5.0674 forget=0.6587 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 260 total=7.2067 mle=1.4867 pcon=5.0645 forget=0.6556 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 310 total=7.2798 mle=1.5666 pcon=5.0623 forget=0.6509 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 6 it 360 total=7.1429 mle=1.4313 pcon=5.0599 forget=0.6517 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 7 it 20 total=7.3532 mle=1.6400 pcon=5.0574 forget=0.6558 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 70 total=7.3286 mle=1.6236 pcon=5.0551 forget=0.6499 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 120 total=7.3034 mle=1.6044 pcon=5.0524 forget=0.6465 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 170 total=7.2581 mle=1.5552 pcon=5.0500 forget=0.6528 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 220 total=7.2763 mle=1.5742 pcon=5.0482 forget=0.6539 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 270 total=7.3122 mle=1.6159 pcon=5.0461 forget=0.6503 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 320 total=7.1047 mle=1.4113 pcon=5.0435 forget=0.6499 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 7 it 370 total=7.2359 mle=1.5443 pcon=5.0415 forget=0.6501 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 8 it 30 total=7.2528 mle=1.5602 pcon=5.0396 forget=0.6529 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 80 total=7.1893 mle=1.4942 pcon=5.0376 forget=0.6575 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 130 total=7.1260 mle=1.4391 pcon=5.0355 forget=0.6514 nr=64 nf=64 protos=540 dmin_norm=NA
 45%|████▌     | 9/20 [01:46<02:06, 11.46s/it] 50%|█████     | 10/20 [01:58<01:54, 11.48s/it] 55%|█████▌    | 11/20 [02:09<01:43, 11.50s/it] 60%|██████    | 12/20 [02:20<01:30, 11.35s/it] 65%|██████▌   | 13/20 [02:32<01:19, 11.29s/it] 70%|███████   | 14/20 [02:43<01:07, 11.31s/it] 75%|███████▌  | 15/20 [02:54<00:56, 11.32s/it] 80%|████████  | 16/20 [03:06<00:45, 11.42s/it][loss] ep 8 it 180 total=7.2184 mle=1.5330 pcon=5.0337 forget=0.6518 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 230 total=7.2911 mle=1.6039 pcon=5.0317 forget=0.6555 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 280 total=7.2296 mle=1.5501 pcon=5.0296 forget=0.6499 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 330 total=7.2801 mle=1.5990 pcon=5.0276 forget=0.6535 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 8 it 380 total=7.0914 mle=1.4140 pcon=5.0257 forget=0.6518 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 9 it 40 total=7.1925 mle=1.5205 pcon=5.0241 forget=0.6479 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 90 total=7.3751 mle=1.7024 pcon=5.0223 forget=0.6504 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 140 total=7.1670 mle=1.4971 pcon=5.0209 forget=0.6490 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 190 total=7.1937 mle=1.5220 pcon=5.0191 forget=0.6526 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 240 total=7.2457 mle=1.5804 pcon=5.0177 forget=0.6476 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 290 total=7.2558 mle=1.5842 pcon=5.0158 forget=0.6558 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 9 it 340 total=7.1628 mle=1.5011 pcon=5.0144 forget=0.6473 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 10 it 0 total=7.2646 mle=1.6044 pcon=5.0129 forget=0.6474 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 50 total=7.2884 mle=1.6280 pcon=5.0114 forget=0.6490 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 100 total=7.0994 mle=1.4368 pcon=5.0097 forget=0.6530 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 150 total=7.2293 mle=1.5740 pcon=5.0082 forget=0.6470 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 200 total=7.1061 mle=1.4529 pcon=5.0069 forget=0.6463 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 250 total=7.3115 mle=1.6503 pcon=5.0054 forget=0.6558 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 300 total=7.3872 mle=1.7374 pcon=5.0038 forget=0.6459 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 10 it 350 total=7.3029 mle=1.6509 pcon=5.0022 forget=0.6498 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 11 it 10 total=7.3122 mle=1.6642 pcon=5.0004 forget=0.6475 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 60 total=7.3616 mle=1.7162 pcon=4.9988 forget=0.6466 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 110 total=7.1473 mle=1.5030 pcon=4.9973 forget=0.6470 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 160 total=7.2090 mle=1.5618 pcon=4.9959 forget=0.6512 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 210 total=7.2950 mle=1.6541 pcon=4.9947 forget=0.6462 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 260 total=7.1721 mle=1.5351 pcon=4.9934 forget=0.6436 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 310 total=7.3241 mle=1.6835 pcon=4.9917 forget=0.6489 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 11 it 360 total=7.2382 mle=1.5986 pcon=4.9901 forget=0.6496 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 12 it 20 total=7.4225 mle=1.7867 pcon=4.9888 forget=0.6471 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 70 total=7.0914 mle=1.4576 pcon=4.9873 forget=0.6466 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 120 total=7.5057 mle=1.8752 pcon=4.9860 forget=0.6445 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 170 total=7.1287 mle=1.5030 pcon=4.9846 forget=0.6411 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 220 total=7.0447 mle=1.4169 pcon=4.9829 forget=0.6449 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 270 total=7.2260 mle=1.5994 pcon=4.9818 forget=0.6448 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 320 total=7.2413 mle=1.6136 pcon=4.9805 forget=0.6473 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 12 it 370 total=7.2316 mle=1.6065 pcon=4.9793 forget=0.6458 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 13 it 30 total=7.2278 mle=1.6027 pcon=4.9778 forget=0.6473 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 80 total=7.1829 mle=1.5558 pcon=4.9766 forget=0.6505 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 130 total=7.0757 mle=1.4606 pcon=4.9753 forget=0.6397 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 180 total=7.3845 mle=1.7608 pcon=4.9740 forget=0.6496 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 230 total=7.3248 mle=1.7053 pcon=4.9725 forget=0.6471 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 280 total=7.2456 mle=1.6280 pcon=4.9714 forget=0.6463 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 330 total=7.1344 mle=1.5211 pcon=4.9702 forget=0.6431 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 13 it 380 total=7.1906 mle=1.5757 pcon=4.9691 forget=0.6459 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 14 it 40 total=7.4444 mle=1.8319 pcon=4.9676 forget=0.6449 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 90 total=7.2909 mle=1.6801 pcon=4.9667 forget=0.6441 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 140 total=7.1713 mle=1.5627 pcon=4.9654 forget=0.6431 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 190 total=7.2243 mle=1.6168 pcon=4.9643 forget=0.6432 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 240 total=7.2458 mle=1.6373 pcon=4.9632 forget=0.6453 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 290 total=7.0722 mle=1.4687 pcon=4.9620 forget=0.6415 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 14 it 340 total=7.2266 mle=1.6219 pcon=4.9610 forget=0.6437 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 15 it 0 total=7.0689 mle=1.4628 pcon=4.9599 forget=0.6461 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 50 total=7.1822 mle=1.5807 pcon=4.9585 forget=0.6430 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 100 total=7.2972 mle=1.6995 pcon=4.9573 forget=0.6404 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 150 total=7.2703 mle=1.6708 pcon=4.9561 forget=0.6434 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 200 total=7.1432 mle=1.5387 pcon=4.9552 forget=0.6493 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 250 total=7.0665 mle=1.4662 pcon=4.9540 forget=0.6463 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 300 total=7.1719 mle=1.5682 pcon=4.9532 forget=0.6505 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 15 it 350 total=7.1845 mle=1.5874 pcon=4.9521 forget=0.6449 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 16 it 10 total=7.0738 mle=1.4753 pcon=4.9510 forget=0.6474 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 60 total=7.2667 mle=1.6731 pcon=4.9501 forget=0.6435 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 110 total=7.2389 mle=1.6441 pcon=4.9493 forget=0.6455 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 160 total=7.1417 mle=1.5470 pcon=4.9482 forget=0.6465 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 210 total=7.2766 mle=1.6851 pcon=4.9471 forget=0.6444 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 260 total=7.2687 mle=1.6783 pcon=4.9459 forget=0.6445 nr=64 nf=64 protos=540 dmin_norm=NA
 85%|████████▌ | 17/20 [03:17<00:34, 11.47s/it] 90%|█████████ | 18/20 [03:29<00:22, 11.48s/it] 95%|█████████▌| 19/20 [03:40<00:11, 11.43s/it]100%|██████████| 20/20 [03:52<00:00, 11.46s/it]100%|██████████| 20/20 [03:52<00:00, 11.62s/it]
[loss] ep 16 it 310 total=7.2835 mle=1.6985 pcon=4.9446 forget=0.6403 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 16 it 360 total=7.2304 mle=1.6425 pcon=4.9435 forget=0.6445 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 17 it 20 total=7.0201 mle=1.4331 pcon=4.9425 forget=0.6444 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 70 total=7.1274 mle=1.5440 pcon=4.9417 forget=0.6417 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 120 total=7.3366 mle=1.7457 pcon=4.9406 forget=0.6503 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 170 total=7.2666 mle=1.6798 pcon=4.9396 forget=0.6472 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 220 total=7.2552 mle=1.6625 pcon=4.9385 forget=0.6542 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 270 total=7.2182 mle=1.6342 pcon=4.9378 forget=0.6462 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 320 total=7.0780 mle=1.4939 pcon=4.9369 forget=0.6472 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 17 it 370 total=7.3359 mle=1.7548 pcon=4.9361 forget=0.6449 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 18 it 30 total=7.1368 mle=1.5578 pcon=4.9355 forget=0.6435 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 80 total=7.1659 mle=1.5882 pcon=4.9346 forget=0.6430 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 130 total=7.0145 mle=1.4288 pcon=4.9341 forget=0.6516 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 180 total=7.0670 mle=1.4875 pcon=4.9331 forget=0.6463 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 230 total=7.1349 mle=1.5578 pcon=4.9321 forget=0.6450 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 280 total=7.1316 mle=1.5542 pcon=4.9315 forget=0.6459 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 330 total=7.3927 mle=1.8166 pcon=4.9309 forget=0.6452 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 18 it 380 total=7.2436 mle=1.6642 pcon=4.9303 forget=0.6492 nr=64 nf=64 protos=540 dmin_norm=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
[loss] ep 19 it 40 total=7.2703 mle=1.6902 pcon=4.9297 forget=0.6504 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 90 total=7.1487 mle=1.5709 pcon=4.9294 forget=0.6484 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 140 total=7.0954 mle=1.5130 pcon=4.9288 forget=0.6537 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 190 total=7.1621 mle=1.5878 pcon=4.9284 forget=0.6459 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 240 total=7.2717 mle=1.6995 pcon=4.9279 forget=0.6443 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 290 total=7.0840 mle=1.4991 pcon=4.9273 forget=0.6576 nr=64 nf=64 protos=540 dmin_norm=NA
[loss] ep 19 it 340 total=7.0704 mle=1.4967 pcon=4.9266 forget=0.6470 nr=64 nf=64 protos=540 dmin_norm=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from /home/shaokun/PALM/checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-0to9_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:15,  2.88it/s]  3%|▎         | 11/391 [00:00<00:12, 30.72it/s]  5%|▌         | 21/391 [00:00<00:07, 50.16it/s]  8%|▊         | 31/391 [00:00<00:05, 63.63it/s] 10%|█         | 41/391 [00:00<00:04, 73.08it/s] 13%|█▎        | 51/391 [00:00<00:04, 79.43it/s] 16%|█▌        | 61/391 [00:00<00:03, 83.71it/s] 18%|█▊        | 71/391 [00:01<00:03, 86.55it/s] 21%|██        | 81/391 [00:01<00:03, 88.89it/s] 23%|██▎       | 91/391 [00:01<00:03, 90.21it/s] 26%|██▌       | 101/391 [00:01<00:03, 91.04it/s] 28%|██▊       | 111/391 [00:01<00:03, 91.84it/s] 31%|███       | 121/391 [00:01<00:02, 92.21it/s] 34%|███▎      | 131/391 [00:01<00:02, 93.16it/s] 36%|███▌      | 141/391 [00:01<00:02, 93.79it/s] 39%|███▊      | 151/391 [00:01<00:02, 94.21it/s] 41%|████      | 161/391 [00:02<00:02, 94.43it/s] 44%|████▎     | 171/391 [00:02<00:02, 94.65it/s] 46%|████▋     | 181/391 [00:02<00:02, 94.87it/s] 49%|████▉     | 191/391 [00:02<00:02, 95.19it/s] 51%|█████▏    | 201/391 [00:02<00:01, 95.24it/s] 54%|█████▍    | 211/391 [00:02<00:01, 95.40it/s] 57%|█████▋    | 221/391 [00:02<00:01, 95.39it/s] 59%|█████▉    | 231/391 [00:02<00:01, 95.33it/s] 62%|██████▏   | 241/391 [00:02<00:01, 95.56it/s] 64%|██████▍   | 251/391 [00:02<00:01, 95.48it/s] 67%|██████▋   | 261/391 [00:03<00:01, 95.39it/s] 69%|██████▉   | 271/391 [00:03<00:01, 95.41it/s] 72%|███████▏  | 281/391 [00:03<00:01, 95.23it/s] 74%|███████▍  | 291/391 [00:03<00:01, 95.37it/s] 77%|███████▋  | 301/391 [00:03<00:00, 95.41it/s] 80%|███████▉  | 311/391 [00:03<00:00, 95.30it/s] 82%|████████▏ | 321/391 [00:03<00:00, 95.32it/s] 85%|████████▍ | 331/391 [00:03<00:00, 95.31it/s] 87%|████████▋ | 341/391 [00:03<00:00, 95.18it/s] 90%|████████▉ | 351/391 [00:04<00:00, 95.01it/s] 92%|█████████▏| 361/391 [00:04<00:00, 94.94it/s] 95%|█████████▍| 371/391 [00:04<00:00, 94.85it/s] 97%|█████████▋| 381/391 [00:04<00:00, 93.95it/s]100%|██████████| 391/391 [00:04<00:00, 91.50it/s]100%|██████████| 391/391 [00:04<00:00, 87.42it/s]
50000 images processed, 4.557288408279419 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.05it/s] 14%|█▍        | 11/79 [00:00<00:02, 31.94it/s] 27%|██▋       | 21/79 [00:00<00:01, 51.36it/s] 39%|███▉      | 31/79 [00:00<00:00, 64.52it/s] 52%|█████▏    | 41/79 [00:00<00:00, 73.77it/s] 65%|██████▍   | 51/79 [00:00<00:00, 79.75it/s] 77%|███████▋  | 61/79 [00:00<00:00, 84.04it/s] 90%|████████▉ | 71/79 [00:01<00:00, 87.53it/s]100%|██████████| 79/79 [00:01<00:00, 48.97it/s]
10000 images processed, 1.6363601684570312 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<00:56,  3.59it/s]  5%|▌         | 11/204 [00:00<00:05, 35.59it/s] 10%|█         | 21/204 [00:00<00:03, 55.28it/s] 15%|█▌        | 31/204 [00:00<00:02, 67.93it/s] 20%|██        | 41/204 [00:00<00:02, 76.36it/s] 25%|██▌       | 51/204 [00:00<00:01, 82.03it/s] 30%|██▉       | 61/204 [00:00<00:01, 85.92it/s] 35%|███▍      | 71/204 [00:01<00:01, 88.64it/s] 40%|███▉      | 81/204 [00:01<00:01, 89.99it/s] 45%|████▍     | 91/204 [00:01<00:01, 91.05it/s] 50%|████▉     | 101/204 [00:01<00:01, 91.34it/s] 54%|█████▍    | 111/204 [00:01<00:01, 92.33it/s] 59%|█████▉    | 121/204 [00:01<00:00, 91.99it/s] 64%|██████▍   | 131/204 [00:01<00:00, 92.63it/s] 69%|██████▉   | 141/204 [00:01<00:00, 93.01it/s] 74%|███████▍  | 151/204 [00:01<00:00, 93.45it/s] 79%|███████▉  | 161/204 [00:01<00:00, 93.74it/s] 84%|████████▍ | 171/204 [00:02<00:00, 93.96it/s] 89%|████████▊ | 181/204 [00:02<00:00, 94.06it/s] 94%|█████████▎| 191/204 [00:02<00:00, 94.00it/s] 99%|█████████▊| 201/204 [00:02<00:00, 94.57it/s]100%|██████████| 204/204 [00:02<00:00, 83.53it/s]
26032 images processed, 2.4984281063079834 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:38,  2.03it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.55it/s] 22%|██▏       | 17/79 [00:00<00:02, 23.03it/s] 32%|███▏      | 25/79 [00:01<00:01, 27.56it/s] 42%|████▏     | 33/79 [00:01<00:01, 30.47it/s] 52%|█████▏    | 41/79 [00:01<00:01, 32.52it/s] 62%|██████▏   | 49/79 [00:01<00:00, 33.98it/s] 72%|███████▏  | 57/79 [00:02<00:00, 35.04it/s] 82%|████████▏ | 65/79 [00:02<00:00, 36.34it/s] 92%|█████████▏| 73/79 [00:02<00:00, 40.78it/s]100%|██████████| 79/79 [00:02<00:00, 32.71it/s]
10000 images processed, 2.4515652656555176 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:29,  2.69it/s] 14%|█▍        | 11/79 [00:00<00:02, 29.07it/s] 27%|██▋       | 21/79 [00:00<00:01, 48.11it/s] 39%|███▉      | 31/79 [00:00<00:00, 61.76it/s] 52%|█████▏    | 41/79 [00:00<00:00, 71.46it/s] 65%|██████▍   | 51/79 [00:00<00:00, 78.36it/s] 77%|███████▋  | 61/79 [00:01<00:00, 83.21it/s] 90%|████████▉ | 71/79 [00:01<00:00, 86.84it/s]100%|██████████| 79/79 [00:01<00:00, 66.47it/s]
10000 images processed, 1.206444263458252 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:20,  3.34it/s] 16%|█▌        | 11/70 [00:00<00:01, 33.66it/s] 30%|███       | 21/70 [00:00<00:00, 53.35it/s] 44%|████▍     | 31/70 [00:00<00:00, 66.56it/s] 59%|█████▊    | 41/70 [00:00<00:00, 75.44it/s] 73%|███████▎  | 51/70 [00:00<00:00, 81.47it/s] 87%|████████▋ | 61/70 [00:00<00:00, 85.87it/s]100%|██████████| 70/70 [00:01<00:00, 68.05it/s]
8925 images processed, 1.0572633743286133 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:37,  1.17it/s] 13%|█▎        | 6/45 [00:00<00:04,  8.05it/s] 20%|██        | 9/45 [00:01<00:03,  9.07it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.30it/s] 31%|███       | 14/45 [00:01<00:02, 11.87it/s] 38%|███▊      | 17/45 [00:01<00:01, 14.60it/s] 44%|████▍     | 20/45 [00:01<00:01, 14.44it/s] 49%|████▉     | 22/45 [00:02<00:01, 12.20it/s] 58%|█████▊    | 26/45 [00:02<00:01, 15.14it/s] 67%|██████▋   | 30/45 [00:02<00:01, 12.13it/s] 73%|███████▎  | 33/45 [00:02<00:00, 13.70it/s] 84%|████████▍ | 38/45 [00:03<00:00, 16.83it/s] 89%|████████▉ | 40/45 [00:03<00:00, 17.18it/s] 93%|█████████▎| 42/45 [00:03<00:00, 11.27it/s]100%|██████████| 45/45 [00:03<00:00, 12.15it/s]
5640 images processed, 3.7262182235717773 seconds used

18.80504584312439
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.67  99.34
places365     68.66  80.92
LSUN          21.51  95.18
iSUN          72.90  81.32
dtd           39.49  91.04
forget        83.70  86.07
AVG           48.15  88.98
Forget-Acc: 0.7550 | Retain-Acc: 0.7441
Forget-as-OOD (retain known vs forget novel):
  FPR: 83.70 AUROC: 86.07 AUIN: 98.31
13.497900247573853
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05_rf.png
