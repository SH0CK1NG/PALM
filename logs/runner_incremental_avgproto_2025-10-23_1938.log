nohup: ignoring input
==== Stage 1: forget_inc={0,8,11,40,51}; forget_seen={}; all={0,8,11,40,51,66,67,88,94,57} ====
Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.1
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1', adapter_load_path=None, adapter_load_paths=None, lora_new_adapter_name='train_s1', lora_stack=True, lora_orth_enable=False, lora_orth_lambda=0.1, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc='0,8,11,40,51', forget_classes_seen=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=True, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] added trainable adapter 'train_s1'
[peft] set_adapter(list) failed: unhashable type: 'list'; fallback to last adapter
[peft] active adapter set to: train_s1
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 22082496
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.train_s1.weight
[debug] trainable: base_model.model.head.2.lora_A.train_s1.weight
[debug] trainable: base_model.model.head.2.lora_B.train_s1.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.train_s1.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.train_s1.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.train_s1.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.train_s1.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.train_s1.weight']
  0%|          | 0/50 [00:00<?, ?it/s]  2%|▏         | 1/50 [01:00<49:05, 60.11s/it]  4%|▍         | 2/50 [01:58<47:04, 58.85s/it]  6%|▌         | 3/50 [02:52<44:32, 56.86s/it]  8%|▊         | 4/50 [03:45<42:17, 55.17s/it] 10%|█         | 5/50 [04:47<43:11, 57.60s/it] 12%|█▏        | 6/50 [05:52<44:16, 60.37s/it] 14%|█▍        | 7/50 [06:45<41:24, 57.77s/it] 16%|█▌        | 8/50 [07:26<36:46, 52.53s/it][loss] ep 0 it 0 total=8.3437 mle=2.1072 pcon=5.2951 forget=1.4584 favg=-0.5171 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 0 it 50 total=7.8327 mle=1.6918 pcon=5.2908 forget=1.4585 favg=-0.6084 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 100 total=8.0912 mle=1.9220 pcon=5.2867 forget=1.4772 favg=-0.5947 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 150 total=8.4335 mle=2.2574 pcon=5.2827 forget=1.4275 favg=-0.5342 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 0 it 200 total=8.3605 mle=2.2335 pcon=5.2787 forget=1.4568 favg=-0.6084 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 250 total=7.7065 mle=1.5965 pcon=5.2747 forget=1.4476 favg=-0.6123 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 0 it 300 total=7.9812 mle=1.7074 pcon=5.2707 forget=1.4297 favg=-0.4265 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 350 total=8.0875 mle=1.7470 pcon=5.2669 forget=1.5067 favg=-0.4331 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 1 it 10 total=8.2183 mle=1.9762 pcon=5.2631 forget=1.4608 favg=-0.4819 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 1 it 60 total=8.3717 mle=2.0226 pcon=5.2595 forget=1.4663 favg=-0.3767 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 1 it 110 total=8.0597 mle=2.0183 pcon=5.2558 forget=1.4360 favg=-0.6504 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 1 it 160 total=8.0853 mle=1.8491 pcon=5.2521 forget=1.5104 favg=-0.5264 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 1 it 210 total=8.2426 mle=2.0060 pcon=5.2486 forget=1.4229 favg=-0.4348 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 1 it 260 total=7.9802 mle=1.8232 pcon=5.2449 forget=1.4751 favg=-0.5630 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 1 it 310 total=7.8289 mle=1.7030 pcon=5.2415 forget=1.4303 favg=-0.5459 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 1 it 360 total=8.1552 mle=2.0524 pcon=5.2378 forget=1.4046 favg=-0.5396 nr=33 nf=33 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 2 it 20 total=7.7004 mle=1.5833 pcon=5.2342 forget=1.4787 favg=-0.5957 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 2 it 70 total=8.0737 mle=2.1086 pcon=5.2304 forget=1.4535 favg=-0.7188 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 2 it 120 total=7.6208 mle=1.8031 pcon=5.2266 forget=1.4007 favg=-0.8096 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 2 it 170 total=7.7961 mle=2.0094 pcon=5.2232 forget=1.4492 favg=-0.8857 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 2 it 220 total=7.9470 mle=2.4311 pcon=5.2197 forget=1.4583 favg=-1.1621 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 2 it 270 total=6.8169 mle=1.5953 pcon=5.2157 forget=1.4747 favg=-1.4688 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 2 it 320 total=7.0214 mle=1.8704 pcon=5.2118 forget=1.5085 favg=-1.5693 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 2 it 370 total=7.6703 mle=2.7146 pcon=5.2078 forget=1.5369 favg=-1.7891 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 3 it 30 total=6.8650 mle=1.9169 pcon=5.2038 forget=1.5783 favg=-1.8340 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 3 it 80 total=7.0405 mle=2.1739 pcon=5.2000 forget=1.6149 favg=-1.9482 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 3 it 130 total=6.6181 mle=1.7593 pcon=5.1958 forget=1.5750 favg=-1.9121 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 3 it 180 total=6.6710 mle=1.7969 pcon=5.1920 forget=1.6177 favg=-1.9355 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 3 it 230 total=6.6651 mle=1.8362 pcon=5.1882 forget=1.6095 favg=-1.9688 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 3 it 280 total=6.6517 mle=1.8416 pcon=5.1846 forget=1.6177 favg=-1.9922 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 3 it 330 total=6.8400 mle=2.0205 pcon=5.1808 forget=1.6523 favg=-2.0137 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 3 it 380 total=6.5070 mle=1.6884 pcon=5.1770 forget=1.6728 favg=-2.0312 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 4 it 40 total=6.6725 mle=1.6885 pcon=5.1736 forget=1.6669 favg=-1.8564 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 4 it 90 total=6.7675 mle=1.6669 pcon=5.1702 forget=1.6981 favg=-1.7676 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 4 it 140 total=7.4930 mle=2.1383 pcon=5.1667 forget=1.7573 favg=-1.5693 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 4 it 190 total=8.7579 mle=2.6564 pcon=5.1632 forget=1.7782 favg=-0.8398 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 4 it 240 total=9.6676 mle=1.9579 pcon=5.1596 forget=1.7888 favg=0.7612 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 4 it 290 total=10.8469 mle=2.1987 pcon=5.1563 forget=1.7858 favg=1.7061 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 4 it 340 total=10.4444 mle=1.7459 pcon=5.1531 forget=1.7329 favg=1.8125 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 5 it 0 total=10.8728 mle=2.2292 pcon=5.1500 forget=1.6557 favg=1.8379 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 5 it 50 total=9.9559 mle=1.7570 pcon=5.1468 forget=1.5228 favg=1.5293 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 5 it 100 total=9.6002 mle=1.8937 pcon=5.1442 forget=1.5018 favg=1.0605 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 5 it 150 total=9.2701 mle=2.0429 pcon=5.1414 forget=1.4383 favg=0.6475 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 5 it 200 total=9.1477 mle=2.1220 pcon=5.1389 forget=1.4988 favg=0.3879 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 5 it 250 total=8.4884 mle=1.8148 pcon=5.1367 forget=1.5199 favg=0.0170 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 5 it 300 total=8.2167 mle=1.7555 pcon=5.1342 forget=1.4065 favg=-0.0795 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 5 it 350 total=8.2536 mle=1.9110 pcon=5.1318 forget=1.4240 favg=-0.2131 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 6 it 10 total=8.8666 mle=1.9810 pcon=5.1297 forget=1.4524 favg=0.3035 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 6 it 60 total=8.5356 mle=1.8626 pcon=5.1274 forget=1.4337 favg=0.1120 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 6 it 110 total=7.8860 mle=1.7124 pcon=5.1250 forget=1.4541 favg=-0.4055 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 6 it 160 total=7.7176 mle=2.0021 pcon=5.1226 forget=1.4572 favg=-0.8643 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 6 it 210 total=7.8514 mle=2.5582 pcon=5.1198 forget=1.5044 favg=-1.3311 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 6 it 260 total=7.2540 mle=1.9333 pcon=5.1172 forget=1.5228 favg=-1.3193 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 6 it 310 total=7.7582 mle=1.9255 pcon=5.1145 forget=1.5869 favg=-0.8687 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 6 it 360 total=7.9369 mle=1.5196 pcon=5.1118 forget=1.5809 favg=-0.2754 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 7 it 20 total=8.5841 mle=1.6971 pcon=5.1092 forget=1.5511 favg=0.2268 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 7 it 70 total=8.9844 mle=1.9134 pcon=5.1066 forget=1.4570 favg=0.5073 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 7 it 120 total=9.1719 mle=1.8512 pcon=5.1042 forget=1.4328 favg=0.7837 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 7 it 170 total=9.9516 mle=2.3515 pcon=5.1017 forget=1.4163 favg=1.0820 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 7 it 220 total=9.5421 mle=1.7934 pcon=5.0998 forget=1.4244 favg=1.2246 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 7 it 270 total=9.5668 mle=1.8165 pcon=5.0977 forget=1.4603 favg=1.1924 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 7 it 320 total=8.9465 mle=1.4125 pcon=5.0956 forget=1.4258 favg=1.0127 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 7 it 370 total=8.5356 mle=1.8061 pcon=5.0932 forget=1.3954 favg=0.2408 nr=27 nf=27 protos=570 fproto_sim=NA
 18%|█▊        | 9/50 [08:26<37:24, 54.74s/it] 20%|██        | 10/50 [09:21<36:42, 55.07s/it] 22%|██▏       | 11/50 [10:05<33:35, 51.69s/it] 24%|██▍       | 12/50 [10:48<30:56, 48.87s/it] 26%|██▌       | 13/50 [11:26<28:10, 45.70s/it] 28%|██▊       | 14/50 [12:06<26:17, 43.81s/it] 30%|███       | 15/50 [12:39<23:44, 40.70s/it] 32%|███▏      | 16/50 [13:10<21:24, 37.77s/it][loss] ep 8 it 30 total=8.0088 mle=1.7404 pcon=5.0912 forget=1.4070 favg=-0.2299 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 8 it 80 total=7.5440 mle=1.7929 pcon=5.0892 forget=1.4324 favg=-0.7705 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 8 it 130 total=8.6089 mle=1.8550 pcon=5.0875 forget=1.4142 favg=0.2522 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 8 it 180 total=9.2540 mle=1.8774 pcon=5.0859 forget=1.4168 favg=0.8740 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 8 it 230 total=8.8964 mle=1.9255 pcon=5.0843 forget=1.4213 favg=0.4653 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 8 it 280 total=8.3002 mle=1.8941 pcon=5.0826 forget=1.3853 favg=-0.0617 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 8 it 330 total=7.5821 mle=1.9281 pcon=5.0805 forget=1.4393 favg=-0.8657 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 8 it 380 total=6.6236 mle=1.4785 pcon=5.0785 forget=1.3987 favg=-1.3320 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 9 it 40 total=7.0990 mle=1.8342 pcon=5.0762 forget=1.4044 favg=-1.2158 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 9 it 90 total=8.1978 mle=1.9743 pcon=5.0743 forget=1.4115 favg=-0.2622 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 9 it 140 total=9.0866 mle=1.6843 pcon=5.0722 forget=1.4404 favg=0.8896 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 9 it 190 total=9.5101 mle=1.8700 pcon=5.0698 forget=1.4140 favg=1.1562 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 9 it 240 total=9.1781 mle=1.7642 pcon=5.0678 forget=1.4253 favg=0.9209 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 9 it 290 total=8.9467 mle=1.8737 pcon=5.0657 forget=1.4301 favg=0.5771 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 9 it 340 total=8.1677 mle=1.6123 pcon=5.0636 forget=1.3915 favg=0.1003 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 10 it 0 total=7.8074 mle=1.7893 pcon=5.0618 forget=1.3802 favg=-0.4238 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 10 it 50 total=7.1012 mle=1.6569 pcon=5.0600 forget=1.3541 favg=-0.9697 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 10 it 100 total=6.6368 mle=1.7282 pcon=5.0578 forget=1.3646 favg=-1.5137 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 10 it 150 total=6.9421 mle=2.1180 pcon=5.0557 forget=1.3641 favg=-1.5957 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 10 it 200 total=8.3660 mle=2.0676 pcon=5.0537 forget=1.4226 favg=-0.1779 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 10 it 250 total=10.0223 mle=2.0532 pcon=5.0518 forget=1.4895 favg=1.4277 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 10 it 300 total=9.8112 mle=1.9177 pcon=5.0494 forget=1.4954 favg=1.3486 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 10 it 350 total=9.0331 mle=1.6806 pcon=5.0469 forget=1.4766 favg=0.8291 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 11 it 10 total=8.7706 mle=2.0823 pcon=5.0449 forget=1.4572 favg=0.1863 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 11 it 60 total=8.2481 mle=2.2494 pcon=5.0425 forget=1.3911 favg=-0.4348 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 11 it 110 total=7.4702 mle=2.0312 pcon=5.0402 forget=1.3724 favg=-0.9736 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 11 it 160 total=7.3608 mle=1.9206 pcon=5.0381 forget=1.4006 favg=-0.9985 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 11 it 210 total=8.3384 mle=1.8499 pcon=5.0359 forget=1.4094 favg=0.0433 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 11 it 260 total=9.2196 mle=1.4965 pcon=5.0338 forget=1.4100 favg=1.2793 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 11 it 310 total=9.3768 mle=1.5633 pcon=5.0317 forget=1.3815 favg=1.4004 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 11 it 360 total=8.9073 mle=1.6127 pcon=5.0294 forget=1.3799 favg=0.8853 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 12 it 20 total=8.1570 mle=1.8847 pcon=5.0267 forget=1.3915 favg=-0.1460 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 70 total=7.2167 mle=1.7105 pcon=5.0241 forget=1.3649 favg=-0.8828 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 12 it 120 total=7.2505 mle=1.7886 pcon=5.0215 forget=1.3599 favg=-0.9194 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 12 it 170 total=7.4534 mle=1.7377 pcon=5.0189 forget=1.3775 favg=-0.6807 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 220 total=7.9432 mle=1.6770 pcon=5.0160 forget=1.3748 favg=-0.1245 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 12 it 270 total=8.0923 mle=1.5787 pcon=5.0132 forget=1.4084 favg=0.0919 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 320 total=8.3591 mle=1.9282 pcon=5.0107 forget=1.3680 favg=0.0522 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 12 it 370 total=7.9591 mle=1.7389 pcon=5.0080 forget=1.3707 favg=-0.1584 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 13 it 30 total=7.9979 mle=1.6023 pcon=5.0055 forget=1.3749 favg=0.0153 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 13 it 80 total=8.5767 mle=1.5971 pcon=5.0030 forget=1.4166 favg=0.5601 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 13 it 130 total=9.0379 mle=1.8054 pcon=5.0005 forget=1.3852 favg=0.8467 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 13 it 180 total=8.9228 mle=1.6166 pcon=4.9982 forget=1.3930 favg=0.9150 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 13 it 230 total=8.5980 mle=1.5639 pcon=4.9954 forget=1.3912 favg=0.6475 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 13 it 280 total=8.3717 mle=1.9913 pcon=4.9929 forget=1.3971 favg=-0.0096 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 13 it 330 total=7.5031 mle=1.6533 pcon=4.9900 forget=1.3808 favg=-0.5210 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 13 it 380 total=6.8105 mle=1.7571 pcon=4.9872 forget=1.3475 favg=-1.2812 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 14 it 40 total=6.1897 mle=1.5544 pcon=4.9841 forget=1.3601 favg=-1.7090 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 90 total=6.1992 mle=1.4351 pcon=4.9815 forget=1.3471 favg=-1.5645 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 140 total=7.1019 mle=1.5350 pcon=4.9787 forget=1.3527 favg=-0.7646 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 14 it 190 total=9.1747 mle=1.7578 pcon=4.9760 forget=1.3589 favg=1.0820 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 240 total=9.6808 mle=1.6874 pcon=4.9732 forget=1.3668 favg=1.6533 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 14 it 290 total=9.7231 mle=1.6416 pcon=4.9702 forget=1.3721 favg=1.7393 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 14 it 340 total=9.7528 mle=1.6902 pcon=4.9673 forget=1.3893 favg=1.7061 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 0 total=9.7018 mle=1.9041 pcon=4.9643 forget=1.3763 favg=1.4570 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 15 it 50 total=9.2630 mle=1.7682 pcon=4.9614 forget=1.3811 favg=1.1523 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 15 it 100 total=8.6835 mle=1.5612 pcon=4.9583 forget=1.3935 favg=0.7705 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 150 total=8.1832 mle=1.5665 pcon=4.9551 forget=1.4040 favg=0.2576 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 200 total=7.3788 mle=1.6355 pcon=4.9520 forget=1.3811 favg=-0.5898 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 15 it 250 total=6.7307 mle=1.5589 pcon=4.9487 forget=1.3452 favg=-1.1221 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 15 it 300 total=6.3291 mle=1.6587 pcon=4.9454 forget=1.3247 favg=-1.5996 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 350 total=6.4445 mle=1.7774 pcon=4.9424 forget=1.3282 favg=-1.6035 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 16 it 10 total=6.7402 mle=1.3916 pcon=4.9396 forget=1.3474 favg=-0.9385 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 16 it 60 total=8.5121 mle=1.4787 pcon=4.9367 forget=1.3775 favg=0.7192 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 16 it 110 total=8.9023 mle=1.7017 pcon=4.9337 forget=1.3870 favg=0.8799 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 16 it 160 total=8.3600 mle=1.7463 pcon=4.9309 forget=1.3749 favg=0.3079 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 16 it 210 total=7.5171 mle=1.6649 pcon=4.9282 forget=1.3445 favg=-0.4204 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 16 it 260 total=6.9775 mle=1.6331 pcon=4.9255 forget=1.3319 favg=-0.9131 nr=25 nf=25 protos=570 fproto_sim=NA
 34%|███▍      | 17/50 [13:38<19:11, 34.88s/it] 36%|███▌      | 18/50 [14:07<17:40, 33.13s/it] 38%|███▊      | 19/50 [14:37<16:37, 32.19s/it] 40%|████      | 20/50 [15:07<15:42, 31.42s/it] 42%|████▏     | 21/50 [15:36<14:51, 30.76s/it] 44%|████▍     | 22/50 [16:08<14:29, 31.06s/it] 46%|████▌     | 23/50 [16:36<13:33, 30.14s/it] 48%|████▊     | 24/50 [17:09<13:28, 31.10s/it] 50%|█████     | 25/50 [17:44<13:25, 32.23s/it][loss] ep 16 it 310 total=7.1660 mle=1.8103 pcon=4.9228 forget=1.3366 favg=-0.9038 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 16 it 360 total=7.3565 mle=1.5886 pcon=4.9202 forget=1.3608 favg=-0.5132 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 17 it 20 total=8.0303 mle=1.6337 pcon=4.9175 forget=1.3879 favg=0.0912 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 70 total=8.5138 mle=1.5302 pcon=4.9151 forget=1.3981 favg=0.6704 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 120 total=8.9940 mle=1.7452 pcon=4.9124 forget=1.3926 favg=0.9438 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 170 total=8.6725 mle=1.5291 pcon=4.9095 forget=1.3878 favg=0.8462 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 17 it 220 total=8.6982 mle=1.8413 pcon=4.9066 forget=1.3830 favg=0.5674 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 17 it 270 total=7.7880 mle=1.5069 pcon=4.9037 forget=1.3698 favg=0.0076 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 17 it 320 total=7.4739 mle=1.7550 pcon=4.9009 forget=1.3751 favg=-0.5571 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 17 it 370 total=7.2915 mle=1.9989 pcon=4.8978 forget=1.3801 favg=-0.9854 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 18 it 30 total=7.0415 mle=1.9397 pcon=4.8949 forget=1.3807 favg=-1.1738 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 18 it 80 total=7.4794 mle=2.1113 pcon=4.8921 forget=1.4066 favg=-0.9307 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 18 it 130 total=7.7969 mle=2.0452 pcon=4.8893 forget=1.4361 favg=-0.5737 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 18 it 180 total=7.9624 mle=1.8046 pcon=4.8865 forget=1.4511 favg=-0.1798 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 18 it 230 total=7.9198 mle=1.7367 pcon=4.8839 forget=1.4649 favg=-0.1656 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 18 it 280 total=7.6396 mle=1.5656 pcon=4.8811 forget=1.4590 favg=-0.2661 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 18 it 330 total=7.1594 mle=1.5370 pcon=4.8781 forget=1.4332 favg=-0.6890 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 18 it 380 total=6.9762 mle=1.6749 pcon=4.8753 forget=1.3996 favg=-0.9736 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 19 it 40 total=6.6493 mle=1.6470 pcon=4.8725 forget=1.3837 favg=-1.2539 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 19 it 90 total=6.5595 mle=1.8350 pcon=4.8697 forget=1.3636 favg=-1.5088 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 19 it 140 total=6.1459 mle=1.6727 pcon=4.8671 forget=1.3376 favg=-1.7314 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 19 it 190 total=6.2657 mle=1.7967 pcon=4.8643 forget=1.3596 favg=-1.7549 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 19 it 240 total=5.9013 mle=1.4720 pcon=4.8616 forget=1.3489 favg=-1.7812 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 19 it 290 total=6.0764 mle=1.5880 pcon=4.8590 forget=1.3579 favg=-1.7285 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 19 it 340 total=6.2023 mle=1.6659 pcon=4.8564 forget=1.3724 favg=-1.6924 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 20 it 0 total=6.3447 mle=1.7701 pcon=4.8537 forget=1.3791 favg=-1.6582 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 20 it 50 total=6.1138 mle=1.5129 pcon=4.8510 forget=1.3934 favg=-1.6436 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 20 it 100 total=6.4359 mle=1.7863 pcon=4.8486 forget=1.4105 favg=-1.6094 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 20 it 150 total=6.4726 mle=1.8942 pcon=4.8458 forget=1.4221 favg=-1.6895 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 20 it 200 total=6.3323 mle=1.8412 pcon=4.8433 forget=1.4183 favg=-1.7705 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 20 it 250 total=5.9842 mle=1.6181 pcon=4.8406 forget=1.4297 favg=-1.9043 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 20 it 300 total=5.8255 mle=1.6348 pcon=4.8381 forget=1.4268 favg=-2.0742 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 20 it 350 total=5.6837 mle=1.5560 pcon=4.8355 forget=1.4309 favg=-2.1387 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 21 it 10 total=5.7970 mle=1.7308 pcon=4.8332 forget=1.4245 favg=-2.1914 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 21 it 60 total=5.8109 mle=1.6430 pcon=4.8308 forget=1.4288 favg=-2.0918 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 21 it 110 total=5.9271 mle=1.4330 pcon=4.8284 forget=1.4390 favg=-1.7734 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 21 it 160 total=6.4942 mle=1.5722 pcon=4.8259 forget=1.4272 favg=-1.3311 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 21 it 210 total=7.3002 mle=1.8532 pcon=4.8237 forget=1.4305 favg=-0.8071 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 21 it 260 total=7.6301 mle=1.5692 pcon=4.8212 forget=1.4363 favg=-0.1967 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 21 it 310 total=8.3951 mle=1.8097 pcon=4.8188 forget=1.4496 favg=0.3169 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 21 it 360 total=8.3480 mle=1.4695 pcon=4.8166 forget=1.4477 favg=0.6143 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 22 it 20 total=8.5675 mle=1.4460 pcon=4.8144 forget=1.4736 favg=0.8335 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 22 it 70 total=8.9051 mle=1.8359 pcon=4.8122 forget=1.5050 favg=0.7520 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 22 it 120 total=8.4513 mle=1.8166 pcon=4.8099 forget=1.5186 favg=0.3062 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 22 it 170 total=7.7234 mle=1.7246 pcon=4.8078 forget=1.5159 favg=-0.3250 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 22 it 220 total=6.3087 mle=1.4204 pcon=4.8055 forget=1.5019 favg=-1.4189 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 22 it 270 total=6.4390 mle=1.6117 pcon=4.8032 forget=1.4958 favg=-1.4717 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 22 it 320 total=6.8095 mle=1.8947 pcon=4.8011 forget=1.4966 favg=-1.3828 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 22 it 370 total=6.2090 mle=1.5086 pcon=4.7989 forget=1.4884 favg=-1.5869 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 23 it 30 total=6.2856 mle=1.7652 pcon=4.7968 forget=1.4834 favg=-1.7598 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 23 it 80 total=5.8868 mle=1.5366 pcon=4.7946 forget=1.4805 favg=-1.9248 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 23 it 130 total=5.8839 mle=1.6680 pcon=4.7926 forget=1.4683 favg=-2.0449 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 23 it 180 total=5.8013 mle=1.6840 pcon=4.7904 forget=1.4577 favg=-2.1309 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 23 it 230 total=5.6070 mle=1.4917 pcon=4.7884 forget=1.4538 favg=-2.1270 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 23 it 280 total=6.0051 mle=1.8736 pcon=4.7865 forget=1.4506 favg=-2.1055 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 23 it 330 total=5.9824 mle=1.6543 pcon=4.7844 forget=1.4686 favg=-1.9248 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 23 it 380 total=6.1616 mle=1.4890 pcon=4.7823 forget=1.4597 favg=-1.5693 nr=21 nf=21 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 24 it 40 total=6.8461 mle=1.6301 pcon=4.7803 forget=1.4641 favg=-1.0283 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 24 it 90 total=7.3025 mle=1.5337 pcon=4.7783 forget=1.4851 favg=-0.4946 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 24 it 140 total=7.6618 mle=1.6819 pcon=4.7763 forget=1.5018 favg=-0.2981 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 24 it 190 total=8.0669 mle=1.8564 pcon=4.7743 forget=1.5217 favg=-0.0856 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 24 it 240 total=8.1541 mle=1.7946 pcon=4.7723 forget=1.5402 favg=0.0471 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 24 it 290 total=7.9312 mle=1.6572 pcon=4.7703 forget=1.5556 favg=-0.0520 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 24 it 340 total=7.4340 mle=1.4305 pcon=4.7683 forget=1.5622 favg=-0.3271 nr=31 nf=31 protos=570 fproto_sim=NA
 52%|█████▏    | 26/50 [18:20<13:17, 33.21s/it] 54%|█████▍    | 27/50 [18:48<12:12, 31.84s/it] 56%|█████▌    | 28/50 [19:17<11:20, 30.92s/it] 58%|█████▊    | 29/50 [19:48<10:46, 30.81s/it] 60%|██████    | 30/50 [20:16<10:00, 30.01s/it] 62%|██████▏   | 31/50 [20:45<09:27, 29.87s/it] 64%|██████▍   | 32/50 [21:14<08:48, 29.39s/it] 66%|██████▌   | 33/50 [21:43<08:21, 29.51s/it][loss] ep 25 it 0 total=7.0857 mle=1.4894 pcon=4.7664 forget=1.5765 favg=-0.7466 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 50 total=6.6827 mle=1.7402 pcon=4.7645 forget=1.5705 favg=-1.3926 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 25 it 100 total=6.4578 mle=1.8706 pcon=4.7628 forget=1.5900 favg=-1.7656 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 25 it 150 total=6.4670 mle=1.8039 pcon=4.7610 forget=1.5837 favg=-1.6816 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 25 it 200 total=6.3610 mle=1.6835 pcon=4.7590 forget=1.6001 favg=-1.6816 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 250 total=6.1376 mle=1.5000 pcon=4.7572 forget=1.6021 favg=-1.7217 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 25 it 300 total=6.0242 mle=1.6815 pcon=4.7554 forget=1.6029 favg=-2.0156 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 350 total=5.8602 mle=1.5933 pcon=4.7536 forget=1.5954 favg=-2.0820 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 10 total=5.9698 mle=1.9035 pcon=4.7518 forget=1.5958 favg=-2.2812 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 26 it 60 total=5.6539 mle=1.6291 pcon=4.7501 forget=1.6028 favg=-2.3281 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 110 total=5.6462 mle=1.6045 pcon=4.7482 forget=1.5903 favg=-2.2969 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 26 it 160 total=5.5270 mle=1.4460 pcon=4.7465 forget=1.5962 favg=-2.2617 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 26 it 210 total=5.7241 mle=1.4703 pcon=4.7448 forget=1.6105 favg=-2.1016 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 26 it 260 total=6.2156 mle=1.8945 pcon=4.7432 forget=1.6131 favg=-2.0352 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 26 it 310 total=5.8345 mle=1.4061 pcon=4.7413 forget=1.6198 favg=-1.9326 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 360 total=6.0081 mle=1.5511 pcon=4.7396 forget=1.6411 favg=-1.9238 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 27 it 20 total=5.9280 mle=1.5247 pcon=4.7380 forget=1.6555 favg=-1.9902 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 27 it 70 total=6.1188 mle=1.7171 pcon=4.7364 forget=1.6790 favg=-2.0137 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 27 it 120 total=6.3072 mle=1.7614 pcon=4.7348 forget=1.6889 favg=-1.8779 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 27 it 170 total=6.3990 mle=1.6119 pcon=4.7332 forget=1.7102 favg=-1.6562 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 27 it 220 total=6.3896 mle=1.5588 pcon=4.7316 forget=1.7222 favg=-1.6230 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 27 it 270 total=6.5363 mle=1.7842 pcon=4.7299 forget=1.7410 favg=-1.7188 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 27 it 320 total=6.1225 mle=1.5477 pcon=4.7283 forget=1.7469 favg=-1.9004 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 27 it 370 total=6.1385 mle=1.6303 pcon=4.7268 forget=1.7658 favg=-1.9844 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 28 it 30 total=6.1749 mle=1.6997 pcon=4.7254 forget=1.7694 favg=-2.0195 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 28 it 80 total=6.0731 mle=1.7247 pcon=4.7239 forget=1.7905 favg=-2.1660 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 28 it 130 total=5.9507 mle=1.5938 pcon=4.7224 forget=1.7967 favg=-2.1621 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 28 it 180 total=5.9035 mle=1.5332 pcon=4.7208 forget=1.8076 favg=-2.1582 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 28 it 230 total=5.8899 mle=1.5725 pcon=4.7193 forget=1.8109 favg=-2.2129 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 28 it 280 total=5.9045 mle=1.6654 pcon=4.7179 forget=1.7986 favg=-2.2773 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 28 it 330 total=5.8897 mle=1.7028 pcon=4.7164 forget=1.8005 favg=-2.3301 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 28 it 380 total=5.5672 mle=1.3673 pcon=4.7149 forget=1.7936 favg=-2.3086 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 29 it 40 total=5.9552 mle=1.7370 pcon=4.7135 forget=1.7977 favg=-2.2930 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 29 it 90 total=5.9284 mle=1.7558 pcon=4.7120 forget=1.8043 favg=-2.3438 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 29 it 140 total=5.6892 mle=1.4819 pcon=4.7107 forget=1.8053 favg=-2.3086 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 29 it 190 total=5.8789 mle=1.7050 pcon=4.7092 forget=1.8143 favg=-2.3496 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 29 it 240 total=5.8695 mle=1.6578 pcon=4.7079 forget=1.8124 favg=-2.3086 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 29 it 290 total=5.9041 mle=1.7003 pcon=4.7066 forget=1.8175 favg=-2.3203 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 29 it 340 total=5.7636 mle=1.5420 pcon=4.7052 forget=1.8230 favg=-2.3066 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 30 it 0 total=5.7614 mle=1.4981 pcon=4.7039 forget=1.8132 favg=-2.2539 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 30 it 50 total=5.9595 mle=1.7156 pcon=4.7026 forget=1.8343 favg=-2.2930 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 30 it 100 total=5.6828 mle=1.4075 pcon=4.7013 forget=1.8357 favg=-2.2617 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 30 it 150 total=5.9468 mle=1.6897 pcon=4.7002 forget=1.8498 favg=-2.2930 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 30 it 200 total=5.8856 mle=1.6027 pcon=4.6989 forget=1.8457 favg=-2.2617 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 30 it 250 total=5.7991 mle=1.5235 pcon=4.6975 forget=1.8554 favg=-2.2773 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 30 it 300 total=5.9281 mle=1.6396 pcon=4.6963 forget=1.8578 favg=-2.2656 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 30 it 350 total=5.9392 mle=1.5933 pcon=4.6953 forget=1.8616 favg=-2.2109 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 31 it 10 total=5.7780 mle=1.4482 pcon=4.6941 forget=1.8739 favg=-2.2383 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 31 it 60 total=6.0541 mle=1.7208 pcon=4.6930 forget=1.8806 favg=-2.2402 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 31 it 110 total=5.9891 mle=1.6855 pcon=4.6918 forget=1.8969 favg=-2.2852 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 31 it 160 total=5.8590 mle=1.5208 pcon=4.6906 forget=1.9034 favg=-2.2559 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 31 it 210 total=6.1691 mle=1.8016 pcon=4.6893 forget=1.9028 favg=-2.2246 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 31 it 260 total=6.0276 mle=1.7251 pcon=4.6881 forget=1.9229 favg=-2.3086 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 31 it 310 total=6.0426 mle=1.7054 pcon=4.6869 forget=1.9277 favg=-2.2773 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 31 it 360 total=5.8487 mle=1.5315 pcon=4.6857 forget=1.9382 favg=-2.3066 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 32 it 20 total=5.8141 mle=1.4899 pcon=4.6846 forget=1.9482 favg=-2.3086 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 32 it 70 total=6.1669 mle=1.7962 pcon=4.6836 forget=1.9567 favg=-2.2695 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 32 it 120 total=5.9118 mle=1.5360 pcon=4.6825 forget=1.9589 favg=-2.2656 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 32 it 170 total=6.0436 mle=1.6736 pcon=4.6815 forget=1.9620 favg=-2.2734 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 32 it 220 total=6.0264 mle=1.6546 pcon=4.6805 forget=1.9589 favg=-2.2676 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 32 it 270 total=5.8079 mle=1.4446 pcon=4.6795 forget=1.9709 favg=-2.2871 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 32 it 320 total=5.9898 mle=1.6337 pcon=4.6785 forget=1.9764 favg=-2.2988 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 32 it 370 total=5.8506 mle=1.5097 pcon=4.6775 forget=1.9954 favg=-2.3320 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 33 it 30 total=5.9835 mle=1.6353 pcon=4.6765 forget=1.9979 favg=-2.3262 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 33 it 80 total=5.8169 mle=1.4685 pcon=4.6754 forget=1.9972 favg=-2.3242 nr=37 nf=37 protos=570 fproto_sim=NA
 68%|██████▊   | 34/50 [22:13<07:50, 29.43s/it] 70%|███████   | 35/50 [22:50<07:56, 31.75s/it] 72%|███████▏  | 36/50 [23:27<07:45, 33.25s/it] 74%|███████▍  | 37/50 [24:01<07:17, 33.67s/it] 76%|███████▌  | 38/50 [24:38<06:54, 34.53s/it] 78%|███████▊  | 39/50 [25:14<06:23, 34.90s/it] 80%|████████  | 40/50 [25:51<05:56, 35.68s/it] 82%|████████▏ | 41/50 [26:27<05:21, 35.67s/it] 84%|████████▍ | 42/50 [27:03<04:47, 35.99s/it][loss] ep 33 it 130 total=5.9277 mle=1.5388 pcon=4.6745 forget=1.9917 favg=-2.2773 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 33 it 180 total=6.2000 mle=1.8471 pcon=4.6735 forget=2.0094 favg=-2.3301 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 33 it 230 total=6.3714 mle=1.9519 pcon=4.6726 forget=1.9891 favg=-2.2422 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 33 it 280 total=6.3786 mle=2.0354 pcon=4.6718 forget=2.0172 favg=-2.3457 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 33 it 330 total=6.4565 mle=2.0743 pcon=4.6709 forget=1.9965 favg=-2.2852 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 33 it 380 total=6.1509 mle=1.8026 pcon=4.6700 forget=2.0240 favg=-2.3457 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 34 it 40 total=6.0314 mle=1.6534 pcon=4.6691 forget=2.0233 favg=-2.3145 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 34 it 90 total=5.8106 mle=1.4670 pcon=4.6681 forget=2.0368 favg=-2.3613 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 34 it 140 total=5.8653 mle=1.5027 pcon=4.6672 forget=2.0293 favg=-2.3340 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 34 it 190 total=6.1287 mle=1.7467 pcon=4.6663 forget=2.0320 favg=-2.3164 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 34 it 240 total=6.3054 mle=1.9383 pcon=4.6654 forget=2.0338 favg=-2.3320 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 34 it 290 total=6.0551 mle=1.6950 pcon=4.6646 forget=2.0450 favg=-2.3496 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 34 it 340 total=5.9243 mle=1.5559 pcon=4.6638 forget=2.0463 favg=-2.3418 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 35 it 0 total=5.9474 mle=1.5769 pcon=4.6630 forget=2.0493 favg=-2.3418 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 35 it 50 total=6.2162 mle=1.8581 pcon=4.6620 forget=2.0672 favg=-2.3711 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 35 it 100 total=6.1389 mle=1.7798 pcon=4.6612 forget=2.0611 favg=-2.3633 nr=43 nf=43 protos=570 fproto_sim=NA
[loss] ep 35 it 150 total=6.2024 mle=1.8203 pcon=4.6604 forget=2.0713 favg=-2.3496 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 35 it 200 total=5.8419 mle=1.4816 pcon=4.6596 forget=2.0739 favg=-2.3730 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 35 it 250 total=6.0392 mle=1.6749 pcon=4.6587 forget=2.0806 favg=-2.3750 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 35 it 300 total=5.9417 mle=1.5567 pcon=4.6580 forget=2.0669 favg=-2.3398 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 35 it 350 total=6.2174 mle=1.8420 pcon=4.6572 forget=2.0639 favg=-2.3457 nr=21 nf=21 protos=570 fproto_sim=NA
[loss] ep 36 it 10 total=6.2510 mle=1.8668 pcon=4.6565 forget=2.0831 favg=-2.3555 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 36 it 60 total=5.9510 mle=1.5911 pcon=4.6557 forget=2.0831 favg=-2.3789 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 36 it 110 total=5.9589 mle=1.5931 pcon=4.6550 forget=2.0897 favg=-2.3789 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 36 it 160 total=6.1326 mle=1.7491 pcon=4.6543 forget=2.0807 favg=-2.3516 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 36 it 210 total=5.8478 mle=1.4761 pcon=4.6536 forget=2.0873 favg=-2.3691 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 36 it 260 total=5.8163 mle=1.4453 pcon=4.6528 forget=2.0971 favg=-2.3789 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 36 it 310 total=6.0865 mle=1.6836 pcon=4.6520 forget=2.0770 favg=-2.3262 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 36 it 360 total=5.8017 mle=1.4244 pcon=4.6513 forget=2.0972 favg=-2.3711 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 37 it 20 total=6.2005 mle=1.8175 pcon=4.6505 forget=2.1036 favg=-2.3711 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 37 it 70 total=6.1994 mle=1.8153 pcon=4.6498 forget=2.1073 favg=-2.3730 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 37 it 120 total=6.0547 mle=1.6564 pcon=4.6491 forget=2.0968 favg=-2.3477 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 37 it 170 total=6.1076 mle=1.7023 pcon=4.6484 forget=2.0968 favg=-2.3398 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 37 it 220 total=5.9041 mle=1.5019 pcon=4.6477 forget=2.1099 favg=-2.3555 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 37 it 270 total=6.0817 mle=1.6875 pcon=4.6470 forget=2.1125 favg=-2.3652 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 37 it 320 total=6.3886 mle=1.9778 pcon=4.6464 forget=2.1198 favg=-2.3555 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 37 it 370 total=5.9836 mle=1.5811 pcon=4.6457 forget=2.1259 favg=-2.3691 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 38 it 30 total=6.3405 mle=1.8843 pcon=4.6451 forget=2.1041 favg=-2.2930 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 38 it 80 total=5.8868 mle=1.4540 pcon=4.6445 forget=2.1184 favg=-2.3301 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 38 it 130 total=5.9475 mle=1.5365 pcon=4.6439 forget=2.1303 favg=-2.3633 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 38 it 180 total=6.0620 mle=1.6271 pcon=4.6433 forget=2.1314 favg=-2.3398 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 38 it 230 total=5.9930 mle=1.5680 pcon=4.6428 forget=2.1396 favg=-2.3574 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 38 it 280 total=6.3003 mle=1.8230 pcon=4.6421 forget=2.1028 favg=-2.2676 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 38 it 330 total=5.9673 mle=1.5092 pcon=4.6416 forget=2.1271 favg=-2.3105 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 38 it 380 total=6.1762 mle=1.7514 pcon=4.6409 forget=2.1414 favg=-2.3574 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 39 it 40 total=6.1512 mle=1.7248 pcon=4.6404 forget=2.1513 favg=-2.3652 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 39 it 90 total=6.0366 mle=1.6025 pcon=4.6397 forget=2.1673 favg=-2.3730 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 39 it 140 total=6.1779 mle=1.7275 pcon=4.6392 forget=2.1530 favg=-2.3418 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 39 it 190 total=6.1323 mle=1.6393 pcon=4.6387 forget=2.1473 favg=-2.2930 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 39 it 240 total=5.9873 mle=1.5138 pcon=4.6381 forget=2.1284 favg=-2.2930 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 39 it 290 total=6.4783 mle=2.0226 pcon=4.6375 forget=2.1542 favg=-2.3359 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 39 it 340 total=6.2420 mle=1.7874 pcon=4.6370 forget=2.1652 favg=-2.3477 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 40 it 0 total=6.0496 mle=1.5373 pcon=4.6366 forget=2.1335 favg=-2.2578 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 40 it 50 total=6.0717 mle=1.5965 pcon=4.6360 forget=2.1556 favg=-2.3164 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 40 it 100 total=5.9240 mle=1.4568 pcon=4.6354 forget=2.1696 favg=-2.3379 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 40 it 150 total=5.9175 mle=1.4659 pcon=4.6348 forget=2.1762 favg=-2.3594 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 40 it 200 total=6.3598 mle=1.8953 pcon=4.6343 forget=2.1856 favg=-2.3555 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 40 it 250 total=6.1303 mle=1.6556 pcon=4.6338 forget=2.1768 favg=-2.3359 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 40 it 300 total=6.2633 mle=1.7910 pcon=4.6333 forget=2.1867 favg=-2.3477 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 40 it 350 total=6.1113 mle=1.6160 pcon=4.6329 forget=2.1691 favg=-2.3066 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 41 it 10 total=6.1363 mle=1.6483 pcon=4.6324 forget=2.1799 favg=-2.3242 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 41 it 60 total=6.3524 mle=1.8811 pcon=4.6319 forget=2.1968 favg=-2.3574 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 41 it 110 total=6.0792 mle=1.5909 pcon=4.6314 forget=2.1870 favg=-2.3301 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 41 it 160 total=6.1454 mle=1.6428 pcon=4.6310 forget=2.1957 favg=-2.3242 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 41 it 210 total=6.5985 mle=2.1175 pcon=4.6306 forget=2.2156 favg=-2.3652 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 41 it 260 total=6.2355 mle=1.7377 pcon=4.6302 forget=2.2114 favg=-2.3438 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 41 it 310 total=6.0313 mle=1.5538 pcon=4.6298 forget=2.2090 favg=-2.3613 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 41 it 360 total=6.2675 mle=1.7482 pcon=4.6293 forget=2.1928 favg=-2.3027 nr=33 nf=33 protos=570 fproto_sim=NA
 86%|████████▌ | 43/50 [27:41<04:16, 36.60s/it] 88%|████████▊ | 44/50 [28:18<03:38, 36.45s/it] 90%|█████████ | 45/50 [28:54<03:02, 36.47s/it] 92%|█████████▏| 46/50 [29:30<02:25, 36.33s/it] 94%|█████████▍| 47/50 [30:05<01:47, 35.87s/it] 96%|█████████▌| 48/50 [30:35<01:08, 34.05s/it] 98%|█████████▊| 49/50 [31:05<00:33, 33.01s/it]100%|██████████| 50/50 [31:33<00:00, 31.42s/it]100%|██████████| 50/50 [31:33<00:00, 37.87s/it]
[loss] ep 42 it 20 total=6.2803 mle=1.7816 pcon=4.6289 forget=2.2038 favg=-2.3340 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 70 total=6.1637 mle=1.6664 pcon=4.6285 forget=2.2146 favg=-2.3457 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 120 total=6.1292 mle=1.6136 pcon=4.6281 forget=2.2117 favg=-2.3242 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 42 it 170 total=6.0399 mle=1.5512 pcon=4.6276 forget=2.2146 favg=-2.3535 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 220 total=6.0723 mle=1.5660 pcon=4.6272 forget=2.2033 favg=-2.3242 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 42 it 270 total=6.1566 mle=1.6743 pcon=4.6268 forget=2.2267 favg=-2.3711 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 42 it 320 total=6.2064 mle=1.6994 pcon=4.6263 forget=2.2206 favg=-2.3398 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 42 it 370 total=6.0272 mle=1.5104 pcon=4.6259 forget=2.2074 favg=-2.3164 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 43 it 30 total=6.2859 mle=1.7292 pcon=4.6255 forget=2.1891 favg=-2.2578 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 43 it 80 total=6.2525 mle=1.7208 pcon=4.6251 forget=2.1996 favg=-2.2930 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 43 it 130 total=6.1969 mle=1.6955 pcon=4.6246 forget=2.2265 favg=-2.3496 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 43 it 180 total=6.2849 mle=1.7577 pcon=4.6241 forget=2.2156 favg=-2.3125 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 43 it 230 total=6.1185 mle=1.6234 pcon=4.6237 forget=2.2327 favg=-2.3613 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 43 it 280 total=6.3280 mle=1.8180 pcon=4.6234 forget=2.2382 favg=-2.3516 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 43 it 330 total=6.0812 mle=1.5469 pcon=4.6231 forget=2.2081 favg=-2.2969 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 43 it 380 total=5.9710 mle=1.4524 pcon=4.6228 forget=2.2239 favg=-2.3281 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 44 it 40 total=6.1568 mle=1.6453 pcon=4.6224 forget=2.2309 favg=-2.3418 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 44 it 90 total=6.2485 mle=1.7475 pcon=4.6221 forget=2.2598 favg=-2.3809 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 44 it 140 total=6.1868 mle=1.6768 pcon=4.6217 forget=2.2438 favg=-2.3555 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 44 it 190 total=6.0168 mle=1.5117 pcon=4.6214 forget=2.2529 favg=-2.3691 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 44 it 240 total=6.4496 mle=1.9275 pcon=4.6210 forget=2.2409 favg=-2.3398 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 44 it 290 total=6.3490 mle=1.8523 pcon=4.6207 forget=2.2686 favg=-2.3926 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 44 it 340 total=6.0558 mle=1.5384 pcon=4.6204 forget=2.2525 favg=-2.3555 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 45 it 0 total=6.0473 mle=1.5434 pcon=4.6200 forget=2.2745 favg=-2.3906 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 50 total=6.1000 mle=1.5625 pcon=4.6197 forget=2.2381 favg=-2.3203 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 100 total=6.1349 mle=1.6099 pcon=4.6194 forget=2.2650 favg=-2.3594 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 45 it 150 total=6.2558 mle=1.7226 pcon=4.6191 forget=2.2501 favg=-2.3359 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 45 it 200 total=6.4137 mle=1.8757 pcon=4.6188 forget=2.2591 favg=-2.3398 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 45 it 250 total=6.1183 mle=1.5893 pcon=4.6184 forget=2.2778 favg=-2.3672 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 300 total=6.4894 mle=1.9270 pcon=4.6182 forget=2.2509 favg=-2.3066 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 45 it 350 total=6.2064 mle=1.6795 pcon=4.6180 forget=2.2800 favg=-2.3711 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 46 it 10 total=6.0412 mle=1.5031 pcon=4.6176 forget=2.2681 favg=-2.3477 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 46 it 60 total=6.0369 mle=1.4956 pcon=4.6173 forget=2.2716 favg=-2.3477 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 46 it 110 total=6.2100 mle=1.6778 pcon=4.6169 forget=2.2687 favg=-2.3535 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 46 it 160 total=6.3831 mle=1.8466 pcon=4.6165 forget=2.2735 favg=-2.3535 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 46 it 210 total=6.0036 mle=1.4741 pcon=4.6162 forget=2.2805 favg=-2.3672 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 46 it 260 total=6.4753 mle=1.9222 pcon=4.6159 forget=2.2692 favg=-2.3320 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 46 it 310 total=6.4791 mle=1.9523 pcon=4.6157 forget=2.2978 favg=-2.3867 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 46 it 360 total=6.0927 mle=1.5351 pcon=4.6155 forget=2.2820 favg=-2.3398 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 47 it 20 total=6.3954 mle=1.8346 pcon=4.6152 forget=2.2776 favg=-2.3320 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 47 it 70 total=6.0146 mle=1.4694 pcon=4.6149 forget=2.2818 favg=-2.3516 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 47 it 120 total=6.0684 mle=1.4883 pcon=4.6147 forget=2.2662 favg=-2.3008 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 47 it 170 total=6.0755 mle=1.5256 pcon=4.6143 forget=2.2929 favg=-2.3574 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 47 it 220 total=6.1061 mle=1.5663 pcon=4.6141 forget=2.3047 favg=-2.3789 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 47 it 270 total=6.4670 mle=1.9147 pcon=4.6138 forget=2.2979 favg=-2.3594 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 47 it 320 total=6.4682 mle=1.8901 pcon=4.6135 forget=2.2888 favg=-2.3242 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 47 it 370 total=6.0411 mle=1.4677 pcon=4.6131 forget=2.3099 favg=-2.3496 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 30 total=6.3523 mle=1.8001 pcon=4.6128 forget=2.3105 favg=-2.3711 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 48 it 80 total=6.2353 mle=1.6687 pcon=4.6125 forget=2.2940 favg=-2.3398 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 48 it 130 total=6.3373 mle=1.7922 pcon=4.6122 forget=2.3118 favg=-2.3789 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 180 total=6.2833 mle=1.7167 pcon=4.6119 forget=2.3023 favg=-2.3477 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 48 it 230 total=6.3068 mle=1.7534 pcon=4.6117 forget=2.3147 favg=-2.3730 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 48 it 280 total=6.2736 mle=1.7145 pcon=4.6116 forget=2.3166 favg=-2.3691 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 48 it 330 total=6.1198 mle=1.5534 pcon=4.6114 forget=2.3026 favg=-2.3477 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 380 total=6.1697 mle=1.6068 pcon=4.6112 forget=2.3111 favg=-2.3594 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 49 it 40 total=6.4107 mle=1.8094 pcon=4.6110 forget=2.2872 favg=-2.2969 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 49 it 90 total=6.4974 mle=1.9367 pcon=4.6107 forget=2.3250 favg=-2.3750 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 49 it 140 total=6.1810 mle=1.6250 pcon=4.6105 forget=2.3205 favg=-2.3750 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 49 it 190 total=6.2751 mle=1.7251 pcon=4.6102 forget=2.3382 favg=-2.3984 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 49 it 240 total=6.5034 mle=1.9449 pcon=4.6101 forget=2.3274 favg=-2.3789 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 49 it 290 total=6.2566 mle=1.6483 pcon=4.6098 forget=2.2896 favg=-2.2910 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 49 it 340 total=6.1033 mle=1.5414 pcon=4.6096 forget=2.3350 favg=-2.3828 nr=34 nf=34 protos=570 fproto_sim=NA
Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.1
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] loaded adapter 'stage1' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] set_adapter(list) failed: unhashable type: 'list'; fallback to last adapter
[peft] active adapter set to: stage1
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1: Number of model parameters: 22082496
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<05:24,  1.20it/s]  3%|▎         | 10/391 [00:00<00:27, 14.01it/s]  5%|▍         | 18/391 [00:01<00:14, 25.21it/s]  7%|▋         | 27/391 [00:01<00:09, 37.27it/s]  9%|▉         | 36/391 [00:01<00:07, 47.49it/s] 11%|█▏        | 44/391 [00:01<00:06, 54.45it/s] 13%|█▎        | 52/391 [00:01<00:05, 59.51it/s] 15%|█▌        | 60/391 [00:01<00:05, 63.45it/s] 18%|█▊        | 69/391 [00:01<00:04, 68.57it/s] 20%|█▉        | 77/391 [00:01<00:04, 70.64it/s] 22%|██▏       | 85/391 [00:01<00:04, 67.86it/s] 24%|██▍       | 94/391 [00:02<00:04, 71.71it/s] 26%|██▋       | 103/391 [00:02<00:03, 75.01it/s] 29%|██▊       | 112/391 [00:02<00:03, 76.42it/s] 31%|███       | 121/391 [00:02<00:03, 78.78it/s] 33%|███▎      | 130/391 [00:02<00:03, 75.79it/s] 35%|███▌      | 138/391 [00:02<00:03, 74.66it/s] 37%|███▋      | 146/391 [00:02<00:03, 72.40it/s] 39%|███▉      | 154/391 [00:02<00:03, 74.10it/s] 41%|████▏     | 162/391 [00:02<00:03, 74.56it/s] 44%|████▎     | 171/391 [00:03<00:02, 76.30it/s] 46%|████▌     | 180/391 [00:03<00:02, 78.33it/s] 48%|████▊     | 188/391 [00:03<00:02, 78.07it/s] 50%|█████     | 196/391 [00:03<00:02, 77.32it/s] 52%|█████▏    | 204/391 [00:03<00:02, 75.89it/s] 54%|█████▍    | 212/391 [00:03<00:02, 74.68it/s] 56%|█████▋    | 220/391 [00:03<00:02, 74.86it/s] 58%|█████▊    | 228/391 [00:03<00:02, 66.23it/s] 60%|██████    | 235/391 [00:03<00:02, 66.98it/s] 62%|██████▏   | 243/391 [00:04<00:02, 68.18it/s] 64%|██████▍   | 250/391 [00:04<00:02, 68.51it/s] 66%|██████▌   | 258/391 [00:04<00:01, 69.56it/s] 68%|██████▊   | 266/391 [00:04<00:01, 69.84it/s] 70%|███████   | 275/391 [00:04<00:01, 70.90it/s] 72%|███████▏  | 283/391 [00:04<00:01, 69.96it/s] 74%|███████▍  | 291/391 [00:04<00:01, 72.40it/s] 77%|███████▋  | 300/391 [00:04<00:01, 76.18it/s] 79%|███████▉  | 308/391 [00:04<00:01, 75.17it/s] 81%|████████  | 316/391 [00:05<00:01, 70.48it/s] 83%|████████▎ | 324/391 [00:05<00:00, 71.47it/s] 85%|████████▍ | 332/391 [00:05<00:00, 71.56it/s] 87%|████████▋ | 340/391 [00:05<00:00, 66.83it/s] 89%|████████▊ | 347/391 [00:05<00:00, 63.96it/s] 91%|█████████ | 354/391 [00:05<00:00, 65.11it/s] 93%|█████████▎| 362/391 [00:05<00:00, 67.33it/s] 95%|█████████▌| 372/391 [00:05<00:00, 74.44it/s] 97%|█████████▋| 381/391 [00:05<00:00, 78.22it/s]100%|█████████▉| 390/391 [00:06<00:00, 81.28it/s]100%|██████████| 391/391 [00:06<00:00, 63.88it/s]
50000 images processed, 6.553605794906616 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:46,  1.66it/s]  4%|▍         | 3/79 [00:00<00:14,  5.10it/s] 15%|█▌        | 12/79 [00:00<00:02, 22.98it/s] 25%|██▌       | 20/79 [00:00<00:01, 35.92it/s] 34%|███▍      | 27/79 [00:01<00:01, 42.66it/s] 44%|████▍     | 35/79 [00:01<00:00, 50.79it/s] 56%|█████▌    | 44/79 [00:01<00:00, 60.13it/s] 66%|██████▌   | 52/79 [00:01<00:00, 65.38it/s] 77%|███████▋  | 61/79 [00:01<00:00, 70.85it/s] 89%|████████▊ | 70/79 [00:01<00:00, 71.92it/s]100%|██████████| 79/79 [00:01<00:00, 66.31it/s]100%|██████████| 79/79 [00:01<00:00, 45.50it/s]
10000 images processed, 1.8670284748077393 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:16,  1.49it/s]  3%|▎         | 7/204 [00:00<00:17, 11.50it/s]  7%|▋         | 14/204 [00:00<00:08, 22.84it/s] 11%|█▏        | 23/204 [00:00<00:04, 36.95it/s] 15%|█▍        | 30/204 [00:01<00:04, 42.14it/s] 19%|█▉        | 39/204 [00:01<00:03, 51.93it/s] 23%|██▎       | 47/204 [00:01<00:02, 57.74it/s] 27%|██▋       | 55/204 [00:01<00:02, 61.89it/s] 31%|███       | 63/204 [00:01<00:02, 65.64it/s] 35%|███▍      | 71/204 [00:01<00:02, 66.48it/s] 39%|███▉      | 80/204 [00:01<00:01, 72.21it/s] 43%|████▎     | 88/204 [00:01<00:01, 73.44it/s] 47%|████▋     | 96/204 [00:01<00:01, 74.41it/s] 51%|█████▏    | 105/204 [00:02<00:01, 76.54it/s] 55%|█████▌    | 113/204 [00:02<00:01, 75.57it/s] 59%|█████▉    | 121/204 [00:02<00:01, 69.65it/s] 63%|██████▎   | 129/204 [00:02<00:01, 64.12it/s] 67%|██████▋   | 137/204 [00:02<00:01, 66.82it/s] 71%|███████   | 145/204 [00:02<00:00, 69.81it/s] 75%|███████▌  | 153/204 [00:02<00:00, 67.76it/s] 79%|███████▉  | 161/204 [00:02<00:00, 70.01it/s] 83%|████████▎ | 170/204 [00:03<00:00, 73.88it/s] 88%|████████▊ | 179/204 [00:03<00:00, 77.67it/s] 92%|█████████▏| 187/204 [00:03<00:00, 76.65it/s] 97%|█████████▋| 197/204 [00:03<00:00, 80.78it/s]100%|██████████| 204/204 [00:03<00:00, 58.55it/s]
26032 images processed, 3.6990864276885986 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:07,  1.15it/s] 13%|█▎        | 10/79 [00:00<00:05, 13.58it/s] 23%|██▎       | 18/79 [00:01<00:02, 24.51it/s] 34%|███▍      | 27/79 [00:01<00:01, 36.20it/s] 47%|████▋     | 37/79 [00:01<00:00, 49.27it/s] 58%|█████▊    | 46/79 [00:01<00:00, 57.88it/s] 70%|██████▉   | 55/79 [00:01<00:00, 63.14it/s] 81%|████████  | 64/79 [00:01<00:00, 69.00it/s] 94%|█████████▎| 74/79 [00:01<00:00, 75.46it/s]100%|██████████| 79/79 [00:01<00:00, 44.35it/s]
10000 images processed, 1.8856608867645264 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:53,  1.46it/s] 11%|█▏        | 9/79 [00:00<00:04, 14.56it/s] 19%|█▉        | 15/79 [00:00<00:02, 23.10it/s] 29%|██▉       | 23/79 [00:01<00:01, 34.47it/s] 39%|███▉      | 31/79 [00:01<00:01, 44.21it/s] 48%|████▊     | 38/79 [00:01<00:00, 48.96it/s] 58%|█████▊    | 46/79 [00:01<00:00, 55.76it/s] 70%|██████▉   | 55/79 [00:01<00:00, 64.15it/s] 82%|████████▏ | 65/79 [00:01<00:00, 71.80it/s] 94%|█████████▎| 74/79 [00:01<00:00, 73.79it/s]100%|██████████| 79/79 [00:01<00:00, 45.36it/s]
10000 images processed, 1.8406181335449219 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:52,  1.31it/s] 11%|█▏        | 8/70 [00:00<00:05, 12.12it/s] 23%|██▎       | 16/70 [00:00<00:02, 24.43it/s] 34%|███▍      | 24/70 [00:01<00:01, 35.49it/s] 47%|████▋     | 33/70 [00:01<00:00, 46.49it/s] 59%|█████▊    | 41/70 [00:01<00:00, 52.16it/s] 71%|███████▏  | 50/70 [00:01<00:00, 59.57it/s] 84%|████████▍ | 59/70 [00:01<00:00, 64.94it/s] 97%|█████████▋| 68/70 [00:01<00:00, 69.86it/s]100%|██████████| 70/70 [00:01<00:00, 41.35it/s]
8925 images processed, 1.7906467914581299 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:58,  1.32s/it] 18%|█▊        | 8/45 [00:01<00:04,  7.46it/s] 31%|███       | 14/45 [00:01<00:02, 12.70it/s] 40%|████      | 18/45 [00:02<00:02, 11.31it/s] 51%|█████     | 23/45 [00:02<00:01, 14.52it/s] 67%|██████▋   | 30/45 [00:02<00:00, 21.12it/s] 76%|███████▌  | 34/45 [00:02<00:00, 13.47it/s] 98%|█████████▊| 44/45 [00:03<00:00, 22.88it/s]100%|██████████| 45/45 [00:03<00:00, 14.50it/s]
5640 images processed, 3.1763956546783447 seconds used

22.68559980392456
Disabling PyTorch because PyTorch >= 2.1 is required but found 2.0.1
None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.
Traceback (most recent call last):
  File "/home/shaokun/PALM/eval_cifar.py", line 19, in <module>
    eval_maha(args)
  File "/home/shaokun/PALM/util/evaluations/mahalanobis.py", line 21, in eval_maha
    feat_log = np.load(f"cache/{args.backbone}-{args.method}/{args.in_dataset}/train_{args.backbone}-{args.method}_features.npy", allow_pickle=True)
  File "/home/shaokun/anaconda3/envs/PALM/lib/python3.10/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'cache/resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1/CIFAR-100/train_resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1_features.npy'
