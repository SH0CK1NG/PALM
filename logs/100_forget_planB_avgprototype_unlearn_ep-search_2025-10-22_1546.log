nohup: ignoring input
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=35, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.2, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=True, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/35 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  3%|▎         | 1/35 [00:54<31:03, 54.82s/it]  6%|▌         | 2/35 [01:48<29:45, 54.10s/it]  9%|▊         | 3/35 [02:44<29:26, 55.21s/it] 11%|█▏        | 4/35 [03:40<28:35, 55.34s/it] 14%|█▍        | 5/35 [04:37<27:59, 55.99s/it] 17%|█▋        | 6/35 [05:34<27:10, 56.22s/it] 20%|██        | 7/35 [06:29<26:08, 56.02s/it][loss] ep 0 it 0 total=7.8809 mle=1.5710 pcon=5.2950 forget=1.3755 favg=-0.3606 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=7.7875 mle=1.5424 pcon=5.2879 forget=1.4013 favg=-0.4441 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=7.9331 mle=1.7006 pcon=5.2809 forget=1.3740 favg=-0.4224 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8.0966 mle=1.8997 pcon=5.2738 forget=1.3696 favg=-0.4465 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=7.9675 mle=1.7131 pcon=5.2670 forget=1.3826 favg=-0.3953 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=7.7255 mle=1.5021 pcon=5.2603 forget=1.3784 favg=-0.4153 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=7.8313 mle=1.5596 pcon=5.2540 forget=1.3819 favg=-0.3643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=7.8704 mle=1.6809 pcon=5.2476 forget=1.3896 favg=-0.4478 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 1 it 10 total=7.8555 mle=1.6715 pcon=5.2409 forget=1.4100 favg=-0.4668 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=7.8237 mle=1.6370 pcon=5.2346 forget=1.3669 favg=-0.4148 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=7.7389 mle=1.5166 pcon=5.2284 forget=1.3893 favg=-0.3955 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=7.9846 mle=1.7799 pcon=5.2224 forget=1.3892 favg=-0.4070 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=7.9263 mle=1.7647 pcon=5.2167 forget=1.3851 favg=-0.4402 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=7.8241 mle=1.6046 pcon=5.2112 forget=1.3679 favg=-0.3596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=7.9436 mle=1.7233 pcon=5.2056 forget=1.3777 favg=-0.3630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=7.9443 mle=1.7941 pcon=5.2002 forget=1.3856 favg=-0.4355 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 2 it 20 total=7.6888 mle=1.5529 pcon=5.1949 forget=1.3760 favg=-0.4351 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=7.9763 mle=1.8881 pcon=5.1898 forget=1.3562 favg=-0.4578 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=7.9107 mle=1.7422 pcon=5.1845 forget=1.3742 favg=-0.3901 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=7.6292 mle=1.5261 pcon=5.1794 forget=1.3617 favg=-0.4380 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=7.6260 mle=1.5861 pcon=5.1742 forget=1.3832 favg=-0.5176 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=7.9281 mle=1.8823 pcon=5.1695 forget=1.3635 favg=-0.4871 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=7.5112 mle=1.5840 pcon=5.1645 forget=1.3500 favg=-0.5874 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=7.6494 mle=1.8072 pcon=5.1596 forget=1.3521 favg=-0.6694 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 3 it 30 total=7.3520 mle=1.6853 pcon=5.1546 forget=1.3612 favg=-0.8491 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=7.1131 mle=1.6679 pcon=5.1499 forget=1.3714 favg=-1.0762 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=6.9764 mle=1.7410 pcon=5.1447 forget=1.3788 favg=-1.2881 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=6.9586 mle=1.9237 pcon=5.1398 forget=1.4069 favg=-1.5117 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=6.4886 mle=1.5771 pcon=5.1346 forget=1.4409 favg=-1.6641 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=6.7059 mle=1.8898 pcon=5.1292 forget=1.4710 favg=-1.7842 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=6.5685 mle=1.8099 pcon=5.1239 forget=1.5038 favg=-1.8691 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=6.4993 mle=1.7712 pcon=5.1194 forget=1.5227 favg=-1.9141 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
[loss] ep 4 it 40 total=6.4374 mle=1.6664 pcon=5.1147 forget=1.5362 favg=-1.8799 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=6.8117 mle=1.8156 pcon=5.1100 forget=1.5834 favg=-1.6973 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8.3945 mle=1.6671 pcon=5.1057 forget=1.6185 favg=0.0033 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=9.9049 mle=1.4750 pcon=5.1016 forget=1.6487 favg=1.6797 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=10.2389 mle=1.7695 pcon=5.0970 forget=1.5970 favg=1.7754 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=9.7297 mle=1.4708 pcon=5.0928 forget=1.5469 favg=1.6191 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=9.8860 mle=1.9416 pcon=5.0887 forget=1.4866 favg=1.3691 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 0 total=9.4082 mle=1.8697 pcon=5.0850 forget=1.4369 favg=1.0166 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=8.8009 mle=1.7835 pcon=5.0812 forget=1.3767 favg=0.5596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=8.6428 mle=1.5917 pcon=5.0781 forget=1.3978 favg=0.5752 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=8.5249 mle=1.6060 pcon=5.0746 forget=1.3885 favg=0.4558 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=8.5345 mle=1.6848 pcon=5.0718 forget=1.3735 favg=0.4043 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=8.5768 mle=1.7826 pcon=5.0690 forget=1.3619 favg=0.3633 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=8.3412 mle=1.5948 pcon=5.0661 forget=1.3711 favg=0.3091 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=8.6024 mle=1.7656 pcon=5.0634 forget=1.3767 favg=0.3967 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 10 total=8.1548 mle=1.4140 pcon=5.0608 forget=1.3963 favg=0.2837 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.8866 mle=1.5575 pcon=5.0583 forget=1.3622 favg=-0.0914 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=8.0107 mle=1.5965 pcon=5.0560 forget=1.3985 favg=-0.0403 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=8.0778 mle=1.6689 pcon=5.0534 forget=1.3689 favg=-0.0134 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=7.7267 mle=1.6162 pcon=5.0512 forget=1.3683 favg=-0.3091 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=7.7202 mle=1.7971 pcon=5.0483 forget=1.3840 favg=-0.5093 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.2637 mle=1.5501 pcon=5.0457 forget=1.4067 favg=-0.7388 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=7.1368 mle=1.6703 pcon=5.0429 forget=1.4143 favg=-0.9907 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 20 total=6.8993 mle=1.6664 pcon=5.0399 forget=1.4449 favg=-1.2520 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=7.6771 mle=1.7840 pcon=5.0373 forget=1.4437 favg=-0.5879 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=9.6225 mle=1.6325 pcon=5.0347 forget=1.4162 favg=1.5391 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=9.0532 mle=1.5740 pcon=5.0325 forget=1.3832 favg=1.0635 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=8.6771 mle=1.6448 pcon=5.0306 forget=1.3806 favg=0.6211 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=8.5290 mle=1.7961 pcon=5.0286 forget=1.3921 favg=0.3123 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=8.2052 mle=1.7365 pcon=5.0263 forget=1.3899 favg=0.0525 nr=64 nf=64 protos=540 fproto_sim=NA
 23%|██▎       | 8/35 [07:26<25:15, 56.15s/it] 26%|██▌       | 9/35 [08:23<24:31, 56.60s/it] 29%|██▊       | 10/35 [09:22<23:47, 57.11s/it] 31%|███▏      | 11/35 [10:19<22:54, 57.27s/it] 34%|███▍      | 12/35 [11:12<21:24, 55.83s/it] 37%|███▋      | 13/35 [12:16<21:25, 58.42s/it] 40%|████      | 14/35 [13:18<20:50, 59.55s/it] 43%|████▎     | 15/35 [14:21<20:07, 60.40s/it] 46%|████▌     | 16/35 [15:18<18:52, 59.60s/it][loss] ep 7 it 370 total=7.6623 mle=1.6448 pcon=5.0244 forget=1.3739 favg=-0.3809 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 30 total=7.0274 mle=1.5412 pcon=5.0226 forget=1.3854 favg=-0.9219 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=7.2111 mle=1.5839 pcon=5.0206 forget=1.4113 favg=-0.8047 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.8689 mle=1.4954 pcon=5.0182 forget=1.4405 favg=-0.0852 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=8.2235 mle=1.7186 pcon=5.0158 forget=1.4461 favg=0.0431 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.8374 mle=1.5205 pcon=5.0138 forget=1.4154 favg=-0.1123 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=7.8598 mle=1.6893 pcon=5.0116 forget=1.3821 favg=-0.2233 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=7.9537 mle=1.7360 pcon=5.0095 forget=1.3926 favg=-0.1843 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=8.6701 mle=1.5077 pcon=5.0078 forget=1.3753 favg=0.7793 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 40 total=9.1989 mle=1.6527 pcon=5.0064 forget=1.3689 favg=1.1709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=8.3017 mle=1.6819 pcon=5.0047 forget=1.3665 favg=0.2487 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=7.3665 mle=1.8204 pcon=5.0034 forget=1.3797 favg=-0.8369 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.0579 mle=1.5663 pcon=5.0019 forget=1.3617 favg=-0.8721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.9155 mle=1.5944 pcon=5.0004 forget=1.4207 favg=-0.1000 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=8.2549 mle=1.5552 pcon=4.9984 forget=1.4598 favg=0.2415 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=8.1105 mle=1.5629 pcon=4.9966 forget=1.4358 favg=0.1152 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 0 total=7.8471 mle=1.6161 pcon=4.9946 forget=1.4000 favg=-0.1637 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=7.6279 mle=1.5902 pcon=4.9929 forget=1.3852 favg=-0.3403 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=8.4666 mle=1.7586 pcon=4.9917 forget=1.3769 favg=0.3394 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=9.5269 mle=1.8441 pcon=4.9909 forget=1.3863 favg=1.3057 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=8.6750 mle=1.8604 pcon=4.9900 forget=1.3720 favg=0.4526 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=7.4994 mle=1.5868 pcon=4.9889 forget=1.3632 favg=-0.4395 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=7.1267 mle=1.5872 pcon=4.9880 forget=1.3490 favg=-0.7974 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=7.1911 mle=1.6622 pcon=4.9867 forget=1.3782 favg=-0.8359 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 10 total=7.4150 mle=1.5951 pcon=4.9852 forget=1.4113 favg=-0.5767 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=7.8425 mle=1.6638 pcon=4.9835 forget=1.4491 favg=-0.2539 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=7.7706 mle=1.5974 pcon=4.9818 forget=1.4510 favg=-0.2595 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=7.8299 mle=1.7008 pcon=4.9798 forget=1.3967 favg=-0.2473 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=9.0942 mle=1.7974 pcon=4.9786 forget=1.3757 favg=0.9424 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=9.5163 mle=1.6500 pcon=4.9781 forget=1.3941 favg=1.4941 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=9.1662 mle=1.9957 pcon=4.9771 forget=1.3956 favg=0.7979 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=7.7458 mle=1.6818 pcon=4.9760 forget=1.3505 favg=-0.2625 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 20 total=6.9077 mle=1.6259 pcon=4.9744 forget=1.3445 favg=-1.0371 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=6.3632 mle=1.7349 pcon=4.9728 forget=1.3411 favg=-1.6855 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=6.1789 mle=1.6267 pcon=4.9713 forget=1.3778 favg=-1.7969 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=8.6594 mle=1.7661 pcon=4.9700 forget=1.3984 favg=0.5249 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=9.6975 mle=1.6255 pcon=4.9688 forget=1.4215 favg=1.6816 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=9.5268 mle=1.5314 pcon=4.9681 forget=1.4278 favg=1.5996 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=9.1841 mle=1.5075 pcon=4.9668 forget=1.4031 favg=1.3066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=9.1240 mle=1.8369 pcon=4.9654 forget=1.3998 favg=0.9219 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 30 total=8.1905 mle=1.4190 pcon=4.9641 forget=1.3777 favg=0.4297 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=7.4703 mle=1.4826 pcon=4.9625 forget=1.3885 favg=-0.3633 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=6.4859 mle=1.6266 pcon=4.9608 forget=1.3535 favg=-1.4551 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=8.5280 mle=1.6423 pcon=4.9598 forget=1.3678 favg=0.5581 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=9.0576 mle=1.7048 pcon=4.9589 forget=1.3724 favg=1.0215 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=8.3876 mle=1.5525 pcon=4.9580 forget=1.3522 favg=0.5249 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=7.7990 mle=1.5984 pcon=4.9568 forget=1.3540 favg=-0.1102 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=7.3953 mle=1.5783 pcon=4.9556 forget=1.3433 favg=-0.4819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 40 total=7.2328 mle=1.7716 pcon=4.9539 forget=1.3423 favg=-0.8350 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=6.9647 mle=1.7847 pcon=4.9525 forget=1.3613 favg=-1.1338 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=6.9328 mle=1.6354 pcon=4.9512 forget=1.3872 favg=-1.0410 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=9.5480 mle=1.8624 pcon=4.9492 forget=1.4102 favg=1.3262 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=9.6646 mle=1.6793 pcon=4.9475 forget=1.4469 favg=1.5908 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=8.9094 mle=1.6508 pcon=4.9455 forget=1.4097 favg=0.9033 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=7.9060 mle=1.6799 pcon=4.9436 forget=1.3670 favg=-0.0845 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 0 total=7.4287 mle=1.7023 pcon=4.9419 forget=1.3553 favg=-0.5708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=6.5827 mle=1.4866 pcon=4.9400 forget=1.3280 favg=-1.1719 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=6.2343 mle=1.5644 pcon=4.9384 forget=1.3175 favg=-1.5859 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=6.3664 mle=1.7181 pcon=4.9370 forget=1.3304 favg=-1.6191 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=8.4999 mle=1.6364 pcon=4.9359 forget=1.3339 favg=0.5938 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=9.8908 mle=1.9229 pcon=4.9346 forget=1.3536 favg=1.6797 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=9.5540 mle=1.5838 pcon=4.9332 forget=1.3758 favg=1.6611 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=9.3128 mle=1.5238 pcon=4.9313 forget=1.3929 favg=1.4648 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 10 total=9.0974 mle=1.5851 pcon=4.9290 forget=1.4095 favg=1.1738 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=8.6305 mle=1.4928 pcon=4.9264 forget=1.3861 favg=0.8252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=8.2164 mle=1.5580 pcon=4.9238 forget=1.3711 favg=0.3635 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=7.6099 mle=1.6374 pcon=4.9206 forget=1.3544 favg=-0.3025 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=6.5498 mle=1.5977 pcon=4.9173 forget=1.3297 favg=-1.2949 nr=64 nf=64 protos=540 fproto_sim=NA
 49%|████▊     | 17/35 [16:11<17:14, 57.49s/it] 51%|█████▏    | 18/35 [16:56<15:14, 53.82s/it] 54%|█████▍    | 19/35 [17:58<14:56, 56.03s/it] 57%|█████▋    | 20/35 [18:54<14:03, 56.22s/it] 60%|██████    | 21/35 [19:45<12:45, 54.65s/it] 63%|██████▎   | 22/35 [20:33<11:24, 52.66s/it] 66%|██████▌   | 23/35 [21:10<09:34, 47.90s/it] 69%|██████▊   | 24/35 [21:49<08:18, 45.28s/it] 71%|███████▏  | 25/35 [22:28<07:13, 43.35s/it][loss] ep 16 it 260 total=6.5982 mle=1.9031 pcon=4.9141 forget=1.3328 favg=-1.5518 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=8.0164 mle=1.8180 pcon=4.9112 forget=1.3611 favg=-0.0739 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=8.9781 mle=1.6161 pcon=4.9085 forget=1.4008 favg=1.0527 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 20 total=9.0449 mle=1.7754 pcon=4.9055 forget=1.4211 favg=0.9429 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=8.6686 mle=1.7478 pcon=4.9028 forget=1.3930 favg=0.6250 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=7.8720 mle=1.5530 pcon=4.8996 forget=1.3725 favg=0.0471 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=7.4091 mle=1.5572 pcon=4.8960 forget=1.3509 favg=-0.3950 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=7.2756 mle=1.6328 pcon=4.8926 forget=1.3518 favg=-0.6016 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=7.0286 mle=1.5170 pcon=4.8897 forget=1.3358 favg=-0.7139 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=8.1631 mle=1.8103 pcon=4.8866 forget=1.3421 favg=0.1241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=9.2356 mle=1.7666 pcon=4.8832 forget=1.3543 favg=1.2314 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=9.1669 mle=1.5693 pcon=4.8807 forget=1.3565 favg=1.3604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=8.7801 mle=1.6534 pcon=4.8778 forget=1.3753 favg=0.8735 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=7.9248 mle=1.6550 pcon=4.8747 forget=1.3588 favg=0.0362 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=7.1387 mle=1.5781 pcon=4.8716 forget=1.3233 favg=-0.6343 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=6.5551 mle=1.6577 pcon=4.8684 forget=1.3005 favg=-1.2715 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=6.3538 mle=1.6452 pcon=4.8654 forget=1.3012 favg=-1.4580 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=6.2919 mle=1.7087 pcon=4.8625 forget=1.2968 favg=-1.5762 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=6.5074 mle=1.7368 pcon=4.8599 forget=1.3208 favg=-1.4102 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 40 total=6.8671 mle=1.5863 pcon=4.8573 forget=1.3463 favg=-0.9229 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=7.8825 mle=1.6332 pcon=4.8548 forget=1.3864 favg=0.0081 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=8.5944 mle=1.6070 pcon=4.8520 forget=1.4088 favg=0.7266 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=9.0659 mle=1.6840 pcon=4.8491 forget=1.4254 favg=1.1074 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=9.3918 mle=1.8257 pcon=4.8461 forget=1.4437 favg=1.2764 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=9.1145 mle=1.6818 pcon=4.8433 forget=1.4351 favg=1.1543 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=8.8989 mle=1.5267 pcon=4.8402 forget=1.4275 favg=1.1045 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 0 total=8.7520 mle=1.6366 pcon=4.8370 forget=1.4020 favg=0.8765 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=8.3696 mle=1.6856 pcon=4.8338 forget=1.3678 favg=0.4824 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=8.2351 mle=1.7604 pcon=4.8310 forget=1.3796 favg=0.2642 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=8.0569 mle=1.8066 pcon=4.8281 forget=1.3607 favg=0.0614 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=7.5925 mle=1.5766 pcon=4.8255 forget=1.3432 favg=-0.1528 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=7.6568 mle=1.9080 pcon=4.8230 forget=1.3372 favg=-0.4114 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=7.1513 mle=1.7590 pcon=4.8206 forget=1.3349 favg=-0.7632 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=6.5473 mle=1.4756 pcon=4.8181 forget=1.3209 favg=-1.0674 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 10 total=6.1725 mle=1.4729 pcon=4.8157 forget=1.3234 favg=-1.4395 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=6.2376 mle=1.6367 pcon=4.8132 forget=1.3365 favg=-1.5488 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=6.3326 mle=1.6112 pcon=4.8108 forget=1.3578 favg=-1.4473 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=6.6798 mle=1.6111 pcon=4.8085 forget=1.3764 favg=-1.1162 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=7.6946 mle=1.7808 pcon=4.8060 forget=1.4182 favg=-0.3105 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=8.7382 mle=1.6594 pcon=4.8035 forget=1.4551 favg=0.8203 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=9.3263 mle=1.5801 pcon=4.8008 forget=1.4796 favg=1.4658 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=9.5504 mle=1.5901 pcon=4.7981 forget=1.5089 favg=1.6533 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 20 total=9.5310 mle=1.5995 pcon=4.7955 forget=1.4934 favg=1.6426 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=9.4243 mle=1.6556 pcon=4.7929 forget=1.4817 favg=1.4941 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=8.9367 mle=1.5779 pcon=4.7902 forget=1.4602 favg=1.1084 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=8.4718 mle=1.5944 pcon=4.7878 forget=1.4163 favg=0.6733 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=7.7670 mle=1.5897 pcon=4.7856 forget=1.3719 favg=0.0198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=7.4982 mle=1.4942 pcon=4.7834 forget=1.3578 favg=-0.1372 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=7.4270 mle=1.6789 pcon=4.7816 forget=1.3390 favg=-0.3726 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=7.6309 mle=1.9185 pcon=4.7797 forget=1.3434 favg=-0.4106 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 30 total=7.2218 mle=1.5174 pcon=4.7784 forget=1.3448 favg=-0.4187 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=7.3690 mle=1.6340 pcon=4.7767 forget=1.3409 favg=-0.3826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=7.4041 mle=1.5917 pcon=4.7750 forget=1.3523 favg=-0.3149 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=7.7006 mle=1.7418 pcon=4.7732 forget=1.3376 favg=-0.1520 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=7.7083 mle=1.6182 pcon=4.7716 forget=1.3574 favg=-0.0388 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=7.8252 mle=1.5963 pcon=4.7696 forget=1.3737 favg=0.0856 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=7.8962 mle=1.6139 pcon=4.7676 forget=1.3857 favg=0.1290 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=7.8517 mle=1.5787 pcon=4.7655 forget=1.4226 favg=0.0849 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 40 total=7.5937 mle=1.5479 pcon=4.7635 forget=1.4276 favg=-0.1453 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=7.6514 mle=1.5970 pcon=4.7614 forget=1.4300 favg=-0.1370 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=7.2459 mle=1.5547 pcon=4.7593 forget=1.4215 favg=-0.4895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=7.0182 mle=1.6350 pcon=4.7574 forget=1.3978 favg=-0.7720 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 240 total=6.9071 mle=1.5198 pcon=4.7555 forget=1.3955 favg=-0.7637 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=7.3720 mle=1.7211 pcon=4.7538 forget=1.3867 favg=-0.4897 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=7.9392 mle=1.5826 pcon=4.7524 forget=1.3702 favg=0.2340 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 0 total=8.5896 mle=1.6184 pcon=4.7511 forget=1.3832 favg=0.8369 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=8.8465 mle=1.7608 pcon=4.7502 forget=1.3918 favg=0.9438 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=8.6495 mle=1.6230 pcon=4.7491 forget=1.3814 favg=0.8960 nr=64 nf=64 protos=540 fproto_sim=NA
 74%|███████▍  | 26/35 [23:13<06:35, 43.94s/it] 77%|███████▋  | 27/35 [23:51<05:36, 42.05s/it] 80%|████████  | 28/35 [24:22<04:30, 38.67s/it] 83%|████████▎ | 29/35 [24:52<03:36, 36.09s/it] 86%|████████▌ | 30/35 [25:21<02:50, 34.13s/it] 89%|████████▊ | 31/35 [25:51<02:10, 32.66s/it] 91%|█████████▏| 32/35 [26:19<01:34, 31.37s/it] 94%|█████████▍| 33/35 [26:41<00:57, 28.71s/it] 97%|█████████▋| 34/35 [27:00<00:25, 25.57s/it][loss] ep 25 it 150 total=8.5935 mle=1.7370 pcon=4.7481 forget=1.3867 favg=0.7217 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 200 total=8.4673 mle=1.8182 pcon=4.7468 forget=1.3930 favg=0.5093 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=8.1585 mle=1.7151 pcon=4.7454 forget=1.3896 favg=0.3083 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=7.6517 mle=1.5910 pcon=4.7439 forget=1.3788 favg=-0.0619 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=7.1944 mle=1.5062 pcon=4.7426 forget=1.3826 favg=-0.4370 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 10 total=7.2204 mle=1.6473 pcon=4.7409 forget=1.3898 favg=-0.5576 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=6.7687 mle=1.5625 pcon=4.7392 forget=1.3786 favg=-0.9116 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=6.7991 mle=1.7010 pcon=4.7376 forget=1.3702 favg=-1.0098 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=7.1488 mle=1.5269 pcon=4.7359 forget=1.3671 favg=-0.4812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=8.8619 mle=1.6803 pcon=4.7343 forget=1.3751 favg=1.0723 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=8.9382 mle=1.5245 pcon=4.7330 forget=1.3848 favg=1.2959 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 310 total=9.1494 mle=1.8710 pcon=4.7318 forget=1.3845 favg=1.1621 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=8.3617 mle=1.5604 pcon=4.7305 forget=1.3784 favg=0.6924 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 20 total=8.2685 mle=1.6462 pcon=4.7295 forget=1.3767 favg=0.5161 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=7.8051 mle=1.5758 pcon=4.7282 forget=1.3681 favg=0.1331 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=7.2785 mle=1.5581 pcon=4.7270 forget=1.3627 favg=-0.3694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=7.1554 mle=1.5403 pcon=4.7259 forget=1.3650 favg=-0.4758 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=7.1207 mle=1.4867 pcon=4.7249 forget=1.3676 favg=-0.4585 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=7.0509 mle=1.6043 pcon=4.7238 forget=1.3634 favg=-0.6406 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=7.1221 mle=1.7207 pcon=4.7230 forget=1.3596 favg=-0.6812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=7.2959 mle=1.7246 pcon=4.7221 forget=1.3907 favg=-0.5415 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 30 total=7.2629 mle=1.6590 pcon=4.7215 forget=1.3644 favg=-0.4819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=7.5552 mle=1.6858 pcon=4.7209 forget=1.3884 favg=-0.2399 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=7.7113 mle=1.6089 pcon=4.7202 forget=1.4092 favg=-0.0270 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=7.8786 mle=1.5581 pcon=4.7195 forget=1.4005 favg=0.2004 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=7.8023 mle=1.4388 pcon=4.7186 forget=1.4147 favg=0.2302 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=8.0281 mle=1.5702 pcon=4.7177 forget=1.4241 favg=0.3162 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=8.2737 mle=1.7631 pcon=4.7167 forget=1.4431 favg=0.3508 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=8.1982 mle=1.6781 pcon=4.7157 forget=1.4474 favg=0.3569 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 40 total=8.1574 mle=1.7026 pcon=4.7146 forget=1.4385 favg=0.3018 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=8.0199 mle=1.5567 pcon=4.7134 forget=1.4321 favg=0.3176 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=7.9822 mle=1.6428 pcon=4.7125 forget=1.4399 favg=0.1870 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=7.7764 mle=1.5910 pcon=4.7115 forget=1.4338 favg=0.0401 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=7.8926 mle=1.5460 pcon=4.7105 forget=1.4330 favg=0.2031 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=7.8901 mle=1.6932 pcon=4.7096 forget=1.4307 favg=0.0566 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=7.8701 mle=1.5577 pcon=4.7087 forget=1.4267 favg=0.1770 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 0 total=7.9056 mle=1.6042 pcon=4.7076 forget=1.4128 favg=0.1810 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=7.9294 mle=1.5934 pcon=4.7068 forget=1.4348 favg=0.1945 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=7.7701 mle=1.5566 pcon=4.7059 forget=1.4224 favg=0.0853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=8.0767 mle=1.7499 pcon=4.7049 forget=1.4156 favg=0.2063 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=7.9035 mle=1.4835 pcon=4.7040 forget=1.4460 favg=0.2700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=8.1826 mle=1.7486 pcon=4.7032 forget=1.4325 favg=0.2983 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=8.0051 mle=1.4908 pcon=4.7024 forget=1.4363 favg=0.3755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=8.1712 mle=1.7233 pcon=4.7018 forget=1.4172 favg=0.3289 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 10 total=8.1171 mle=1.6262 pcon=4.7011 forget=1.4036 favg=0.3862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=8.0070 mle=1.5359 pcon=4.7006 forget=1.4042 favg=0.3665 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=8.0177 mle=1.5577 pcon=4.7000 forget=1.4119 favg=0.3481 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=7.9810 mle=1.5460 pcon=4.6993 forget=1.4012 favg=0.3345 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=7.9782 mle=1.5465 pcon=4.6987 forget=1.3951 favg=0.3379 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 260 total=7.9359 mle=1.5179 pcon=4.6981 forget=1.3935 favg=0.3264 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=7.8798 mle=1.5911 pcon=4.6975 forget=1.3954 favg=0.1957 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=7.8036 mle=1.5424 pcon=4.6969 forget=1.3785 favg=0.1858 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 20 total=7.8745 mle=1.7104 pcon=4.6964 forget=1.3774 favg=0.0903 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=7.6069 mle=1.5514 pcon=4.6957 forget=1.3760 favg=-0.0163 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=7.7642 mle=1.5652 pcon=4.6953 forget=1.4071 favg=0.0966 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 170 total=7.6256 mle=1.5550 pcon=4.6950 forget=1.3964 favg=-0.0207 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=7.9691 mle=1.6481 pcon=4.6946 forget=1.3960 favg=0.2305 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=7.7816 mle=1.5752 pcon=4.6942 forget=1.3866 favg=0.1255 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=7.5614 mle=1.4640 pcon=4.6938 forget=1.3762 favg=0.0274 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=7.5804 mle=1.4578 pcon=4.6936 forget=1.3933 favg=0.0357 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 30 total=7.5821 mle=1.6051 pcon=4.6933 forget=1.3955 favg=-0.1118 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=7.9815 mle=1.8961 pcon=4.6930 forget=1.3762 favg=0.0162 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=7.6627 mle=1.4896 pcon=4.6927 forget=1.3985 favg=0.0818 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=7.7647 mle=1.6589 pcon=4.6924 forget=1.4091 favg=0.0043 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=7.9273 mle=1.6295 pcon=4.6921 forget=1.4023 favg=0.2034 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=7.8975 mle=1.6917 pcon=4.6918 forget=1.3925 favg=0.1215 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=7.8347 mle=1.6895 pcon=4.6916 forget=1.3796 favg=0.0740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 380 total=7.7469 mle=1.5766 pcon=4.6912 forget=1.4301 favg=0.0490 nr=64 nf=64 protos=540 fproto_sim=NA
100%|██████████| 35/35 [27:18<00:00, 23.40s/it]100%|██████████| 35/35 [27:18<00:00, 46.82s/it]
[loss] ep 34 it 40 total=7.9207 mle=1.7048 pcon=4.6908 forget=1.4021 favg=0.1230 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=7.6997 mle=1.5109 pcon=4.6903 forget=1.4034 favg=0.0951 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 140 total=7.7139 mle=1.6078 pcon=4.6901 forget=1.3637 favg=0.0522 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=7.7921 mle=1.6153 pcon=4.6899 forget=1.3975 favg=0.0895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 240 total=7.7894 mle=1.6907 pcon=4.6895 forget=1.3958 favg=0.0134 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=7.8075 mle=1.6119 pcon=4.6892 forget=1.3977 favg=0.1088 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=7.7791 mle=1.5254 pcon=4.6889 forget=1.4218 favg=0.1429 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0-forget_avgproto_enable-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:09,  3.01it/s]  3%|▎         | 11/391 [00:00<00:11, 31.80it/s]  5%|▌         | 21/391 [00:00<00:07, 51.64it/s]  8%|▊         | 31/391 [00:00<00:05, 65.34it/s] 10%|█         | 41/391 [00:00<00:04, 74.85it/s] 13%|█▎        | 51/391 [00:00<00:04, 80.79it/s] 16%|█▌        | 61/391 [00:00<00:03, 83.56it/s] 18%|█▊        | 71/391 [00:01<00:03, 87.33it/s] 21%|██        | 81/391 [00:01<00:03, 90.12it/s] 23%|██▎       | 91/391 [00:01<00:03, 91.99it/s] 26%|██▌       | 101/391 [00:01<00:03, 93.23it/s] 28%|██▊       | 111/391 [00:01<00:02, 94.21it/s] 31%|███       | 121/391 [00:01<00:02, 93.51it/s] 34%|███▎      | 131/391 [00:01<00:02, 93.57it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.43it/s] 39%|███▊      | 151/391 [00:01<00:02, 95.08it/s] 41%|████      | 161/391 [00:02<00:02, 95.26it/s] 44%|████▎     | 171/391 [00:02<00:02, 95.41it/s] 46%|████▋     | 181/391 [00:02<00:02, 95.44it/s] 49%|████▉     | 191/391 [00:02<00:02, 95.89it/s] 51%|█████▏    | 201/391 [00:02<00:01, 95.71it/s] 54%|█████▍    | 211/391 [00:02<00:01, 95.86it/s] 57%|█████▋    | 221/391 [00:02<00:01, 95.94it/s] 59%|█████▉    | 231/391 [00:02<00:01, 96.18it/s] 62%|██████▏   | 241/391 [00:02<00:01, 95.85it/s] 64%|██████▍   | 251/391 [00:02<00:01, 95.95it/s] 67%|██████▋   | 261/391 [00:03<00:01, 96.00it/s] 69%|██████▉   | 271/391 [00:03<00:01, 96.07it/s] 72%|███████▏  | 281/391 [00:03<00:01, 96.16it/s] 74%|███████▍  | 291/391 [00:03<00:01, 96.44it/s] 77%|███████▋  | 301/391 [00:03<00:00, 96.56it/s] 80%|███████▉  | 311/391 [00:03<00:00, 96.67it/s] 82%|████████▏ | 321/391 [00:03<00:00, 96.50it/s] 85%|████████▍ | 331/391 [00:03<00:00, 96.42it/s] 87%|████████▋ | 341/391 [00:03<00:00, 92.55it/s] 90%|████████▉ | 351/391 [00:04<00:00, 92.56it/s] 92%|█████████▏| 361/391 [00:04<00:00, 93.48it/s] 95%|█████████▍| 371/391 [00:04<00:00, 94.40it/s] 97%|█████████▋| 381/391 [00:04<00:00, 94.61it/s]100%|██████████| 391/391 [00:04<00:00, 92.59it/s]100%|██████████| 391/391 [00:04<00:00, 88.27it/s]
50000 images processed, 4.53371787071228 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.01it/s] 14%|█▍        | 11/79 [00:00<00:02, 31.56it/s] 27%|██▋       | 21/79 [00:00<00:01, 50.79it/s] 39%|███▉      | 31/79 [00:00<00:00, 64.18it/s] 52%|█████▏    | 41/79 [00:00<00:00, 73.41it/s] 65%|██████▍   | 51/79 [00:00<00:00, 78.97it/s] 77%|███████▋  | 61/79 [00:00<00:00, 83.00it/s] 90%|████████▉ | 71/79 [00:01<00:00, 86.96it/s]100%|██████████| 79/79 [00:01<00:00, 67.70it/s]
10000 images processed, 1.185774326324463 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:01,  3.31it/s]  5%|▌         | 11/204 [00:00<00:05, 33.85it/s] 10%|█         | 21/204 [00:00<00:03, 53.61it/s] 15%|█▍        | 30/204 [00:00<00:02, 64.00it/s] 19%|█▉        | 39/204 [00:00<00:02, 70.70it/s] 24%|██▍       | 49/204 [00:00<00:01, 78.26it/s] 29%|██▉       | 59/204 [00:00<00:01, 83.56it/s] 34%|███▍      | 69/204 [00:01<00:01, 87.22it/s] 39%|███▊      | 79/204 [00:01<00:01, 89.82it/s] 44%|████▎     | 89/204 [00:01<00:01, 91.67it/s] 49%|████▊     | 99/204 [00:01<00:01, 91.97it/s] 53%|█████▎    | 109/204 [00:01<00:01, 90.41it/s] 58%|█████▊    | 119/204 [00:01<00:00, 91.83it/s] 63%|██████▎   | 129/204 [00:01<00:00, 92.92it/s] 68%|██████▊   | 139/204 [00:01<00:00, 93.62it/s] 73%|███████▎  | 149/204 [00:01<00:00, 94.10it/s] 78%|███████▊  | 159/204 [00:01<00:00, 94.20it/s] 83%|████████▎ | 169/204 [00:02<00:00, 94.53it/s] 88%|████████▊ | 179/204 [00:02<00:00, 89.71it/s] 93%|█████████▎| 189/204 [00:02<00:00, 91.23it/s] 98%|█████████▊| 199/204 [00:02<00:00, 92.70it/s]100%|██████████| 204/204 [00:02<00:00, 82.10it/s]
26032 images processed, 2.5199942588806152 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:42,  1.85it/s] 11%|█▏        | 9/79 [00:00<00:04, 16.97it/s] 22%|██▏       | 17/79 [00:00<00:02, 28.52it/s] 29%|██▉       | 23/79 [00:00<00:01, 34.79it/s] 35%|███▌      | 28/79 [00:01<00:01, 34.64it/s] 43%|████▎     | 34/79 [00:01<00:01, 36.12it/s] 53%|█████▎    | 42/79 [00:01<00:00, 41.81it/s] 63%|██████▎   | 50/79 [00:01<00:00, 45.63it/s] 73%|███████▎  | 58/79 [00:01<00:00, 49.12it/s] 84%|████████▎ | 66/79 [00:01<00:00, 52.45it/s] 94%|█████████▎| 74/79 [00:01<00:00, 53.86it/s]100%|██████████| 79/79 [00:01<00:00, 40.15it/s]
10000 images processed, 2.0055973529815674 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:21,  3.59it/s] 14%|█▍        | 11/79 [00:00<00:01, 35.66it/s] 27%|██▋       | 21/79 [00:00<00:01, 55.55it/s] 39%|███▉      | 31/79 [00:00<00:00, 68.19it/s] 52%|█████▏    | 41/79 [00:00<00:00, 76.65it/s] 65%|██████▍   | 51/79 [00:00<00:00, 82.37it/s] 77%|███████▋  | 61/79 [00:00<00:00, 85.87it/s] 90%|████████▉ | 71/79 [00:01<00:00, 88.93it/s]100%|██████████| 79/79 [00:01<00:00, 72.42it/s]
10000 images processed, 1.1129279136657715 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:23,  2.93it/s] 16%|█▌        | 11/70 [00:00<00:01, 31.25it/s] 30%|███       | 21/70 [00:00<00:00, 50.83it/s] 44%|████▍     | 31/70 [00:00<00:00, 64.22it/s] 59%|█████▊    | 41/70 [00:00<00:00, 73.27it/s] 73%|███████▎  | 51/70 [00:00<00:00, 79.85it/s] 87%|████████▋ | 61/70 [00:00<00:00, 84.57it/s]100%|██████████| 70/70 [00:01<00:00, 65.58it/s]
8925 images processed, 1.095369815826416 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:41,  1.07it/s] 20%|██        | 9/45 [00:01<00:04,  7.35it/s] 31%|███       | 14/45 [00:01<00:03,  9.33it/s] 38%|███▊      | 17/45 [00:02<00:02,  9.37it/s] 49%|████▉     | 22/45 [00:02<00:02, 11.39it/s] 56%|█████▌    | 25/45 [00:02<00:01, 12.45it/s] 67%|██████▋   | 30/45 [00:02<00:01, 12.98it/s] 73%|███████▎  | 33/45 [00:03<00:00, 12.44it/s] 91%|█████████ | 41/45 [00:03<00:00, 14.66it/s]100%|██████████| 45/45 [00:03<00:00, 12.05it/s]
5640 images processed, 3.7544772624969482 seconds used

17.722941160202026
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.51  99.36
places365     67.82  81.18
LSUN          17.47  96.08
iSUN          72.30  81.66
dtd           37.78  91.39
AVG           39.58  89.93
Retain-Acc: 0.7408
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
9.369561195373535
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fpw1.0_rf.png
