nohup: ignoring input
==== Stage 1: forget_inc={0,8,11,40,51}; forget_seen={}; all={0,8,11,40,51,66,67,88,94,57} ====
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1', adapter_load_path=None, adapter_load_paths=None, lora_new_adapter_name='train_s1', lora_stack=True, lora_orth_enable=False, lora_orth_lambda=0.1, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc='0,8,11,40,51', forget_classes_seen=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] added trainable adapter 'train_s1'
[peft] set_adapter(list) failed: unhashable type: 'list'; try adapter fusion
[peft] adapter fusion failed: adapter fusion API not available or single adapter provided; fallback to last adapter
[peft] active adapter set to: train_s1
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 22082496
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.train_s1.weight
[debug] trainable: base_model.model.head.2.lora_A.train_s1.weight
[debug] trainable: base_model.model.head.2.lora_B.train_s1.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.train_s1.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.train_s1.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.train_s1.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.train_s1.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.train_s1.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [01:10<57:31, 70.43s/it]  4%|▍         | 2/50 [01:22<29:00, 36.27s/it]  6%|▌         | 3/50 [01:33<19:15, 24.59s/it]  8%|▊         | 4/50 [01:46<15:18, 19.97s/it] 10%|█         | 5/50 [01:56<12:22, 16.50s/it] 12%|█▏        | 6/50 [02:07<10:36, 14.47s/it] 14%|█▍        | 7/50 [02:17<09:23, 13.11s/it][loss] ep 0 it 0 total=8.8609 mle=2.1075 pcon=5.2951 forget=1.4584 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 0 it 50 total=8.4405 mle=1.6912 pcon=5.2908 forget=1.4585 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 100 total=8.6865 mle=1.9226 pcon=5.2867 forget=1.4772 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 150 total=8.9678 mle=2.2575 pcon=5.2827 forget=1.4275 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 0 it 200 total=8.9688 mle=2.2334 pcon=5.2787 forget=1.4568 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 250 total=8.3179 mle=1.5955 pcon=5.2747 forget=1.4477 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 0 it 300 total=8.4077 mle=1.7072 pcon=5.2707 forget=1.4298 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 350 total=8.5211 mle=1.7473 pcon=5.2669 forget=1.5068 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 1 it 10 total=8.7004 mle=1.9761 pcon=5.2632 forget=1.4612 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 1 it 60 total=8.7489 mle=2.0227 pcon=5.2595 forget=1.4667 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 1 it 110 total=8.7102 mle=2.0185 pcon=5.2558 forget=1.4359 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 1 it 160 total=8.6122 mle=1.8491 pcon=5.2521 forget=1.5110 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 1 it 210 total=8.6773 mle=2.0049 pcon=5.2486 forget=1.4238 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 1 it 260 total=8.5456 mle=1.8253 pcon=5.2449 forget=1.4755 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 1 it 310 total=8.3754 mle=1.7016 pcon=5.2416 forget=1.4322 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 1 it 360 total=8.7013 mle=2.0531 pcon=5.2379 forget=1.4102 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 2 it 20 total=8.2991 mle=1.5821 pcon=5.2343 forget=1.4827 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 2 it 70 total=8.7941 mle=2.1033 pcon=5.2306 forget=1.4603 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 2 it 120 total=8.4498 mle=1.8135 pcon=5.2269 forget=1.4094 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 2 it 170 total=8.7154 mle=2.0183 pcon=5.2237 forget=1.4733 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 2 it 220 total=9.1173 mle=2.4229 pcon=5.2206 forget=1.4738 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 2 it 270 total=8.3079 mle=1.6396 pcon=5.2170 forget=1.4513 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 2 it 320 total=8.5448 mle=1.8620 pcon=5.2136 forget=1.4692 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 2 it 370 total=9.5172 mle=2.8536 pcon=5.2104 forget=1.4533 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 3 it 30 total=8.5906 mle=1.9173 pcon=5.2070 forget=1.4663 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 3 it 80 total=8.8388 mle=2.1276 pcon=5.2040 forget=1.5072 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 3 it 130 total=8.2119 mle=1.5745 pcon=5.2005 forget=1.4369 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 3 it 180 total=8.4239 mle=1.7296 pcon=5.1973 forget=1.4970 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 3 it 230 total=8.4264 mle=1.7721 pcon=5.1941 forget=1.4602 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 3 it 280 total=8.4944 mle=1.8559 pcon=5.1911 forget=1.4474 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 3 it 330 total=8.6046 mle=1.9613 pcon=5.1879 forget=1.4553 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 3 it 380 total=8.3558 mle=1.7303 pcon=5.1846 forget=1.4409 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 4 it 40 total=8.3437 mle=1.7108 pcon=5.1817 forget=1.4512 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 4 it 90 total=8.2441 mle=1.6470 pcon=5.1787 forget=1.4184 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 4 it 140 total=8.5747 mle=1.9410 pcon=5.1757 forget=1.4580 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 4 it 190 total=9.3094 mle=2.6871 pcon=5.1728 forget=1.4495 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 4 it 240 total=8.5687 mle=1.9817 pcon=5.1698 forget=1.4172 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 4 it 290 total=8.6276 mle=2.0604 pcon=5.1670 forget=1.4002 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 4 it 340 total=8.4402 mle=1.7887 pcon=5.1642 forget=1.4872 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 5 it 0 total=8.9113 mle=2.2842 pcon=5.1615 forget=1.4656 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 5 it 50 total=8.3445 mle=1.7633 pcon=5.1586 forget=1.4227 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 5 it 100 total=8.4856 mle=1.8781 pcon=5.1559 forget=1.4516 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 5 it 150 total=8.5974 mle=2.0411 pcon=5.1530 forget=1.4034 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 5 it 200 total=8.7823 mle=2.1573 pcon=5.1503 forget=1.4747 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 5 it 250 total=8.4603 mle=1.8021 pcon=5.1479 forget=1.5103 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 5 it 300 total=8.2414 mle=1.7145 pcon=5.1451 forget=1.3818 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 5 it 350 total=8.4164 mle=1.8781 pcon=5.1424 forget=1.3958 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 6 it 10 total=8.5680 mle=1.9879 pcon=5.1398 forget=1.4403 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 6 it 60 total=8.3280 mle=1.7741 pcon=5.1370 forget=1.4170 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 6 it 110 total=8.2849 mle=1.7220 pcon=5.1345 forget=1.4284 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 6 it 160 total=8.5880 mle=2.0200 pcon=5.1319 forget=1.4361 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 6 it 210 total=9.1595 mle=2.6069 pcon=5.1293 forget=1.4233 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 6 it 260 total=8.4237 mle=1.9034 pcon=5.1268 forget=1.3935 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 6 it 310 total=8.4958 mle=1.9473 pcon=5.1244 forget=1.4241 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 6 it 360 total=8.0245 mle=1.4968 pcon=5.1219 forget=1.4058 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 7 it 20 total=8.2416 mle=1.6982 pcon=5.1195 forget=1.4239 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 7 it 70 total=8.5261 mle=2.0003 pcon=5.1171 forget=1.4087 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 7 it 120 total=8.4204 mle=1.8651 pcon=5.1146 forget=1.4407 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 7 it 170 total=8.8599 mle=2.3531 pcon=5.1120 forget=1.3947 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
 16%|█▌        | 8/50 [02:28<08:45, 12.50s/it] 18%|█▊        | 9/50 [02:39<08:09, 11.93s/it] 20%|██        | 10/50 [02:50<07:41, 11.53s/it] 22%|██▏       | 11/50 [03:00<07:19, 11.27s/it] 24%|██▍       | 12/50 [03:16<07:58, 12.58s/it] 26%|██▌       | 13/50 [03:26<07:22, 11.97s/it] 28%|██▊       | 14/50 [03:37<06:58, 11.63s/it][loss] ep 7 it 220 total=8.3496 mle=1.8050 pcon=5.1098 forget=1.4348 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 7 it 270 total=8.4042 mle=1.8432 pcon=5.1074 forget=1.4536 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 7 it 320 total=7.9219 mle=1.4073 pcon=5.1051 forget=1.4096 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 7 it 370 total=8.2964 mle=1.8113 pcon=5.1025 forget=1.3826 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 8 it 30 total=8.1980 mle=1.7243 pcon=5.1003 forget=1.3734 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 8 it 80 total=8.2463 mle=1.7451 pcon=5.0979 forget=1.4033 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 8 it 130 total=8.1185 mle=1.6552 pcon=5.0956 forget=1.3677 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 8 it 180 total=8.2860 mle=1.8495 pcon=5.0933 forget=1.3432 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 8 it 230 total=8.3424 mle=1.8624 pcon=5.0911 forget=1.3889 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 8 it 280 total=8.2979 mle=1.8399 pcon=5.0888 forget=1.3692 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 8 it 330 total=8.3914 mle=1.8458 pcon=5.0864 forget=1.4592 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 8 it 380 total=7.9398 mle=1.4837 pcon=5.0841 forget=1.3719 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 9 it 40 total=8.2811 mle=1.8451 pcon=5.0816 forget=1.3544 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 9 it 90 total=8.4304 mle=1.9538 pcon=5.0793 forget=1.3973 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 9 it 140 total=8.0597 mle=1.5849 pcon=5.0769 forget=1.3980 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 9 it 190 total=8.2714 mle=1.8421 pcon=5.0742 forget=1.3551 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 9 it 240 total=8.1777 mle=1.7489 pcon=5.0718 forget=1.3569 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 9 it 290 total=8.3676 mle=1.9150 pcon=5.0695 forget=1.3832 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 9 it 340 total=8.0069 mle=1.6049 pcon=5.0670 forget=1.3351 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 10 it 0 total=8.2044 mle=1.7876 pcon=5.0649 forget=1.3520 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 10 it 50 total=8.0531 mle=1.6472 pcon=5.0627 forget=1.3432 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 10 it 100 total=8.1568 mle=1.7184 pcon=5.0602 forget=1.3782 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 10 it 150 total=8.5037 mle=2.0841 pcon=5.0578 forget=1.3618 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 10 it 200 total=8.3314 mle=1.8927 pcon=5.0556 forget=1.3831 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 10 it 250 total=8.4938 mle=2.0580 pcon=5.0535 forget=1.3824 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 10 it 300 total=8.3329 mle=1.9045 pcon=5.0510 forget=1.3774 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 10 it 350 total=8.0306 mle=1.5978 pcon=5.0486 forget=1.3843 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 11 it 10 total=8.4485 mle=2.0031 pcon=5.0465 forget=1.3989 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 11 it 60 total=8.5640 mle=2.1456 pcon=5.0443 forget=1.3742 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 11 it 110 total=8.3822 mle=1.9534 pcon=5.0421 forget=1.3867 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 11 it 160 total=8.2676 mle=1.8032 pcon=5.0401 forget=1.4243 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 11 it 210 total=8.3403 mle=1.8859 pcon=5.0378 forget=1.4166 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 11 it 260 total=7.9116 mle=1.4538 pcon=5.0357 forget=1.4221 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 11 it 310 total=7.9451 mle=1.5070 pcon=5.0335 forget=1.4046 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 11 it 360 total=8.1112 mle=1.6688 pcon=5.0313 forget=1.4111 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 12 it 20 total=8.3518 mle=1.8801 pcon=5.0289 forget=1.4429 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 70 total=8.1435 mle=1.6898 pcon=5.0265 forget=1.4271 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 12 it 120 total=8.2337 mle=1.7790 pcon=5.0242 forget=1.4305 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 12 it 170 total=8.0917 mle=1.6290 pcon=5.0221 forget=1.4405 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 220 total=8.1069 mle=1.6494 pcon=5.0196 forget=1.4379 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 12 it 270 total=8.0712 mle=1.5792 pcon=5.0171 forget=1.4749 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 320 total=8.3453 mle=1.8900 pcon=5.0149 forget=1.4404 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 12 it 370 total=8.2733 mle=1.7916 pcon=5.0124 forget=1.4693 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 13 it 30 total=7.9601 mle=1.4929 pcon=5.0100 forget=1.4571 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 13 it 80 total=8.1046 mle=1.5828 pcon=5.0077 forget=1.5141 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 13 it 130 total=8.2520 mle=1.7808 pcon=5.0053 forget=1.4660 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 13 it 180 total=8.1099 mle=1.6433 pcon=5.0029 forget=1.4636 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 13 it 230 total=8.0073 mle=1.5408 pcon=5.0004 forget=1.4661 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 13 it 280 total=8.3491 mle=1.8606 pcon=4.9980 forget=1.4904 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 13 it 330 total=8.1451 mle=1.6712 pcon=4.9955 forget=1.4784 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 13 it 380 total=8.1419 mle=1.6664 pcon=4.9929 forget=1.4826 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 14 it 40 total=8.0290 mle=1.5342 pcon=4.9904 forget=1.5044 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 90 total=7.7878 mle=1.3280 pcon=4.9882 forget=1.4717 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 140 total=7.9696 mle=1.4979 pcon=4.9859 forget=1.4857 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 14 it 190 total=8.1979 mle=1.7391 pcon=4.9837 forget=1.4752 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 240 total=8.0744 mle=1.6461 pcon=4.9814 forget=1.4469 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 14 it 290 total=8.0495 mle=1.5865 pcon=4.9791 forget=1.4840 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 14 it 340 total=7.9536 mle=1.4895 pcon=4.9767 forget=1.4873 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
 30%|███       | 15/50 [03:48<06:40, 11.45s/it] 32%|███▏      | 16/50 [04:02<06:49, 12.03s/it] 34%|███▍      | 17/50 [04:12<06:23, 11.63s/it] 36%|███▌      | 18/50 [04:23<06:01, 11.29s/it] 38%|███▊      | 19/50 [04:34<05:48, 11.25s/it] 40%|████      | 20/50 [04:45<05:34, 11.14s/it] 42%|████▏     | 21/50 [04:56<05:19, 11.02s/it] 44%|████▍     | 22/50 [05:08<05:16, 11.31s/it][peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 15 it 0 total=8.3913 mle=1.9522 pcon=4.9745 forget=1.4646 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 15 it 50 total=8.1447 mle=1.7319 pcon=4.9723 forget=1.4404 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 15 it 100 total=8.0245 mle=1.5983 pcon=4.9701 forget=1.4561 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 150 total=7.9774 mle=1.5214 pcon=4.9679 forget=1.4881 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 200 total=7.9514 mle=1.5562 pcon=4.9659 forget=1.4292 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 15 it 250 total=7.8188 mle=1.5227 pcon=4.9636 forget=1.3325 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 15 it 300 total=7.9175 mle=1.6658 pcon=4.9615 forget=1.2903 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 350 total=7.9984 mle=1.7483 pcon=4.9595 forget=1.2906 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 16 it 10 total=7.7046 mle=1.3599 pcon=4.9576 forget=1.3871 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 16 it 60 total=7.7074 mle=1.4651 pcon=4.9555 forget=1.2869 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 16 it 110 total=7.8671 mle=1.6543 pcon=4.9533 forget=1.2596 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 16 it 160 total=7.9388 mle=1.7201 pcon=4.9512 forget=1.2675 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 16 it 210 total=7.8413 mle=1.6321 pcon=4.9492 forget=1.2600 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 16 it 260 total=7.8172 mle=1.6492 pcon=4.9472 forget=1.2208 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 16 it 310 total=8.0096 mle=1.8067 pcon=4.9452 forget=1.2577 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 16 it 360 total=7.7752 mle=1.5919 pcon=4.9432 forget=1.2401 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 17 it 20 total=7.6993 mle=1.5637 pcon=4.9412 forget=1.1943 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 70 total=7.7078 mle=1.5156 pcon=4.9394 forget=1.2529 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 120 total=7.8445 mle=1.7383 pcon=4.9374 forget=1.1689 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 170 total=7.6320 mle=1.5234 pcon=4.9353 forget=1.1732 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 17 it 220 total=7.7961 mle=1.7287 pcon=4.9332 forget=1.1341 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 17 it 270 total=7.5379 mle=1.4609 pcon=4.9311 forget=1.1459 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 17 it 320 total=7.6391 mle=1.6475 pcon=4.9290 forget=1.0626 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 17 it 370 total=7.9150 mle=1.9115 pcon=4.9266 forget=1.0769 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 18 it 30 total=7.7889 mle=1.8083 pcon=4.9241 forget=1.0565 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 18 it 80 total=8.0291 mle=2.0652 pcon=4.9216 forget=1.0423 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 18 it 130 total=7.8318 mle=1.8832 pcon=4.9189 forget=1.0297 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 18 it 180 total=7.6210 mle=1.6974 pcon=4.9161 forget=1.0074 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 18 it 230 total=7.5875 mle=1.7035 pcon=4.9136 forget=0.9704 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 18 it 280 total=7.4528 mle=1.5291 pcon=4.9108 forget=1.0130 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 18 it 330 total=7.4420 mle=1.5207 pcon=4.9078 forget=1.0135 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 18 it 380 total=7.5290 mle=1.5973 pcon=4.9049 forget=1.0269 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 19 it 40 total=7.5039 mle=1.6094 pcon=4.9020 forget=0.9926 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 19 it 90 total=7.7422 mle=1.8294 pcon=4.8990 forget=1.0138 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 19 it 140 total=7.4759 mle=1.5791 pcon=4.8961 forget=1.0007 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 19 it 190 total=7.6236 mle=1.7125 pcon=4.8932 forget=1.0179 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 19 it 240 total=7.2880 mle=1.4009 pcon=4.8903 forget=0.9967 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 19 it 290 total=7.4560 mle=1.5805 pcon=4.8876 forget=0.9878 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 19 it 340 total=7.5880 mle=1.6963 pcon=4.8849 forget=1.0068 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 20 it 0 total=7.4799 mle=1.6025 pcon=4.8821 forget=0.9953 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 20 it 50 total=7.3783 mle=1.5093 pcon=4.8793 forget=0.9897 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 20 it 100 total=7.5946 mle=1.6872 pcon=4.8767 forget=1.0306 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 20 it 150 total=7.6741 mle=1.7897 pcon=4.8739 forget=1.0105 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 20 it 200 total=7.5248 mle=1.6554 pcon=4.8712 forget=0.9981 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 20 it 250 total=7.4999 mle=1.6194 pcon=4.8685 forget=1.0119 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 20 it 300 total=7.5347 mle=1.6643 pcon=4.8659 forget=1.0045 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 20 it 350 total=7.4310 mle=1.5093 pcon=4.8632 forget=1.0586 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 21 it 10 total=7.4517 mle=1.5504 pcon=4.8608 forget=1.0405 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 21 it 60 total=7.4657 mle=1.5563 pcon=4.8582 forget=1.0511 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 21 it 110 total=7.2695 mle=1.3707 pcon=4.8558 forget=1.0431 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 21 it 160 total=7.4383 mle=1.5498 pcon=4.8531 forget=1.0353 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 21 it 210 total=7.7719 mle=1.8810 pcon=4.8508 forget=1.0401 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 21 it 260 total=7.5130 mle=1.6039 pcon=4.8483 forget=1.0608 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 21 it 310 total=7.6152 mle=1.7007 pcon=4.8459 forget=1.0686 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 21 it 360 total=7.3202 mle=1.4226 pcon=4.8436 forget=1.0540 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 22 it 20 total=7.3214 mle=1.4304 pcon=4.8413 forget=1.0497 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 22 it 70 total=7.6102 mle=1.6970 pcon=4.8391 forget=1.0741 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 22 it 120 total=7.6539 mle=1.7544 pcon=4.8368 forget=1.0626 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 22 it 170 total=7.5268 mle=1.6192 pcon=4.8347 forget=1.0729 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
 46%|████▌     | 23/50 [05:17<04:46, 10.63s/it] 48%|████▊     | 24/50 [05:26<04:24, 10.16s/it] 50%|█████     | 25/50 [05:35<04:04,  9.78s/it] 52%|█████▏    | 26/50 [05:47<04:16, 10.68s/it] 54%|█████▍    | 27/50 [05:56<03:54, 10.19s/it] 56%|█████▌    | 28/50 [06:06<03:43, 10.15s/it] 58%|█████▊    | 29/50 [06:20<03:51, 11.03s/it] 60%|██████    | 30/50 [06:33<03:54, 11.74s/it] 62%|██████▏   | 31/50 [06:47<03:53, 12.30s/it][loss] ep 22 it 220 total=7.2741 mle=1.3669 pcon=4.8325 forget=1.0747 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 22 it 270 total=7.5066 mle=1.5862 pcon=4.8302 forget=1.0902 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 22 it 320 total=7.8551 mle=1.9428 pcon=4.8281 forget=1.0841 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 22 it 370 total=7.4446 mle=1.5331 pcon=4.8260 forget=1.0855 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 23 it 30 total=7.7089 mle=1.7895 pcon=4.8239 forget=1.0955 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 23 it 80 total=7.4797 mle=1.5430 pcon=4.8217 forget=1.1150 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 23 it 130 total=7.5576 mle=1.6303 pcon=4.8196 forget=1.1077 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 23 it 180 total=7.6941 mle=1.7748 pcon=4.8175 forget=1.1018 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 23 it 230 total=7.3762 mle=1.4413 pcon=4.8156 forget=1.1194 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 23 it 280 total=7.7844 mle=1.8537 pcon=4.8137 forget=1.1170 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 23 it 330 total=7.5101 mle=1.5739 pcon=4.8117 forget=1.1245 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 23 it 380 total=7.4732 mle=1.5390 pcon=4.8097 forget=1.1245 favg=0.0000 nr=21 nf=21 protos=570 fproto_sim=NA
[loss] ep 24 it 40 total=7.5234 mle=1.5872 pcon=4.8077 forget=1.1285 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 24 it 90 total=7.3929 mle=1.4366 pcon=4.8058 forget=1.1505 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 24 it 140 total=7.6088 mle=1.6583 pcon=4.8041 forget=1.1464 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 24 it 190 total=7.7746 mle=1.8300 pcon=4.8022 forget=1.1425 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 24 it 240 total=7.6131 mle=1.6637 pcon=4.8004 forget=1.1490 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 24 it 290 total=7.6076 mle=1.6417 pcon=4.7986 forget=1.1673 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 24 it 340 total=7.4076 mle=1.4458 pcon=4.7968 forget=1.1650 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 0 total=7.3838 mle=1.4143 pcon=4.7951 forget=1.1745 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 50 total=7.6575 mle=1.6936 pcon=4.7934 forget=1.1706 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 25 it 100 total=7.7263 mle=1.7597 pcon=4.7918 forget=1.1748 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 25 it 150 total=7.7389 mle=1.7689 pcon=4.7903 forget=1.1797 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 25 it 200 total=7.6186 mle=1.6487 pcon=4.7886 forget=1.1813 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 250 total=7.4472 mle=1.4703 pcon=4.7870 forget=1.1899 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 25 it 300 total=7.6565 mle=1.6684 pcon=4.7855 forget=1.2026 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 350 total=7.5228 mle=1.5410 pcon=4.7838 forget=1.1980 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 10 total=7.8173 mle=1.8421 pcon=4.7823 forget=1.1928 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 26 it 60 total=7.5621 mle=1.5610 pcon=4.7808 forget=1.2203 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 110 total=7.6186 mle=1.6252 pcon=4.7792 forget=1.2142 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 26 it 160 total=7.4231 mle=1.4355 pcon=4.7778 forget=1.2098 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 26 it 210 total=7.4415 mle=1.4380 pcon=4.7764 forget=1.2271 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 26 it 260 total=7.9169 mle=1.9187 pcon=4.7750 forget=1.2232 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 26 it 310 total=7.4020 mle=1.4086 pcon=4.7733 forget=1.2200 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 360 total=7.5934 mle=1.5986 pcon=4.7720 forget=1.2227 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 27 it 20 total=7.5486 mle=1.5363 pcon=4.7707 forget=1.2416 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 27 it 70 total=7.6960 mle=1.6761 pcon=4.7695 forget=1.2504 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 27 it 120 total=7.6852 mle=1.6770 pcon=4.7681 forget=1.2402 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 27 it 170 total=7.5486 mle=1.5322 pcon=4.7667 forget=1.2497 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 27 it 220 total=7.5840 mle=1.5777 pcon=4.7655 forget=1.2408 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 27 it 270 total=7.7829 mle=1.7735 pcon=4.7641 forget=1.2453 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 27 it 320 total=7.5129 mle=1.5045 pcon=4.7628 forget=1.2456 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 27 it 370 total=7.5324 mle=1.5056 pcon=4.7616 forget=1.2653 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 28 it 30 total=7.5637 mle=1.5247 pcon=4.7604 forget=1.2787 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 28 it 80 total=7.7056 mle=1.6818 pcon=4.7592 forget=1.2645 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 28 it 130 total=7.4883 mle=1.4659 pcon=4.7579 forget=1.2645 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 28 it 180 total=7.5350 mle=1.5113 pcon=4.7566 forget=1.2671 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 28 it 230 total=7.6040 mle=1.5697 pcon=4.7553 forget=1.2790 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 28 it 280 total=7.6335 mle=1.6033 pcon=4.7542 forget=1.2760 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 28 it 330 total=7.6803 mle=1.6372 pcon=4.7530 forget=1.2901 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 28 it 380 total=7.3949 mle=1.3453 pcon=4.7518 forget=1.2978 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 29 it 40 total=7.6701 mle=1.6117 pcon=4.7506 forget=1.3078 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 29 it 90 total=7.6992 mle=1.6597 pcon=4.7494 forget=1.2900 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 29 it 140 total=7.5451 mle=1.4784 pcon=4.7484 forget=1.3183 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 29 it 190 total=7.6825 mle=1.6401 pcon=4.7473 forget=1.2952 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 29 it 240 total=7.6356 mle=1.5829 pcon=4.7462 forget=1.3065 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 29 it 290 total=7.7625 mle=1.7145 pcon=4.7451 forget=1.3028 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 29 it 340 total=7.5162 mle=1.4715 pcon=4.7440 forget=1.3007 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 30 it 0 total=7.4937 mle=1.4426 pcon=4.7430 forget=1.3080 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 30 it 50 total=7.7666 mle=1.7162 pcon=4.7420 forget=1.3085 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 30 it 100 total=7.4452 mle=1.3771 pcon=4.7410 forget=1.3271 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 30 it 150 total=7.6487 mle=1.5930 pcon=4.7402 forget=1.3155 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 30 it 200 total=7.5692 mle=1.5051 pcon=4.7392 forget=1.3249 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 30 it 250 total=7.5898 mle=1.5262 pcon=4.7381 forget=1.3255 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 30 it 300 total=7.6712 mle=1.6080 pcon=4.7372 forget=1.3260 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 30 it 350 total=7.5933 mle=1.5205 pcon=4.7364 forget=1.3364 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 31 it 10 total=7.4829 mle=1.4227 pcon=4.7355 forget=1.3247 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 31 it 60 total=7.6967 mle=1.6250 pcon=4.7347 forget=1.3370 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 31 it 110 total=7.6533 mle=1.5963 pcon=4.7338 forget=1.3232 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
 64%|██████▍   | 32/50 [07:00<03:46, 12.58s/it] 66%|██████▌   | 33/50 [07:13<03:39, 12.90s/it] 68%|██████▊   | 34/50 [07:27<03:27, 12.95s/it] 70%|███████   | 35/50 [07:40<03:17, 13.18s/it] 72%|███████▏  | 36/50 [07:54<03:08, 13.44s/it] 74%|███████▍  | 37/50 [08:08<02:56, 13.56s/it] 76%|███████▌  | 38/50 [08:22<02:44, 13.68s/it] 78%|███████▊  | 39/50 [08:36<02:32, 13.84s/it] 80%|████████  | 40/50 [08:50<02:16, 13.68s/it][loss] ep 31 it 160 total=7.5829 mle=1.5098 pcon=4.7328 forget=1.3402 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 31 it 210 total=7.8302 mle=1.7620 pcon=4.7320 forget=1.3362 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 31 it 260 total=7.6554 mle=1.5897 pcon=4.7311 forget=1.3346 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 31 it 310 total=7.7433 mle=1.6575 pcon=4.7301 forget=1.3556 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 31 it 360 total=7.5482 mle=1.4815 pcon=4.7292 forget=1.3374 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 32 it 20 total=7.5091 mle=1.4343 pcon=4.7285 forget=1.3463 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 32 it 70 total=7.8873 mle=1.7876 pcon=4.7277 forget=1.3720 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 32 it 120 total=7.6022 mle=1.5222 pcon=4.7270 forget=1.3530 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 32 it 170 total=7.6876 mle=1.6132 pcon=4.7262 forget=1.3482 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 32 it 220 total=7.6992 mle=1.6181 pcon=4.7254 forget=1.3557 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 32 it 270 total=7.4973 mle=1.4035 pcon=4.7248 forget=1.3691 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 32 it 320 total=7.6904 mle=1.6087 pcon=4.7240 forget=1.3577 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 32 it 370 total=7.6601 mle=1.5808 pcon=4.7233 forget=1.3560 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 33 it 30 total=7.6735 mle=1.5958 pcon=4.7226 forget=1.3551 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 33 it 80 total=7.5013 mle=1.4215 pcon=4.7218 forget=1.3580 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 33 it 130 total=7.6113 mle=1.5103 pcon=4.7212 forget=1.3799 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 33 it 180 total=7.7786 mle=1.6953 pcon=4.7204 forget=1.3629 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 33 it 230 total=8.0183 mle=1.9133 pcon=4.7197 forget=1.3853 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 33 it 280 total=7.8969 mle=1.8104 pcon=4.7192 forget=1.3673 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 33 it 330 total=8.1593 mle=2.0710 pcon=4.7186 forget=1.3698 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 33 it 380 total=7.8497 mle=1.7575 pcon=4.7180 forget=1.3743 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 34 it 40 total=7.6352 mle=1.5351 pcon=4.7173 forget=1.3828 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 34 it 90 total=7.5537 mle=1.4615 pcon=4.7166 forget=1.3756 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 34 it 140 total=7.5540 mle=1.4581 pcon=4.7159 forget=1.3799 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 34 it 190 total=7.8088 mle=1.7068 pcon=4.7153 forget=1.3867 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 34 it 240 total=7.8934 mle=1.7987 pcon=4.7147 forget=1.3799 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 34 it 290 total=7.7143 mle=1.6153 pcon=4.7141 forget=1.3849 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 34 it 340 total=7.6052 mle=1.5046 pcon=4.7136 forget=1.3871 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 35 it 0 total=7.6816 mle=1.5725 pcon=4.7129 forget=1.3961 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 35 it 50 total=7.9688 mle=1.8623 pcon=4.7122 forget=1.3943 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 35 it 100 total=7.8372 mle=1.7307 pcon=4.7116 forget=1.3949 favg=0.0000 nr=43 nf=43 protos=570 fproto_sim=NA
[loss] ep 35 it 150 total=7.9218 mle=1.8032 pcon=4.7110 forget=1.4076 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 35 it 200 total=7.5383 mle=1.4300 pcon=4.7104 forget=1.3979 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 35 it 250 total=7.7239 mle=1.6023 pcon=4.7097 forget=1.4118 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 35 it 300 total=7.6309 mle=1.5113 pcon=4.7092 forget=1.4104 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 35 it 350 total=7.7800 mle=1.6722 pcon=4.7087 forget=1.3991 favg=0.0000 nr=21 nf=21 protos=570 fproto_sim=NA
[loss] ep 36 it 10 total=7.9424 mle=1.8237 pcon=4.7082 forget=1.4104 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 36 it 60 total=7.6866 mle=1.5650 pcon=4.7077 forget=1.4139 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 36 it 110 total=7.6650 mle=1.5457 pcon=4.7072 forget=1.4122 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 36 it 160 total=7.7605 mle=1.6387 pcon=4.7067 forget=1.4151 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 36 it 210 total=7.5519 mle=1.4258 pcon=4.7061 forget=1.4200 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 36 it 260 total=7.5462 mle=1.4128 pcon=4.7056 forget=1.4278 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 36 it 310 total=7.7193 mle=1.5904 pcon=4.7050 forget=1.4239 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 36 it 360 total=7.5466 mle=1.4148 pcon=4.7045 forget=1.4273 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 37 it 20 total=7.8272 mle=1.6968 pcon=4.7039 forget=1.4265 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 37 it 70 total=7.8832 mle=1.7518 pcon=4.7034 forget=1.4280 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 37 it 120 total=7.7734 mle=1.6425 pcon=4.7030 forget=1.4279 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 37 it 170 total=7.7613 mle=1.6234 pcon=4.7024 forget=1.4355 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 37 it 220 total=7.6173 mle=1.4733 pcon=4.7019 forget=1.4421 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 37 it 270 total=7.7883 mle=1.6505 pcon=4.7014 forget=1.4365 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 37 it 320 total=8.0611 mle=1.9076 pcon=4.7010 forget=1.4525 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 37 it 370 total=7.6681 mle=1.5218 pcon=4.7005 forget=1.4458 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 38 it 30 total=7.9414 mle=1.7848 pcon=4.7001 forget=1.4565 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 38 it 80 total=7.5830 mle=1.4243 pcon=4.6996 forget=1.4590 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 38 it 130 total=7.5562 mle=1.4012 pcon=4.6992 forget=1.4558 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 38 it 180 total=7.7877 mle=1.6252 pcon=4.6989 forget=1.4636 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 38 it 230 total=7.6378 mle=1.4719 pcon=4.6985 forget=1.4675 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 38 it 280 total=7.9387 mle=1.7791 pcon=4.6980 forget=1.4615 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 38 it 330 total=7.6500 mle=1.4868 pcon=4.6976 forget=1.4656 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 38 it 380 total=7.9441 mle=1.7798 pcon=4.6971 forget=1.4672 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 39 it 40 total=7.9392 mle=1.7647 pcon=4.6968 forget=1.4777 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 39 it 90 total=7.7485 mle=1.5673 pcon=4.6964 forget=1.4848 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 39 it 140 total=7.8483 mle=1.6767 pcon=4.6960 forget=1.4755 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 39 it 190 total=7.7406 mle=1.5466 pcon=4.6957 forget=1.4983 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 39 it 240 total=7.5969 mle=1.4277 pcon=4.6953 forget=1.4739 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 39 it 290 total=8.1176 mle=1.9333 pcon=4.6947 forget=1.4895 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 39 it 340 total=7.9152 mle=1.7287 pcon=4.6945 forget=1.4921 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 40 it 0 total=7.6559 mle=1.4569 pcon=4.6942 forget=1.5048 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 40 it 50 total=7.7115 mle=1.5213 pcon=4.6938 forget=1.4964 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
 82%|████████▏ | 41/50 [09:03<02:02, 13.64s/it] 84%|████████▍ | 42/50 [09:17<01:49, 13.65s/it] 86%|████████▌ | 43/50 [09:31<01:36, 13.76s/it] 88%|████████▊ | 44/50 [09:45<01:22, 13.81s/it] 90%|█████████ | 45/50 [09:59<01:09, 13.84s/it] 92%|█████████▏| 46/50 [10:13<00:55, 13.89s/it] 94%|█████████▍| 47/50 [10:27<00:41, 13.90s/it] 96%|█████████▌| 48/50 [10:40<00:27, 13.79s/it][loss] ep 40 it 100 total=7.6267 mle=1.4404 pcon=4.6933 forget=1.4929 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 40 it 150 total=7.6276 mle=1.4386 pcon=4.6929 forget=1.4961 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 40 it 200 total=8.0723 mle=1.8637 pcon=4.6926 forget=1.5160 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 40 it 250 total=7.8290 mle=1.6211 pcon=4.6923 forget=1.5156 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 40 it 300 total=7.9336 mle=1.7232 pcon=4.6920 forget=1.5184 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 40 it 350 total=7.7808 mle=1.5731 pcon=4.6918 forget=1.5159 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 41 it 10 total=7.7772 mle=1.5732 pcon=4.6915 forget=1.5125 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 41 it 60 total=7.9619 mle=1.7521 pcon=4.6912 forget=1.5186 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 41 it 110 total=7.7137 mle=1.5040 pcon=4.6909 forget=1.5187 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 41 it 160 total=7.8850 mle=1.6653 pcon=4.6907 forget=1.5290 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 41 it 210 total=8.1181 mle=1.9004 pcon=4.6905 forget=1.5272 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 41 it 260 total=7.9287 mle=1.6995 pcon=4.6902 forget=1.5389 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 41 it 310 total=7.7758 mle=1.5600 pcon=4.6899 forget=1.5258 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 41 it 360 total=7.9220 mle=1.6857 pcon=4.6896 forget=1.5467 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 42 it 20 total=7.9060 mle=1.6673 pcon=4.6894 forget=1.5493 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 70 total=7.8738 mle=1.6373 pcon=4.6891 forget=1.5473 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 120 total=7.7920 mle=1.5439 pcon=4.6889 forget=1.5591 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 42 it 170 total=7.6954 mle=1.4694 pcon=4.6886 forget=1.5373 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 220 total=7.7358 mle=1.4956 pcon=4.6883 forget=1.5519 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 42 it 270 total=7.8040 mle=1.5667 pcon=4.6881 forget=1.5492 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 42 it 320 total=7.8869 mle=1.6382 pcon=4.6879 forget=1.5609 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 42 it 370 total=7.7117 mle=1.4630 pcon=4.6876 forget=1.5611 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 43 it 30 total=7.9448 mle=1.6792 pcon=4.6874 forget=1.5782 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 43 it 80 total=7.9588 mle=1.7075 pcon=4.6872 forget=1.5641 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 43 it 130 total=7.9266 mle=1.6717 pcon=4.6869 forget=1.5680 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 43 it 180 total=7.9367 mle=1.6723 pcon=4.6866 forget=1.5778 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 43 it 230 total=7.8107 mle=1.5557 pcon=4.6864 forget=1.5686 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 43 it 280 total=8.0414 mle=1.7795 pcon=4.6863 forget=1.5756 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 43 it 330 total=7.7717 mle=1.5082 pcon=4.6861 forget=1.5773 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 43 it 380 total=7.6985 mle=1.4275 pcon=4.6860 forget=1.5849 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 44 it 40 total=7.8895 mle=1.6159 pcon=4.6858 forget=1.5879 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 44 it 90 total=7.9539 mle=1.6798 pcon=4.6856 forget=1.5885 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 44 it 140 total=7.9249 mle=1.6517 pcon=4.6855 forget=1.5877 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 44 it 190 total=7.7105 mle=1.4300 pcon=4.6853 forget=1.5953 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 44 it 240 total=8.0902 mle=1.8110 pcon=4.6851 forget=1.5941 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 44 it 290 total=8.0490 mle=1.7679 pcon=4.6848 forget=1.5963 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 44 it 340 total=7.7846 mle=1.4935 pcon=4.6847 forget=1.6063 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 45 it 0 total=7.7913 mle=1.5051 pcon=4.6845 forget=1.6016 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 50 total=7.8071 mle=1.5340 pcon=4.6844 forget=1.5887 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 100 total=7.8420 mle=1.5442 pcon=4.6842 forget=1.6136 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 45 it 150 total=8.0111 mle=1.7068 pcon=4.6840 forget=1.6202 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 45 it 200 total=8.0044 mle=1.7040 pcon=4.6840 forget=1.6165 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 45 it 250 total=7.8624 mle=1.5340 pcon=4.6838 forget=1.6446 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 300 total=8.1827 mle=1.8682 pcon=4.6837 forget=1.6308 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 45 it 350 total=7.9040 mle=1.5999 pcon=4.6837 forget=1.6204 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 46 it 10 total=7.7903 mle=1.4857 pcon=4.6835 forget=1.6210 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 46 it 60 total=7.7595 mle=1.4516 pcon=4.6833 forget=1.6246 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 46 it 110 total=7.9524 mle=1.6433 pcon=4.6831 forget=1.6260 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 46 it 160 total=8.0625 mle=1.7531 pcon=4.6829 forget=1.6265 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 46 it 210 total=7.7152 mle=1.4166 pcon=4.6826 forget=1.6160 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 46 it 260 total=8.1359 mle=1.8121 pcon=4.6825 forget=1.6412 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 46 it 310 total=7.9862 mle=1.6574 pcon=4.6824 forget=1.6464 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 46 it 360 total=7.7718 mle=1.4528 pcon=4.6823 forget=1.6366 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 47 it 20 total=8.0968 mle=1.7591 pcon=4.6823 forget=1.6555 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 47 it 70 total=7.6682 mle=1.3450 pcon=4.6821 forget=1.6411 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 47 it 120 total=7.7836 mle=1.4491 pcon=4.6820 forget=1.6525 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 47 it 170 total=7.7874 mle=1.4621 pcon=4.6819 forget=1.6434 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 47 it 220 total=7.8371 mle=1.4983 pcon=4.6819 forget=1.6569 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 47 it 270 total=8.1516 mle=1.8169 pcon=4.6818 forget=1.6529 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 47 it 320 total=8.0962 mle=1.7533 pcon=4.6816 forget=1.6612 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 47 it 370 total=7.7773 mle=1.4245 pcon=4.6814 forget=1.6714 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 30 total=8.1131 mle=1.7756 pcon=4.6812 forget=1.6563 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 48 it 80 total=7.9914 mle=1.6427 pcon=4.6811 forget=1.6677 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 48 it 130 total=8.2495 mle=1.8944 pcon=4.6809 forget=1.6742 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 180 total=7.9796 mle=1.6346 pcon=4.6808 forget=1.6642 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 48 it 230 total=8.0325 mle=1.6723 pcon=4.6807 forget=1.6795 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 48 it 280 total=8.0059 mle=1.6490 pcon=4.6807 forget=1.6761 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 48 it 330 total=7.8513 mle=1.4961 pcon=4.6807 forget=1.6746 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
 98%|█████████▊| 49/50 [10:54<00:13, 13.69s/it]100%|██████████| 50/50 [11:09<00:00, 14.28s/it]100%|██████████| 50/50 [11:09<00:00, 13.39s/it]
[loss] ep 48 it 380 total=7.9602 mle=1.5896 pcon=4.6806 forget=1.6899 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 49 it 40 total=8.1360 mle=1.7632 pcon=4.6806 forget=1.6922 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 49 it 90 total=8.1163 mle=1.7484 pcon=4.6804 forget=1.6875 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 49 it 140 total=7.9541 mle=1.5970 pcon=4.6803 forget=1.6769 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 49 it 190 total=8.0275 mle=1.6592 pcon=4.6802 forget=1.6881 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 49 it 240 total=8.2714 mle=1.8971 pcon=4.6801 forget=1.6942 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 49 it 290 total=7.9768 mle=1.6160 pcon=4.6800 forget=1.6808 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 49 it 340 total=7.9074 mle=1.5342 pcon=4.6800 forget=1.6932 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] loaded adapter 'stage1' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] set_adapter(list) failed: unhashable type: 'list'; try adapter fusion
[peft] adapter fusion failed: adapter fusion API not available or single adapter provided; fallback to last adapter
[peft] active adapter set to: stage1
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1: Number of model parameters: 22082496
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:04,  2.12it/s]  3%|▎         | 11/391 [00:00<00:15, 24.59it/s]  5%|▌         | 21/391 [00:00<00:08, 42.71it/s]  8%|▊         | 31/391 [00:00<00:06, 56.82it/s] 10%|█         | 41/391 [00:00<00:05, 67.45it/s] 13%|█▎        | 51/391 [00:00<00:04, 75.01it/s] 16%|█▌        | 61/391 [00:01<00:04, 80.84it/s] 18%|█▊        | 71/391 [00:01<00:03, 85.27it/s] 21%|██        | 81/391 [00:01<00:03, 88.52it/s] 23%|██▎       | 91/391 [00:01<00:03, 90.73it/s] 26%|██▌       | 101/391 [00:01<00:03, 92.04it/s] 28%|██▊       | 111/391 [00:01<00:03, 93.16it/s] 31%|███       | 121/391 [00:01<00:02, 94.06it/s] 34%|███▎      | 131/391 [00:01<00:02, 94.49it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.07it/s] 39%|███▊      | 151/391 [00:02<00:02, 94.68it/s] 41%|████      | 161/391 [00:02<00:02, 94.69it/s] 44%|████▎     | 171/391 [00:02<00:02, 93.36it/s] 46%|████▋     | 181/391 [00:02<00:02, 93.89it/s] 49%|████▉     | 191/391 [00:02<00:02, 92.18it/s] 51%|█████▏    | 201/391 [00:02<00:02, 93.23it/s] 54%|█████▍    | 211/391 [00:02<00:01, 93.64it/s] 57%|█████▋    | 221/391 [00:02<00:01, 92.62it/s] 59%|█████▉    | 231/391 [00:02<00:01, 93.76it/s] 62%|██████▏   | 241/391 [00:03<00:01, 94.41it/s] 64%|██████▍   | 251/391 [00:03<00:01, 94.73it/s] 67%|██████▋   | 261/391 [00:03<00:01, 95.06it/s] 69%|██████▉   | 271/391 [00:03<00:01, 95.30it/s] 72%|███████▏  | 281/391 [00:03<00:01, 94.97it/s] 74%|███████▍  | 291/391 [00:03<00:01, 94.49it/s] 77%|███████▋  | 301/391 [00:03<00:00, 94.92it/s] 80%|███████▉  | 311/391 [00:03<00:00, 95.04it/s] 82%|████████▏ | 321/391 [00:03<00:00, 95.35it/s] 85%|████████▍ | 331/391 [00:03<00:00, 95.63it/s] 87%|████████▋ | 341/391 [00:04<00:00, 95.09it/s] 90%|████████▉ | 351/391 [00:04<00:00, 94.92it/s] 92%|█████████▏| 361/391 [00:04<00:00, 94.70it/s] 95%|█████████▍| 371/391 [00:04<00:00, 95.47it/s] 97%|█████████▋| 381/391 [00:04<00:00, 96.07it/s]100%|██████████| 391/391 [00:04<00:00, 94.31it/s]100%|██████████| 391/391 [00:04<00:00, 85.26it/s]
50000 images processed, 4.698672771453857 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:38,  2.04it/s] 14%|█▍        | 11/79 [00:00<00:02, 23.67it/s] 27%|██▋       | 21/79 [00:00<00:01, 41.16it/s] 39%|███▉      | 31/79 [00:00<00:00, 54.85it/s] 52%|█████▏    | 41/79 [00:00<00:00, 65.55it/s] 63%|██████▎   | 50/79 [00:01<00:00, 72.01it/s] 76%|███████▌  | 60/79 [00:01<00:00, 79.00it/s] 89%|████████▊ | 70/79 [00:01<00:00, 84.07it/s]100%|██████████| 79/79 [00:01<00:00, 42.12it/s]
10000 images processed, 1.9029030799865723 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:49,  1.85it/s]  5%|▌         | 11/204 [00:00<00:08, 22.07it/s] 10%|█         | 21/204 [00:00<00:04, 38.94it/s] 15%|█▌        | 31/204 [00:00<00:03, 52.41it/s] 20%|██        | 41/204 [00:00<00:02, 63.20it/s] 25%|██▌       | 51/204 [00:01<00:02, 71.02it/s] 30%|██▉       | 61/204 [00:01<00:01, 77.34it/s] 35%|███▍      | 71/204 [00:01<00:01, 81.89it/s] 40%|███▉      | 81/204 [00:01<00:01, 85.33it/s] 45%|████▍     | 91/204 [00:01<00:01, 87.96it/s] 50%|████▉     | 101/204 [00:01<00:01, 89.77it/s] 54%|█████▍    | 111/204 [00:01<00:01, 90.25it/s] 59%|█████▉    | 121/204 [00:01<00:00, 91.19it/s] 64%|██████▍   | 131/204 [00:01<00:00, 92.31it/s] 69%|██████▉   | 141/204 [00:02<00:00, 92.65it/s] 74%|███████▍  | 151/204 [00:02<00:00, 93.20it/s] 79%|███████▉  | 161/204 [00:02<00:00, 93.75it/s] 84%|████████▍ | 171/204 [00:02<00:00, 93.15it/s] 89%|████████▊ | 181/204 [00:02<00:00, 93.97it/s] 94%|█████████▎| 191/204 [00:02<00:00, 94.87it/s] 99%|█████████▊| 201/204 [00:02<00:00, 95.27it/s]100%|██████████| 204/204 [00:02<00:00, 75.16it/s]
26032 images processed, 2.7628118991851807 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:48,  1.62it/s] 11%|█▏        | 9/79 [00:00<00:04, 16.27it/s] 24%|██▍       | 19/79 [00:00<00:01, 33.21it/s] 37%|███▋      | 29/79 [00:00<00:01, 47.51it/s] 49%|████▉     | 39/79 [00:01<00:00, 59.15it/s] 62%|██████▏   | 49/79 [00:01<00:00, 68.69it/s] 75%|███████▍  | 59/79 [00:01<00:00, 75.51it/s] 87%|████████▋ | 69/79 [00:01<00:00, 81.35it/s]100%|██████████| 79/79 [00:01<00:00, 54.20it/s]
10000 images processed, 1.5033049583435059 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:38,  2.05it/s] 13%|█▎        | 10/79 [00:00<00:03, 21.96it/s] 25%|██▌       | 20/79 [00:00<00:01, 40.54it/s] 38%|███▊      | 30/79 [00:00<00:00, 55.02it/s] 51%|█████     | 40/79 [00:00<00:00, 66.05it/s] 63%|██████▎   | 50/79 [00:01<00:00, 74.28it/s] 76%|███████▌  | 60/79 [00:01<00:00, 80.68it/s] 89%|████████▊ | 70/79 [00:01<00:00, 85.42it/s]100%|██████████| 79/79 [00:01<00:00, 60.73it/s]
10000 images processed, 1.3222155570983887 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:34,  1.98it/s] 16%|█▌        | 11/70 [00:00<00:02, 23.22it/s] 30%|███       | 21/70 [00:00<00:01, 40.75it/s] 44%|████▍     | 31/70 [00:00<00:00, 54.65it/s] 59%|█████▊    | 41/70 [00:00<00:00, 65.41it/s] 73%|███████▎  | 51/70 [00:01<00:00, 74.07it/s] 87%|████████▋ | 61/70 [00:01<00:00, 80.49it/s]100%|██████████| 70/70 [00:01<00:00, 56.70it/s]
8925 images processed, 1.2713451385498047 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:41,  1.07it/s]  4%|▍         | 2/45 [00:01<00:25,  1.71it/s] 27%|██▋       | 12/45 [00:01<00:02, 13.98it/s] 40%|████      | 18/45 [00:01<00:01, 15.51it/s] 62%|██████▏   | 28/45 [00:01<00:00, 27.04it/s] 76%|███████▌  | 34/45 [00:02<00:00, 19.68it/s] 96%|█████████▌| 43/45 [00:02<00:00, 28.43it/s]100%|██████████| 45/45 [00:02<00:00, 18.49it/s]
5640 images processed, 2.4567368030548096 seconds used

17.584287881851196
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.50  99.36
places365     67.90  80.57
LSUN          21.54  95.06
iSUN          72.28  81.40
dtd           38.17  91.27
AVG           40.48  89.53
Retain-Acc: 0.7433
Forget-as-OOD (retain known vs forget novel):
  FPR: 84.00 AUROC: 85.57 AUIN: 99.08
8.483344554901123
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1_rf.png
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] loaded adapter 'stage1' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] set_adapter(list) failed: unhashable type: 'list'; try adapter fusion
[peft] adapter fusion failed: adapter fusion API not available or single adapter provided; fallback to last adapter
[peft] active adapter set to: stage1
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-seen: Number of model parameters: 22082496
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:52,  2.26it/s]  3%|▎         | 11/391 [00:00<00:14, 25.79it/s]  5%|▌         | 21/391 [00:00<00:08, 43.76it/s]  8%|▊         | 31/391 [00:00<00:06, 57.83it/s] 10%|█         | 41/391 [00:00<00:05, 68.45it/s] 13%|█▎        | 51/391 [00:00<00:04, 76.24it/s] 16%|█▌        | 61/391 [00:01<00:04, 81.78it/s] 18%|█▊        | 71/391 [00:01<00:03, 85.73it/s] 21%|██        | 81/391 [00:01<00:03, 88.85it/s] 23%|██▎       | 91/391 [00:01<00:03, 90.91it/s] 26%|██▌       | 101/391 [00:01<00:03, 91.03it/s] 28%|██▊       | 111/391 [00:01<00:03, 92.48it/s] 31%|███       | 121/391 [00:01<00:02, 93.47it/s] 34%|███▎      | 131/391 [00:01<00:02, 93.96it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.53it/s] 39%|███▊      | 151/391 [00:02<00:02, 94.83it/s] 41%|████      | 161/391 [00:02<00:02, 95.17it/s] 44%|████▎     | 171/391 [00:02<00:02, 94.26it/s] 46%|████▋     | 181/391 [00:02<00:02, 94.65it/s] 49%|████▉     | 191/391 [00:02<00:02, 95.17it/s] 51%|█████▏    | 201/391 [00:02<00:01, 95.49it/s] 54%|█████▍    | 211/391 [00:02<00:01, 95.51it/s] 57%|█████▋    | 221/391 [00:02<00:01, 95.04it/s] 59%|█████▉    | 231/391 [00:02<00:01, 95.29it/s] 62%|██████▏   | 241/391 [00:02<00:01, 95.43it/s] 64%|██████▍   | 251/391 [00:03<00:01, 94.45it/s] 67%|██████▋   | 261/391 [00:03<00:01, 93.76it/s] 69%|██████▉   | 271/391 [00:03<00:01, 94.45it/s] 72%|███████▏  | 281/391 [00:03<00:01, 94.77it/s] 74%|███████▍  | 291/391 [00:03<00:01, 94.89it/s] 77%|███████▋  | 301/391 [00:03<00:00, 94.48it/s] 80%|███████▉  | 311/391 [00:03<00:00, 94.79it/s] 82%|████████▏ | 321/391 [00:03<00:00, 95.11it/s] 85%|████████▍ | 331/391 [00:03<00:00, 95.16it/s] 87%|████████▋ | 341/391 [00:04<00:00, 95.23it/s] 90%|████████▉ | 351/391 [00:04<00:00, 95.57it/s] 92%|█████████▏| 361/391 [00:04<00:00, 95.25it/s] 95%|█████████▍| 371/391 [00:04<00:00, 95.84it/s] 97%|█████████▋| 381/391 [00:04<00:00, 96.33it/s]100%|██████████| 391/391 [00:04<00:00, 94.52it/s]100%|██████████| 391/391 [00:04<00:00, 85.98it/s]
50000 images processed, 4.619920969009399 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:36,  2.11it/s] 11%|█▏        | 9/79 [00:00<00:03, 20.12it/s] 24%|██▍       | 19/79 [00:00<00:01, 39.49it/s] 37%|███▋      | 29/79 [00:00<00:00, 54.51it/s] 49%|████▉     | 39/79 [00:00<00:00, 65.75it/s] 62%|██████▏   | 49/79 [00:00<00:00, 73.35it/s] 75%|███████▍  | 59/79 [00:01<00:00, 80.01it/s] 87%|████████▋ | 69/79 [00:01<00:00, 84.87it/s]100%|██████████| 79/79 [00:01<00:00, 86.02it/s]100%|██████████| 79/79 [00:01<00:00, 59.93it/s]
10000 images processed, 1.3413326740264893 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:47,  1.89it/s]  5%|▌         | 11/204 [00:00<00:08, 22.37it/s] 10%|█         | 21/204 [00:00<00:04, 39.66it/s] 15%|█▌        | 31/204 [00:00<00:03, 53.67it/s] 20%|██        | 41/204 [00:00<00:02, 64.51it/s] 25%|██▌       | 51/204 [00:01<00:02, 72.87it/s] 30%|██▉       | 61/204 [00:01<00:01, 78.31it/s] 34%|███▍      | 70/204 [00:01<00:01, 80.26it/s] 39%|███▉      | 80/204 [00:01<00:01, 84.01it/s] 44%|████▍     | 90/204 [00:01<00:01, 86.99it/s] 49%|████▉     | 100/204 [00:01<00:01, 89.07it/s] 54%|█████▍    | 110/204 [00:01<00:01, 90.64it/s] 59%|█████▉    | 120/204 [00:01<00:00, 90.92it/s] 64%|██████▎   | 130/204 [00:01<00:00, 90.22it/s] 69%|██████▊   | 140/204 [00:02<00:00, 91.47it/s] 74%|███████▎  | 150/204 [00:02<00:00, 92.27it/s] 78%|███████▊  | 160/204 [00:02<00:00, 92.88it/s] 83%|████████▎ | 170/204 [00:02<00:00, 93.06it/s] 88%|████████▊ | 180/204 [00:02<00:00, 93.70it/s] 93%|█████████▎| 190/204 [00:02<00:00, 94.55it/s] 98%|█████████▊| 200/204 [00:02<00:00, 94.94it/s]100%|██████████| 204/204 [00:02<00:00, 75.27it/s]
26032 images processed, 2.7578611373901367 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:54,  1.43it/s] 13%|█▎        | 10/79 [00:00<00:04, 16.42it/s] 24%|██▍       | 19/79 [00:00<00:01, 30.07it/s] 37%|███▋      | 29/79 [00:01<00:01, 44.44it/s] 49%|████▉     | 39/79 [00:01<00:00, 56.59it/s] 62%|██████▏   | 49/79 [00:01<00:00, 66.33it/s] 75%|███████▍  | 59/79 [00:01<00:00, 74.11it/s] 87%|████████▋ | 69/79 [00:01<00:00, 80.17it/s]100%|██████████| 79/79 [00:01<00:00, 51.48it/s]
10000 images processed, 1.5717172622680664 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:50,  1.56it/s] 14%|█▍        | 11/79 [00:00<00:03, 19.24it/s] 27%|██▋       | 21/79 [00:00<00:01, 35.27it/s] 39%|███▉      | 31/79 [00:00<00:00, 49.08it/s] 52%|█████▏    | 41/79 [00:01<00:00, 60.34it/s] 65%|██████▍   | 51/79 [00:01<00:00, 69.43it/s] 77%|███████▋  | 61/79 [00:01<00:00, 76.55it/s] 90%|████████▉ | 71/79 [00:01<00:00, 82.17it/s]100%|██████████| 79/79 [00:01<00:00, 54.21it/s]
10000 images processed, 1.478560447692871 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:37,  1.84it/s] 14%|█▍        | 10/70 [00:00<00:02, 20.21it/s] 29%|██▊       | 20/70 [00:00<00:01, 38.02it/s] 43%|████▎     | 30/70 [00:00<00:00, 52.53it/s] 57%|█████▋    | 40/70 [00:00<00:00, 63.96it/s] 71%|███████▏  | 50/70 [00:01<00:00, 72.83it/s] 86%|████████▌ | 60/70 [00:01<00:00, 79.37it/s]100%|██████████| 70/70 [00:01<00:00, 83.36it/s]100%|██████████| 70/70 [00:01<00:00, 54.97it/s]
8925 images processed, 1.3021883964538574 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:45,  1.04s/it]  4%|▍         | 2/45 [00:01<00:21,  2.04it/s] 24%|██▍       | 11/45 [00:01<00:02, 14.68it/s] 38%|███▊      | 17/45 [00:01<00:01, 15.01it/s] 51%|█████     | 23/45 [00:01<00:01, 19.33it/s] 73%|███████▎  | 33/45 [00:02<00:00, 17.72it/s] 96%|█████████▌| 43/45 [00:02<00:00, 26.60it/s]100%|██████████| 45/45 [00:02<00:00, 17.54it/s]
5640 images processed, 2.587838888168335 seconds used

17.366228342056274
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.50  99.36
places365     67.90  80.57
LSUN          21.54  95.06
iSUN          72.28  81.40
dtd           38.17  91.27
AVG           40.48  89.53
Retain-Acc: 0.7433
Forget-as-OOD (retain known vs forget novel):
  FPR: 84.00 AUROC: 85.57 AUIN: 99.08
8.791903734207153
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-seen_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-seen_rf.png
==== Stage 2: forget_inc={66,67,88,94,57}; forget_seen={0,8,11,40,51}; all={0,8,11,40,51,66,67,88,94,57} ====
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2', adapter_load_path=None, adapter_load_paths='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1', lora_new_adapter_name='train_s2', lora_stack=True, lora_orth_enable=False, lora_orth_lambda=0.1, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc='66,67,88,94,57', forget_classes_seen='0,8,11,40,51', forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] loaded adapter 'stage1' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] added trainable adapter 'train_s2'
[peft] set_adapter(list) failed: unhashable type: 'list'; try adapter fusion
[peft] adapter fusion failed: adapter fusion API not available or single adapter provided; fallback to last adapter
[peft] active adapter set to: train_s2
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 22321088
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.train_s2.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.train_s2.weight
[debug] trainable: base_model.model.head.2.lora_A.train_s2.weight
[debug] trainable: base_model.model.head.2.lora_B.train_s2.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.train_s2.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.train_s2.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.train_s2.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.train_s2.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.train_s2.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.train_s2.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.train_s2.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.train_s2.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.train_s2.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.train_s2.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [01:16<1:02:18, 76.29s/it]  4%|▍         | 2/50 [01:30<31:58, 39.98s/it]    6%|▌         | 3/50 [01:44<21:44, 27.75s/it]  8%|▊         | 4/50 [01:58<17:12, 22.45s/it] 10%|█         | 5/50 [02:12<14:29, 19.32s/it] 12%|█▏        | 6/50 [02:24<12:27, 16.98s/it] 14%|█▍        | 7/50 [02:37<11:06, 15.50s/it][loss] ep 0 it 0 total=8.8252 mle=2.0445 pcon=5.2951 forget=1.4856 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=8.7391 mle=1.9720 pcon=5.2900 forget=1.4771 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=8.5840 mle=1.8877 pcon=5.2851 forget=1.4111 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8.3386 mle=1.6112 pcon=5.2801 forget=1.4473 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=8.6982 mle=2.0166 pcon=5.2751 forget=1.4065 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=8.7875 mle=2.1330 pcon=5.2700 forget=1.3845 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=8.6989 mle=2.0092 pcon=5.2651 forget=1.4246 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=8.4771 mle=1.8043 pcon=5.2604 forget=1.4124 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 1 it 10 total=8.3174 mle=1.6167 pcon=5.2557 forget=1.4449 favg=0.0000 nr=23 nf=23 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=8.4686 mle=1.7964 pcon=5.2509 forget=1.4212 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=8.7829 mle=2.1157 pcon=5.2464 forget=1.4208 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=8.6090 mle=1.9453 pcon=5.2417 forget=1.4220 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=8.7176 mle=2.0460 pcon=5.2366 forget=1.4349 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=8.6942 mle=2.0023 pcon=5.2320 forget=1.4599 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=8.6224 mle=1.9420 pcon=5.2276 forget=1.4528 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=8.5366 mle=1.8567 pcon=5.2232 forget=1.4568 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 2 it 20 total=8.5951 mle=1.9605 pcon=5.2189 forget=1.4157 favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=8.7811 mle=2.1409 pcon=5.2149 forget=1.4253 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=8.9545 mle=2.2976 pcon=5.2104 forget=1.4465 favg=0.0000 nr=24 nf=24 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=8.5230 mle=1.8590 pcon=5.2062 forget=1.4579 favg=0.0000 nr=40 nf=40 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=8.5367 mle=1.9154 pcon=5.2021 forget=1.4193 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=8.5853 mle=1.9320 pcon=5.1978 forget=1.4555 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=9.1996 mle=2.5641 pcon=5.1937 forget=1.4418 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=8.7217 mle=2.0914 pcon=5.1896 forget=1.4407 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 3 it 30 total=8.6140 mle=1.9929 pcon=5.1856 forget=1.4356 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=8.1751 mle=1.6187 pcon=5.1816 forget=1.3748 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=8.4192 mle=1.8068 pcon=5.1776 forget=1.4348 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=8.6425 mle=2.0225 pcon=5.1741 forget=1.4459 favg=0.0000 nr=23 nf=23 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=8.3136 mle=1.7307 pcon=5.1701 forget=1.4128 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=8.7761 mle=2.2010 pcon=5.1665 forget=1.4087 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=8.8235 mle=2.2113 pcon=5.1626 forget=1.4496 favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=8.3460 mle=1.7742 pcon=5.1591 forget=1.4128 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 4 it 40 total=8.4053 mle=1.8550 pcon=5.1554 forget=1.3949 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=8.8690 mle=2.3296 pcon=5.1517 forget=1.3877 favg=0.0000 nr=23 nf=23 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8.5761 mle=2.0170 pcon=5.1485 forget=1.4106 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=8.4400 mle=1.8950 pcon=5.1450 forget=1.4001 favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=8.2652 mle=1.7160 pcon=5.1418 forget=1.4074 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=8.3527 mle=1.8378 pcon=5.1384 forget=1.3764 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=8.6405 mle=2.1137 pcon=5.1351 forget=1.3917 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 5 it 0 total=8.8709 mle=2.3507 pcon=5.1317 forget=1.3885 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=8.2620 mle=1.7740 pcon=5.1286 forget=1.3594 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=8.4720 mle=1.9869 pcon=5.1253 forget=1.3597 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=8.2884 mle=1.7748 pcon=5.1221 forget=1.3915 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=8.2822 mle=1.8026 pcon=5.1186 forget=1.3610 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=8.2696 mle=1.7980 pcon=5.1156 forget=1.3560 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=8.3261 mle=1.8384 pcon=5.1125 forget=1.3752 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=8.6637 mle=2.1635 pcon=5.1093 forget=1.3909 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 6 it 10 total=8.5575 mle=2.0723 pcon=5.1061 forget=1.3791 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=8.6878 mle=2.2196 pcon=5.1034 forget=1.3648 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=8.5142 mle=2.0262 pcon=5.1005 forget=1.3875 favg=0.0000 nr=24 nf=24 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=8.3019 mle=1.8148 pcon=5.0977 forget=1.3895 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=8.0843 mle=1.6131 pcon=5.0946 forget=1.3765 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=8.2385 mle=1.7231 pcon=5.0919 forget=1.4235 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=8.0457 mle=1.5390 pcon=5.0889 forget=1.4177 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=8.3518 mle=1.8833 pcon=5.0860 forget=1.3824 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 7 it 20 total=8.0404 mle=1.5753 pcon=5.0829 forget=1.3822 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=8.6273 mle=2.1826 pcon=5.0802 forget=1.3645 favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=9.0060 mle=2.5300 pcon=5.0776 forget=1.3985 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=8.6454 mle=2.1832 pcon=5.0748 forget=1.3874 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
 16%|█▌        | 8/50 [02:49<10:13, 14.60s/it] 18%|█▊        | 9/50 [03:02<09:29, 13.89s/it] 20%|██        | 10/50 [03:14<09:02, 13.55s/it] 22%|██▏       | 11/50 [03:27<08:36, 13.23s/it] 24%|██▍       | 12/50 [03:42<08:42, 13.76s/it] 26%|██▌       | 13/50 [03:55<08:19, 13.49s/it] 28%|██▊       | 14/50 [04:07<07:55, 13.21s/it] 30%|███       | 15/50 [04:16<06:57, 11.93s/it][loss] ep 7 it 220 total=8.4762 mle=1.9962 pcon=5.0724 forget=1.4076 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=8.5753 mle=2.1248 pcon=5.0695 forget=1.3809 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=8.1340 mle=1.7142 pcon=5.0672 forget=1.3526 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=8.2859 mle=1.8452 pcon=5.0642 forget=1.3765 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 8 it 30 total=8.6987 mle=2.2652 pcon=5.0619 forget=1.3716 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=8.2951 mle=1.7984 pcon=5.0594 forget=1.4372 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=8.1343 mle=1.6848 pcon=5.0568 forget=1.3926 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=8.3182 mle=1.9006 pcon=5.0544 forget=1.3632 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=8.0156 mle=1.6054 pcon=5.0518 forget=1.3584 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=8.5446 mle=2.1024 pcon=5.0491 forget=1.3931 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=8.4426 mle=2.0086 pcon=5.0463 forget=1.3877 favg=0.0000 nr=41 nf=41 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=8.2139 mle=1.8230 pcon=5.0438 forget=1.3470 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 9 it 40 total=8.5505 mle=2.1851 pcon=5.0410 forget=1.3244 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=8.4850 mle=2.0705 pcon=5.0385 forget=1.3760 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=8.4069 mle=2.0401 pcon=5.0361 forget=1.3307 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=8.2155 mle=1.8585 pcon=5.0338 forget=1.3232 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=8.3323 mle=1.9657 pcon=5.0312 forget=1.3354 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=8.2486 mle=1.8744 pcon=5.0286 forget=1.3456 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=8.2049 mle=1.8122 pcon=5.0264 forget=1.3662 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 10 it 0 total=8.4440 mle=2.0396 pcon=5.0238 forget=1.3806 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=8.3129 mle=1.9278 pcon=5.0215 forget=1.3636 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=8.4070 mle=2.0319 pcon=5.0193 forget=1.3558 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=8.4889 mle=2.0856 pcon=5.0168 forget=1.3865 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=8.4768 mle=2.1211 pcon=5.0145 forget=1.3412 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=8.0294 mle=1.6778 pcon=5.0121 forget=1.3395 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=8.2408 mle=1.8845 pcon=5.0101 forget=1.3462 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=8.0880 mle=1.7250 pcon=5.0080 forget=1.3551 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 11 it 10 total=8.2079 mle=1.8137 pcon=5.0055 forget=1.3887 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=7.9589 mle=1.6047 pcon=5.0034 forget=1.3508 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=8.2959 mle=1.9154 pcon=5.0011 forget=1.3795 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=8.4139 mle=2.0395 pcon=4.9989 forget=1.3754 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=7.9801 mle=1.5997 pcon=4.9969 forget=1.3835 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=8.1247 mle=1.7596 pcon=4.9951 forget=1.3700 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=8.1441 mle=1.7623 pcon=4.9929 forget=1.3889 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=8.1595 mle=1.7730 pcon=4.9908 forget=1.3958 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 12 it 20 total=8.3222 mle=1.8811 pcon=4.9887 forget=1.4524 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=8.0719 mle=1.6890 pcon=4.9865 forget=1.3963 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=8.1698 mle=1.7814 pcon=4.9842 forget=1.4042 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=8.5471 mle=2.1624 pcon=4.9816 forget=1.4031 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=8.4201 mle=2.0435 pcon=4.9791 forget=1.3975 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=8.0517 mle=1.6572 pcon=4.9766 forget=1.4179 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=8.1499 mle=1.7604 pcon=4.9743 forget=1.4152 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=8.2381 mle=1.8413 pcon=4.9716 forget=1.4251 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 13 it 30 total=8.1318 mle=1.7198 pcon=4.9692 forget=1.4428 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=7.9962 mle=1.5975 pcon=4.9668 forget=1.4319 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=8.4146 mle=2.0167 pcon=4.9642 forget=1.4337 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=8.0629 mle=1.6578 pcon=4.9617 forget=1.4434 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=7.9982 mle=1.6162 pcon=4.9591 forget=1.4229 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=8.2880 mle=1.8835 pcon=4.9565 forget=1.4481 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=8.2129 mle=1.7655 pcon=4.9538 forget=1.4936 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=8.1168 mle=1.7396 pcon=4.9511 forget=1.4261 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 14 it 40 total=8.0373 mle=1.6241 pcon=4.9485 forget=1.4647 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=8.4846 mle=2.1268 pcon=4.9461 forget=1.4117 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=8.2798 mle=1.9412 pcon=4.9434 forget=1.3952 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=8.0126 mle=1.6535 pcon=4.9409 forget=1.4182 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=8.1416 mle=1.7879 pcon=4.9384 forget=1.4153 favg=0.0000 nr=23 nf=23 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=8.1216 mle=1.8169 pcon=4.9358 forget=1.3689 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=8.1056 mle=1.8164 pcon=4.9333 forget=1.3559 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 15 it 0 total=7.9254 mle=1.6398 pcon=4.9310 forget=1.3546 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
 32%|███▏      | 16/50 [04:27<06:36, 11.66s/it] 34%|███▍      | 17/50 [04:36<05:52, 10.68s/it] 36%|███▌      | 18/50 [04:45<05:25, 10.17s/it] 38%|███▊      | 19/50 [04:53<05:01,  9.72s/it] 40%|████      | 20/50 [05:02<04:41,  9.39s/it] 42%|████▏     | 21/50 [05:11<04:25,  9.17s/it] 44%|████▍     | 22/50 [05:23<04:44, 10.17s/it][loss] ep 15 it 50 total=8.1690 mle=1.9091 pcon=4.9285 forget=1.3314 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=7.9983 mle=1.7504 pcon=4.9264 forget=1.3216 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=8.2481 mle=1.9892 pcon=4.9242 forget=1.3347 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=7.9036 mle=1.7352 pcon=4.9219 forget=1.2466 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=8.0003 mle=1.8218 pcon=4.9197 forget=1.2587 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=7.8869 mle=1.7477 pcon=4.9177 forget=1.2215 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=7.7145 mle=1.6543 pcon=4.9159 forget=1.1443 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 16 it 10 total=7.7209 mle=1.6692 pcon=4.9139 forget=1.1377 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=7.6327 mle=1.6209 pcon=4.9120 forget=1.0998 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=7.6439 mle=1.6510 pcon=4.9101 forget=1.0827 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=7.8945 mle=1.9097 pcon=4.9079 forget=1.0769 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=7.9585 mle=1.9662 pcon=4.9060 forget=1.0863 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=7.7042 mle=1.7520 pcon=4.9042 forget=1.0480 favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=7.5761 mle=1.6538 pcon=4.9023 forget=1.0200 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=7.4716 mle=1.5761 pcon=4.9004 forget=0.9950 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 17 it 20 total=7.8933 mle=1.9707 pcon=4.8988 forget=1.0239 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=7.8889 mle=1.9823 pcon=4.8969 forget=1.0097 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=7.6483 mle=1.7564 pcon=4.8951 forget=0.9968 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=7.4541 mle=1.5778 pcon=4.8932 forget=0.9831 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=7.7756 mle=1.9034 pcon=4.8913 forget=0.9810 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=7.6172 mle=1.7709 pcon=4.8894 forget=0.9569 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=7.5737 mle=1.7448 pcon=4.8875 forget=0.9414 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=7.8444 mle=2.0084 pcon=4.8855 forget=0.9506 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 18 it 30 total=7.4614 mle=1.5946 pcon=4.8836 forget=0.9832 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=7.5021 mle=1.6537 pcon=4.8817 forget=0.9667 favg=0.0000 nr=41 nf=41 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=7.4633 mle=1.6274 pcon=4.8799 forget=0.9560 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=7.6960 mle=1.8883 pcon=4.8780 forget=0.9297 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=7.4587 mle=1.6534 pcon=4.8761 forget=0.9291 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=7.4714 mle=1.6521 pcon=4.8742 forget=0.9451 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=7.4459 mle=1.6376 pcon=4.8723 forget=0.9359 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=7.2962 mle=1.4859 pcon=4.8704 forget=0.9399 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 19 it 40 total=7.2701 mle=1.4655 pcon=4.8684 forget=0.9362 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=7.3989 mle=1.5963 pcon=4.8665 forget=0.9361 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=7.4036 mle=1.6143 pcon=4.8647 forget=0.9246 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=7.3826 mle=1.6055 pcon=4.8628 forget=0.9144 favg=0.0000 nr=24 nf=24 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=7.3214 mle=1.5269 pcon=4.8609 forget=0.9335 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=7.3277 mle=1.5534 pcon=4.8589 forget=0.9154 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=7.3451 mle=1.5673 pcon=4.8569 forget=0.9209 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 20 it 0 total=7.3117 mle=1.5232 pcon=4.8550 forget=0.9335 favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=7.4484 mle=1.6692 pcon=4.8529 forget=0.9263 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=7.6125 mle=1.8382 pcon=4.8510 forget=0.9233 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=7.3295 mle=1.5598 pcon=4.8489 forget=0.9207 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=7.2623 mle=1.4681 pcon=4.8470 forget=0.9473 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=7.5051 mle=1.7459 pcon=4.8450 forget=0.9142 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=7.3182 mle=1.5459 pcon=4.8429 forget=0.9294 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=7.3791 mle=1.6157 pcon=4.8409 forget=0.9225 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[loss] ep 21 it 10 total=7.4122 mle=1.6387 pcon=4.8388 forget=0.9347 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=7.5337 mle=1.7576 pcon=4.8367 forget=0.9394 favg=0.0000 nr=40 nf=40 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=7.3611 mle=1.6024 pcon=4.8347 forget=0.9240 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=7.5941 mle=1.8235 pcon=4.8328 forget=0.9377 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=7.5296 mle=1.7420 pcon=4.8308 forget=0.9567 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=7.4803 mle=1.7138 pcon=4.8288 forget=0.9377 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=7.3892 mle=1.6218 pcon=4.8267 forget=0.9407 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=7.4050 mle=1.6338 pcon=4.8247 forget=0.9465 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 22 it 20 total=7.3977 mle=1.6203 pcon=4.8227 forget=0.9546 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=7.4783 mle=1.7133 pcon=4.8208 forget=0.9443 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=7.5299 mle=1.7517 pcon=4.8187 forget=0.9595 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=7.4322 mle=1.6493 pcon=4.8167 forget=0.9662 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=7.6067 mle=1.8332 pcon=4.8147 forget=0.9588 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=7.5818 mle=1.7974 pcon=4.8127 forget=0.9718 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
 46%|████▌     | 23/50 [05:33<04:34, 10.17s/it] 48%|████▊     | 24/50 [05:43<04:22, 10.09s/it] 50%|█████     | 25/50 [05:53<04:12, 10.09s/it] 52%|█████▏    | 26/50 [06:06<04:19, 10.82s/it] 54%|█████▍    | 27/50 [06:16<04:04, 10.63s/it] 56%|█████▌    | 28/50 [06:26<03:50, 10.48s/it] 58%|█████▊    | 29/50 [06:36<03:36, 10.29s/it] 60%|██████    | 30/50 [06:46<03:24, 10.24s/it] 62%|██████▏   | 31/50 [06:56<03:13, 10.16s/it][loss] ep 22 it 320 total=7.3443 mle=1.5691 pcon=4.8106 forget=0.9646 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=7.3980 mle=1.6033 pcon=4.8088 forget=0.9859 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 23 it 30 total=7.4232 mle=1.6418 pcon=4.8069 forget=0.9745 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=7.4315 mle=1.6424 pcon=4.8051 forget=0.9839 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=7.4818 mle=1.6967 pcon=4.8031 forget=0.9820 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=7.4299 mle=1.6080 pcon=4.8014 forget=1.0205 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=7.4013 mle=1.6164 pcon=4.7996 forget=0.9853 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=7.5136 mle=1.7234 pcon=4.7978 forget=0.9924 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=7.3866 mle=1.5993 pcon=4.7960 forget=0.9913 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=7.4416 mle=1.6547 pcon=4.7942 forget=0.9926 favg=0.0000 nr=43 nf=43 protos=540 fproto_sim=NA
[loss] ep 24 it 40 total=7.3749 mle=1.5765 pcon=4.7925 forget=1.0058 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=7.7851 mle=1.9919 pcon=4.7907 forget=1.0025 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=7.4667 mle=1.6512 pcon=4.7890 forget=1.0266 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=7.6167 mle=1.8190 pcon=4.7872 forget=1.0105 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 24 it 240 total=7.4502 mle=1.6575 pcon=4.7855 forget=1.0073 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=7.5405 mle=1.7379 pcon=4.7836 forget=1.0189 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=7.3160 mle=1.5100 pcon=4.7819 forget=1.0241 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 25 it 0 total=7.3650 mle=1.5530 pcon=4.7802 forget=1.0318 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=8.0441 mle=2.2184 pcon=4.7785 forget=1.0473 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=7.5595 mle=1.7378 pcon=4.7769 forget=1.0448 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 25 it 150 total=7.4416 mle=1.6230 pcon=4.7752 forget=1.0434 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 25 it 200 total=7.5961 mle=1.7574 pcon=4.7734 forget=1.0653 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=7.2563 mle=1.4329 pcon=4.7716 forget=1.0517 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=7.3989 mle=1.5682 pcon=4.7700 forget=1.0607 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=7.4796 mle=1.6579 pcon=4.7682 forget=1.0534 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 26 it 10 total=7.3375 mle=1.5130 pcon=4.7667 forget=1.0578 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=7.5767 mle=1.7392 pcon=4.7653 forget=1.0722 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=7.4463 mle=1.6138 pcon=4.7636 forget=1.0688 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=7.5042 mle=1.6703 pcon=4.7620 forget=1.0718 favg=0.0000 nr=23 nf=23 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=7.4879 mle=1.6522 pcon=4.7604 forget=1.0753 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=7.3584 mle=1.5144 pcon=4.7588 forget=1.0851 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 26 it 310 total=7.9765 mle=2.1285 pcon=4.7572 forget=1.0909 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=7.4656 mle=1.6204 pcon=4.7558 forget=1.0894 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 27 it 20 total=7.4644 mle=1.6247 pcon=4.7543 forget=1.0855 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=7.3869 mle=1.5355 pcon=4.7528 forget=1.0986 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=7.4918 mle=1.6053 pcon=4.7514 forget=1.1351 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=7.5725 mle=1.7160 pcon=4.7499 forget=1.1066 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=7.5762 mle=1.7070 pcon=4.7485 forget=1.1207 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=7.3772 mle=1.5133 pcon=4.7471 forget=1.1168 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=7.5297 mle=1.6712 pcon=4.7457 forget=1.1128 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=7.4010 mle=1.5402 pcon=4.7441 forget=1.1168 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 28 it 30 total=7.3983 mle=1.5327 pcon=4.7428 forget=1.1227 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=7.5884 mle=1.7090 pcon=4.7415 forget=1.1380 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=7.8182 mle=1.9461 pcon=4.7401 forget=1.1319 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=7.7640 mle=1.8869 pcon=4.7389 forget=1.1383 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=7.5789 mle=1.6995 pcon=4.7375 forget=1.1418 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=7.5127 mle=1.6317 pcon=4.7363 forget=1.1447 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=7.8170 mle=1.9174 pcon=4.7348 forget=1.1648 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=7.5681 mle=1.6751 pcon=4.7336 forget=1.1594 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 29 it 40 total=7.5486 mle=1.6566 pcon=4.7322 forget=1.1598 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=7.5165 mle=1.6283 pcon=4.7309 forget=1.1573 favg=0.0000 nr=40 nf=40 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=7.5691 mle=1.6495 pcon=4.7297 forget=1.1898 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=7.7639 mle=1.8534 pcon=4.7284 forget=1.1820 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=7.8489 mle=1.9529 pcon=4.7272 forget=1.1688 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=7.4838 mle=1.5796 pcon=4.7259 forget=1.1783 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=7.5854 mle=1.6902 pcon=4.7247 forget=1.1704 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 30 it 0 total=7.5661 mle=1.6638 pcon=4.7236 forget=1.1787 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=7.3978 mle=1.4895 pcon=4.7224 forget=1.1859 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=7.6516 mle=1.7385 pcon=4.7213 forget=1.1919 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=7.5835 mle=1.6787 pcon=4.7201 forget=1.1848 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=7.7504 mle=1.8319 pcon=4.7190 forget=1.1996 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=7.5201 mle=1.6136 pcon=4.7178 forget=1.1886 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=7.3387 mle=1.4243 pcon=4.7166 forget=1.1977 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=7.8801 mle=1.9560 pcon=4.7155 forget=1.2085 favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 31 it 10 total=7.6582 mle=1.7228 pcon=4.7144 forget=1.2210 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=7.6728 mle=1.7487 pcon=4.7134 forget=1.2107 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=7.7653 mle=1.8071 pcon=4.7124 forget=1.2458 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=7.5731 mle=1.6552 pcon=4.7114 forget=1.2066 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=7.6919 mle=1.7565 pcon=4.7104 forget=1.2250 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
 64%|██████▍   | 32/50 [07:06<03:02, 10.13s/it] 66%|██████▌   | 33/50 [07:16<02:52, 10.16s/it] 68%|██████▊   | 34/50 [07:27<02:43, 10.19s/it] 70%|███████   | 35/50 [07:37<02:33, 10.21s/it] 72%|███████▏  | 36/50 [07:47<02:22, 10.19s/it] 74%|███████▍  | 37/50 [07:57<02:13, 10.25s/it] 76%|███████▌  | 38/50 [08:07<02:02, 10.19s/it] 78%|███████▊  | 39/50 [08:18<01:52, 10.20s/it] 80%|████████  | 40/50 [08:28<01:41, 10.14s/it][loss] ep 31 it 260 total=7.5893 mle=1.6615 pcon=4.7094 forget=1.2184 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=7.5748 mle=1.6476 pcon=4.7084 forget=1.2189 favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=7.4294 mle=1.4856 pcon=4.7074 forget=1.2364 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 32 it 20 total=7.4984 mle=1.5699 pcon=4.7065 forget=1.2221 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=7.6764 mle=1.7388 pcon=4.7056 forget=1.2320 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=7.5483 mle=1.6073 pcon=4.7045 forget=1.2365 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 32 it 170 total=7.7273 mle=1.7759 pcon=4.7037 forget=1.2477 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=7.6425 mle=1.6971 pcon=4.7027 forget=1.2427 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=7.4951 mle=1.5486 pcon=4.7018 forget=1.2446 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=7.6021 mle=1.6634 pcon=4.7009 forget=1.2378 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=7.5341 mle=1.5914 pcon=4.7000 forget=1.2427 favg=0.0000 nr=41 nf=41 protos=540 fproto_sim=NA
[loss] ep 33 it 30 total=7.4284 mle=1.4822 pcon=4.6992 forget=1.2470 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=7.7080 mle=1.7585 pcon=4.6983 forget=1.2512 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=7.5631 mle=1.6180 pcon=4.6975 forget=1.2475 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=7.6374 mle=1.6872 pcon=4.6967 forget=1.2535 favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=7.6244 mle=1.6751 pcon=4.6960 forget=1.2534 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=7.7651 mle=1.8045 pcon=4.6950 forget=1.2656 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=7.6180 mle=1.6702 pcon=4.6941 forget=1.2537 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 33 it 380 total=7.6076 mle=1.6510 pcon=4.6934 forget=1.2632 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 34 it 40 total=7.6663 mle=1.6876 pcon=4.6925 forget=1.2862 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=7.6139 mle=1.6622 pcon=4.6917 forget=1.2599 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 34 it 140 total=7.6842 mle=1.7225 pcon=4.6910 forget=1.2707 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=7.7086 mle=1.7523 pcon=4.6903 forget=1.2660 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 34 it 240 total=7.6982 mle=1.7311 pcon=4.6895 forget=1.2776 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=8.1024 mle=2.1256 pcon=4.6888 forget=1.2880 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=7.5457 mle=1.5711 pcon=4.6882 forget=1.2864 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 35 it 0 total=7.7273 mle=1.7674 pcon=4.6875 forget=1.2724 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 35 it 50 total=7.6718 mle=1.6788 pcon=4.6868 forget=1.3061 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 35 it 100 total=7.9107 mle=1.9476 pcon=4.6861 forget=1.2770 favg=0.0000 nr=21 nf=21 protos=540 fproto_sim=NA
[loss] ep 35 it 150 total=7.7425 mle=1.7615 pcon=4.6854 forget=1.2956 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 35 it 200 total=7.7815 mle=1.8204 pcon=4.6847 forget=1.2764 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 35 it 250 total=7.8367 mle=1.8715 pcon=4.6839 forget=1.2812 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 35 it 300 total=7.5582 mle=1.5870 pcon=4.6833 forget=1.2879 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 35 it 350 total=7.6375 mle=1.6571 pcon=4.6826 forget=1.2978 favg=0.0000 nr=43 nf=43 protos=540 fproto_sim=NA
[loss] ep 36 it 10 total=7.9389 mle=1.9639 pcon=4.6819 forget=1.2932 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 36 it 60 total=7.7767 mle=1.7848 pcon=4.6813 forget=1.3106 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 36 it 110 total=7.5760 mle=1.5982 pcon=4.6807 forget=1.2971 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 36 it 160 total=7.7888 mle=1.7977 pcon=4.6801 forget=1.3110 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 36 it 210 total=7.5332 mle=1.5529 pcon=4.6794 forget=1.3008 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 36 it 260 total=7.6123 mle=1.6387 pcon=4.6787 forget=1.2949 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 36 it 310 total=7.5769 mle=1.5935 pcon=4.6782 forget=1.3051 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 36 it 360 total=7.5455 mle=1.5635 pcon=4.6776 forget=1.3044 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 37 it 20 total=7.5813 mle=1.5954 pcon=4.6770 forget=1.3089 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 37 it 70 total=7.5620 mle=1.5670 pcon=4.6764 forget=1.3186 favg=0.0000 nr=24 nf=24 protos=540 fproto_sim=NA
[loss] ep 37 it 120 total=7.5703 mle=1.5876 pcon=4.6759 forget=1.3069 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 37 it 170 total=7.7793 mle=1.7757 pcon=4.6753 forget=1.3282 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 37 it 220 total=7.5674 mle=1.5832 pcon=4.6747 forget=1.3095 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 37 it 270 total=7.5367 mle=1.5466 pcon=4.6741 forget=1.3160 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 37 it 320 total=7.4383 mle=1.4494 pcon=4.6736 forget=1.3153 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 37 it 370 total=7.8529 mle=1.8560 pcon=4.6731 forget=1.3239 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 38 it 30 total=7.7377 mle=1.7494 pcon=4.6726 forget=1.3156 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 38 it 80 total=7.6690 mle=1.6681 pcon=4.6720 forget=1.3289 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 38 it 130 total=7.7393 mle=1.7434 pcon=4.6715 forget=1.3244 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 38 it 180 total=7.7397 mle=1.7449 pcon=4.6709 forget=1.3239 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 38 it 230 total=7.8023 mle=1.8025 pcon=4.6703 forget=1.3294 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 38 it 280 total=7.7708 mle=1.7624 pcon=4.6699 forget=1.3385 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 38 it 330 total=7.5913 mle=1.5942 pcon=4.6693 forget=1.3277 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 38 it 380 total=7.6613 mle=1.6310 pcon=4.6688 forget=1.3615 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 39 it 40 total=7.5261 mle=1.5237 pcon=4.6683 forget=1.3340 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 39 it 90 total=7.5857 mle=1.5867 pcon=4.6678 forget=1.3311 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 39 it 140 total=7.7118 mle=1.6845 pcon=4.6673 forget=1.3600 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 39 it 190 total=7.6243 mle=1.6077 pcon=4.6668 forget=1.3498 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 39 it 240 total=7.6571 mle=1.6539 pcon=4.6663 forget=1.3369 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 39 it 290 total=7.6398 mle=1.6290 pcon=4.6658 forget=1.3450 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 39 it 340 total=7.6475 mle=1.6292 pcon=4.6654 forget=1.3529 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 40 it 0 total=7.7614 mle=1.7450 pcon=4.6650 forget=1.3514 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 40 it 50 total=7.5107 mle=1.4713 pcon=4.6647 forget=1.3747 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 40 it 100 total=7.5896 mle=1.5745 pcon=4.6642 forget=1.3509 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 40 it 150 total=7.7285 mle=1.7076 pcon=4.6638 forget=1.3571 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
 82%|████████▏ | 41/50 [08:38<01:31, 10.15s/it] 84%|████████▍ | 42/50 [08:48<01:21, 10.18s/it] 86%|████████▌ | 43/50 [08:58<01:10, 10.12s/it] 88%|████████▊ | 44/50 [09:08<01:00, 10.15s/it] 90%|█████████ | 45/50 [09:18<00:50, 10.15s/it] 92%|█████████▏| 46/50 [09:29<00:40, 10.22s/it] 94%|█████████▍| 47/50 [09:39<00:30, 10.14s/it] 96%|█████████▌| 48/50 [09:49<00:20, 10.18s/it] 98%|█████████▊| 49/50 [09:59<00:10, 10.26s/it][loss] ep 40 it 200 total=7.7560 mle=1.7359 pcon=4.6633 forget=1.3568 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 40 it 250 total=7.6432 mle=1.6164 pcon=4.6629 forget=1.3640 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 40 it 300 total=7.6183 mle=1.5778 pcon=4.6625 forget=1.3780 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 40 it 350 total=7.7292 mle=1.7020 pcon=4.6621 forget=1.3651 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 41 it 10 total=7.6643 mle=1.6328 pcon=4.6618 forget=1.3697 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 41 it 60 total=7.8738 mle=1.8348 pcon=4.6614 forget=1.3776 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 41 it 110 total=7.5989 mle=1.5566 pcon=4.6610 forget=1.3813 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 41 it 160 total=7.5098 mle=1.4775 pcon=4.6606 forget=1.3717 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 41 it 210 total=7.5707 mle=1.5401 pcon=4.6603 forget=1.3703 favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 41 it 260 total=7.7189 mle=1.6833 pcon=4.6599 forget=1.3757 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 41 it 310 total=7.6463 mle=1.6120 pcon=4.6596 forget=1.3747 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 41 it 360 total=7.6452 mle=1.6143 pcon=4.6592 forget=1.3716 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 42 it 20 total=7.9016 mle=1.8630 pcon=4.6589 forget=1.3797 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 42 it 70 total=7.6636 mle=1.6175 pcon=4.6586 forget=1.3876 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 42 it 120 total=7.7079 mle=1.6648 pcon=4.6583 forget=1.3848 favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 42 it 170 total=7.8236 mle=1.7708 pcon=4.6579 forget=1.3949 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 42 it 220 total=7.7576 mle=1.7152 pcon=4.6577 forget=1.3847 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 42 it 270 total=8.0370 mle=1.9777 pcon=4.6574 forget=1.4020 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 42 it 320 total=7.6846 mle=1.6332 pcon=4.6570 forget=1.3945 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 42 it 370 total=7.8041 mle=1.7569 pcon=4.6568 forget=1.3904 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 43 it 30 total=7.6471 mle=1.5955 pcon=4.6565 forget=1.3951 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 43 it 80 total=7.7649 mle=1.7139 pcon=4.6562 forget=1.3948 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 43 it 130 total=7.7287 mle=1.6683 pcon=4.6559 forget=1.4045 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 43 it 180 total=7.8894 mle=1.8240 pcon=4.6558 forget=1.4096 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 43 it 230 total=7.7717 mle=1.7120 pcon=4.6554 forget=1.4044 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 43 it 280 total=7.7028 mle=1.6437 pcon=4.6551 forget=1.4040 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 43 it 330 total=7.8381 mle=1.7458 pcon=4.6548 forget=1.4374 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 43 it 380 total=7.5830 mle=1.5197 pcon=4.6546 forget=1.4087 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 44 it 40 total=7.7918 mle=1.7328 pcon=4.6543 forget=1.4047 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 44 it 90 total=7.6243 mle=1.5499 pcon=4.6541 forget=1.4204 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 44 it 140 total=7.6470 mle=1.5840 pcon=4.6538 forget=1.4092 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 44 it 190 total=7.6427 mle=1.5738 pcon=4.6535 forget=1.4153 favg=0.0000 nr=24 nf=24 protos=540 fproto_sim=NA
[loss] ep 44 it 240 total=7.7927 mle=1.6929 pcon=4.6533 forget=1.4466 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 44 it 290 total=8.0038 mle=1.9253 pcon=4.6531 forget=1.4254 favg=0.0000 nr=36 nf=36 protos=540 fproto_sim=NA
[loss] ep 44 it 340 total=7.8629 mle=1.7812 pcon=4.6528 forget=1.4289 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 45 it 0 total=7.8635 mle=1.7780 pcon=4.6525 forget=1.4330 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 45 it 50 total=7.7473 mle=1.6543 pcon=4.6523 forget=1.4407 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 45 it 100 total=7.6482 mle=1.5444 pcon=4.6520 forget=1.4517 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 45 it 150 total=7.8797 mle=1.7918 pcon=4.6518 forget=1.4361 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 45 it 200 total=7.5842 mle=1.4986 pcon=4.6515 forget=1.4340 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 45 it 250 total=7.6518 mle=1.5593 pcon=4.6512 forget=1.4412 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 45 it 300 total=7.8057 mle=1.7074 pcon=4.6511 forget=1.4472 favg=0.0000 nr=40 nf=40 protos=540 fproto_sim=NA
[loss] ep 45 it 350 total=7.6203 mle=1.5134 pcon=4.6508 forget=1.4561 favg=0.0000 nr=39 nf=39 protos=540 fproto_sim=NA
[loss] ep 46 it 10 total=7.8100 mle=1.7089 pcon=4.6506 forget=1.4506 favg=0.0000 nr=25 nf=25 protos=540 fproto_sim=NA
[loss] ep 46 it 60 total=7.7401 mle=1.6351 pcon=4.6503 forget=1.4547 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 46 it 110 total=7.7626 mle=1.6640 pcon=4.6500 forget=1.4486 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 46 it 160 total=7.5036 mle=1.4077 pcon=4.6498 forget=1.4461 favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 46 it 210 total=7.8714 mle=1.7650 pcon=4.6497 forget=1.4567 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 46 it 260 total=7.7491 mle=1.6447 pcon=4.6494 forget=1.4550 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 46 it 310 total=7.9319 mle=1.8240 pcon=4.6493 forget=1.4586 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 46 it 360 total=7.8183 mle=1.7102 pcon=4.6492 forget=1.4590 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 47 it 20 total=7.6995 mle=1.5964 pcon=4.6490 forget=1.4541 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 47 it 70 total=7.6945 mle=1.5598 pcon=4.6488 forget=1.4859 favg=0.0000 nr=33 nf=33 protos=540 fproto_sim=NA
[loss] ep 47 it 120 total=7.6398 mle=1.5238 pcon=4.6486 forget=1.4674 favg=0.0000 nr=26 nf=26 protos=540 fproto_sim=NA
[loss] ep 47 it 170 total=7.8845 mle=1.7574 pcon=4.6484 forget=1.4786 favg=0.0000 nr=27 nf=27 protos=540 fproto_sim=NA
[loss] ep 47 it 220 total=7.8301 mle=1.7041 pcon=4.6482 forget=1.4778 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 47 it 270 total=7.9129 mle=1.7897 pcon=4.6481 forget=1.4751 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 47 it 320 total=7.8527 mle=1.7219 pcon=4.6479 forget=1.4829 favg=0.0000 nr=38 nf=38 protos=540 fproto_sim=NA
[loss] ep 47 it 370 total=7.8623 mle=1.7364 pcon=4.6477 forget=1.4782 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 48 it 30 total=7.8339 mle=1.7086 pcon=4.6475 forget=1.4778 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 48 it 80 total=7.9409 mle=1.8029 pcon=4.6473 forget=1.4907 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 48 it 130 total=7.7529 mle=1.6154 pcon=4.6472 forget=1.4903 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 48 it 180 total=7.6906 mle=1.5533 pcon=4.6471 forget=1.4903 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 48 it 230 total=7.9180 mle=1.7865 pcon=4.6469 forget=1.4846 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
[loss] ep 48 it 280 total=7.7508 mle=1.6145 pcon=4.6468 forget=1.4895 favg=0.0000 nr=37 nf=37 protos=540 fproto_sim=NA
[loss] ep 48 it 330 total=7.9692 mle=1.8188 pcon=4.6466 forget=1.5038 favg=0.0000 nr=35 nf=35 protos=540 fproto_sim=NA
[loss] ep 48 it 380 total=8.1913 mle=2.0444 pcon=4.6465 forget=1.5005 favg=0.0000 nr=31 nf=31 protos=540 fproto_sim=NA
[loss] ep 49 it 40 total=7.8735 mle=1.7283 pcon=4.6463 forget=1.4988 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 49 it 90 total=7.7789 mle=1.6187 pcon=4.6462 forget=1.5139 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
100%|██████████| 50/50 [10:11<00:00, 10.76s/it]100%|██████████| 50/50 [10:11<00:00, 12.24s/it]
[loss] ep 49 it 140 total=7.7613 mle=1.6123 pcon=4.6461 forget=1.5030 favg=0.0000 nr=32 nf=32 protos=540 fproto_sim=NA
[loss] ep 49 it 190 total=7.8235 mle=1.6838 pcon=4.6460 forget=1.4937 favg=0.0000 nr=29 nf=29 protos=540 fproto_sim=NA
[loss] ep 49 it 240 total=7.8604 mle=1.7056 pcon=4.6459 forget=1.5088 favg=0.0000 nr=28 nf=28 protos=540 fproto_sim=NA
[loss] ep 49 it 290 total=7.9645 mle=1.8079 pcon=4.6457 forget=1.5109 favg=0.0000 nr=34 nf=34 protos=540 fproto_sim=NA
[loss] ep 49 it 340 total=7.8507 mle=1.6905 pcon=4.6456 forget=1.5146 favg=0.0000 nr=30 nf=30 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] loaded adapter 'stage1' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] loaded adapter 'stage2' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[peft] set_adapter(list) failed: unhashable type: 'list'; try adapter fusion
[peft] adapter fusion failed: adapter fusion API not available or single adapter provided; fallback to last adapter
[peft] active adapter set to: stage2
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc1: Number of model parameters: 22321088
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:55,  2.22it/s]  3%|▎         | 11/391 [00:00<00:14, 25.35it/s]  5%|▌         | 21/391 [00:00<00:08, 43.37it/s]  8%|▊         | 31/391 [00:00<00:06, 57.53it/s] 10%|█         | 41/391 [00:00<00:05, 67.51it/s] 13%|█▎        | 51/391 [00:00<00:04, 74.75it/s] 16%|█▌        | 61/391 [00:01<00:04, 80.63it/s] 18%|█▊        | 71/391 [00:01<00:03, 84.95it/s] 21%|██        | 81/391 [00:01<00:03, 87.99it/s] 23%|██▎       | 91/391 [00:01<00:03, 90.24it/s] 26%|██▌       | 101/391 [00:01<00:03, 91.97it/s] 28%|██▊       | 111/391 [00:01<00:03, 91.83it/s] 31%|███       | 121/391 [00:01<00:02, 92.08it/s] 34%|███▎      | 131/391 [00:01<00:02, 93.06it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.05it/s] 39%|███▊      | 151/391 [00:02<00:02, 94.54it/s] 41%|████      | 161/391 [00:02<00:02, 95.00it/s] 44%|████▎     | 171/391 [00:02<00:02, 95.44it/s] 46%|████▋     | 181/391 [00:02<00:02, 95.48it/s] 49%|████▉     | 191/391 [00:02<00:02, 95.48it/s] 51%|█████▏    | 201/391 [00:02<00:01, 95.55it/s] 54%|█████▍    | 211/391 [00:02<00:01, 95.58it/s] 57%|█████▋    | 221/391 [00:02<00:01, 95.72it/s] 59%|█████▉    | 231/391 [00:02<00:01, 95.81it/s] 62%|██████▏   | 241/391 [00:02<00:01, 96.05it/s] 64%|██████▍   | 251/391 [00:03<00:01, 95.90it/s] 67%|██████▋   | 261/391 [00:03<00:01, 95.75it/s] 69%|██████▉   | 271/391 [00:03<00:01, 95.74it/s] 72%|███████▏  | 281/391 [00:03<00:01, 95.70it/s] 74%|███████▍  | 291/391 [00:03<00:01, 95.77it/s] 77%|███████▋  | 301/391 [00:03<00:00, 95.74it/s] 80%|███████▉  | 311/391 [00:03<00:00, 95.94it/s] 82%|████████▏ | 321/391 [00:03<00:00, 95.92it/s] 85%|████████▍ | 331/391 [00:03<00:00, 96.06it/s] 87%|████████▋ | 341/391 [00:04<00:00, 94.35it/s] 90%|████████▉ | 351/391 [00:04<00:00, 94.97it/s] 92%|█████████▏| 361/391 [00:04<00:00, 94.98it/s] 95%|█████████▍| 371/391 [00:04<00:00, 95.71it/s] 97%|█████████▋| 381/391 [00:04<00:00, 96.31it/s]100%|██████████| 391/391 [00:04<00:00, 93.28it/s]100%|██████████| 391/391 [00:04<00:00, 85.82it/s]
50000 images processed, 4.634426593780518 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:34,  2.24it/s] 14%|█▍        | 11/79 [00:00<00:02, 25.39it/s] 27%|██▋       | 21/79 [00:00<00:01, 43.30it/s] 39%|███▉      | 31/79 [00:00<00:00, 57.20it/s] 52%|█████▏    | 41/79 [00:00<00:00, 67.71it/s] 65%|██████▍   | 51/79 [00:00<00:00, 75.32it/s] 77%|███████▋  | 61/79 [00:01<00:00, 81.39it/s] 90%|████████▉ | 71/79 [00:01<00:00, 85.87it/s]100%|██████████| 79/79 [00:01<00:00, 61.68it/s]
10000 images processed, 1.3044748306274414 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:46,  1.91it/s]  5%|▌         | 11/204 [00:00<00:08, 22.68it/s] 10%|█         | 21/204 [00:00<00:04, 40.16it/s] 15%|█▌        | 31/204 [00:00<00:03, 54.33it/s] 20%|█▉        | 40/204 [00:00<00:02, 63.54it/s] 25%|██▍       | 50/204 [00:01<00:02, 72.32it/s] 29%|██▉       | 60/204 [00:01<00:01, 78.35it/s] 34%|███▍      | 70/204 [00:01<00:01, 83.12it/s] 39%|███▉      | 80/204 [00:01<00:01, 86.69it/s] 44%|████▍     | 90/204 [00:01<00:01, 89.25it/s] 49%|████▉     | 100/204 [00:01<00:01, 90.85it/s] 54%|█████▍    | 110/204 [00:01<00:01, 91.72it/s] 59%|█████▉    | 120/204 [00:01<00:00, 92.74it/s] 64%|██████▎   | 130/204 [00:01<00:00, 93.43it/s] 69%|██████▊   | 140/204 [00:01<00:00, 93.78it/s] 74%|███████▎  | 150/204 [00:02<00:00, 93.79it/s] 78%|███████▊  | 160/204 [00:02<00:00, 93.83it/s] 83%|████████▎ | 170/204 [00:02<00:00, 94.14it/s] 88%|████████▊ | 180/204 [00:02<00:00, 94.65it/s] 93%|█████████▎| 190/204 [00:02<00:00, 95.30it/s] 98%|█████████▊| 200/204 [00:02<00:00, 95.71it/s]100%|██████████| 204/204 [00:02<00:00, 76.43it/s]
26032 images processed, 2.716914653778076 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:53,  1.46it/s] 14%|█▍        | 11/79 [00:00<00:03, 18.15it/s] 27%|██▋       | 21/79 [00:00<00:01, 33.68it/s] 39%|███▉      | 31/79 [00:01<00:01, 47.43it/s] 52%|█████▏    | 41/79 [00:01<00:00, 57.81it/s] 63%|██████▎   | 50/79 [00:01<00:00, 63.72it/s] 76%|███████▌  | 60/79 [00:01<00:00, 72.21it/s] 89%|████████▊ | 70/79 [00:01<00:00, 78.91it/s]100%|██████████| 79/79 [00:01<00:00, 51.75it/s]
10000 images processed, 1.5603649616241455 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:39,  1.98it/s] 14%|█▍        | 11/79 [00:00<00:02, 23.32it/s] 27%|██▋       | 21/79 [00:00<00:01, 41.09it/s] 39%|███▉      | 31/79 [00:00<00:00, 55.30it/s] 52%|█████▏    | 41/79 [00:00<00:00, 66.09it/s] 65%|██████▍   | 51/79 [00:01<00:00, 74.36it/s] 77%|███████▋  | 61/79 [00:01<00:00, 80.78it/s] 90%|████████▉ | 71/79 [00:01<00:00, 85.37it/s]100%|██████████| 79/79 [00:01<00:00, 60.29it/s]
10000 images processed, 1.3383569717407227 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:36,  1.91it/s] 16%|█▌        | 11/70 [00:00<00:02, 22.50it/s] 30%|███       | 21/70 [00:00<00:01, 39.77it/s] 44%|████▍     | 31/70 [00:00<00:00, 53.86it/s] 59%|█████▊    | 41/70 [00:00<00:00, 64.86it/s] 73%|███████▎  | 51/70 [00:01<00:00, 73.59it/s] 87%|████████▋ | 61/70 [00:01<00:00, 80.17it/s]100%|██████████| 70/70 [00:01<00:00, 55.89it/s]
8925 images processed, 1.2815964221954346 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:45,  1.02s/it]  4%|▍         | 2/45 [00:01<00:20,  2.05it/s] 27%|██▋       | 12/45 [00:01<00:02, 16.36it/s] 38%|███▊      | 17/45 [00:01<00:01, 17.15it/s] 49%|████▉     | 22/45 [00:01<00:01, 18.35it/s] 67%|██████▋   | 30/45 [00:01<00:00, 24.72it/s] 76%|███████▌  | 34/45 [00:02<00:00, 21.56it/s] 96%|█████████▌| 43/45 [00:02<00:00, 30.14it/s]100%|██████████| 45/45 [00:02<00:00, 19.01it/s]
5640 images processed, 2.39453387260437 seconds used

16.824538707733154
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.50  99.36
places365     67.90  80.57
LSUN          21.54  95.06
iSUN          72.28  81.40
dtd           38.17  91.27
AVG           40.48  89.53
Retain-Acc: 0.7433
Forget-as-OOD (retain known vs forget novel):
  FPR: 84.00 AUROC: 85.57 AUIN: 99.08
8.189259052276611
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc1_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc1_rf.png
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] loaded adapter 'stage1' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] loaded adapter 'stage2' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[peft] set_adapter(list) failed: unhashable type: 'list'; try adapter fusion
[peft] adapter fusion failed: adapter fusion API not available or single adapter provided; fallback to last adapter
[peft] active adapter set to: stage2
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc2: Number of model parameters: 22321088
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:44,  2.37it/s]  3%|▎         | 11/391 [00:00<00:14, 26.59it/s]  5%|▌         | 21/391 [00:00<00:08, 45.33it/s]  8%|▊         | 31/391 [00:00<00:06, 59.25it/s] 10%|█         | 41/391 [00:00<00:05, 69.31it/s] 13%|█▎        | 51/391 [00:00<00:04, 77.04it/s] 16%|█▌        | 61/391 [00:01<00:03, 82.79it/s] 18%|█▊        | 71/391 [00:01<00:03, 86.85it/s] 21%|██        | 81/391 [00:01<00:03, 89.75it/s] 23%|██▎       | 91/391 [00:01<00:03, 91.31it/s] 26%|██▌       | 101/391 [00:01<00:03, 92.68it/s] 28%|██▊       | 111/391 [00:01<00:02, 93.82it/s] 31%|███       | 121/391 [00:01<00:02, 94.51it/s] 34%|███▎      | 131/391 [00:01<00:02, 95.04it/s] 36%|███▌      | 141/391 [00:01<00:02, 95.47it/s] 39%|███▊      | 151/391 [00:01<00:02, 95.68it/s] 41%|████      | 161/391 [00:02<00:02, 95.79it/s] 44%|████▎     | 171/391 [00:02<00:02, 95.12it/s] 46%|████▋     | 181/391 [00:02<00:02, 95.46it/s] 49%|████▉     | 191/391 [00:02<00:02, 95.56it/s] 51%|█████▏    | 201/391 [00:02<00:01, 95.65it/s] 54%|█████▍    | 211/391 [00:02<00:01, 95.67it/s] 57%|█████▋    | 221/391 [00:02<00:01, 95.67it/s] 59%|█████▉    | 231/391 [00:02<00:01, 95.77it/s] 62%|██████▏   | 241/391 [00:02<00:01, 95.68it/s] 64%|██████▍   | 251/391 [00:03<00:01, 95.33it/s] 67%|██████▋   | 261/391 [00:03<00:01, 95.68it/s] 69%|██████▉   | 271/391 [00:03<00:01, 95.63it/s] 72%|███████▏  | 281/391 [00:03<00:01, 95.67it/s] 74%|███████▍  | 291/391 [00:03<00:01, 95.99it/s] 77%|███████▋  | 301/391 [00:03<00:00, 95.79it/s] 80%|███████▉  | 311/391 [00:03<00:00, 95.34it/s] 82%|████████▏ | 321/391 [00:03<00:00, 95.56it/s] 85%|████████▍ | 331/391 [00:03<00:00, 95.94it/s] 87%|████████▋ | 341/391 [00:03<00:00, 96.20it/s] 90%|████████▉ | 351/391 [00:04<00:00, 95.95it/s] 92%|█████████▏| 361/391 [00:04<00:00, 95.63it/s] 95%|█████████▍| 371/391 [00:04<00:00, 96.24it/s] 97%|█████████▋| 381/391 [00:04<00:00, 96.54it/s]100%|██████████| 391/391 [00:04<00:00, 94.63it/s]100%|██████████| 391/391 [00:04<00:00, 86.92it/s]
50000 images processed, 4.585920095443726 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:38,  2.05it/s] 11%|█▏        | 9/79 [00:00<00:03, 19.54it/s] 24%|██▍       | 19/79 [00:00<00:01, 38.72it/s] 37%|███▋      | 29/79 [00:00<00:00, 53.53it/s] 49%|████▉     | 39/79 [00:00<00:00, 65.02it/s] 62%|██████▏   | 49/79 [00:01<00:00, 73.31it/s] 75%|███████▍  | 59/79 [00:01<00:00, 80.07it/s] 87%|████████▋ | 69/79 [00:01<00:00, 84.99it/s]100%|██████████| 79/79 [00:01<00:00, 87.13it/s]100%|██████████| 79/79 [00:01<00:00, 59.53it/s]
10000 images processed, 1.3480792045593262 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:56,  1.75it/s]  5%|▌         | 11/204 [00:00<00:09, 21.03it/s] 10%|█         | 21/204 [00:00<00:04, 37.87it/s] 15%|█▌        | 31/204 [00:00<00:03, 51.94it/s] 20%|██        | 41/204 [00:00<00:02, 63.21it/s] 25%|██▌       | 51/204 [00:01<00:02, 72.01it/s] 30%|██▉       | 61/204 [00:01<00:01, 78.58it/s] 35%|███▍      | 71/204 [00:01<00:01, 83.42it/s] 40%|███▉      | 81/204 [00:01<00:01, 86.72it/s] 45%|████▍     | 91/204 [00:01<00:01, 89.01it/s] 50%|████▉     | 101/204 [00:01<00:01, 90.60it/s] 54%|█████▍    | 111/204 [00:01<00:01, 91.73it/s] 59%|█████▉    | 121/204 [00:01<00:00, 92.84it/s] 64%|██████▍   | 131/204 [00:01<00:00, 93.52it/s] 69%|██████▉   | 141/204 [00:02<00:00, 94.24it/s] 74%|███████▍  | 151/204 [00:02<00:00, 94.55it/s] 79%|███████▉  | 161/204 [00:02<00:00, 94.52it/s] 84%|████████▍ | 171/204 [00:02<00:00, 93.73it/s] 89%|████████▊ | 181/204 [00:02<00:00, 94.48it/s] 94%|█████████▎| 191/204 [00:02<00:00, 95.19it/s] 99%|█████████▊| 201/204 [00:02<00:00, 95.61it/s]100%|██████████| 204/204 [00:02<00:00, 75.25it/s]
26032 images processed, 2.750901460647583 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:59,  1.31it/s] 14%|█▍        | 11/79 [00:00<00:04, 16.68it/s] 25%|██▌       | 20/79 [00:00<00:02, 28.56it/s] 37%|███▋      | 29/79 [00:01<00:01, 40.58it/s] 47%|████▋     | 37/79 [00:01<00:00, 48.37it/s] 59%|█████▉    | 47/79 [00:01<00:00, 59.69it/s] 71%|███████   | 56/79 [00:01<00:00, 62.32it/s] 84%|████████▎ | 66/79 [00:01<00:00, 70.85it/s] 96%|█████████▌| 76/79 [00:01<00:00, 77.73it/s]100%|██████████| 79/79 [00:01<00:00, 47.01it/s]
10000 images processed, 1.7207541465759277 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.91it/s]  9%|▉         | 7/79 [00:00<00:05, 14.14it/s] 22%|██▏       | 17/79 [00:00<00:01, 33.59it/s] 34%|███▍      | 27/79 [00:00<00:01, 49.14it/s] 47%|████▋     | 37/79 [00:00<00:00, 61.42it/s] 59%|█████▉    | 47/79 [00:01<00:00, 70.65it/s] 72%|███████▏  | 57/79 [00:01<00:00, 77.78it/s] 85%|████████▍ | 67/79 [00:01<00:00, 83.11it/s] 97%|█████████▋| 77/79 [00:01<00:00, 87.05it/s]100%|██████████| 79/79 [00:01<00:00, 57.26it/s]
10000 images processed, 1.400374412536621 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:40,  1.71it/s] 16%|█▌        | 11/70 [00:00<00:02, 20.59it/s] 30%|███       | 21/70 [00:00<00:01, 37.25it/s] 44%|████▍     | 31/70 [00:00<00:00, 51.12it/s] 59%|█████▊    | 41/70 [00:01<00:00, 62.29it/s] 73%|███████▎  | 51/70 [00:01<00:00, 71.36it/s] 87%|████████▋ | 61/70 [00:01<00:00, 78.26it/s]100%|██████████| 70/70 [00:01<00:00, 53.15it/s]
8925 images processed, 1.345590353012085 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:44,  1.00s/it]  4%|▍         | 2/45 [00:01<00:21,  2.04it/s] 16%|█▌        | 7/45 [00:01<00:04,  8.92it/s] 38%|███▊      | 17/45 [00:01<00:01, 17.84it/s] 44%|████▍     | 20/45 [00:01<00:01, 15.83it/s] 58%|█████▊    | 26/45 [00:01<00:00, 22.17it/s] 73%|███████▎  | 33/45 [00:02<00:00, 23.82it/s] 82%|████████▏ | 37/45 [00:02<00:00, 24.31it/s] 91%|█████████ | 41/45 [00:02<00:00, 27.01it/s]100%|██████████| 45/45 [00:02<00:00, 18.05it/s]
5640 images processed, 2.5165016651153564 seconds used

17.34193754196167
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.53  99.36
places365     68.01  81.23
LSUN          17.64  96.11
iSUN          72.54  81.90
dtd           37.73  91.47
AVG           39.69  90.01
Retain-Acc: 0.7418
Forget-as-OOD (retain known vs forget novel):
  FPR: 73.20 AUROC: 88.35 AUIN: 99.33
8.607141494750977
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc2_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-inc2_rf.png
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] loaded adapter 'stage1' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] loaded adapter 'stage2' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage2
[peft] set_adapter(list) failed: unhashable type: 'list'; try adapter fusion
[peft] adapter fusion failed: adapter fusion API not available or single adapter provided; fallback to last adapter
[peft] active adapter set to: stage2
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-seen: Number of model parameters: 22321088
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:03,  2.13it/s]  3%|▎         | 11/391 [00:00<00:15, 24.70it/s]  5%|▌         | 21/391 [00:00<00:08, 43.00it/s]  8%|▊         | 31/391 [00:00<00:06, 57.08it/s] 10%|█         | 41/391 [00:00<00:05, 67.76it/s] 13%|█▎        | 51/391 [00:00<00:04, 75.75it/s] 16%|█▌        | 61/391 [00:01<00:04, 80.67it/s] 18%|█▊        | 71/391 [00:01<00:03, 85.18it/s] 21%|██        | 81/391 [00:01<00:03, 88.42it/s] 23%|██▎       | 91/391 [00:01<00:03, 90.67it/s] 26%|██▌       | 101/391 [00:01<00:03, 92.33it/s] 28%|██▊       | 111/391 [00:01<00:02, 93.41it/s] 31%|███       | 121/391 [00:01<00:02, 94.08it/s] 34%|███▎      | 131/391 [00:01<00:02, 94.28it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.84it/s] 39%|███▊      | 151/391 [00:02<00:02, 95.42it/s] 41%|████      | 161/391 [00:02<00:02, 95.37it/s] 44%|████▎     | 171/391 [00:02<00:02, 95.51it/s] 46%|████▋     | 181/391 [00:02<00:02, 95.57it/s] 49%|████▉     | 191/391 [00:02<00:02, 95.59it/s] 51%|█████▏    | 201/391 [00:02<00:01, 95.61it/s] 54%|█████▍    | 211/391 [00:02<00:01, 95.49it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.60it/s] 59%|█████▉    | 231/391 [00:02<00:01, 92.42it/s] 62%|██████▏   | 241/391 [00:02<00:01, 93.50it/s] 64%|██████▍   | 251/391 [00:03<00:01, 94.24it/s] 67%|██████▋   | 261/391 [00:03<00:01, 94.69it/s] 69%|██████▉   | 271/391 [00:03<00:01, 95.07it/s] 72%|███████▏  | 281/391 [00:03<00:01, 94.68it/s] 74%|███████▍  | 291/391 [00:03<00:01, 95.04it/s] 77%|███████▋  | 301/391 [00:03<00:00, 94.98it/s] 80%|███████▉  | 311/391 [00:03<00:00, 95.35it/s] 82%|████████▏ | 321/391 [00:03<00:00, 95.40it/s] 85%|████████▍ | 331/391 [00:03<00:00, 95.59it/s] 87%|████████▋ | 341/391 [00:04<00:00, 95.64it/s] 90%|████████▉ | 351/391 [00:04<00:00, 95.71it/s] 92%|█████████▏| 361/391 [00:04<00:00, 95.01it/s] 95%|█████████▍| 371/391 [00:04<00:00, 95.76it/s] 97%|█████████▋| 381/391 [00:04<00:00, 96.37it/s]100%|██████████| 391/391 [00:04<00:00, 93.19it/s]100%|██████████| 391/391 [00:04<00:00, 85.57it/s]
50000 images processed, 4.6872718334198 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:36,  2.14it/s] 14%|█▍        | 11/79 [00:00<00:02, 24.83it/s] 27%|██▋       | 21/79 [00:00<00:01, 43.09it/s] 39%|███▉      | 31/79 [00:00<00:00, 57.28it/s] 52%|█████▏    | 41/79 [00:00<00:00, 67.53it/s] 65%|██████▍   | 51/79 [00:00<00:00, 75.42it/s] 77%|███████▋  | 61/79 [00:01<00:00, 81.50it/s] 90%|████████▉ | 71/79 [00:01<00:00, 85.88it/s]100%|██████████| 79/79 [00:01<00:00, 40.62it/s]
10000 images processed, 1.9739999771118164 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:39,  2.04it/s]  5%|▍         | 10/204 [00:00<00:09, 21.51it/s] 10%|▉         | 20/204 [00:00<00:04, 39.78it/s] 15%|█▍        | 30/204 [00:00<00:03, 54.06it/s] 20%|█▉        | 40/204 [00:00<00:02, 65.02it/s] 25%|██▍       | 50/204 [00:01<00:02, 73.42it/s] 29%|██▉       | 60/204 [00:01<00:01, 79.73it/s] 34%|███▍      | 70/204 [00:01<00:01, 84.36it/s] 39%|███▉      | 80/204 [00:01<00:01, 87.53it/s] 44%|████▍     | 90/204 [00:01<00:01, 89.80it/s] 49%|████▉     | 100/204 [00:01<00:01, 91.40it/s] 54%|█████▍    | 110/204 [00:01<00:01, 92.48it/s] 59%|█████▉    | 120/204 [00:01<00:00, 93.20it/s] 64%|██████▎   | 130/204 [00:01<00:00, 92.64it/s] 69%|██████▊   | 140/204 [00:01<00:00, 93.47it/s] 74%|███████▎  | 150/204 [00:02<00:00, 93.79it/s] 78%|███████▊  | 160/204 [00:02<00:00, 93.93it/s] 83%|████████▎ | 170/204 [00:02<00:00, 94.29it/s] 88%|████████▊ | 180/204 [00:02<00:00, 94.70it/s] 93%|█████████▎| 190/204 [00:02<00:00, 95.43it/s] 98%|█████████▊| 200/204 [00:02<00:00, 95.90it/s]100%|██████████| 204/204 [00:02<00:00, 77.16it/s]
26032 images processed, 2.6831748485565186 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:49,  1.58it/s] 14%|█▍        | 11/79 [00:00<00:03, 19.53it/s] 25%|██▌       | 20/79 [00:00<00:01, 33.89it/s] 38%|███▊      | 30/79 [00:00<00:01, 48.03it/s] 51%|█████     | 40/79 [00:01<00:00, 59.46it/s] 63%|██████▎   | 50/79 [00:01<00:00, 68.23it/s] 76%|███████▌  | 60/79 [00:01<00:00, 75.21it/s] 89%|████████▊ | 70/79 [00:01<00:00, 80.90it/s]100%|██████████| 79/79 [00:01<00:00, 53.98it/s]
10000 images processed, 1.5005664825439453 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:50,  1.54it/s] 14%|█▍        | 11/79 [00:00<00:03, 19.01it/s] 27%|██▋       | 21/79 [00:00<00:01, 34.57it/s] 39%|███▉      | 31/79 [00:00<00:00, 48.27it/s] 52%|█████▏    | 41/79 [00:01<00:00, 59.67it/s] 65%|██████▍   | 51/79 [00:01<00:00, 68.88it/s] 77%|███████▋  | 61/79 [00:01<00:00, 76.22it/s] 90%|████████▉ | 71/79 [00:01<00:00, 81.65it/s]100%|██████████| 79/79 [00:01<00:00, 53.65it/s]
10000 images processed, 1.491459608078003 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:35,  1.95it/s] 16%|█▌        | 11/70 [00:00<00:02, 22.90it/s] 30%|███       | 21/70 [00:00<00:01, 40.46it/s] 44%|████▍     | 31/70 [00:00<00:00, 54.42it/s] 59%|█████▊    | 41/70 [00:00<00:00, 65.04it/s] 73%|███████▎  | 51/70 [00:01<00:00, 73.67it/s] 87%|████████▋ | 61/70 [00:01<00:00, 80.03it/s]100%|██████████| 70/70 [00:01<00:00, 56.27it/s]
8925 images processed, 1.2741360664367676 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:47,  1.09s/it]  9%|▉         | 4/45 [00:01<00:09,  4.27it/s] 16%|█▌        | 7/45 [00:01<00:05,  7.39it/s] 38%|███▊      | 17/45 [00:01<00:01, 17.77it/s] 44%|████▍     | 20/45 [00:01<00:01, 19.15it/s] 51%|█████     | 23/45 [00:01<00:01, 17.24it/s] 73%|███████▎  | 33/45 [00:02<00:00, 23.46it/s] 80%|████████  | 36/45 [00:02<00:00, 21.95it/s]100%|██████████| 45/45 [00:02<00:00, 17.94it/s]
5640 images processed, 2.527602195739746 seconds used

17.788605451583862
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.53  99.36
places365     67.87  81.18
LSUN          17.60  96.07
iSUN          72.36  81.68
dtd           37.87  91.39
AVG           39.65  89.93
Retain-Acc: 0.7406
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
8.400340795516968
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-seen_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage2-seen_rf.png
