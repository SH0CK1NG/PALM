nohup: ignoring input
[Phase 1] Searching lambda over: 1.0 0.5 0.2 0.1
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase1-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:18<02:42, 18.02s/it] 20%|██        | 2/10 [00:32<02:08, 16.06s/it] 30%|███       | 3/10 [00:47<01:47, 15.30s/it] 40%|████      | 4/10 [01:01<01:30, 15.07s/it] 50%|█████     | 5/10 [01:16<01:14, 14.97s/it] 60%|██████    | 6/10 [01:31<00:59, 14.79s/it] 70%|███████   | 7/10 [01:45<00:43, 14.62s/it] 80%|████████  | 8/10 [01:59<00:29, 14.50s/it][loss] ep 0 it 0 total=13.7435 mle=1.5710 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8370 mle=1.5424 pcon=5.2879 forget=7.0066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8513 mle=1.7005 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0218 mle=1.8998 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8931 mle=1.7132 pcon=5.2670 forget=6.9130 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6543 mle=1.5020 pcon=5.2603 forget=6.8919 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7233 mle=1.5597 pcon=5.2540 forget=6.9095 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8764 mle=1.6810 pcon=5.2476 forget=6.9478 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 1 it 10 total=13.9620 mle=1.6716 pcon=5.2409 forget=7.0496 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7062 mle=1.6371 pcon=5.2346 forget=6.8344 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6914 mle=1.5165 pcon=5.2284 forget=6.9464 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9483 mle=1.7804 pcon=5.2224 forget=6.9455 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9067 mle=1.7648 pcon=5.2167 forget=6.9252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6563 mle=1.6050 pcon=5.2112 forget=6.8401 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8179 mle=1.7235 pcon=5.2056 forget=6.8888 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9237 mle=1.7944 pcon=5.2003 forget=6.9290 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 2 it 20 total=13.6297 mle=1.5530 pcon=5.1950 forget=6.8817 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8629 mle=1.8883 pcon=5.1899 forget=6.7848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8038 mle=1.7420 pcon=5.1846 forget=6.8771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5268 mle=1.5263 pcon=5.1796 forget=6.8210 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6859 mle=1.5857 pcon=5.1745 forget=6.9256 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9021 mle=1.8840 pcon=5.1698 forget=6.8483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5393 mle=1.5911 pcon=5.1651 forget=6.7831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7886 mle=1.8112 pcon=5.1604 forget=6.8170 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 3 it 30 total=13.7066 mle=1.7049 pcon=5.1558 forget=6.8459 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7121 mle=1.6713 pcon=5.1518 forget=6.8890 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7348 mle=1.7664 pcon=5.1474 forget=6.8211 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8712 mle=1.9270 pcon=5.1434 forget=6.8008 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.4987 mle=1.5695 pcon=5.1393 forget=6.7899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7424 mle=1.8358 pcon=5.1351 forget=6.7715 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.6962 mle=1.7028 pcon=5.1308 forget=6.8626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.6910 mle=1.6954 pcon=5.1272 forget=6.8685 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 4 it 40 total=13.4700 mle=1.6521 pcon=5.1233 forget=6.6946 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6123 mle=1.7318 pcon=5.1192 forget=6.7613 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5723 mle=1.6523 pcon=5.1155 forget=6.8046 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3639 mle=1.4533 pcon=5.1119 forget=6.7987 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6082 mle=1.7665 pcon=5.1079 forget=6.7338 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3131 mle=1.4891 pcon=5.1042 forget=6.7198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7343 mle=1.8758 pcon=5.1006 forget=6.7580 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 5 it 0 total=13.6935 mle=1.8653 pcon=5.0970 forget=6.7313 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=13.5489 mle=1.8061 pcon=5.0931 forget=6.6497 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=13.4059 mle=1.5904 pcon=5.0897 forget=6.7258 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=13.3917 mle=1.5924 pcon=5.0858 forget=6.7134 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=13.4118 mle=1.6919 pcon=5.0826 forget=6.6374 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=13.4545 mle=1.7800 pcon=5.0792 forget=6.5953 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=13.2879 mle=1.6036 pcon=5.0757 forget=6.6086 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=13.4639 mle=1.7402 pcon=5.0724 forget=6.6514 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 6 it 10 total=13.2321 mle=1.4104 pcon=5.0692 forget=6.7525 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=13.1498 mle=1.5274 pcon=5.0660 forget=6.5565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=13.3005 mle=1.5942 pcon=5.0630 forget=6.6433 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=13.3710 mle=1.6816 pcon=5.0600 forget=6.6295 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=13.3247 mle=1.6310 pcon=5.0576 forget=6.6361 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=13.5425 mle=1.7990 pcon=5.0546 forget=6.6889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=13.3507 mle=1.5471 pcon=5.0521 forget=6.7515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=13.4034 mle=1.6703 pcon=5.0498 forget=6.6833 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 7 it 20 total=13.4032 mle=1.6510 pcon=5.0474 forget=6.7049 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=13.5263 mle=1.7659 pcon=5.0453 forget=6.7151 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=13.4399 mle=1.6176 pcon=5.0432 forget=6.7790 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=13.4468 mle=1.5965 pcon=5.0415 forget=6.8089 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=13.5854 mle=1.6445 pcon=5.0401 forget=6.9008 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=13.7965 mle=1.7800 pcon=5.0388 forget=6.9777 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=13.7837 mle=1.7537 pcon=5.0372 forget=6.9927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=13.6791 mle=1.6352 pcon=5.0361 forget=7.0077 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 30 total=13.7225 mle=1.5611 pcon=5.0352 forget=7.1262 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=13.7310 mle=1.5985 pcon=5.0343 forget=7.0982 nr=64 nf=64 protos=540 fproto_sim=NA
 90%|█████████ | 9/10 [02:13<00:14, 14.42s/it]100%|██████████| 10/10 [02:28<00:00, 14.50s/it]100%|██████████| 10/10 [02:28<00:00, 14.85s/it]
[loss] ep 8 it 130 total=13.6543 mle=1.4968 pcon=5.0331 forget=7.1244 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=13.8711 mle=1.7137 pcon=5.0322 forget=7.1252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=13.6506 mle=1.5297 pcon=5.0315 forget=7.0894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=13.7763 mle=1.7034 pcon=5.0302 forget=7.0427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=13.8859 mle=1.7307 pcon=5.0286 forget=7.1266 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=13.4632 mle=1.4731 pcon=5.0270 forget=6.9631 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 40 total=13.4101 mle=1.6204 pcon=5.0252 forget=6.7645 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=13.3207 mle=1.6540 pcon=5.0228 forget=6.6439 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=13.2958 mle=1.8051 pcon=5.0206 forget=6.4701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=12.7674 mle=1.5773 pcon=5.0179 forget=6.1721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=12.6481 mle=1.5922 pcon=5.0147 forget=6.0411 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=12.5617 mle=1.5531 pcon=5.0109 forget=5.9977 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=12.5520 mle=1.5802 pcon=5.0074 forget=5.9644 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase1: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:00,  3.24it/s]  3%|▎         | 11/391 [00:00<00:11, 33.33it/s]  5%|▌         | 21/391 [00:00<00:06, 53.05it/s]  8%|▊         | 31/391 [00:00<00:05, 66.22it/s] 10%|█         | 41/391 [00:00<00:04, 75.21it/s] 13%|█▎        | 51/391 [00:00<00:04, 81.00it/s] 16%|█▌        | 61/391 [00:00<00:03, 85.18it/s] 18%|█▊        | 71/391 [00:01<00:03, 88.12it/s] 21%|██        | 81/391 [00:01<00:03, 89.84it/s] 23%|██▎       | 91/391 [00:01<00:03, 91.19it/s] 26%|██▌       | 101/391 [00:01<00:03, 92.08it/s] 28%|██▊       | 111/391 [00:01<00:03, 92.86it/s] 31%|███       | 121/391 [00:01<00:02, 93.52it/s] 34%|███▎      | 131/391 [00:01<00:02, 93.97it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.17it/s] 39%|███▊      | 151/391 [00:01<00:02, 94.16it/s] 41%|████      | 161/391 [00:01<00:02, 94.29it/s] 44%|████▎     | 171/391 [00:02<00:02, 94.40it/s] 46%|████▋     | 181/391 [00:02<00:02, 94.26it/s] 49%|████▉     | 191/391 [00:02<00:02, 94.37it/s] 51%|█████▏    | 201/391 [00:02<00:02, 94.45it/s] 54%|█████▍    | 211/391 [00:02<00:01, 94.50it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.52it/s] 59%|█████▉    | 231/391 [00:02<00:01, 94.44it/s] 62%|██████▏   | 241/391 [00:02<00:01, 94.58it/s] 64%|██████▍   | 251/391 [00:02<00:01, 94.38it/s] 67%|██████▋   | 261/391 [00:03<00:01, 93.94it/s] 69%|██████▉   | 271/391 [00:03<00:01, 94.33it/s] 72%|███████▏  | 281/391 [00:03<00:01, 90.84it/s] 74%|███████▍  | 291/391 [00:03<00:01, 92.02it/s] 77%|███████▋  | 301/391 [00:03<00:00, 92.64it/s] 80%|███████▉  | 311/391 [00:03<00:00, 93.07it/s] 82%|████████▏ | 321/391 [00:03<00:00, 93.48it/s] 85%|████████▍ | 331/391 [00:03<00:00, 93.73it/s] 87%|████████▋ | 341/391 [00:03<00:00, 93.95it/s] 90%|████████▉ | 351/391 [00:04<00:00, 93.97it/s] 92%|█████████▏| 361/391 [00:04<00:00, 93.88it/s] 95%|█████████▍| 371/391 [00:04<00:00, 93.85it/s] 97%|█████████▋| 381/391 [00:04<00:00, 93.78it/s]100%|██████████| 391/391 [00:04<00:00, 91.39it/s]100%|██████████| 391/391 [00:04<00:00, 87.63it/s]
50000 images processed, 4.563409805297852 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:29,  2.63it/s] 14%|█▍        | 11/79 [00:00<00:02, 28.74it/s] 27%|██▋       | 21/79 [00:00<00:01, 47.47it/s] 39%|███▉      | 31/79 [00:00<00:00, 60.97it/s] 51%|█████     | 40/79 [00:00<00:00, 68.52it/s] 63%|██████▎   | 50/79 [00:00<00:00, 75.49it/s] 76%|███████▌  | 60/79 [00:01<00:00, 80.49it/s] 89%|████████▊ | 70/79 [00:01<00:00, 84.61it/s]100%|██████████| 79/79 [00:01<00:00, 63.81it/s]
10000 images processed, 1.2626862525939941 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:31,  2.22it/s]  5%|▌         | 11/204 [00:00<00:07, 25.33it/s] 10%|█         | 21/204 [00:00<00:04, 43.28it/s] 15%|█▌        | 31/204 [00:00<00:03, 56.56it/s] 20%|██        | 41/204 [00:00<00:02, 66.80it/s] 25%|██▌       | 51/204 [00:00<00:02, 74.29it/s] 30%|██▉       | 61/204 [00:01<00:01, 79.86it/s] 35%|███▍      | 71/204 [00:01<00:01, 83.69it/s] 40%|███▉      | 81/204 [00:01<00:01, 86.41it/s] 45%|████▍     | 91/204 [00:01<00:01, 88.24it/s] 50%|████▉     | 101/204 [00:01<00:01, 89.41it/s] 54%|█████▍    | 111/204 [00:01<00:01, 90.34it/s] 59%|█████▉    | 121/204 [00:01<00:00, 91.14it/s] 64%|██████▍   | 131/204 [00:01<00:00, 91.64it/s] 69%|██████▉   | 141/204 [00:01<00:00, 91.84it/s] 74%|███████▍  | 151/204 [00:02<00:00, 91.92it/s] 79%|███████▉  | 161/204 [00:02<00:00, 92.00it/s] 84%|████████▍ | 171/204 [00:02<00:00, 92.19it/s] 89%|████████▊ | 181/204 [00:02<00:00, 92.23it/s] 94%|█████████▎| 191/204 [00:02<00:00, 92.97it/s] 99%|█████████▊| 201/204 [00:02<00:00, 93.84it/s]100%|██████████| 204/204 [00:02<00:00, 77.20it/s]
26032 images processed, 2.6852309703826904 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:38,  2.02it/s] 11%|█▏        | 9/79 [00:00<00:04, 17.09it/s] 22%|██▏       | 17/79 [00:00<00:02, 27.16it/s] 32%|███▏      | 25/79 [00:00<00:01, 34.06it/s] 42%|████▏     | 33/79 [00:01<00:01, 38.76it/s] 52%|█████▏    | 41/79 [00:01<00:00, 41.92it/s] 62%|██████▏   | 49/79 [00:01<00:00, 44.27it/s] 72%|███████▏  | 57/79 [00:01<00:00, 45.70it/s] 82%|████████▏ | 65/79 [00:01<00:00, 46.83it/s] 92%|█████████▏| 73/79 [00:01<00:00, 47.58it/s]100%|██████████| 79/79 [00:02<00:00, 39.30it/s]
10000 images processed, 2.0438895225524902 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:26,  2.90it/s] 14%|█▍        | 11/79 [00:00<00:02, 30.81it/s] 27%|██▋       | 21/79 [00:00<00:01, 50.06it/s] 39%|███▉      | 31/79 [00:00<00:00, 63.52it/s] 52%|█████▏    | 41/79 [00:00<00:00, 72.94it/s] 65%|██████▍   | 51/79 [00:00<00:00, 79.59it/s] 77%|███████▋  | 61/79 [00:00<00:00, 83.44it/s] 90%|████████▉ | 71/79 [00:01<00:00, 87.11it/s]100%|██████████| 79/79 [00:01<00:00, 67.89it/s]
10000 images processed, 1.1861231327056885 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:23,  2.90it/s] 16%|█▌        | 11/70 [00:00<00:01, 30.88it/s] 30%|███       | 21/70 [00:00<00:00, 50.33it/s] 44%|████▍     | 31/70 [00:00<00:00, 63.87it/s] 59%|█████▊    | 41/70 [00:00<00:00, 73.28it/s] 73%|███████▎  | 51/70 [00:00<00:00, 79.92it/s] 87%|████████▋ | 61/70 [00:00<00:00, 84.73it/s]100%|██████████| 70/70 [00:01<00:00, 65.21it/s]
8925 images processed, 1.1029272079467773 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:38,  1.15it/s] 11%|█         | 5/45 [00:00<00:06,  6.57it/s] 20%|██        | 9/45 [00:01<00:04,  8.64it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.63it/s] 31%|███       | 14/45 [00:01<00:02, 11.18it/s] 38%|███▊      | 17/45 [00:01<00:02, 11.71it/s] 44%|████▍     | 20/45 [00:02<00:01, 13.18it/s] 49%|████▉     | 22/45 [00:02<00:01, 12.47it/s] 56%|█████▌    | 25/45 [00:02<00:01, 13.70it/s] 62%|██████▏   | 28/45 [00:02<00:01, 15.34it/s] 67%|██████▋   | 30/45 [00:02<00:01, 11.77it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.14it/s] 87%|████████▋ | 39/45 [00:03<00:00, 17.04it/s] 93%|█████████▎| 42/45 [00:03<00:00, 11.15it/s]100%|██████████| 45/45 [00:03<00:00, 11.41it/s]
5640 images processed, 3.9629549980163574 seconds used

18.60898780822754
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.56  99.35
places365     68.17  81.15
LSUN          17.19  96.14
iSUN          71.75  81.93
dtd           38.33  91.32
forget        78.50  87.02
AVG           46.08  89.49
Retain-Acc: 0.7397
Forget-as-OOD (retain known vs forget novel):
  FPR: 78.50 AUROC: 87.02 AUIN: 98.38
15.448493719100952
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase1_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase1_rf.png
lambda=1.0 score=-23.0421
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-phase1-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.5, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:17<02:39, 17.71s/it] 20%|██        | 2/10 [00:31<02:05, 15.66s/it] 30%|███       | 3/10 [00:46<01:45, 15.09s/it] 40%|████      | 4/10 [01:00<01:28, 14.77s/it] 50%|█████     | 5/10 [01:14<01:13, 14.62s/it] 60%|██████    | 6/10 [01:29<00:57, 14.45s/it] 70%|███████   | 7/10 [01:43<00:43, 14.50s/it] 80%|████████  | 8/10 [01:58<00:29, 14.58s/it][loss] ep 0 it 0 total=10.3047 mle=1.5709 pcon=5.2950 forget=3.4387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=10.3337 mle=1.5424 pcon=5.2879 forget=3.5033 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=10.4163 mle=1.7004 pcon=5.2809 forget=3.4350 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=10.5976 mle=1.8997 pcon=5.2738 forget=3.4241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=10.4366 mle=1.7131 pcon=5.2670 forget=3.4565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=10.2084 mle=1.5020 pcon=5.2603 forget=3.4460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=10.2685 mle=1.5597 pcon=5.2540 forget=3.4548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=10.4026 mle=1.6809 pcon=5.2476 forget=3.4741 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 1 it 10 total=10.4376 mle=1.6717 pcon=5.2409 forget=3.5250 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=10.2889 mle=1.6369 pcon=5.2346 forget=3.4174 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=10.2185 mle=1.5167 pcon=5.2284 forget=3.4734 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=10.4760 mle=1.7802 pcon=5.2224 forget=3.4733 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=10.4442 mle=1.7645 pcon=5.2167 forget=3.4630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=10.2368 mle=1.6051 pcon=5.2112 forget=3.4204 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=10.3742 mle=1.7235 pcon=5.2056 forget=3.4450 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=10.4594 mle=1.7941 pcon=5.2003 forget=3.4651 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 2 it 20 total=10.1895 mle=1.5529 pcon=5.1950 forget=3.4416 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=10.4716 mle=1.8883 pcon=5.1899 forget=3.3934 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=10.3658 mle=1.7419 pcon=5.1847 forget=3.4392 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=10.1176 mle=1.5264 pcon=5.1796 forget=3.4116 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=10.2244 mle=1.5859 pcon=5.1745 forget=3.4641 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=10.4795 mle=1.8840 pcon=5.1699 forget=3.4257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=10.1501 mle=1.5918 pcon=5.1651 forget=3.3931 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=10.3824 mle=1.8111 pcon=5.1605 forget=3.4108 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 3 it 30 total=10.2867 mle=1.7054 pcon=5.1559 forget=3.4254 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=10.2694 mle=1.6708 pcon=5.1518 forget=3.4469 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=10.3276 mle=1.7669 pcon=5.1474 forget=3.4132 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=10.4704 mle=1.9230 pcon=5.1435 forget=3.4040 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=10.1074 mle=1.5698 pcon=5.1394 forget=3.3981 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=10.3599 mle=1.8357 pcon=5.1352 forget=3.3890 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=10.2708 mle=1.7034 pcon=5.1310 forget=3.4364 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=10.2652 mle=1.6951 pcon=5.1274 forget=3.4426 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 4 it 40 total=10.1304 mle=1.6536 pcon=5.1235 forget=3.3532 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=10.2369 mle=1.7293 pcon=5.1195 forget=3.3881 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=10.1789 mle=1.6509 pcon=5.1158 forget=3.4122 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=9.9773 mle=1.4535 pcon=5.1123 forget=3.4114 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=10.2617 mle=1.7668 pcon=5.1085 forget=3.3864 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=9.9752 mle=1.4910 pcon=5.1049 forget=3.3793 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=10.3800 mle=1.8718 pcon=5.1014 forget=3.4068 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 5 it 0 total=10.3537 mle=1.8670 pcon=5.0980 forget=3.3887 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=10.2539 mle=1.8006 pcon=5.0944 forget=3.3590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=10.0708 mle=1.5854 pcon=5.0911 forget=3.3943 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=10.0721 mle=1.5943 pcon=5.0876 forget=3.3902 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=10.1320 mle=1.6913 pcon=5.0847 forget=3.3561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=10.1976 mle=1.7777 pcon=5.0816 forget=3.3383 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=10.0382 mle=1.5975 pcon=5.0784 forget=3.3623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=10.1814 mle=1.7450 pcon=5.0754 forget=3.3609 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 6 it 10 total=9.8878 mle=1.3990 pcon=5.0725 forget=3.4163 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=9.9058 mle=1.5309 pcon=5.0696 forget=3.3053 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=10.0348 mle=1.5902 pcon=5.0668 forget=3.3779 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=10.0683 mle=1.6676 pcon=5.0638 forget=3.3369 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=10.0214 mle=1.6292 pcon=5.0614 forget=3.3307 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=10.2301 mle=1.8141 pcon=5.0582 forget=3.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=9.9797 mle=1.5467 pcon=5.0556 forget=3.3773 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=10.0579 mle=1.6718 pcon=5.0528 forget=3.3332 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 7 it 20 total=9.9895 mle=1.6304 pcon=5.0498 forget=3.3092 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=10.0893 mle=1.7446 pcon=5.0471 forget=3.2976 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=9.9561 mle=1.6080 pcon=5.0442 forget=3.3039 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=9.8957 mle=1.5601 pcon=5.0414 forget=3.2942 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=10.0128 mle=1.6416 pcon=5.0390 forget=3.3322 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=10.1746 mle=1.7871 pcon=5.0363 forget=3.3512 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=10.1079 mle=1.7319 pcon=5.0334 forget=3.3427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=9.9720 mle=1.6297 pcon=5.0309 forget=3.3115 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
 90%|█████████ | 9/10 [02:12<00:14, 14.55s/it]100%|██████████| 10/10 [02:27<00:00, 14.57s/it]100%|██████████| 10/10 [02:27<00:00, 14.75s/it]
[loss] ep 8 it 30 total=9.9188 mle=1.5421 pcon=5.0284 forget=3.3483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=9.9352 mle=1.5750 pcon=5.0260 forget=3.3342 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=9.8493 mle=1.5033 pcon=5.0234 forget=3.3225 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=10.0494 mle=1.7138 pcon=5.0211 forget=3.3145 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=9.8558 mle=1.5289 pcon=5.0192 forget=3.3076 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=10.0134 mle=1.6959 pcon=5.0169 forget=3.3006 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=10.1237 mle=1.7366 pcon=5.0145 forget=3.3726 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=9.8277 mle=1.4794 pcon=5.0124 forget=3.3359 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 9 it 40 total=9.9538 mle=1.6300 pcon=5.0104 forget=3.3135 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=10.0336 mle=1.6681 pcon=5.0082 forget=3.3573 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=10.2206 mle=1.8095 pcon=5.0067 forget=3.4043 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=9.9541 mle=1.5895 pcon=5.0052 forget=3.3594 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=9.9955 mle=1.5873 pcon=5.0038 forget=3.4044 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=9.9832 mle=1.5610 pcon=5.0022 forget=3.4200 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=9.9685 mle=1.5520 pcon=5.0010 forget=3.4155 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-phase1: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:07,  3.07it/s]  3%|▎         | 11/391 [00:00<00:11, 32.04it/s]  5%|▌         | 21/391 [00:00<00:07, 51.52it/s]  8%|▊         | 31/391 [00:00<00:05, 64.82it/s] 10%|█         | 41/391 [00:00<00:04, 74.23it/s] 13%|█▎        | 51/391 [00:00<00:04, 80.52it/s] 16%|█▌        | 61/391 [00:00<00:03, 85.12it/s] 18%|█▊        | 71/391 [00:01<00:03, 88.17it/s] 21%|██        | 81/391 [00:01<00:03, 90.38it/s] 23%|██▎       | 91/391 [00:01<00:03, 91.79it/s] 26%|██▌       | 101/391 [00:01<00:03, 92.76it/s] 28%|██▊       | 111/391 [00:01<00:02, 93.44it/s] 31%|███       | 121/391 [00:01<00:02, 93.94it/s] 34%|███▎      | 131/391 [00:01<00:02, 94.04it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.35it/s] 39%|███▊      | 151/391 [00:01<00:02, 94.52it/s] 41%|████      | 161/391 [00:02<00:02, 93.67it/s] 44%|████▎     | 171/391 [00:02<00:02, 94.11it/s] 46%|████▋     | 181/391 [00:02<00:02, 94.38it/s] 49%|████▉     | 191/391 [00:02<00:02, 94.36it/s] 51%|█████▏    | 201/391 [00:02<00:02, 94.56it/s] 54%|█████▍    | 211/391 [00:02<00:01, 94.53it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.57it/s] 59%|█████▉    | 231/391 [00:02<00:01, 94.51it/s] 62%|██████▏   | 241/391 [00:02<00:01, 94.83it/s] 64%|██████▍   | 251/391 [00:02<00:01, 94.74it/s] 67%|██████▋   | 261/391 [00:03<00:01, 94.75it/s] 69%|██████▉   | 271/391 [00:03<00:01, 94.76it/s] 72%|███████▏  | 281/391 [00:03<00:01, 93.57it/s] 74%|███████▍  | 291/391 [00:03<00:01, 93.49it/s] 77%|███████▋  | 301/391 [00:03<00:00, 93.84it/s] 80%|███████▉  | 311/391 [00:03<00:00, 92.76it/s] 82%|████████▏ | 321/391 [00:03<00:00, 93.23it/s] 85%|████████▍ | 331/391 [00:03<00:00, 93.63it/s] 87%|████████▋ | 341/391 [00:03<00:00, 93.87it/s] 90%|████████▉ | 351/391 [00:04<00:00, 94.26it/s] 92%|█████████▏| 361/391 [00:04<00:00, 94.43it/s] 95%|█████████▍| 371/391 [00:04<00:00, 94.39it/s] 97%|█████████▋| 381/391 [00:04<00:00, 94.42it/s]100%|██████████| 391/391 [00:04<00:00, 92.21it/s]100%|██████████| 391/391 [00:04<00:00, 87.63it/s]
50000 images processed, 4.563554286956787 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:23,  3.34it/s] 14%|█▍        | 11/79 [00:00<00:02, 33.84it/s] 27%|██▋       | 21/79 [00:00<00:01, 52.59it/s] 39%|███▉      | 31/79 [00:00<00:00, 65.32it/s] 52%|█████▏    | 41/79 [00:00<00:00, 73.89it/s] 65%|██████▍   | 51/79 [00:00<00:00, 79.69it/s] 77%|███████▋  | 61/79 [00:00<00:00, 83.79it/s] 90%|████████▉ | 71/79 [00:01<00:00, 87.36it/s]100%|██████████| 79/79 [00:01<00:00, 69.00it/s]
10000 images processed, 1.166137933731079 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:10,  2.87it/s]  5%|▌         | 11/204 [00:00<00:06, 30.56it/s] 10%|█         | 21/204 [00:00<00:03, 49.56it/s] 15%|█▌        | 31/204 [00:00<00:02, 62.70it/s] 20%|██        | 41/204 [00:00<00:02, 71.92it/s] 25%|██▌       | 51/204 [00:00<00:01, 77.80it/s] 30%|██▉       | 61/204 [00:00<00:01, 82.56it/s] 35%|███▍      | 71/204 [00:01<00:01, 85.73it/s] 40%|███▉      | 81/204 [00:01<00:01, 87.85it/s] 45%|████▍     | 91/204 [00:01<00:01, 89.05it/s] 50%|████▉     | 101/204 [00:01<00:01, 89.87it/s] 54%|█████▍    | 111/204 [00:01<00:01, 90.54it/s] 59%|█████▉    | 121/204 [00:01<00:00, 90.71it/s] 64%|██████▍   | 131/204 [00:01<00:00, 90.25it/s] 69%|██████▉   | 141/204 [00:01<00:00, 90.56it/s] 74%|███████▍  | 151/204 [00:01<00:00, 90.37it/s] 79%|███████▉  | 161/204 [00:02<00:00, 90.75it/s] 84%|████████▍ | 171/204 [00:02<00:00, 91.34it/s] 89%|████████▊ | 181/204 [00:02<00:00, 91.66it/s] 94%|█████████▎| 191/204 [00:02<00:00, 91.77it/s] 99%|█████████▊| 201/204 [00:02<00:00, 92.95it/s]100%|██████████| 204/204 [00:02<00:00, 79.87it/s]
26032 images processed, 2.595611333847046 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:38,  2.03it/s] 11%|█▏        | 9/79 [00:00<00:04, 17.27it/s] 22%|██▏       | 17/79 [00:00<00:02, 27.27it/s] 32%|███▏      | 25/79 [00:00<00:01, 34.08it/s] 42%|████▏     | 33/79 [00:01<00:01, 38.46it/s] 52%|█████▏    | 41/79 [00:01<00:00, 41.73it/s] 61%|██████    | 48/79 [00:01<00:00, 47.34it/s] 68%|██████▊   | 54/79 [00:01<00:00, 44.74it/s] 75%|███████▍  | 59/79 [00:01<00:00, 45.89it/s] 82%|████████▏ | 65/79 [00:01<00:00, 45.00it/s] 89%|████████▊ | 70/79 [00:01<00:00, 45.15it/s] 96%|█████████▌| 76/79 [00:02<00:00, 43.33it/s]100%|██████████| 79/79 [00:02<00:00, 37.82it/s]
10000 images processed, 2.121164560317993 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:26,  2.89it/s] 14%|█▍        | 11/79 [00:00<00:02, 30.45it/s] 27%|██▋       | 21/79 [00:00<00:01, 49.72it/s] 39%|███▉      | 31/79 [00:00<00:00, 63.24it/s] 52%|█████▏    | 41/79 [00:00<00:00, 72.62it/s] 65%|██████▍   | 51/79 [00:00<00:00, 79.27it/s] 77%|███████▋  | 61/79 [00:00<00:00, 83.95it/s] 90%|████████▉ | 71/79 [00:01<00:00, 87.53it/s]100%|██████████| 79/79 [00:01<00:00, 67.87it/s]
10000 images processed, 1.1840779781341553 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:25,  2.70it/s] 16%|█▌        | 11/70 [00:00<00:02, 29.34it/s] 30%|███       | 21/70 [00:00<00:01, 48.50it/s] 44%|████▍     | 31/70 [00:00<00:00, 62.09it/s] 59%|█████▊    | 41/70 [00:00<00:00, 71.71it/s] 73%|███████▎  | 51/70 [00:00<00:00, 78.53it/s] 87%|████████▋ | 61/70 [00:01<00:00, 83.60it/s]100%|██████████| 70/70 [00:01<00:00, 63.45it/s]
8925 images processed, 1.133779525756836 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:39,  1.13it/s]  4%|▍         | 2/45 [00:01<00:18,  2.28it/s] 20%|██        | 9/45 [00:01<00:03,  9.51it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.20it/s] 36%|███▌      | 16/45 [00:01<00:01, 15.09it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.34it/s] 49%|████▉     | 22/45 [00:02<00:01, 13.10it/s] 56%|█████▌    | 25/45 [00:02<00:01, 13.17it/s] 60%|██████    | 27/45 [00:02<00:01, 12.20it/s] 67%|██████▋   | 30/45 [00:02<00:01, 13.19it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.27it/s] 78%|███████▊  | 35/45 [00:03<00:00, 10.76it/s] 91%|█████████ | 41/45 [00:03<00:00, 13.84it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.65it/s]100%|██████████| 45/45 [00:04<00:00, 10.59it/s]
5640 images processed, 4.2735559940338135 seconds used

18.791834115982056
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.91  81.20
LSUN          17.27  96.12
iSUN          72.13  81.76
dtd           37.82  91.37
forget        79.30  86.89
AVG           46.16  89.45
Retain-Acc: 0.7410
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.30 AUROC: 86.89 AUIN: 98.36
24.359310388565063
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-phase1_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-phase1_rf.png
lambda=0.5 score=-23.0791
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-phase1-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.2, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:17<02:40, 17.84s/it] 20%|██        | 2/10 [00:32<02:08, 16.00s/it] 30%|███       | 3/10 [00:46<01:46, 15.26s/it] 40%|████      | 4/10 [01:01<01:29, 14.91s/it] 50%|█████     | 5/10 [01:16<01:14, 14.84s/it] 60%|██████    | 6/10 [01:30<00:59, 14.81s/it] 70%|███████   | 7/10 [01:45<00:44, 14.74s/it] 80%|████████  | 8/10 [01:59<00:29, 14.62s/it][loss] ep 0 it 0 total=8.2415 mle=1.5709 pcon=5.2950 forget=1.3755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=8.2318 mle=1.5425 pcon=5.2879 forget=1.4013 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=8.3552 mle=1.7003 pcon=5.2809 forget=1.3740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8.5434 mle=1.8999 pcon=5.2738 forget=1.3696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=8.3627 mle=1.7131 pcon=5.2670 forget=1.3826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=8.1408 mle=1.5021 pcon=5.2603 forget=1.3784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=8.1956 mle=1.5596 pcon=5.2541 forget=1.3819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=8.3183 mle=1.6810 pcon=5.2476 forget=1.3896 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 1 it 10 total=8.3226 mle=1.6717 pcon=5.2409 forget=1.4100 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=8.2386 mle=1.6370 pcon=5.2346 forget=1.3670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=8.1345 mle=1.5166 pcon=5.2284 forget=1.3895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=8.3920 mle=1.7802 pcon=5.2224 forget=1.3894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=8.3663 mle=1.7644 pcon=5.2167 forget=1.3853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=8.1849 mle=1.6054 pcon=5.2112 forget=1.3683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=8.3076 mle=1.7238 pcon=5.2056 forget=1.3782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=8.3807 mle=1.7943 pcon=5.2003 forget=1.3861 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 2 it 20 total=8.1247 mle=1.5529 pcon=5.1950 forget=1.3768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=8.4354 mle=1.8879 pcon=5.1899 forget=1.3576 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=8.3023 mle=1.7418 pcon=5.1847 forget=1.3758 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=8.0708 mle=1.5264 pcon=5.1796 forget=1.3648 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=8.1462 mle=1.5858 pcon=5.1745 forget=1.3858 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=8.4244 mle=1.8839 pcon=5.1699 forget=1.3706 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=8.1150 mle=1.5923 pcon=5.1652 forget=1.3575 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=8.3363 mle=1.8111 pcon=5.1605 forget=1.3647 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 3 it 30 total=8.2324 mle=1.7058 pcon=5.1559 forget=1.3706 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=8.2021 mle=1.6711 pcon=5.1519 forget=1.3791 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=8.2801 mle=1.7670 pcon=5.1475 forget=1.3657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=8.4237 mle=1.9181 pcon=5.1435 forget=1.3621 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=8.0692 mle=1.5700 pcon=5.1395 forget=1.3597 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=8.3271 mle=1.8357 pcon=5.1353 forget=1.3560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=8.2095 mle=1.7032 pcon=5.1311 forget=1.3752 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=8.2005 mle=1.6951 pcon=5.1275 forget=1.3780 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 4 it 40 total=8.1192 mle=1.6536 pcon=5.1236 forget=1.3420 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=8.2050 mle=1.7294 pcon=5.1196 forget=1.3560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8.1324 mle=1.6506 pcon=5.1159 forget=1.3659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.9313 mle=1.4533 pcon=5.1125 forget=1.3655 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=8.2315 mle=1.7669 pcon=5.1086 forget=1.3561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.9494 mle=1.4913 pcon=5.1050 forget=1.3531 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=8.3376 mle=1.8714 pcon=5.1016 forget=1.3647 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 5 it 0 total=8.3221 mle=1.8669 pcon=5.0982 forget=1.3570 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=8.2400 mle=1.7998 pcon=5.0946 forget=1.3456 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=8.0354 mle=1.5842 pcon=5.0914 forget=1.3598 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=8.0412 mle=1.5949 pcon=5.0879 forget=1.3584 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=8.1208 mle=1.6906 pcon=5.0850 forget=1.3452 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=8.1942 mle=1.7737 pcon=5.0820 forget=1.3384 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=8.0256 mle=1.5964 pcon=5.0789 forget=1.3503 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=8.1710 mle=1.7467 pcon=5.0760 forget=1.3483 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 6 it 10 total=7.8448 mle=1.4002 pcon=5.0731 forget=1.3715 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.9327 mle=1.5344 pcon=5.0703 forget=1.3280 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=8.0193 mle=1.5912 pcon=5.0676 forget=1.3605 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=8.0751 mle=1.6654 pcon=5.0649 forget=1.3448 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=8.0327 mle=1.6293 pcon=5.0626 forget=1.3408 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=8.2357 mle=1.8211 pcon=5.0597 forget=1.3549 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.9564 mle=1.5364 pcon=5.0573 forget=1.3627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=8.0749 mle=1.6739 pcon=5.0547 forget=1.3463 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 7 it 20 total=8.0211 mle=1.6310 pcon=5.0521 forget=1.3381 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=8.1262 mle=1.7429 pcon=5.0497 forget=1.3336 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=8.0118 mle=1.6248 pcon=5.0470 forget=1.3399 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.9521 mle=1.5800 pcon=5.0446 forget=1.3275 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=8.0429 mle=1.6503 pcon=5.0426 forget=1.3500 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=8.1941 mle=1.7947 pcon=5.0404 forget=1.3590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=8.1350 mle=1.7407 pcon=5.0378 forget=1.3565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=8.0216 mle=1.6429 pcon=5.0357 forget=1.3430 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 8 it 30 total=7.9265 mle=1.5365 pcon=5.0337 forget=1.3562 nr=64 nf=64 protos=540 fproto_sim=NA
 90%|█████████ | 9/10 [02:14<00:14, 14.51s/it]100%|██████████| 10/10 [02:28<00:00, 14.50s/it]100%|██████████| 10/10 [02:28<00:00, 14.85s/it]
[loss] ep 8 it 80 total=7.9650 mle=1.5882 pcon=5.0316 forget=1.3452 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.8853 mle=1.5045 pcon=5.0294 forget=1.3514 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=8.0653 mle=1.7018 pcon=5.0274 forget=1.3362 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.8904 mle=1.5303 pcon=5.0257 forget=1.3344 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=8.0252 mle=1.6769 pcon=5.0236 forget=1.3247 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=8.1358 mle=1.7547 pcon=5.0213 forget=1.3598 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.8295 mle=1.4832 pcon=5.0192 forget=1.3270 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 9 it 40 total=7.9582 mle=1.6320 pcon=5.0171 forget=1.3091 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=8.0126 mle=1.6676 pcon=5.0148 forget=1.3302 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=8.1877 mle=1.8314 pcon=5.0131 forget=1.3431 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.9054 mle=1.5696 pcon=5.0112 forget=1.3246 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.9098 mle=1.5694 pcon=5.0094 forget=1.3310 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.9041 mle=1.5625 pcon=5.0072 forget=1.3344 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.8948 mle=1.5646 pcon=5.0054 forget=1.3248 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-phase1: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:56,  3.35it/s]  3%|▎         | 11/391 [00:00<00:11, 33.79it/s]  5%|▌         | 21/391 [00:00<00:06, 53.21it/s]  8%|▊         | 31/391 [00:00<00:05, 65.94it/s] 10%|█         | 41/391 [00:00<00:04, 74.46it/s] 13%|█▎        | 51/391 [00:00<00:04, 80.23it/s] 16%|█▌        | 61/391 [00:00<00:03, 84.39it/s] 18%|█▊        | 71/391 [00:01<00:03, 87.32it/s] 21%|██        | 81/391 [00:01<00:03, 88.97it/s] 23%|██▎       | 91/391 [00:01<00:03, 90.39it/s] 26%|██▌       | 101/391 [00:01<00:03, 91.29it/s] 28%|██▊       | 111/391 [00:01<00:03, 92.04it/s] 31%|███       | 121/391 [00:01<00:02, 92.55it/s] 34%|███▎      | 131/391 [00:01<00:02, 92.95it/s] 36%|███▌      | 141/391 [00:01<00:02, 93.05it/s] 39%|███▊      | 151/391 [00:01<00:02, 93.49it/s] 41%|████      | 161/391 [00:02<00:02, 93.88it/s] 44%|████▎     | 171/391 [00:02<00:02, 94.11it/s] 46%|████▋     | 181/391 [00:02<00:02, 94.03it/s] 49%|████▉     | 191/391 [00:02<00:02, 94.52it/s] 51%|█████▏    | 201/391 [00:02<00:02, 93.79it/s] 54%|█████▍    | 211/391 [00:02<00:01, 94.44it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.68it/s] 59%|█████▉    | 231/391 [00:02<00:01, 94.85it/s] 62%|██████▏   | 241/391 [00:02<00:01, 95.17it/s] 64%|██████▍   | 251/391 [00:02<00:01, 95.30it/s] 67%|██████▋   | 261/391 [00:03<00:01, 95.42it/s] 69%|██████▉   | 271/391 [00:03<00:01, 94.66it/s] 72%|███████▏  | 281/391 [00:03<00:01, 94.74it/s] 74%|███████▍  | 291/391 [00:03<00:01, 94.87it/s] 77%|███████▋  | 301/391 [00:03<00:00, 94.30it/s] 80%|███████▉  | 311/391 [00:03<00:00, 94.25it/s] 82%|████████▏ | 321/391 [00:03<00:00, 94.24it/s] 85%|████████▍ | 331/391 [00:03<00:00, 94.28it/s] 87%|████████▋ | 341/391 [00:03<00:00, 94.42it/s] 90%|████████▉ | 351/391 [00:04<00:00, 94.84it/s] 92%|█████████▏| 361/391 [00:04<00:00, 94.19it/s] 95%|█████████▍| 371/391 [00:04<00:00, 94.50it/s] 97%|█████████▋| 381/391 [00:04<00:00, 94.82it/s]100%|██████████| 391/391 [00:04<00:00, 93.55it/s]100%|██████████| 391/391 [00:04<00:00, 88.01it/s]
50000 images processed, 4.508934259414673 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:16,  4.84it/s] 14%|█▍        | 11/79 [00:00<00:01, 42.66it/s] 27%|██▋       | 21/79 [00:00<00:00, 61.74it/s] 39%|███▉      | 31/79 [00:00<00:00, 72.43it/s] 52%|█████▏    | 41/79 [00:00<00:00, 79.03it/s] 65%|██████▍   | 51/79 [00:00<00:00, 83.24it/s] 77%|███████▋  | 61/79 [00:00<00:00, 86.34it/s] 90%|████████▉ | 71/79 [00:00<00:00, 88.91it/s]100%|██████████| 79/79 [00:01<00:00, 75.05it/s]
10000 images processed, 1.0726251602172852 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:02,  3.26it/s]  5%|▌         | 11/204 [00:00<00:05, 32.98it/s] 10%|█         | 21/204 [00:00<00:03, 52.30it/s] 15%|█▌        | 31/204 [00:00<00:02, 65.02it/s] 20%|██        | 41/204 [00:00<00:02, 73.79it/s] 25%|██▌       | 51/204 [00:00<00:01, 79.76it/s] 30%|██▉       | 61/204 [00:00<00:01, 83.70it/s] 35%|███▍      | 71/204 [00:01<00:01, 86.11it/s] 40%|███▉      | 81/204 [00:01<00:01, 88.30it/s] 45%|████▍     | 91/204 [00:01<00:01, 88.95it/s] 50%|████▉     | 101/204 [00:01<00:01, 90.09it/s] 54%|█████▍    | 111/204 [00:01<00:01, 90.66it/s] 59%|█████▉    | 121/204 [00:01<00:00, 91.00it/s] 64%|██████▍   | 131/204 [00:01<00:00, 91.09it/s] 69%|██████▉   | 141/204 [00:01<00:00, 91.77it/s] 74%|███████▍  | 151/204 [00:01<00:00, 91.58it/s] 79%|███████▉  | 161/204 [00:02<00:00, 92.07it/s] 84%|████████▍ | 171/204 [00:02<00:00, 92.66it/s] 89%|████████▊ | 181/204 [00:02<00:00, 92.78it/s] 94%|█████████▎| 191/204 [00:02<00:00, 92.91it/s] 99%|█████████▊| 201/204 [00:02<00:00, 93.84it/s]100%|██████████| 204/204 [00:02<00:00, 81.51it/s]
26032 images processed, 2.546053647994995 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:35,  2.21it/s] 11%|█▏        | 9/79 [00:00<00:03, 18.29it/s] 22%|██▏       | 17/79 [00:00<00:02, 28.45it/s] 29%|██▉       | 23/79 [00:00<00:01, 34.75it/s] 35%|███▌      | 28/79 [00:01<00:01, 33.73it/s] 43%|████▎     | 34/79 [00:01<00:01, 34.37it/s] 53%|█████▎    | 42/79 [00:01<00:00, 37.46it/s] 63%|██████▎   | 50/79 [00:01<00:00, 39.62it/s] 73%|███████▎  | 58/79 [00:01<00:00, 39.42it/s] 84%|████████▎ | 66/79 [00:01<00:00, 39.17it/s] 94%|█████████▎| 74/79 [00:02<00:00, 41.72it/s]100%|██████████| 79/79 [00:02<00:00, 35.94it/s]
10000 images processed, 2.235353708267212 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.02it/s] 14%|█▍        | 11/79 [00:00<00:02, 31.63it/s] 27%|██▋       | 21/79 [00:00<00:01, 51.11it/s] 39%|███▉      | 31/79 [00:00<00:00, 64.23it/s] 52%|█████▏    | 41/79 [00:00<00:00, 73.56it/s] 65%|██████▍   | 51/79 [00:00<00:00, 79.97it/s] 77%|███████▋  | 61/79 [00:00<00:00, 84.43it/s] 90%|████████▉ | 71/79 [00:01<00:00, 87.88it/s]100%|██████████| 79/79 [00:01<00:00, 68.89it/s]
10000 images processed, 1.166161298751831 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:25,  2.70it/s] 16%|█▌        | 11/70 [00:00<00:02, 29.31it/s] 30%|███       | 21/70 [00:00<00:01, 48.44it/s] 44%|████▍     | 31/70 [00:00<00:00, 62.04it/s] 59%|█████▊    | 41/70 [00:00<00:00, 71.73it/s] 73%|███████▎  | 51/70 [00:00<00:00, 78.59it/s] 87%|████████▋ | 61/70 [00:01<00:00, 83.66it/s]100%|██████████| 70/70 [00:01<00:00, 63.43it/s]
8925 images processed, 1.133544683456421 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:40,  1.09it/s]  7%|▋         | 3/45 [00:01<00:11,  3.65it/s] 20%|██        | 9/45 [00:01<00:04,  8.99it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.26it/s] 31%|███       | 14/45 [00:01<00:02, 12.17it/s] 38%|███▊      | 17/45 [00:01<00:02, 11.56it/s] 42%|████▏     | 19/45 [00:02<00:02, 12.54it/s] 49%|████▉     | 22/45 [00:02<00:01, 13.48it/s] 56%|█████▌    | 25/45 [00:02<00:01, 13.11it/s] 60%|██████    | 27/45 [00:02<00:01, 13.38it/s] 67%|██████▋   | 30/45 [00:02<00:01, 13.05it/s] 73%|███████▎  | 33/45 [00:03<00:01, 10.86it/s] 78%|███████▊  | 35/45 [00:03<00:00, 11.74it/s] 89%|████████▉ | 40/45 [00:03<00:00, 17.58it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.86it/s]100%|██████████| 45/45 [00:04<00:00, 10.75it/s]
5640 images processed, 4.207908868789673 seconds used

18.554105520248413
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     68.02  81.18
LSUN          17.30  96.13
iSUN          72.29  81.68
dtd           37.96  91.36
forget        79.40  86.87
AVG           46.25  89.43
Retain-Acc: 0.7412
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.40 AUROC: 86.87 AUIN: 98.36
11.3393235206604
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-phase1_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-phase1_rf.png
lambda=0.2 score=-23.1247
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-phase1-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.1, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:19<02:51, 19.02s/it] 20%|██        | 2/10 [00:33<02:11, 16.45s/it] 30%|███       | 3/10 [00:48<01:50, 15.85s/it] 40%|████      | 4/10 [01:04<01:34, 15.68s/it] 50%|█████     | 5/10 [01:19<01:16, 15.39s/it] 60%|██████    | 6/10 [01:34<01:01, 15.34s/it] 70%|███████   | 7/10 [01:49<00:45, 15.20s/it] 80%|████████  | 8/10 [02:03<00:30, 15.01s/it][loss] ep 0 it 0 total=7.5538 mle=1.5710 pcon=5.2950 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=7.5309 mle=1.5423 pcon=5.2879 forget=0.7007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=7.6683 mle=1.7004 pcon=5.2809 forget=0.6870 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=7.8585 mle=1.8998 pcon=5.2738 forget=0.6848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=7.6712 mle=1.7129 pcon=5.2670 forget=0.6913 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=7.4516 mle=1.5020 pcon=5.2603 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=7.5046 mle=1.5596 pcon=5.2540 forget=0.6910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=7.6234 mle=1.6810 pcon=5.2476 forget=0.6948 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 1 it 10 total=7.6174 mle=1.6716 pcon=5.2409 forget=0.7050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=7.5552 mle=1.6371 pcon=5.2346 forget=0.6835 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=7.4397 mle=1.5165 pcon=5.2284 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=7.6973 mle=1.7802 pcon=5.2224 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=7.6736 mle=1.7643 pcon=5.2167 forget=0.6926 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=7.5008 mle=1.6055 pcon=5.2112 forget=0.6841 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=7.6186 mle=1.7239 pcon=5.2056 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=7.6877 mle=1.7943 pcon=5.2003 forget=0.6931 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 2 it 20 total=7.4362 mle=1.5528 pcon=5.1950 forget=0.6884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=7.7570 mle=1.8883 pcon=5.1899 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=7.6142 mle=1.7416 pcon=5.1847 forget=0.6879 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=7.3887 mle=1.5266 pcon=5.1796 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=7.4533 mle=1.5858 pcon=5.1745 forget=0.6930 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=7.7392 mle=1.8840 pcon=5.1699 forget=0.6853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=7.4362 mle=1.5922 pcon=5.1652 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=7.6540 mle=1.8111 pcon=5.1605 forget=0.6824 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 3 it 30 total=7.5472 mle=1.7059 pcon=5.1560 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=7.5123 mle=1.6708 pcon=5.1519 forget=0.6896 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=7.5973 mle=1.7669 pcon=5.1475 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=7.7426 mle=1.9180 pcon=5.1435 forget=0.6811 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=7.3893 mle=1.5699 pcon=5.1395 forget=0.6799 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=7.6491 mle=1.8357 pcon=5.1353 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=7.5217 mle=1.7029 pcon=5.1311 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=7.5118 mle=1.6952 pcon=5.1275 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 4 it 40 total=7.4482 mle=1.6535 pcon=5.1236 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=7.5270 mle=1.7293 pcon=5.1196 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=7.4492 mle=1.6502 pcon=5.1160 forget=0.6831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.2485 mle=1.4532 pcon=5.1125 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=7.5534 mle=1.7666 pcon=5.1086 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.2731 mle=1.4914 pcon=5.1051 forget=0.6767 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=7.6553 mle=1.8712 pcon=5.1016 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 5 it 0 total=7.6439 mle=1.8670 pcon=5.0983 forget=0.6787 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=7.5676 mle=1.8000 pcon=5.0947 forget=0.6730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=7.3557 mle=1.5842 pcon=5.0915 forget=0.6801 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=7.3622 mle=1.5948 pcon=5.0880 forget=0.6794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=7.4484 mle=1.6904 pcon=5.0851 forget=0.6728 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=7.5282 mle=1.7767 pcon=5.0821 forget=0.6694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=7.3508 mle=1.5963 pcon=5.0790 forget=0.6755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=7.4970 mle=1.7465 pcon=5.0761 forget=0.6744 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 6 it 10 total=7.1595 mle=1.4002 pcon=5.0732 forget=0.6860 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.2697 mle=1.5349 pcon=5.0705 forget=0.6643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=7.3399 mle=1.5915 pcon=5.0678 forget=0.6807 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=7.4031 mle=1.6652 pcon=5.0650 forget=0.6729 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=7.3627 mle=1.6292 pcon=5.0627 forget=0.6708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=7.5592 mle=1.8215 pcon=5.0599 forget=0.6779 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.2747 mle=1.5354 pcon=5.0574 forget=0.6818 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=7.4023 mle=1.6737 pcon=5.0549 forget=0.6737 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 7 it 20 total=7.3530 mle=1.6311 pcon=5.0523 forget=0.6697 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=7.4611 mle=1.7437 pcon=5.0499 forget=0.6675 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=7.3440 mle=1.6258 pcon=5.0473 forget=0.6709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.2905 mle=1.5812 pcon=5.0449 forget=0.6643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=7.3702 mle=1.6514 pcon=5.0429 forget=0.6759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=7.5162 mle=1.7948 pcon=5.0407 forget=0.6807 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=7.4581 mle=1.7403 pcon=5.0383 forget=0.6796 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=7.3483 mle=1.6392 pcon=5.0362 forget=0.6729 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 8 it 30 total=7.2502 mle=1.5361 pcon=5.0342 forget=0.6799 nr=64 nf=64 protos=540 fproto_sim=NA
 90%|█████████ | 9/10 [02:17<00:14, 14.72s/it]100%|██████████| 10/10 [02:32<00:00, 14.64s/it]100%|██████████| 10/10 [02:32<00:00, 15.24s/it]
[loss] ep 8 it 80 total=7.2957 mle=1.5893 pcon=5.0322 forget=0.6741 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.2134 mle=1.5051 pcon=5.0300 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=7.4008 mle=1.7023 pcon=5.0281 forget=0.6704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.2264 mle=1.5303 pcon=5.0265 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=7.3664 mle=1.6770 pcon=5.0245 forget=0.6650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=7.4606 mle=1.7552 pcon=5.0223 forget=0.6831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.1723 mle=1.4857 pcon=5.0204 forget=0.6662 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
[loss] ep 9 it 40 total=7.3096 mle=1.6341 pcon=5.0184 forget=0.6571 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=7.3527 mle=1.6688 pcon=5.0162 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=7.5278 mle=1.8379 pcon=5.0146 forget=0.6752 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.2519 mle=1.5712 pcon=5.0130 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.2523 mle=1.5714 pcon=5.0113 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.2465 mle=1.5646 pcon=5.0094 forget=0.6726 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.2359 mle=1.5607 pcon=5.0078 forget=0.6675 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-phase1-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-phase1: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:52,  3.46it/s]  3%|▎         | 11/391 [00:00<00:10, 34.78it/s]  5%|▌         | 21/391 [00:00<00:06, 54.51it/s]  8%|▊         | 31/391 [00:00<00:05, 67.15it/s] 10%|█         | 41/391 [00:00<00:04, 75.59it/s] 13%|█▎        | 51/391 [00:00<00:04, 81.40it/s] 16%|█▌        | 61/391 [00:00<00:03, 85.69it/s] 18%|█▊        | 71/391 [00:01<00:03, 88.50it/s] 21%|██        | 81/391 [00:01<00:03, 90.62it/s] 23%|██▎       | 91/391 [00:01<00:03, 92.03it/s] 26%|██▌       | 101/391 [00:01<00:03, 92.98it/s] 28%|██▊       | 111/391 [00:01<00:02, 93.64it/s] 31%|███       | 121/391 [00:01<00:02, 93.86it/s] 34%|███▎      | 131/391 [00:01<00:02, 94.26it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.54it/s] 39%|███▊      | 151/391 [00:01<00:02, 94.79it/s] 41%|████      | 161/391 [00:01<00:02, 94.95it/s] 44%|████▎     | 171/391 [00:02<00:02, 95.04it/s] 46%|████▋     | 181/391 [00:02<00:02, 95.05it/s] 49%|████▉     | 191/391 [00:02<00:02, 95.10it/s] 51%|█████▏    | 201/391 [00:02<00:01, 95.35it/s] 54%|█████▍    | 211/391 [00:02<00:01, 95.26it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.93it/s] 59%|█████▉    | 231/391 [00:02<00:01, 95.07it/s] 62%|██████▏   | 241/391 [00:02<00:01, 95.13it/s] 64%|██████▍   | 251/391 [00:02<00:01, 95.19it/s] 67%|██████▋   | 261/391 [00:03<00:01, 94.41it/s] 69%|██████▉   | 271/391 [00:03<00:01, 94.76it/s] 72%|███████▏  | 281/391 [00:03<00:01, 94.90it/s] 74%|███████▍  | 291/391 [00:03<00:01, 95.06it/s] 77%|███████▋  | 301/391 [00:03<00:00, 95.29it/s] 80%|███████▉  | 311/391 [00:03<00:00, 95.40it/s] 82%|████████▏ | 321/391 [00:03<00:00, 95.35it/s] 85%|████████▍ | 331/391 [00:03<00:00, 95.54it/s] 87%|████████▋ | 341/391 [00:03<00:00, 94.78it/s] 90%|████████▉ | 351/391 [00:03<00:00, 95.08it/s] 92%|█████████▏| 361/391 [00:04<00:00, 95.17it/s] 95%|█████████▍| 371/391 [00:04<00:00, 94.88it/s] 97%|█████████▋| 381/391 [00:04<00:00, 94.91it/s]100%|██████████| 391/391 [00:04<00:00, 93.58it/s]100%|██████████| 391/391 [00:04<00:00, 88.88it/s]
50000 images processed, 4.461978912353516 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:17,  4.41it/s] 14%|█▍        | 11/79 [00:00<00:01, 40.34it/s] 27%|██▋       | 21/79 [00:00<00:00, 59.76it/s] 39%|███▉      | 31/79 [00:00<00:00, 71.22it/s] 52%|█████▏    | 41/79 [00:00<00:00, 78.34it/s] 65%|██████▍   | 51/79 [00:00<00:00, 83.12it/s] 77%|███████▋  | 61/79 [00:00<00:00, 86.55it/s] 90%|████████▉ | 71/79 [00:00<00:00, 89.46it/s]100%|██████████| 79/79 [00:01<00:00, 74.45it/s]
10000 images processed, 1.0796642303466797 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<00:52,  3.85it/s]  5%|▌         | 11/204 [00:00<00:05, 36.68it/s] 10%|▉         | 20/204 [00:00<00:03, 54.12it/s] 15%|█▍        | 30/204 [00:00<00:02, 66.76it/s] 20%|█▉        | 40/204 [00:00<00:02, 75.01it/s] 25%|██▍       | 50/204 [00:00<00:01, 80.32it/s] 29%|██▉       | 60/204 [00:00<00:01, 83.95it/s] 34%|███▍      | 70/204 [00:01<00:01, 86.65it/s] 39%|███▉      | 80/204 [00:01<00:01, 88.29it/s] 44%|████▍     | 90/204 [00:01<00:01, 89.56it/s] 49%|████▉     | 100/204 [00:01<00:01, 90.27it/s] 54%|█████▍    | 110/204 [00:01<00:01, 90.88it/s] 59%|█████▉    | 120/204 [00:01<00:00, 91.18it/s] 64%|██████▎   | 130/204 [00:01<00:00, 91.41it/s] 69%|██████▊   | 140/204 [00:01<00:00, 91.90it/s] 74%|███████▎  | 150/204 [00:01<00:00, 92.32it/s] 78%|███████▊  | 160/204 [00:01<00:00, 91.64it/s] 83%|████████▎ | 170/204 [00:02<00:00, 91.76it/s] 88%|████████▊ | 180/204 [00:02<00:00, 91.91it/s] 93%|█████████▎| 190/204 [00:02<00:00, 91.95it/s] 98%|█████████▊| 200/204 [00:02<00:00, 93.22it/s]100%|██████████| 204/204 [00:02<00:00, 82.63it/s]
26032 images processed, 2.511232376098633 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:35,  2.19it/s] 11%|█▏        | 9/79 [00:00<00:03, 18.45it/s] 22%|██▏       | 17/79 [00:00<00:02, 26.56it/s] 32%|███▏      | 25/79 [00:01<00:01, 31.13it/s] 42%|████▏     | 33/79 [00:01<00:01, 33.88it/s] 52%|█████▏    | 41/79 [00:01<00:01, 35.77it/s] 62%|██████▏   | 49/79 [00:01<00:00, 37.27it/s] 72%|███████▏  | 57/79 [00:01<00:00, 41.06it/s] 82%|████████▏ | 65/79 [00:01<00:00, 44.14it/s] 92%|█████████▏| 73/79 [00:02<00:00, 46.58it/s]100%|██████████| 79/79 [00:02<00:00, 37.15it/s]
10000 images processed, 2.1592111587524414 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.11it/s] 14%|█▍        | 11/79 [00:00<00:02, 32.33it/s] 27%|██▋       | 21/79 [00:00<00:01, 51.99it/s] 39%|███▉      | 31/79 [00:00<00:00, 65.33it/s] 52%|█████▏    | 41/79 [00:00<00:00, 74.51it/s] 65%|██████▍   | 51/79 [00:00<00:00, 80.92it/s] 77%|███████▋  | 61/79 [00:00<00:00, 85.33it/s] 90%|████████▉ | 71/79 [00:01<00:00, 88.63it/s]100%|██████████| 79/79 [00:01<00:00, 69.87it/s]
10000 images processed, 1.148160696029663 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:23,  2.89it/s] 16%|█▌        | 11/70 [00:00<00:01, 30.74it/s] 30%|███       | 21/70 [00:00<00:00, 50.09it/s] 44%|████▍     | 31/70 [00:00<00:00, 63.63it/s] 59%|█████▊    | 41/70 [00:00<00:00, 73.05it/s] 73%|███████▎  | 51/70 [00:00<00:00, 79.69it/s] 87%|████████▋ | 61/70 [00:00<00:00, 84.52it/s]100%|██████████| 70/70 [00:01<00:00, 65.01it/s]
8925 images processed, 1.1044807434082031 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:38,  1.14it/s]  4%|▍         | 2/45 [00:00<00:18,  2.31it/s] 20%|██        | 9/45 [00:01<00:03,  9.61it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.32it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.22it/s] 42%|████▏     | 19/45 [00:02<00:02, 12.14it/s] 51%|█████     | 23/45 [00:02<00:01, 15.65it/s] 56%|█████▌    | 25/45 [00:02<00:01, 13.00it/s] 60%|██████    | 27/45 [00:02<00:01, 12.51it/s] 69%|██████▉   | 31/45 [00:02<00:00, 15.62it/s] 73%|███████▎  | 33/45 [00:03<00:01, 10.52it/s] 78%|███████▊  | 35/45 [00:03<00:00, 10.75it/s] 91%|█████████ | 41/45 [00:03<00:00, 13.21it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.84it/s]100%|██████████| 45/45 [00:04<00:00, 10.73it/s]
5640 images processed, 4.213033199310303 seconds used

18.335070848464966
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.95  81.18
LSUN          17.28  96.12
iSUN          72.25  81.65
dtd           37.80  91.36
forget        79.40  86.86
AVG           46.20  89.42
Retain-Acc: 0.7412
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.40 AUROC: 86.86 AUIN: 98.36
10.336272954940796
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-phase1_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-phase1_rf.png
lambda=0.1 score=-23.0998
[Phase 1] best lambda= (score=-1)
[Phase 2] Searching lr over: 0.001 0.0005 0.0001
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:18<02:42, 18.09s/it] 20%|██        | 2/10 [00:32<02:08, 16.05s/it] 30%|███       | 3/10 [00:47<01:47, 15.39s/it] 40%|████      | 4/10 [01:01<01:29, 14.94s/it] 50%|█████     | 5/10 [01:15<01:13, 14.67s/it] 60%|██████    | 6/10 [01:29<00:58, 14.51s/it] 70%|███████   | 7/10 [01:44<00:43, 14.40s/it] 80%|████████  | 8/10 [01:58<00:28, 14.31s/it][loss] ep 0 it 0 total=13.7434 mle=1.5709 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8369 mle=1.5424 pcon=5.2879 forget=7.0066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8513 mle=1.7005 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0218 mle=1.8998 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8932 mle=1.7132 pcon=5.2670 forget=6.9130 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6542 mle=1.5019 pcon=5.2603 forget=6.8920 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7232 mle=1.5597 pcon=5.2540 forget=6.9095 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8764 mle=1.6809 pcon=5.2476 forget=6.9479 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 1 it 10 total=13.9619 mle=1.6715 pcon=5.2409 forget=7.0496 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7061 mle=1.6371 pcon=5.2346 forget=6.8345 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6915 mle=1.5166 pcon=5.2284 forget=6.9465 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9482 mle=1.7802 pcon=5.2224 forget=6.9456 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9066 mle=1.7647 pcon=5.2167 forget=6.9252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6560 mle=1.6048 pcon=5.2112 forget=6.8400 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8180 mle=1.7236 pcon=5.2056 forget=6.8888 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9235 mle=1.7942 pcon=5.2003 forget=6.9290 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 2 it 20 total=13.6298 mle=1.5530 pcon=5.1950 forget=6.8818 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8628 mle=1.8882 pcon=5.1899 forget=6.7847 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8038 mle=1.7421 pcon=5.1846 forget=6.8771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5267 mle=1.5262 pcon=5.1796 forget=6.8210 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6858 mle=1.5856 pcon=5.1745 forget=6.9256 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9022 mle=1.8841 pcon=5.1698 forget=6.8483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5395 mle=1.5913 pcon=5.1651 forget=6.7831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7883 mle=1.8109 pcon=5.1604 forget=6.8170 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 3 it 30 total=13.7065 mle=1.7048 pcon=5.1558 forget=6.8458 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7120 mle=1.6712 pcon=5.1518 forget=6.8890 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7348 mle=1.7664 pcon=5.1474 forget=6.8210 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8713 mle=1.9271 pcon=5.1434 forget=6.8009 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.4987 mle=1.5695 pcon=5.1393 forget=6.7899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7423 mle=1.8357 pcon=5.1351 forget=6.7716 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.6962 mle=1.7029 pcon=5.1308 forget=6.8625 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.6910 mle=1.6953 pcon=5.1272 forget=6.8685 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 4 it 40 total=13.4701 mle=1.6522 pcon=5.1233 forget=6.6946 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6123 mle=1.7317 pcon=5.1192 forget=6.7614 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5721 mle=1.6523 pcon=5.1155 forget=6.8044 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3637 mle=1.4532 pcon=5.1119 forget=6.7986 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6081 mle=1.7664 pcon=5.1079 forget=6.7338 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3090 mle=1.4851 pcon=5.1042 forget=6.7198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7346 mle=1.8759 pcon=5.1006 forget=6.7581 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 5 it 0 total=13.6934 mle=1.8652 pcon=5.0970 forget=6.7311 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=13.5491 mle=1.8061 pcon=5.0931 forget=6.6498 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=13.4060 mle=1.5906 pcon=5.0897 forget=6.7258 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=13.3915 mle=1.5923 pcon=5.0858 forget=6.7134 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=13.4120 mle=1.6921 pcon=5.0826 forget=6.6373 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=13.4547 mle=1.7802 pcon=5.0792 forget=6.5954 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=13.2878 mle=1.6035 pcon=5.0757 forget=6.6086 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=13.4640 mle=1.7402 pcon=5.0724 forget=6.6514 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 6 it 10 total=13.2319 mle=1.4104 pcon=5.0691 forget=6.7523 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=13.1496 mle=1.5272 pcon=5.0660 forget=6.5564 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=13.3005 mle=1.5941 pcon=5.0630 forget=6.6434 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=13.3711 mle=1.6816 pcon=5.0600 forget=6.6295 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=13.3246 mle=1.6309 pcon=5.0575 forget=6.6361 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=13.5426 mle=1.7991 pcon=5.0545 forget=6.6889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=13.3507 mle=1.5470 pcon=5.0521 forget=6.7516 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=13.4034 mle=1.6703 pcon=5.0498 forget=6.6833 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 7 it 20 total=13.4032 mle=1.6509 pcon=5.0474 forget=6.7049 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=13.5264 mle=1.7660 pcon=5.0453 forget=6.7151 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=13.4397 mle=1.6174 pcon=5.0432 forget=6.7791 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=13.4466 mle=1.5963 pcon=5.0414 forget=6.8089 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=13.5856 mle=1.6446 pcon=5.0401 forget=6.9009 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=13.7965 mle=1.7800 pcon=5.0388 forget=6.9778 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=13.7836 mle=1.7536 pcon=5.0372 forget=6.9927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=13.6791 mle=1.6352 pcon=5.0361 forget=7.0077 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 30 total=13.7224 mle=1.5610 pcon=5.0352 forget=7.1261 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=13.7310 mle=1.5985 pcon=5.0343 forget=7.0982 nr=64 nf=64 protos=540 fproto_sim=NA
 90%|█████████ | 9/10 [02:12<00:14, 14.34s/it]100%|██████████| 10/10 [02:27<00:00, 14.36s/it]100%|██████████| 10/10 [02:27<00:00, 14.71s/it]
[loss] ep 8 it 130 total=13.6543 mle=1.4969 pcon=5.0331 forget=7.1243 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=13.8711 mle=1.7137 pcon=5.0322 forget=7.1252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=13.6501 mle=1.5293 pcon=5.0314 forget=7.0894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=13.7776 mle=1.7047 pcon=5.0302 forget=7.0427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=13.8855 mle=1.7304 pcon=5.0286 forget=7.1266 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=13.4632 mle=1.4733 pcon=5.0270 forget=6.9630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 40 total=13.4102 mle=1.6206 pcon=5.0252 forget=6.7645 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=13.3206 mle=1.6539 pcon=5.0228 forget=6.6439 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=13.2963 mle=1.8054 pcon=5.0206 forget=6.4702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=12.7674 mle=1.5773 pcon=5.0178 forget=6.1722 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=12.6482 mle=1.5924 pcon=5.0147 forget=6.0411 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=12.5617 mle=1.5531 pcon=5.0109 forget=5.9977 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=12.5521 mle=1.5802 pcon=5.0074 forget=5.9645 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:13,  2.93it/s]  3%|▎         | 11/391 [00:00<00:12, 30.99it/s]  5%|▌         | 21/391 [00:00<00:07, 50.48it/s]  8%|▊         | 31/391 [00:00<00:05, 64.06it/s] 10%|█         | 41/391 [00:00<00:04, 73.29it/s] 13%|█▎        | 51/391 [00:00<00:04, 79.50it/s] 16%|█▌        | 61/391 [00:00<00:03, 84.16it/s] 18%|█▊        | 71/391 [00:01<00:03, 87.34it/s] 21%|██        | 81/391 [00:01<00:03, 89.71it/s] 23%|██▎       | 91/391 [00:01<00:03, 91.29it/s] 26%|██▌       | 101/391 [00:01<00:03, 92.30it/s] 28%|██▊       | 111/391 [00:01<00:03, 93.24it/s] 31%|███       | 121/391 [00:01<00:02, 93.62it/s] 34%|███▎      | 131/391 [00:01<00:02, 94.13it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.41it/s] 39%|███▊      | 151/391 [00:01<00:02, 94.83it/s] 41%|████      | 161/391 [00:02<00:02, 94.90it/s] 44%|████▎     | 171/391 [00:02<00:02, 94.95it/s] 46%|████▋     | 181/391 [00:02<00:02, 95.06it/s] 49%|████▉     | 191/391 [00:02<00:02, 93.91it/s] 51%|█████▏    | 201/391 [00:02<00:02, 94.05it/s] 54%|█████▍    | 211/391 [00:02<00:01, 94.39it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.09it/s] 59%|█████▉    | 231/391 [00:02<00:01, 94.52it/s] 62%|██████▏   | 241/391 [00:02<00:01, 94.70it/s] 64%|██████▍   | 251/391 [00:02<00:01, 94.68it/s] 67%|██████▋   | 261/391 [00:03<00:01, 94.89it/s] 69%|██████▉   | 271/391 [00:03<00:01, 94.87it/s] 72%|███████▏  | 281/391 [00:03<00:01, 95.11it/s] 74%|███████▍  | 291/391 [00:03<00:01, 95.20it/s] 77%|███████▋  | 301/391 [00:03<00:00, 93.43it/s] 80%|███████▉  | 311/391 [00:03<00:00, 93.63it/s] 82%|████████▏ | 321/391 [00:03<00:00, 94.06it/s] 85%|████████▍ | 331/391 [00:03<00:00, 94.36it/s] 87%|████████▋ | 341/391 [00:03<00:00, 94.39it/s] 90%|████████▉ | 351/391 [00:04<00:00, 94.60it/s] 92%|█████████▏| 361/391 [00:04<00:00, 94.49it/s] 95%|█████████▍| 371/391 [00:04<00:00, 94.62it/s] 97%|█████████▋| 381/391 [00:04<00:00, 94.59it/s]100%|██████████| 391/391 [00:04<00:00, 92.22it/s]100%|██████████| 391/391 [00:04<00:00, 87.45it/s]
50000 images processed, 4.547974586486816 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.10it/s] 14%|█▍        | 11/79 [00:00<00:02, 32.30it/s] 27%|██▋       | 21/79 [00:00<00:01, 51.64it/s] 39%|███▉      | 31/79 [00:00<00:00, 64.71it/s] 52%|█████▏    | 41/79 [00:00<00:00, 73.46it/s] 65%|██████▍   | 51/79 [00:00<00:00, 79.40it/s] 77%|███████▋  | 61/79 [00:00<00:00, 83.24it/s] 90%|████████▉ | 71/79 [00:01<00:00, 86.91it/s]100%|██████████| 79/79 [00:01<00:00, 68.03it/s]
10000 images processed, 1.1816916465759277 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<00:59,  3.40it/s]  5%|▌         | 11/204 [00:00<00:05, 34.28it/s] 10%|█         | 21/204 [00:00<00:03, 53.70it/s] 15%|█▌        | 31/204 [00:00<00:02, 66.18it/s] 20%|██        | 41/204 [00:00<00:02, 74.24it/s] 25%|██▌       | 51/204 [00:00<00:01, 80.05it/s] 30%|██▉       | 61/204 [00:00<00:01, 83.56it/s] 35%|███▍      | 71/204 [00:01<00:01, 86.34it/s] 40%|███▉      | 81/204 [00:01<00:01, 88.34it/s] 45%|████▍     | 91/204 [00:01<00:01, 89.59it/s] 50%|████▉     | 101/204 [00:01<00:01, 90.53it/s] 54%|█████▍    | 111/204 [00:01<00:01, 91.30it/s] 59%|█████▉    | 121/204 [00:01<00:00, 91.74it/s] 64%|██████▍   | 131/204 [00:01<00:00, 92.17it/s] 69%|██████▉   | 141/204 [00:01<00:00, 92.59it/s] 74%|███████▍  | 151/204 [00:01<00:00, 92.75it/s] 79%|███████▉  | 161/204 [00:02<00:00, 92.49it/s] 84%|████████▍ | 171/204 [00:02<00:00, 92.63it/s] 89%|████████▊ | 181/204 [00:02<00:00, 92.45it/s] 94%|█████████▎| 191/204 [00:02<00:00, 90.27it/s] 99%|█████████▊| 201/204 [00:02<00:00, 91.99it/s]100%|██████████| 204/204 [00:02<00:00, 81.82it/s]
26032 images processed, 2.5345242023468018 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:35,  2.19it/s] 11%|█▏        | 9/79 [00:00<00:03, 18.69it/s] 22%|██▏       | 17/79 [00:00<00:02, 29.56it/s] 32%|███▏      | 25/79 [00:00<00:01, 36.89it/s] 42%|████▏     | 33/79 [00:01<00:01, 41.68it/s] 51%|█████     | 40/79 [00:01<00:00, 47.26it/s] 58%|█████▊    | 46/79 [00:01<00:00, 46.07it/s] 66%|██████▌   | 52/79 [00:01<00:00, 44.70it/s] 73%|███████▎  | 58/79 [00:01<00:00, 44.29it/s] 84%|████████▎ | 66/79 [00:01<00:00, 45.97it/s] 94%|█████████▎| 74/79 [00:01<00:00, 47.90it/s]100%|██████████| 79/79 [00:01<00:00, 40.43it/s]
10000 images processed, 1.988621711730957 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.12it/s] 14%|█▍        | 11/79 [00:00<00:02, 32.17it/s] 27%|██▋       | 21/79 [00:00<00:01, 51.76it/s] 39%|███▉      | 31/79 [00:00<00:00, 65.10it/s] 52%|█████▏    | 41/79 [00:00<00:00, 74.21it/s] 65%|██████▍   | 51/79 [00:00<00:00, 80.54it/s] 77%|███████▋  | 61/79 [00:00<00:00, 84.96it/s] 90%|████████▉ | 71/79 [00:01<00:00, 88.29it/s]100%|██████████| 79/79 [00:01<00:00, 69.57it/s]
10000 images processed, 1.1549715995788574 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:20,  3.33it/s] 16%|█▌        | 11/70 [00:00<00:01, 33.92it/s] 30%|███       | 21/70 [00:00<00:00, 53.59it/s] 44%|████▍     | 31/70 [00:00<00:00, 66.56it/s] 59%|█████▊    | 41/70 [00:00<00:00, 75.39it/s] 73%|███████▎  | 51/70 [00:00<00:00, 81.48it/s] 87%|████████▋ | 61/70 [00:00<00:00, 85.77it/s]100%|██████████| 70/70 [00:01<00:00, 67.84it/s]
8925 images processed, 1.0604767799377441 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:35,  1.25it/s]  4%|▍         | 2/45 [00:00<00:17,  2.44it/s] 20%|██        | 9/45 [00:01<00:03, 10.67it/s] 24%|██▍       | 11/45 [00:01<00:03, 10.70it/s] 31%|███       | 14/45 [00:01<00:02, 13.44it/s] 36%|███▌      | 16/45 [00:01<00:02, 13.90it/s] 40%|████      | 18/45 [00:01<00:02, 13.40it/s] 44%|████▍     | 20/45 [00:01<00:01, 12.71it/s] 49%|████▉     | 22/45 [00:02<00:01, 12.44it/s] 53%|█████▎    | 24/45 [00:02<00:01, 13.06it/s] 60%|██████    | 27/45 [00:02<00:01, 15.30it/s] 64%|██████▍   | 29/45 [00:02<00:01, 11.26it/s] 73%|███████▎  | 33/45 [00:02<00:00, 14.03it/s] 78%|███████▊  | 35/45 [00:03<00:00, 14.73it/s] 82%|████████▏ | 37/45 [00:03<00:00, 13.72it/s] 89%|████████▉ | 40/45 [00:03<00:00, 12.20it/s] 96%|█████████▌| 43/45 [00:03<00:00, 12.94it/s]100%|██████████| 45/45 [00:03<00:00, 13.16it/s]100%|██████████| 45/45 [00:03<00:00, 11.62it/s]
5640 images processed, 3.892921209335327 seconds used

17.971094369888306
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.56  99.35
places365     68.17  81.15
LSUN          17.19  96.14
iSUN          71.75  81.93
dtd           38.33  91.32
forget        78.50  87.02
AVG           46.08  89.49
Retain-Acc: 0.7397
Forget-as-OOD (retain known vs forget novel):
  FPR: 78.50 AUROC: 87.02 AUIN: 98.38
13.279773473739624
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2_rf.png
lr=0.001 score=-23.0421
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:18<02:45, 18.44s/it] 20%|██        | 2/10 [00:33<02:11, 16.48s/it] 30%|███       | 3/10 [00:48<01:51, 15.97s/it] 40%|████      | 4/10 [01:04<01:34, 15.68s/it] 50%|█████     | 5/10 [01:19<01:17, 15.44s/it] 60%|██████    | 6/10 [01:33<01:00, 15.20s/it] 70%|███████   | 7/10 [01:48<00:45, 15.05s/it] 80%|████████  | 8/10 [02:03<00:30, 15.10s/it][loss] ep 0 it 0 total=13.7434 mle=1.5709 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8369 mle=1.5425 pcon=5.2879 forget=7.0065 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8514 mle=1.7005 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0217 mle=1.8998 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8929 mle=1.7129 pcon=5.2670 forget=6.9130 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6545 mle=1.5021 pcon=5.2603 forget=6.8921 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7233 mle=1.5597 pcon=5.2540 forget=6.9096 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8767 mle=1.6810 pcon=5.2476 forget=6.9481 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 1 it 10 total=13.9625 mle=1.6715 pcon=5.2409 forget=7.0501 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7066 mle=1.6372 pcon=5.2346 forget=6.8348 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6922 mle=1.5167 pcon=5.2284 forget=6.9471 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9490 mle=1.7801 pcon=5.2224 forget=6.9464 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9072 mle=1.7645 pcon=5.2167 forget=6.9260 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6575 mle=1.6054 pcon=5.2112 forget=6.8409 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8193 mle=1.7237 pcon=5.2056 forget=6.8900 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9244 mle=1.7940 pcon=5.2003 forget=6.9301 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 2 it 20 total=13.6309 mle=1.5529 pcon=5.1950 forget=6.8830 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8652 mle=1.8883 pcon=5.1899 forget=6.7869 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8050 mle=1.7419 pcon=5.1847 forget=6.8784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5293 mle=1.5264 pcon=5.1796 forget=6.8233 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6881 mle=1.5855 pcon=5.1745 forget=6.9281 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9051 mle=1.8837 pcon=5.1699 forget=6.8515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5437 mle=1.5923 pcon=5.1652 forget=6.7862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7933 mle=1.8113 pcon=5.1605 forget=6.8215 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 3 it 30 total=13.7124 mle=1.7055 pcon=5.1559 forget=6.8509 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7168 mle=1.6709 pcon=5.1519 forget=6.8940 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7410 mle=1.7667 pcon=5.1475 forget=6.8267 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8746 mle=1.9231 pcon=5.1435 forget=6.8080 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.5058 mle=1.5699 pcon=5.1395 forget=6.7964 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7497 mle=1.8362 pcon=5.1353 forget=6.7782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.7071 mle=1.7030 pcon=5.1311 forget=6.8731 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.7088 mle=1.6957 pcon=5.1275 forget=6.8856 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 4 it 40 total=13.4839 mle=1.6535 pcon=5.1236 forget=6.7067 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6257 mle=1.7294 pcon=5.1196 forget=6.7767 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5918 mle=1.6512 pcon=5.1159 forget=6.8247 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3892 mle=1.4536 pcon=5.1125 forget=6.8231 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6495 mle=1.7672 pcon=5.1086 forget=6.7737 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3557 mle=1.4911 pcon=5.1050 forget=6.7596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7883 mle=1.8723 pcon=5.1015 forget=6.8145 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 5 it 0 total=13.7433 mle=1.8673 pcon=5.0982 forget=6.7777 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=13.6149 mle=1.8008 pcon=5.0946 forget=6.7196 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=13.4658 mle=1.5851 pcon=5.0914 forget=6.7894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=13.4639 mle=1.5944 pcon=5.0879 forget=6.7817 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=13.4908 mle=1.6916 pcon=5.0849 forget=6.7142 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=13.5357 mle=1.7747 pcon=5.0819 forget=6.6791 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=13.4048 mle=1.5975 pcon=5.0788 forget=6.7285 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=13.5452 mle=1.7451 pcon=5.0758 forget=6.7243 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 6 it 10 total=13.3084 mle=1.3997 pcon=5.0729 forget=6.8358 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=13.2167 mle=1.5322 pcon=5.0700 forget=6.6145 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=13.4180 mle=1.5904 pcon=5.0673 forget=6.7603 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=13.4136 mle=1.6674 pcon=5.0644 forget=6.6818 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=13.3604 mle=1.6307 pcon=5.0620 forget=6.6677 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=13.5975 mle=1.8165 pcon=5.0590 forget=6.7221 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=13.3615 mle=1.5451 pcon=5.0564 forget=6.7601 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=13.3987 mle=1.6758 pcon=5.0537 forget=6.6692 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 7 it 20 total=13.3041 mle=1.6300 pcon=5.0508 forget=6.6234 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=13.3898 mle=1.7451 pcon=5.0481 forget=6.5966 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=13.2667 mle=1.6109 pcon=5.0453 forget=6.6106 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=13.1940 mle=1.5653 pcon=5.0426 forget=6.5861 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=13.3480 mle=1.6451 pcon=5.0402 forget=6.6627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=13.5273 mle=1.7898 pcon=5.0377 forget=6.6999 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=13.4456 mle=1.7320 pcon=5.0348 forget=6.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=13.2732 mle=1.6252 pcon=5.0324 forget=6.6155 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
 90%|█████████ | 9/10 [02:18<00:15, 15.08s/it]100%|██████████| 10/10 [02:33<00:00, 15.04s/it]100%|██████████| 10/10 [02:33<00:00, 15.38s/it]
[loss] ep 8 it 30 total=13.2572 mle=1.5432 pcon=5.0301 forget=6.6839 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=13.2589 mle=1.5773 pcon=5.0277 forget=6.6539 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=13.1649 mle=1.5040 pcon=5.0252 forget=6.6357 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=13.3487 mle=1.7124 pcon=5.0229 forget=6.6134 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=13.1491 mle=1.5289 pcon=5.0210 forget=6.5991 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=13.2950 mle=1.6905 pcon=5.0187 forget=6.5857 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=13.4826 mle=1.7408 pcon=5.0164 forget=6.7253 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=13.1434 mle=1.4823 pcon=5.0143 forget=6.6468 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 9 it 40 total=13.2487 mle=1.6329 pcon=5.0123 forget=6.6035 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=13.3710 mle=1.6719 pcon=5.0101 forget=6.6889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=13.6087 mle=1.8198 pcon=5.0086 forget=6.7802 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=13.2773 mle=1.5803 pcon=5.0071 forget=6.6898 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=13.3692 mle=1.5858 pcon=5.0057 forget=6.7777 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=13.3709 mle=1.5610 pcon=5.0040 forget=6.8059 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=13.3539 mle=1.5534 pcon=5.0028 forget=6.7977 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:09,  3.02it/s]  3%|▎         | 11/391 [00:00<00:12, 31.64it/s]  5%|▌         | 21/391 [00:00<00:07, 51.07it/s]  8%|▊         | 31/391 [00:00<00:05, 64.10it/s] 10%|█         | 41/391 [00:00<00:04, 73.37it/s] 13%|█▎        | 51/391 [00:00<00:04, 79.69it/s] 16%|█▌        | 61/391 [00:00<00:03, 84.26it/s] 18%|█▊        | 71/391 [00:01<00:03, 87.50it/s] 21%|██        | 81/391 [00:01<00:03, 89.80it/s] 23%|██▎       | 91/391 [00:01<00:03, 90.82it/s] 26%|██▌       | 101/391 [00:01<00:03, 91.93it/s] 28%|██▊       | 111/391 [00:01<00:03, 92.70it/s] 31%|███       | 121/391 [00:01<00:02, 92.90it/s] 34%|███▎      | 131/391 [00:01<00:02, 93.54it/s] 36%|███▌      | 141/391 [00:01<00:02, 93.69it/s] 39%|███▊      | 151/391 [00:01<00:02, 93.72it/s] 41%|████      | 161/391 [00:02<00:02, 93.85it/s] 44%|████▎     | 171/391 [00:02<00:02, 93.46it/s] 46%|████▋     | 181/391 [00:02<00:02, 93.59it/s] 49%|████▉     | 191/391 [00:02<00:02, 93.54it/s] 51%|█████▏    | 201/391 [00:02<00:02, 93.25it/s] 54%|█████▍    | 211/391 [00:02<00:01, 93.79it/s] 57%|█████▋    | 221/391 [00:02<00:01, 93.41it/s] 59%|█████▉    | 231/391 [00:02<00:01, 93.98it/s] 62%|██████▏   | 241/391 [00:02<00:01, 94.24it/s] 64%|██████▍   | 251/391 [00:02<00:01, 94.35it/s] 67%|██████▋   | 261/391 [00:03<00:01, 94.53it/s] 69%|██████▉   | 271/391 [00:03<00:01, 94.41it/s] 72%|███████▏  | 281/391 [00:03<00:01, 94.21it/s] 74%|███████▍  | 291/391 [00:03<00:01, 94.42it/s] 77%|███████▋  | 301/391 [00:03<00:00, 94.60it/s] 80%|███████▉  | 311/391 [00:03<00:00, 94.59it/s] 82%|████████▏ | 321/391 [00:03<00:00, 94.53it/s] 85%|████████▍ | 331/391 [00:03<00:00, 94.64it/s] 87%|████████▋ | 341/391 [00:03<00:00, 94.81it/s] 90%|████████▉ | 351/391 [00:04<00:00, 93.92it/s] 92%|█████████▏| 361/391 [00:04<00:00, 93.19it/s] 95%|█████████▍| 371/391 [00:04<00:00, 93.84it/s] 97%|█████████▋| 381/391 [00:04<00:00, 93.93it/s]100%|██████████| 391/391 [00:04<00:00, 91.52it/s]100%|██████████| 391/391 [00:04<00:00, 87.21it/s]
50000 images processed, 4.569021940231323 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:32,  2.40it/s] 14%|█▍        | 11/79 [00:00<00:02, 26.87it/s] 27%|██▋       | 21/79 [00:00<00:01, 45.39it/s] 39%|███▉      | 31/79 [00:00<00:00, 59.27it/s] 52%|█████▏    | 41/79 [00:00<00:00, 68.55it/s] 65%|██████▍   | 51/79 [00:00<00:00, 75.77it/s] 77%|███████▋  | 61/79 [00:01<00:00, 81.06it/s] 90%|████████▉ | 71/79 [00:01<00:00, 84.97it/s]100%|██████████| 79/79 [00:01<00:00, 62.87it/s]
10000 images processed, 1.2782678604125977 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:27,  2.32it/s]  5%|▌         | 11/204 [00:00<00:07, 26.13it/s] 10%|█         | 21/204 [00:00<00:04, 44.30it/s] 15%|█▌        | 31/204 [00:00<00:02, 57.93it/s] 20%|██        | 41/204 [00:00<00:02, 67.83it/s] 25%|██▌       | 51/204 [00:00<00:02, 74.89it/s] 30%|██▉       | 61/204 [00:01<00:01, 80.27it/s] 35%|███▍      | 71/204 [00:01<00:01, 84.22it/s] 40%|███▉      | 81/204 [00:01<00:01, 87.13it/s] 45%|████▍     | 91/204 [00:01<00:01, 88.98it/s] 50%|████▉     | 101/204 [00:01<00:01, 90.07it/s] 54%|█████▍    | 111/204 [00:01<00:01, 91.21it/s] 59%|█████▉    | 121/204 [00:01<00:00, 91.94it/s] 64%|██████▍   | 131/204 [00:01<00:00, 92.25it/s] 69%|██████▉   | 141/204 [00:01<00:00, 92.06it/s] 74%|███████▍  | 151/204 [00:02<00:00, 92.26it/s] 79%|███████▉  | 161/204 [00:02<00:00, 92.21it/s] 84%|████████▍ | 171/204 [00:02<00:00, 92.24it/s] 89%|████████▊ | 181/204 [00:02<00:00, 92.49it/s] 94%|█████████▎| 191/204 [00:02<00:00, 92.88it/s] 99%|█████████▊| 201/204 [00:02<00:00, 93.64it/s]100%|██████████| 204/204 [00:02<00:00, 77.92it/s]
26032 images processed, 2.6653740406036377 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:47,  1.66it/s] 11%|█▏        | 9/79 [00:00<00:04, 14.89it/s] 22%|██▏       | 17/79 [00:00<00:02, 24.39it/s] 32%|███▏      | 25/79 [00:01<00:01, 31.04it/s] 42%|████▏     | 33/79 [00:01<00:01, 36.78it/s] 52%|█████▏    | 41/79 [00:01<00:00, 41.42it/s] 62%|██████▏   | 49/79 [00:01<00:00, 44.62it/s] 72%|███████▏  | 57/79 [00:01<00:00, 45.51it/s] 82%|████████▏ | 65/79 [00:01<00:00, 46.31it/s] 92%|█████████▏| 73/79 [00:02<00:00, 46.88it/s]100%|██████████| 79/79 [00:02<00:00, 37.28it/s]
10000 images processed, 2.153709888458252 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:34,  2.25it/s] 14%|█▍        | 11/79 [00:00<00:02, 25.59it/s] 27%|██▋       | 21/79 [00:00<00:01, 43.78it/s] 39%|███▉      | 31/79 [00:00<00:00, 57.18it/s] 52%|█████▏    | 41/79 [00:00<00:00, 67.62it/s] 65%|██████▍   | 51/79 [00:00<00:00, 75.28it/s] 77%|███████▋  | 61/79 [00:01<00:00, 80.70it/s] 90%|████████▉ | 71/79 [00:01<00:00, 85.00it/s]100%|██████████| 79/79 [00:01<00:00, 62.22it/s]
10000 images processed, 1.2905857563018799 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:31,  2.21it/s] 16%|█▌        | 11/70 [00:00<00:02, 25.36it/s] 30%|███       | 21/70 [00:00<00:01, 43.44it/s] 44%|████▍     | 31/70 [00:00<00:00, 57.28it/s] 59%|█████▊    | 41/70 [00:00<00:00, 67.65it/s] 73%|███████▎  | 51/70 [00:00<00:00, 75.41it/s] 87%|████████▋ | 61/70 [00:01<00:00, 81.15it/s]100%|██████████| 70/70 [00:01<00:00, 58.91it/s]
8925 images processed, 1.2184042930603027 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:42,  1.04it/s]  4%|▍         | 2/45 [00:01<00:20,  2.07it/s] 20%|██        | 9/45 [00:01<00:03,  9.53it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.88it/s] 31%|███       | 14/45 [00:01<00:02, 11.75it/s] 38%|███▊      | 17/45 [00:01<00:02, 13.31it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.08it/s] 49%|████▉     | 22/45 [00:02<00:01, 12.56it/s] 58%|█████▊    | 26/45 [00:02<00:01, 11.53it/s] 67%|██████▋   | 30/45 [00:03<00:01, 12.54it/s] 73%|███████▎  | 33/45 [00:03<00:00, 13.26it/s] 78%|███████▊  | 35/45 [00:03<00:00, 11.85it/s] 91%|█████████ | 41/45 [00:03<00:00, 15.21it/s] 96%|█████████▌| 43/45 [00:04<00:00, 11.53it/s]100%|██████████| 45/45 [00:04<00:00, 10.89it/s]
5640 images processed, 4.154879331588745 seconds used

19.06135320663452
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.82  81.20
LSUN          17.44  96.10
iSUN          72.20  81.79
dtd           37.82  91.38
forget        79.50  86.91
AVG           46.22  89.46
Retain-Acc: 0.7403
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.91 AUIN: 98.36
14.592551708221436
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2_rf.png
lr=0.0005 score=-23.1087
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:18<02:50, 18.90s/it] 20%|██        | 2/10 [00:34<02:16, 17.02s/it] 30%|███       | 3/10 [00:49<01:53, 16.15s/it] 40%|████      | 4/10 [01:05<01:35, 15.94s/it] 50%|█████     | 5/10 [01:20<01:18, 15.70s/it] 60%|██████    | 6/10 [01:35<01:02, 15.55s/it] 70%|███████   | 7/10 [01:51<00:46, 15.59s/it] 80%|████████  | 8/10 [02:06<00:30, 15.49s/it][loss] ep 0 it 0 total=13.7434 mle=1.5709 pcon=5.2950 forget=6.8774 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8368 mle=1.5423 pcon=5.2879 forget=7.0066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8512 mle=1.7003 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0218 mle=1.8998 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8932 mle=1.7131 pcon=5.2670 forget=6.9131 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6546 mle=1.5020 pcon=5.2603 forget=6.8922 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7235 mle=1.5597 pcon=5.2540 forget=6.9097 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8769 mle=1.6809 pcon=5.2476 forget=6.9484 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 1 it 10 total=13.9626 mle=1.6715 pcon=5.2409 forget=7.0503 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7067 mle=1.6371 pcon=5.2346 forget=6.8350 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6925 mle=1.5167 pcon=5.2285 forget=6.9474 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9496 mle=1.7801 pcon=5.2225 forget=6.9471 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9079 mle=1.7646 pcon=5.2167 forget=6.9266 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6581 mle=1.6054 pcon=5.2112 forget=6.8415 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8206 mle=1.7240 pcon=5.2056 forget=6.8910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9255 mle=1.7945 pcon=5.2003 forget=6.9308 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 2 it 20 total=13.6319 mle=1.5528 pcon=5.1951 forget=6.8841 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8667 mle=1.8884 pcon=5.1899 forget=6.7884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8056 mle=1.7415 pcon=5.1847 forget=6.8793 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5307 mle=1.5263 pcon=5.1797 forget=6.8248 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6899 mle=1.5857 pcon=5.1746 forget=6.9296 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9069 mle=1.8836 pcon=5.1699 forget=6.8534 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5460 mle=1.5926 pcon=5.1652 forget=6.7882 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7962 mle=1.8116 pcon=5.1605 forget=6.8241 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 3 it 30 total=13.7162 mle=1.7062 pcon=5.1560 forget=6.8540 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7193 mle=1.6709 pcon=5.1519 forget=6.8964 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7439 mle=1.7669 pcon=5.1476 forget=6.8294 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8735 mle=1.9184 pcon=5.1436 forget=6.8115 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.5094 mle=1.5702 pcon=5.1396 forget=6.7997 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7529 mle=1.8363 pcon=5.1354 forget=6.7812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.7116 mle=1.7032 pcon=5.1312 forget=6.8772 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.7153 mle=1.6960 pcon=5.1276 forget=6.8916 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 4 it 40 total=13.4885 mle=1.6541 pcon=5.1238 forget=6.7107 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6308 mle=1.7297 pcon=5.1198 forget=6.7813 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5972 mle=1.6503 pcon=5.1161 forget=6.8308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3951 mle=1.4537 pcon=5.1127 forget=6.8288 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6586 mle=1.7671 pcon=5.1088 forget=6.7827 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3638 mle=1.4912 pcon=5.1053 forget=6.7673 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7996 mle=1.8719 pcon=5.1018 forget=6.8258 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 5 it 0 total=13.7525 mle=1.8673 pcon=5.0985 forget=6.7866 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=13.6251 mle=1.7997 pcon=5.0949 forget=6.7305 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=13.4766 mle=1.5840 pcon=5.0918 forget=6.8009 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=13.4767 mle=1.5944 pcon=5.0883 forget=6.7940 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=13.5058 mle=1.6913 pcon=5.0854 forget=6.7290 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=13.5503 mle=1.7734 pcon=5.0824 forget=6.6945 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=13.4313 mle=1.5965 pcon=5.0794 forget=6.7554 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=13.5686 mle=1.7471 pcon=5.0765 forget=6.7449 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 6 it 10 total=13.3357 mle=1.4012 pcon=5.0737 forget=6.8608 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=13.2502 mle=1.5358 pcon=5.0709 forget=6.6435 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=13.4678 mle=1.5916 pcon=5.0683 forget=6.8079 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=13.4616 mle=1.6655 pcon=5.0655 forget=6.7306 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=13.4029 mle=1.6304 pcon=5.0633 forget=6.7091 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=13.6642 mle=1.8233 pcon=5.0604 forget=6.7804 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=13.4129 mle=1.5351 pcon=5.0581 forget=6.8198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=13.4688 mle=1.6751 pcon=5.0556 forget=6.7381 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 7 it 20 total=13.3819 mle=1.6306 pcon=5.0530 forget=6.6984 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=13.4715 mle=1.7449 pcon=5.0506 forget=6.6759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=13.3863 mle=1.6271 pcon=5.0481 forget=6.7111 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=13.2729 mle=1.5820 pcon=5.0458 forget=6.6451 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=13.4596 mle=1.6543 pcon=5.0438 forget=6.7615 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=13.6463 mle=1.7959 pcon=5.0417 forget=6.8087 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=13.5804 mle=1.7420 pcon=5.0393 forget=6.7990 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=13.4096 mle=1.6405 pcon=5.0373 forget=6.7319 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
 90%|█████████ | 9/10 [02:22<00:15, 15.48s/it]100%|██████████| 10/10 [02:37<00:00, 15.36s/it]100%|██████████| 10/10 [02:37<00:00, 15.74s/it]
[loss] ep 8 it 30 total=13.3746 mle=1.5371 pcon=5.0354 forget=6.8021 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=13.3672 mle=1.5904 pcon=5.0335 forget=6.7434 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=13.3200 mle=1.5003 pcon=5.0313 forget=6.7884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=13.4416 mle=1.7035 pcon=5.0294 forget=6.7087 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=13.2591 mle=1.5309 pcon=5.0279 forget=6.7003 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=13.3599 mle=1.6776 pcon=5.0260 forget=6.6563 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=13.6169 mle=1.7565 pcon=5.0239 forget=6.8365 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=13.1773 mle=1.4892 pcon=5.0220 forget=6.6661 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
[loss] ep 9 it 40 total=13.2316 mle=1.6364 pcon=5.0201 forget=6.5751 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=13.3681 mle=1.6695 pcon=5.0180 forget=6.6805 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=13.6156 mle=1.8420 pcon=5.0166 forget=6.7570 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=13.2797 mle=1.5783 pcon=5.0150 forget=6.6863 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=13.2833 mle=1.5674 pcon=5.0135 forget=6.7024 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=13.3052 mle=1.5599 pcon=5.0116 forget=6.7336 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=13.2497 mle=1.5581 pcon=5.0102 forget=6.6814 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:56,  3.35it/s]  3%|▎         | 11/391 [00:00<00:11, 34.18it/s]  5%|▌         | 21/391 [00:00<00:06, 53.95it/s]  8%|▊         | 31/391 [00:00<00:05, 67.27it/s] 10%|█         | 41/391 [00:00<00:04, 75.89it/s] 13%|█▎        | 51/391 [00:00<00:04, 81.83it/s] 16%|█▌        | 61/391 [00:00<00:03, 85.12it/s] 18%|█▊        | 71/391 [00:01<00:03, 88.29it/s] 21%|██        | 81/391 [00:01<00:03, 90.12it/s] 23%|██▎       | 91/391 [00:01<00:03, 89.60it/s] 26%|██▌       | 101/391 [00:01<00:03, 90.52it/s] 28%|██▊       | 111/391 [00:01<00:03, 91.92it/s] 31%|███       | 121/391 [00:01<00:02, 93.10it/s] 34%|███▎      | 131/391 [00:01<00:02, 91.87it/s] 36%|███▌      | 141/391 [00:01<00:02, 92.80it/s] 39%|███▊      | 151/391 [00:01<00:02, 92.61it/s] 41%|████      | 161/391 [00:02<00:02, 93.50it/s] 44%|████▎     | 171/391 [00:02<00:02, 91.22it/s] 46%|████▋     | 181/391 [00:02<00:02, 91.47it/s] 49%|████▉     | 191/391 [00:02<00:02, 91.46it/s] 51%|█████▏    | 201/391 [00:02<00:02, 92.63it/s] 54%|█████▍    | 211/391 [00:02<00:01, 93.46it/s] 57%|█████▋    | 221/391 [00:02<00:01, 92.68it/s] 59%|█████▉    | 231/391 [00:02<00:01, 93.04it/s] 62%|██████▏   | 241/391 [00:02<00:01, 93.61it/s] 64%|██████▍   | 251/391 [00:02<00:01, 92.89it/s] 67%|██████▋   | 261/391 [00:03<00:01, 92.16it/s] 69%|██████▉   | 271/391 [00:03<00:01, 92.72it/s] 72%|███████▏  | 281/391 [00:03<00:01, 93.25it/s] 74%|███████▍  | 291/391 [00:03<00:01, 93.81it/s] 77%|███████▋  | 301/391 [00:03<00:00, 94.26it/s] 80%|███████▉  | 311/391 [00:03<00:00, 94.48it/s] 82%|████████▏ | 321/391 [00:03<00:00, 94.49it/s] 85%|████████▍ | 331/391 [00:03<00:00, 94.60it/s] 87%|████████▋ | 341/391 [00:03<00:00, 93.95it/s] 90%|████████▉ | 351/391 [00:04<00:00, 94.20it/s] 92%|█████████▏| 361/391 [00:04<00:00, 94.63it/s] 95%|█████████▍| 371/391 [00:04<00:00, 94.75it/s] 97%|█████████▋| 381/391 [00:04<00:00, 94.72it/s]100%|██████████| 391/391 [00:04<00:00, 93.25it/s]100%|██████████| 391/391 [00:04<00:00, 87.56it/s]
50000 images processed, 4.568130970001221 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:17,  4.38it/s] 13%|█▎        | 10/79 [00:00<00:01, 36.14it/s] 25%|██▌       | 20/79 [00:00<00:01, 57.00it/s] 38%|███▊      | 30/79 [00:00<00:00, 69.43it/s] 49%|████▉     | 39/79 [00:00<00:00, 74.70it/s] 62%|██████▏   | 49/79 [00:00<00:00, 80.62it/s] 75%|███████▍  | 59/79 [00:00<00:00, 84.69it/s] 87%|████████▋ | 69/79 [00:00<00:00, 87.15it/s]100%|██████████| 79/79 [00:01<00:00, 86.28it/s]100%|██████████| 79/79 [00:01<00:00, 72.04it/s]
10000 images processed, 1.1190612316131592 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<00:50,  4.00it/s]  5%|▌         | 11/204 [00:00<00:05, 38.14it/s] 10%|█         | 21/204 [00:00<00:03, 57.74it/s] 15%|█▍        | 30/204 [00:00<00:02, 66.39it/s] 20%|█▉        | 40/204 [00:00<00:02, 75.41it/s] 25%|██▍       | 50/204 [00:00<00:01, 80.95it/s] 29%|██▉       | 60/204 [00:00<00:01, 85.07it/s] 34%|███▍      | 70/204 [00:00<00:01, 88.14it/s] 39%|███▉      | 80/204 [00:01<00:01, 89.73it/s] 44%|████▍     | 90/204 [00:01<00:01, 89.69it/s] 49%|████▉     | 100/204 [00:01<00:01, 90.41it/s] 54%|█████▍    | 110/204 [00:01<00:01, 90.46it/s] 59%|█████▉    | 120/204 [00:01<00:00, 91.01it/s] 64%|██████▎   | 130/204 [00:01<00:00, 92.07it/s] 69%|██████▊   | 140/204 [00:01<00:00, 91.05it/s] 74%|███████▎  | 150/204 [00:01<00:00, 88.10it/s] 78%|███████▊  | 160/204 [00:01<00:00, 89.36it/s] 83%|████████▎ | 170/204 [00:02<00:00, 90.48it/s] 88%|████████▊ | 180/204 [00:02<00:00, 90.45it/s] 93%|█████████▎| 190/204 [00:02<00:00, 90.75it/s] 98%|█████████▊| 200/204 [00:02<00:00, 92.26it/s]100%|██████████| 204/204 [00:02<00:00, 82.61it/s]
26032 images processed, 2.5142135620117188 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:33,  2.32it/s] 11%|█▏        | 9/79 [00:00<00:03, 19.63it/s] 22%|██▏       | 17/79 [00:00<00:02, 30.74it/s] 32%|███▏      | 25/79 [00:00<00:01, 38.54it/s] 39%|███▉      | 31/79 [00:00<00:01, 41.50it/s] 46%|████▌     | 36/79 [00:01<00:01, 42.38it/s] 53%|█████▎    | 42/79 [00:01<00:00, 46.21it/s] 61%|██████    | 48/79 [00:01<00:00, 42.55it/s] 70%|██████▉   | 55/79 [00:01<00:00, 40.73it/s] 80%|███████▉  | 63/79 [00:01<00:00, 45.38it/s] 90%|████████▉ | 71/79 [00:01<00:00, 49.16it/s]100%|██████████| 79/79 [00:01<00:00, 41.43it/s]
10000 images processed, 1.9460909366607666 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:23,  3.30it/s] 14%|█▍        | 11/79 [00:00<00:02, 33.23it/s] 27%|██▋       | 21/79 [00:00<00:01, 52.61it/s] 39%|███▉      | 31/79 [00:00<00:00, 65.63it/s] 52%|█████▏    | 41/79 [00:00<00:00, 73.63it/s] 65%|██████▍   | 51/79 [00:00<00:00, 79.75it/s] 76%|███████▌  | 60/79 [00:00<00:00, 82.36it/s] 89%|████████▊ | 70/79 [00:01<00:00, 86.21it/s]100%|██████████| 79/79 [00:01<00:00, 69.51it/s]
10000 images processed, 1.1557459831237793 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:24,  2.79it/s] 16%|█▌        | 11/70 [00:00<00:01, 29.84it/s] 30%|███       | 21/70 [00:00<00:01, 48.54it/s] 44%|████▍     | 31/70 [00:00<00:00, 62.11it/s] 57%|█████▋    | 40/70 [00:00<00:00, 69.78it/s] 71%|███████▏  | 50/70 [00:00<00:00, 77.15it/s] 86%|████████▌ | 60/70 [00:00<00:00, 82.62it/s]100%|██████████| 70/70 [00:01<00:00, 84.90it/s]100%|██████████| 70/70 [00:01<00:00, 63.35it/s]
8925 images processed, 1.1380951404571533 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:36,  1.20it/s]  4%|▍         | 2/45 [00:01<00:19,  2.19it/s] 20%|██        | 9/45 [00:01<00:03,  9.90it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.72it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.84it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.29it/s] 56%|█████▌    | 25/45 [00:02<00:01, 14.40it/s] 60%|██████    | 27/45 [00:02<00:01, 13.15it/s] 73%|███████▎  | 33/45 [00:03<00:00, 12.42it/s] 78%|███████▊  | 35/45 [00:03<00:00, 13.07it/s] 91%|█████████ | 41/45 [00:03<00:00, 13.69it/s] 96%|█████████▌| 43/45 [00:04<00:00, 11.32it/s]100%|██████████| 45/45 [00:04<00:00, 11.13it/s]
5640 images processed, 4.061439275741577 seconds used

18.165644884109497
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.89  81.18
LSUN          17.53  96.08
iSUN          72.37  81.70
dtd           37.87  91.39
forget        79.50  86.90
AVG           46.28  89.44
Retain-Acc: 0.7404
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
13.954315662384033
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase2_rf.png
lr=0.0001 score=-23.1402
[Phase 2] best lr= (score=-1)
[Phase 3] Searching epochs over: 5 10 20 50
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:18<01:15, 18.89s/it] 40%|████      | 2/5 [00:33<00:49, 16.65s/it] 60%|██████    | 3/5 [00:48<00:31, 15.85s/it] 80%|████████  | 4/5 [01:04<00:15, 15.57s/it]100%|██████████| 5/5 [01:19<00:00, 15.49s/it]100%|██████████| 5/5 [01:19<00:00, 15.87s/it]
[loss] ep 0 it 0 total=13.7434 mle=1.5709 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8369 mle=1.5424 pcon=5.2879 forget=7.0066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8513 mle=1.7004 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0218 mle=1.8998 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8933 mle=1.7133 pcon=5.2670 forget=6.9130 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6543 mle=1.5021 pcon=5.2603 forget=6.8919 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7231 mle=1.5597 pcon=5.2540 forget=6.9094 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8765 mle=1.6810 pcon=5.2476 forget=6.9479 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 1 it 10 total=13.9621 mle=1.6716 pcon=5.2409 forget=7.0497 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7060 mle=1.6369 pcon=5.2346 forget=6.8345 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6915 mle=1.5166 pcon=5.2284 forget=6.9465 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9482 mle=1.7803 pcon=5.2224 forget=6.9455 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9066 mle=1.7646 pcon=5.2167 forget=6.9253 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6563 mle=1.6051 pcon=5.2112 forget=6.8401 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8181 mle=1.7236 pcon=5.2056 forget=6.8889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9235 mle=1.7943 pcon=5.2003 forget=6.9289 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 2 it 20 total=13.6296 mle=1.5529 pcon=5.1950 forget=6.8817 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8630 mle=1.8883 pcon=5.1899 forget=6.7848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8039 mle=1.7421 pcon=5.1846 forget=6.8771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5267 mle=1.5262 pcon=5.1796 forget=6.8210 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6858 mle=1.5856 pcon=5.1745 forget=6.9257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9022 mle=1.8841 pcon=5.1698 forget=6.8483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5395 mle=1.5914 pcon=5.1651 forget=6.7831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7885 mle=1.8111 pcon=5.1604 forget=6.8170 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 3 it 30 total=13.7066 mle=1.7049 pcon=5.1558 forget=6.8459 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7120 mle=1.6712 pcon=5.1518 forget=6.8890 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7349 mle=1.7665 pcon=5.1474 forget=6.8210 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8714 mle=1.9270 pcon=5.1434 forget=6.8011 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.4988 mle=1.5696 pcon=5.1393 forget=6.7899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7424 mle=1.8358 pcon=5.1351 forget=6.7715 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.6962 mle=1.7028 pcon=5.1308 forget=6.8626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.6913 mle=1.6956 pcon=5.1272 forget=6.8685 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 4 it 40 total=13.4701 mle=1.6521 pcon=5.1233 forget=6.6946 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6125 mle=1.7319 pcon=5.1192 forget=6.7614 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5720 mle=1.6521 pcon=5.1155 forget=6.8044 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3638 mle=1.4532 pcon=5.1119 forget=6.7987 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6079 mle=1.7661 pcon=5.1079 forget=6.7338 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3131 mle=1.4891 pcon=5.1042 forget=6.7197 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7347 mle=1.8760 pcon=5.1006 forget=6.7581 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:35,  2.51it/s]  3%|▎         | 11/391 [00:00<00:13, 27.87it/s]  5%|▌         | 21/391 [00:00<00:07, 46.71it/s]  8%|▊         | 31/391 [00:00<00:05, 60.14it/s] 10%|█         | 41/391 [00:00<00:04, 70.08it/s] 13%|█▎        | 50/391 [00:00<00:04, 75.45it/s] 15%|█▌        | 60/391 [00:01<00:04, 81.18it/s] 18%|█▊        | 70/391 [00:01<00:03, 84.74it/s] 20%|██        | 80/391 [00:01<00:03, 87.87it/s] 23%|██▎       | 90/391 [00:01<00:03, 90.32it/s] 26%|██▌       | 100/391 [00:01<00:03, 92.16it/s] 28%|██▊       | 110/391 [00:01<00:03, 92.92it/s] 31%|███       | 120/391 [00:01<00:02, 93.90it/s] 33%|███▎      | 130/391 [00:01<00:02, 94.33it/s] 36%|███▌      | 140/391 [00:01<00:02, 94.50it/s] 38%|███▊      | 150/391 [00:01<00:02, 94.72it/s] 41%|████      | 160/391 [00:02<00:02, 95.02it/s] 43%|████▎     | 170/391 [00:02<00:02, 95.16it/s] 46%|████▌     | 180/391 [00:02<00:02, 95.16it/s] 49%|████▊     | 190/391 [00:02<00:02, 95.02it/s] 51%|█████     | 200/391 [00:02<00:02, 94.85it/s] 54%|█████▎    | 210/391 [00:02<00:01, 95.06it/s] 56%|█████▋    | 220/391 [00:02<00:01, 95.37it/s] 59%|█████▉    | 230/391 [00:02<00:01, 94.27it/s] 61%|██████▏   | 240/391 [00:02<00:01, 94.53it/s] 64%|██████▍   | 250/391 [00:03<00:01, 93.57it/s] 66%|██████▋   | 260/391 [00:03<00:01, 94.01it/s] 69%|██████▉   | 270/391 [00:03<00:01, 94.44it/s] 72%|███████▏  | 280/391 [00:03<00:01, 94.18it/s] 74%|███████▍  | 290/391 [00:03<00:01, 94.11it/s] 77%|███████▋  | 300/391 [00:03<00:00, 93.65it/s] 79%|███████▉  | 310/391 [00:03<00:00, 93.41it/s] 82%|████████▏ | 320/391 [00:03<00:00, 92.52it/s] 84%|████████▍ | 330/391 [00:03<00:00, 92.93it/s] 87%|████████▋ | 340/391 [00:04<00:00, 91.47it/s] 90%|████████▉ | 350/391 [00:04<00:00, 92.28it/s] 92%|█████████▏| 360/391 [00:04<00:00, 92.12it/s] 95%|█████████▍| 370/391 [00:04<00:00, 92.82it/s] 97%|█████████▋| 380/391 [00:04<00:00, 92.51it/s]100%|█████████▉| 390/391 [00:04<00:00, 93.86it/s]100%|██████████| 391/391 [00:04<00:00, 85.78it/s]
50000 images processed, 4.64235258102417 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.02it/s] 14%|█▍        | 11/79 [00:00<00:02, 31.76it/s] 25%|██▌       | 20/79 [00:00<00:01, 48.38it/s] 38%|███▊      | 30/79 [00:00<00:00, 62.24it/s] 51%|█████     | 40/79 [00:00<00:00, 72.04it/s] 63%|██████▎   | 50/79 [00:00<00:00, 78.72it/s] 76%|███████▌  | 60/79 [00:00<00:00, 83.25it/s] 87%|████████▋ | 69/79 [00:01<00:00, 84.64it/s]100%|██████████| 79/79 [00:01<00:00, 86.11it/s]100%|██████████| 79/79 [00:01<00:00, 66.81it/s]
10000 images processed, 1.2041184902191162 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:04,  3.14it/s]  5%|▍         | 10/204 [00:00<00:06, 29.66it/s] 10%|▉         | 20/204 [00:00<00:03, 49.71it/s] 15%|█▍        | 30/204 [00:00<00:02, 63.29it/s] 20%|█▉        | 40/204 [00:00<00:02, 72.89it/s] 25%|██▍       | 50/204 [00:00<00:01, 78.92it/s] 29%|██▉       | 60/204 [00:00<00:01, 82.11it/s] 34%|███▍      | 70/204 [00:01<00:01, 85.52it/s] 39%|███▉      | 80/204 [00:01<00:01, 87.57it/s] 44%|████▍     | 90/204 [00:01<00:01, 89.47it/s] 49%|████▉     | 100/204 [00:01<00:01, 90.48it/s] 54%|█████▍    | 110/204 [00:01<00:01, 90.70it/s] 59%|█████▉    | 120/204 [00:01<00:00, 91.52it/s] 64%|██████▎   | 130/204 [00:01<00:00, 91.75it/s] 69%|██████▊   | 140/204 [00:01<00:00, 92.39it/s] 74%|███████▎  | 150/204 [00:01<00:00, 92.96it/s] 78%|███████▊  | 160/204 [00:02<00:00, 90.83it/s] 83%|████████▎ | 170/204 [00:02<00:00, 91.67it/s] 88%|████████▊ | 180/204 [00:02<00:00, 92.33it/s] 93%|█████████▎| 190/204 [00:02<00:00, 92.96it/s] 98%|█████████▊| 200/204 [00:02<00:00, 93.99it/s]100%|██████████| 204/204 [00:02<00:00, 81.11it/s]
26032 images processed, 2.558633804321289 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.92it/s] 10%|█         | 8/79 [00:00<00:04, 16.24it/s] 15%|█▌        | 12/79 [00:00<00:03, 20.31it/s] 23%|██▎       | 18/79 [00:00<00:02, 26.76it/s] 32%|███▏      | 25/79 [00:01<00:01, 35.80it/s] 39%|███▉      | 31/79 [00:01<00:01, 41.11it/s] 46%|████▌     | 36/79 [00:01<00:01, 39.60it/s] 53%|█████▎    | 42/79 [00:01<00:00, 40.07it/s] 63%|██████▎   | 50/79 [00:01<00:00, 42.96it/s] 73%|███████▎  | 58/79 [00:01<00:00, 44.64it/s] 84%|████████▎ | 66/79 [00:01<00:00, 46.83it/s] 94%|█████████▎| 74/79 [00:02<00:00, 50.44it/s]100%|██████████| 79/79 [00:02<00:00, 38.09it/s]
10000 images processed, 2.109886646270752 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:30,  2.52it/s] 14%|█▍        | 11/79 [00:00<00:02, 27.93it/s] 25%|██▌       | 20/79 [00:00<00:01, 44.26it/s] 38%|███▊      | 30/79 [00:00<00:00, 58.68it/s] 51%|█████     | 40/79 [00:00<00:00, 68.38it/s] 63%|██████▎   | 50/79 [00:00<00:00, 75.87it/s] 76%|███████▌  | 60/79 [00:01<00:00, 80.88it/s] 89%|████████▊ | 70/79 [00:01<00:00, 85.06it/s]100%|██████████| 79/79 [00:01<00:00, 64.30it/s]
10000 images processed, 1.2475848197937012 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:25,  2.66it/s] 14%|█▍        | 10/70 [00:00<00:02, 25.95it/s] 29%|██▊       | 20/70 [00:00<00:01, 45.54it/s] 43%|████▎     | 30/70 [00:00<00:00, 59.43it/s] 57%|█████▋    | 40/70 [00:00<00:00, 69.22it/s] 71%|███████▏  | 50/70 [00:00<00:00, 76.06it/s] 86%|████████▌ | 60/70 [00:01<00:00, 80.83it/s]100%|██████████| 70/70 [00:01<00:00, 84.29it/s]100%|██████████| 70/70 [00:01<00:00, 61.64it/s]
8925 images processed, 1.165156602859497 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:37,  1.16it/s] 11%|█         | 5/45 [00:00<00:06,  6.63it/s] 20%|██        | 9/45 [00:01<00:04,  8.85it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.35it/s] 31%|███       | 14/45 [00:01<00:02, 11.50it/s] 36%|███▌      | 16/45 [00:01<00:02, 11.52it/s] 40%|████      | 18/45 [00:01<00:02, 11.70it/s] 49%|████▉     | 22/45 [00:02<00:01, 13.00it/s] 53%|█████▎    | 24/45 [00:02<00:02, 10.08it/s] 60%|██████    | 27/45 [00:02<00:01, 10.57it/s] 71%|███████   | 32/45 [00:03<00:01, 11.57it/s] 78%|███████▊  | 35/45 [00:03<00:00, 10.81it/s] 89%|████████▉ | 40/45 [00:03<00:00, 12.32it/s] 93%|█████████▎| 42/45 [00:04<00:00, 12.39it/s] 98%|█████████▊| 44/45 [00:04<00:00, 13.29it/s]100%|██████████| 45/45 [00:04<00:00, 10.85it/s]
5640 images processed, 4.166344165802002 seconds used

18.63772439956665
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.53  99.36
places365     67.95  81.19
LSUN          17.51  96.09
iSUN          72.25  81.77
dtd           37.89  91.39
forget        79.50  86.90
AVG           46.27  89.45
Retain-Acc: 0.7407
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
15.837071895599365
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3_rf.png
epochs=5 score=-23.1354
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:18<02:49, 18.78s/it] 20%|██        | 2/10 [00:34<02:15, 16.94s/it] 30%|███       | 3/10 [00:50<01:54, 16.32s/it] 40%|████      | 4/10 [01:05<01:36, 16.08s/it] 50%|█████     | 5/10 [01:21<01:19, 15.86s/it] 60%|██████    | 6/10 [01:37<01:03, 15.84s/it] 70%|███████   | 7/10 [01:52<00:47, 15.77s/it] 80%|████████  | 8/10 [02:08<00:31, 15.74s/it][loss] ep 0 it 0 total=13.7434 mle=1.5709 pcon=5.2950 forget=6.8774 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8370 mle=1.5425 pcon=5.2879 forget=7.0065 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8512 mle=1.7004 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0218 mle=1.8999 pcon=5.2738 forget=6.8481 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8930 mle=1.7130 pcon=5.2670 forget=6.9130 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6542 mle=1.5020 pcon=5.2603 forget=6.8919 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7232 mle=1.5598 pcon=5.2540 forget=6.9094 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8765 mle=1.6810 pcon=5.2476 forget=6.9479 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 1 it 10 total=13.9621 mle=1.6716 pcon=5.2409 forget=7.0496 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7061 mle=1.6371 pcon=5.2346 forget=6.8345 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6915 mle=1.5166 pcon=5.2284 forget=6.9465 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9483 mle=1.7802 pcon=5.2224 forget=6.9456 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9065 mle=1.7646 pcon=5.2167 forget=6.9253 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6563 mle=1.6050 pcon=5.2112 forget=6.8401 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8180 mle=1.7235 pcon=5.2056 forget=6.8889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9235 mle=1.7942 pcon=5.2003 forget=6.9290 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 2 it 20 total=13.6297 mle=1.5530 pcon=5.1950 forget=6.8817 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8627 mle=1.8882 pcon=5.1899 forget=6.7847 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8039 mle=1.7421 pcon=5.1846 forget=6.8771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5267 mle=1.5262 pcon=5.1796 forget=6.8210 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6860 mle=1.5858 pcon=5.1745 forget=6.9258 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9020 mle=1.8839 pcon=5.1698 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5394 mle=1.5912 pcon=5.1651 forget=6.7831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7884 mle=1.8109 pcon=5.1604 forget=6.8171 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 3 it 30 total=13.7067 mle=1.7050 pcon=5.1558 forget=6.8459 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7119 mle=1.6712 pcon=5.1518 forget=6.8890 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7348 mle=1.7665 pcon=5.1473 forget=6.8209 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8714 mle=1.9270 pcon=5.1434 forget=6.8010 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.4986 mle=1.5694 pcon=5.1393 forget=6.7898 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7425 mle=1.8359 pcon=5.1351 forget=6.7715 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.6961 mle=1.7028 pcon=5.1308 forget=6.8625 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.6913 mle=1.6956 pcon=5.1272 forget=6.8685 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 4 it 40 total=13.4700 mle=1.6521 pcon=5.1233 forget=6.6946 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6125 mle=1.7319 pcon=5.1192 forget=6.7614 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5720 mle=1.6522 pcon=5.1154 forget=6.8043 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3640 mle=1.4534 pcon=5.1119 forget=6.7987 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6080 mle=1.7663 pcon=5.1079 forget=6.7338 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3132 mle=1.4892 pcon=5.1042 forget=6.7197 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7345 mle=1.8759 pcon=5.1006 forget=6.7581 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 5 it 0 total=13.6934 mle=1.8653 pcon=5.0970 forget=6.7311 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=13.5489 mle=1.8060 pcon=5.0931 forget=6.6498 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=13.4058 mle=1.5904 pcon=5.0897 forget=6.7257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=13.3917 mle=1.5925 pcon=5.0858 forget=6.7134 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=13.4120 mle=1.6921 pcon=5.0826 forget=6.6374 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=13.4547 mle=1.7801 pcon=5.0792 forget=6.5954 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=13.2879 mle=1.6036 pcon=5.0757 forget=6.6086 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=13.4637 mle=1.7400 pcon=5.0724 forget=6.6513 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 6 it 10 total=13.2319 mle=1.4103 pcon=5.0691 forget=6.7524 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=13.1498 mle=1.5274 pcon=5.0660 forget=6.5564 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=13.3005 mle=1.5942 pcon=5.0630 forget=6.6433 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=13.3710 mle=1.6816 pcon=5.0600 forget=6.6294 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=13.3246 mle=1.6309 pcon=5.0576 forget=6.6361 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=13.5427 mle=1.7992 pcon=5.0545 forget=6.6889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=13.3508 mle=1.5471 pcon=5.0521 forget=6.7515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=13.4034 mle=1.6703 pcon=5.0498 forget=6.6833 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 7 it 20 total=13.4032 mle=1.6509 pcon=5.0474 forget=6.7049 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=13.5263 mle=1.7659 pcon=5.0453 forget=6.7151 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=13.4397 mle=1.6174 pcon=5.0432 forget=6.7791 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=13.4470 mle=1.5966 pcon=5.0414 forget=6.8089 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=13.5856 mle=1.6447 pcon=5.0401 forget=6.9008 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=13.7966 mle=1.7801 pcon=5.0388 forget=6.9777 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=13.7837 mle=1.7537 pcon=5.0372 forget=6.9927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=13.6789 mle=1.6351 pcon=5.0361 forget=7.0077 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 30 total=13.7227 mle=1.5613 pcon=5.0352 forget=7.1262 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=13.7310 mle=1.5986 pcon=5.0343 forget=7.0981 nr=64 nf=64 protos=540 fproto_sim=NA
 90%|█████████ | 9/10 [02:24<00:15, 15.83s/it]100%|██████████| 10/10 [02:40<00:00, 15.83s/it]100%|██████████| 10/10 [02:40<00:00, 16.01s/it]
[loss] ep 8 it 130 total=13.6545 mle=1.4970 pcon=5.0331 forget=7.1244 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=13.8710 mle=1.7136 pcon=5.0322 forget=7.1252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=13.6504 mle=1.5295 pcon=5.0314 forget=7.0894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=13.7763 mle=1.7034 pcon=5.0302 forget=7.0427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=13.8859 mle=1.7307 pcon=5.0286 forget=7.1266 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=13.4633 mle=1.4732 pcon=5.0270 forget=6.9630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 40 total=13.4102 mle=1.6205 pcon=5.0252 forget=6.7645 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=13.3209 mle=1.6542 pcon=5.0228 forget=6.6439 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=13.2960 mle=1.8052 pcon=5.0206 forget=6.4701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=12.7673 mle=1.5773 pcon=5.0179 forget=6.1721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=12.6481 mle=1.5923 pcon=5.0147 forget=6.0411 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=12.5618 mle=1.5532 pcon=5.0109 forget=5.9977 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=12.5520 mle=1.5802 pcon=5.0074 forget=5.9644 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:00,  3.23it/s]  3%|▎         | 11/391 [00:00<00:11, 33.00it/s]  5%|▌         | 21/391 [00:00<00:07, 52.78it/s]  8%|▊         | 31/391 [00:00<00:05, 66.15it/s] 10%|█         | 41/391 [00:00<00:04, 73.64it/s] 13%|█▎        | 51/391 [00:00<00:04, 79.14it/s] 16%|█▌        | 61/391 [00:00<00:03, 83.96it/s] 18%|█▊        | 71/391 [00:01<00:03, 87.10it/s] 21%|██        | 81/391 [00:01<00:03, 89.31it/s] 23%|██▎       | 91/391 [00:01<00:03, 90.14it/s] 26%|██▌       | 101/391 [00:01<00:03, 91.20it/s] 28%|██▊       | 111/391 [00:01<00:03, 92.38it/s] 31%|███       | 121/391 [00:01<00:02, 92.82it/s] 34%|███▎      | 131/391 [00:01<00:02, 93.32it/s] 36%|███▌      | 141/391 [00:01<00:02, 92.95it/s] 39%|███▊      | 151/391 [00:01<00:02, 93.41it/s] 41%|████      | 161/391 [00:02<00:02, 93.48it/s] 44%|████▎     | 171/391 [00:02<00:02, 93.80it/s] 46%|████▋     | 181/391 [00:02<00:02, 93.92it/s] 49%|████▉     | 191/391 [00:02<00:02, 93.50it/s] 51%|█████▏    | 201/391 [00:02<00:02, 93.65it/s] 54%|█████▍    | 211/391 [00:02<00:01, 94.15it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.08it/s] 59%|█████▉    | 231/391 [00:02<00:01, 94.52it/s] 62%|██████▏   | 241/391 [00:02<00:01, 94.72it/s] 64%|██████▍   | 251/391 [00:02<00:01, 94.69it/s] 67%|██████▋   | 261/391 [00:03<00:01, 94.94it/s] 69%|██████▉   | 271/391 [00:03<00:01, 95.16it/s] 72%|███████▏  | 281/391 [00:03<00:01, 94.86it/s] 74%|███████▍  | 291/391 [00:03<00:01, 95.03it/s] 77%|███████▋  | 301/391 [00:03<00:00, 94.01it/s] 80%|███████▉  | 311/391 [00:03<00:00, 94.49it/s] 82%|████████▏ | 321/391 [00:03<00:00, 93.97it/s] 85%|████████▍ | 331/391 [00:03<00:00, 94.48it/s] 87%|████████▋ | 341/391 [00:03<00:00, 94.70it/s] 90%|████████▉ | 351/391 [00:04<00:00, 95.02it/s] 92%|█████████▏| 361/391 [00:04<00:00, 92.83it/s] 95%|█████████▍| 371/391 [00:04<00:00, 93.44it/s] 97%|█████████▋| 381/391 [00:04<00:00, 93.70it/s]100%|██████████| 391/391 [00:04<00:00, 91.53it/s]100%|██████████| 391/391 [00:04<00:00, 87.53it/s]
50000 images processed, 4.543082237243652 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:21,  3.68it/s] 14%|█▍        | 11/79 [00:00<00:01, 35.93it/s] 27%|██▋       | 21/79 [00:00<00:01, 55.81it/s] 39%|███▉      | 31/79 [00:00<00:00, 68.32it/s] 52%|█████▏    | 41/79 [00:00<00:00, 76.29it/s] 65%|██████▍   | 51/79 [00:00<00:00, 82.03it/s] 77%|███████▋  | 61/79 [00:00<00:00, 85.72it/s] 90%|████████▉ | 71/79 [00:01<00:00, 88.59it/s]100%|██████████| 79/79 [00:01<00:00, 71.60it/s]
10000 images processed, 1.1248040199279785 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:00,  3.37it/s]  5%|▌         | 11/204 [00:00<00:05, 34.14it/s] 10%|█         | 21/204 [00:00<00:03, 53.34it/s] 15%|█▍        | 30/204 [00:00<00:02, 63.48it/s] 20%|█▉        | 40/204 [00:00<00:02, 72.63it/s] 25%|██▍       | 50/204 [00:00<00:01, 78.40it/s] 29%|██▉       | 60/204 [00:00<00:01, 82.76it/s] 34%|███▍      | 69/204 [00:01<00:01, 83.87it/s] 39%|███▊      | 79/204 [00:01<00:01, 86.30it/s] 44%|████▎     | 89/204 [00:01<00:01, 88.34it/s] 49%|████▊     | 99/204 [00:01<00:01, 86.52it/s] 53%|█████▎    | 109/204 [00:01<00:01, 88.47it/s] 58%|█████▊    | 118/204 [00:01<00:00, 88.06it/s] 63%|██████▎   | 128/204 [00:01<00:00, 89.74it/s] 68%|██████▊   | 138/204 [00:01<00:00, 90.67it/s] 73%|███████▎  | 148/204 [00:01<00:00, 90.33it/s] 77%|███████▋  | 158/204 [00:02<00:00, 91.35it/s] 82%|████████▏ | 168/204 [00:02<00:00, 89.49it/s] 87%|████████▋ | 178/204 [00:02<00:00, 90.61it/s] 92%|█████████▏| 188/204 [00:02<00:00, 91.24it/s] 97%|█████████▋| 198/204 [00:02<00:00, 92.34it/s]100%|██████████| 204/204 [00:02<00:00, 80.55it/s]
26032 images processed, 2.585507392883301 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:35,  2.17it/s]  9%|▉         | 7/79 [00:00<00:04, 15.71it/s] 14%|█▍        | 11/79 [00:00<00:03, 21.05it/s] 22%|██▏       | 17/79 [00:00<00:02, 26.59it/s] 32%|███▏      | 25/79 [00:01<00:01, 32.44it/s] 42%|████▏     | 33/79 [00:01<00:01, 35.61it/s] 52%|█████▏    | 41/79 [00:01<00:00, 41.92it/s] 62%|██████▏   | 49/79 [00:01<00:00, 45.92it/s] 70%|██████▉   | 55/79 [00:01<00:00, 48.67it/s] 77%|███████▋  | 61/79 [00:01<00:00, 42.29it/s] 84%|████████▎ | 66/79 [00:01<00:00, 38.24it/s] 94%|█████████▎| 74/79 [00:02<00:00, 44.31it/s]100%|██████████| 79/79 [00:02<00:00, 36.91it/s]
10000 images processed, 2.181971549987793 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:23,  3.31it/s] 14%|█▍        | 11/79 [00:00<00:02, 33.58it/s] 25%|██▌       | 20/79 [00:00<00:01, 50.80it/s] 38%|███▊      | 30/79 [00:00<00:00, 64.58it/s] 51%|█████     | 40/79 [00:00<00:00, 73.02it/s] 63%|██████▎   | 50/79 [00:00<00:00, 79.62it/s] 76%|███████▌  | 60/79 [00:00<00:00, 84.18it/s] 89%|████████▊ | 70/79 [00:01<00:00, 87.53it/s]100%|██████████| 79/79 [00:01<00:00, 69.86it/s]
10000 images processed, 1.1530733108520508 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:22,  3.03it/s] 16%|█▌        | 11/70 [00:00<00:01, 31.79it/s] 29%|██▊       | 20/70 [00:00<00:01, 49.00it/s] 43%|████▎     | 30/70 [00:00<00:00, 63.19it/s] 57%|█████▋    | 40/70 [00:00<00:00, 72.87it/s] 71%|███████▏  | 50/70 [00:00<00:00, 79.41it/s] 86%|████████▌ | 60/70 [00:00<00:00, 84.01it/s]100%|██████████| 70/70 [00:01<00:00, 86.70it/s]100%|██████████| 70/70 [00:01<00:00, 65.58it/s]
8925 images processed, 1.0941493511199951 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:39,  1.13it/s]  4%|▍         | 2/45 [00:01<00:20,  2.09it/s] 20%|██        | 9/45 [00:01<00:03,  9.78it/s] 24%|██▍       | 11/45 [00:01<00:04,  8.05it/s] 38%|███▊      | 17/45 [00:01<00:02, 13.07it/s] 42%|████▏     | 19/45 [00:02<00:02,  9.78it/s] 53%|█████▎    | 24/45 [00:02<00:01, 14.61it/s] 60%|██████    | 27/45 [00:02<00:01, 10.99it/s] 67%|██████▋   | 30/45 [00:03<00:01, 12.58it/s] 76%|███████▌  | 34/45 [00:03<00:01,  9.65it/s] 93%|█████████▎| 42/45 [00:04<00:00, 10.81it/s]100%|██████████| 45/45 [00:04<00:00, 10.39it/s]
5640 images processed, 4.353352785110474 seconds used

18.8703396320343
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.55  99.35
places365     68.17  81.15
LSUN          17.19  96.14
iSUN          71.75  81.93
dtd           38.33  91.32
forget        78.50  87.02
AVG           46.08  89.49
Retain-Acc: 0.7397
Forget-as-OOD (retain known vs forget novel):
  FPR: 78.50 AUROC: 87.02 AUIN: 98.38
20.835750102996826
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3_rf.png
epochs=10 score=-23.0418
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:17<05:39, 17.87s/it] 10%|█         | 2/20 [00:32<04:50, 16.14s/it] 15%|█▌        | 3/20 [00:47<04:26, 15.66s/it] 20%|██        | 4/20 [01:02<04:06, 15.40s/it] 25%|██▌       | 5/20 [01:18<03:49, 15.32s/it] 30%|███       | 6/20 [01:33<03:34, 15.33s/it] 35%|███▌      | 7/20 [01:48<03:17, 15.21s/it] 40%|████      | 8/20 [02:03<03:02, 15.21s/it][loss] ep 0 it 0 total=13.7435 mle=1.5710 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8370 mle=1.5424 pcon=5.2879 forget=7.0066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8513 mle=1.7005 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0218 mle=1.8998 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8931 mle=1.7132 pcon=5.2670 forget=6.9130 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6543 mle=1.5020 pcon=5.2603 forget=6.8919 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7233 mle=1.5597 pcon=5.2540 forget=6.9095 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8764 mle=1.6810 pcon=5.2476 forget=6.9478 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 1 it 10 total=13.9620 mle=1.6716 pcon=5.2409 forget=7.0496 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7062 mle=1.6371 pcon=5.2346 forget=6.8344 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6914 mle=1.5165 pcon=5.2284 forget=6.9464 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9483 mle=1.7804 pcon=5.2224 forget=6.9455 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9067 mle=1.7648 pcon=5.2167 forget=6.9252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6563 mle=1.6050 pcon=5.2112 forget=6.8401 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8179 mle=1.7235 pcon=5.2056 forget=6.8888 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9237 mle=1.7944 pcon=5.2003 forget=6.9290 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 2 it 20 total=13.6297 mle=1.5530 pcon=5.1950 forget=6.8817 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8629 mle=1.8883 pcon=5.1899 forget=6.7848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8038 mle=1.7420 pcon=5.1846 forget=6.8771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5268 mle=1.5263 pcon=5.1796 forget=6.8210 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6859 mle=1.5857 pcon=5.1745 forget=6.9256 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9021 mle=1.8840 pcon=5.1698 forget=6.8483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5393 mle=1.5911 pcon=5.1651 forget=6.7831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7886 mle=1.8112 pcon=5.1604 forget=6.8170 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 3 it 30 total=13.7066 mle=1.7049 pcon=5.1558 forget=6.8459 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7121 mle=1.6713 pcon=5.1518 forget=6.8890 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7348 mle=1.7664 pcon=5.1474 forget=6.8211 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8712 mle=1.9270 pcon=5.1434 forget=6.8008 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.4987 mle=1.5695 pcon=5.1393 forget=6.7899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7424 mle=1.8358 pcon=5.1351 forget=6.7715 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.6962 mle=1.7028 pcon=5.1308 forget=6.8626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.6910 mle=1.6954 pcon=5.1272 forget=6.8685 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 4 it 40 total=13.4700 mle=1.6521 pcon=5.1233 forget=6.6946 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6123 mle=1.7318 pcon=5.1192 forget=6.7613 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5723 mle=1.6523 pcon=5.1155 forget=6.8046 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3639 mle=1.4533 pcon=5.1119 forget=6.7987 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6082 mle=1.7665 pcon=5.1079 forget=6.7338 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3131 mle=1.4891 pcon=5.1042 forget=6.7198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7343 mle=1.8758 pcon=5.1006 forget=6.7580 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 5 it 0 total=13.6935 mle=1.8653 pcon=5.0970 forget=6.7313 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=13.5489 mle=1.8061 pcon=5.0931 forget=6.6497 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=13.4059 mle=1.5904 pcon=5.0897 forget=6.7258 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=13.3917 mle=1.5924 pcon=5.0858 forget=6.7134 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=13.4118 mle=1.6919 pcon=5.0826 forget=6.6374 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=13.4545 mle=1.7800 pcon=5.0792 forget=6.5953 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=13.2879 mle=1.6036 pcon=5.0757 forget=6.6086 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=13.4639 mle=1.7402 pcon=5.0724 forget=6.6514 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 6 it 10 total=13.2321 mle=1.4104 pcon=5.0692 forget=6.7525 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=13.1498 mle=1.5274 pcon=5.0660 forget=6.5565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=13.3005 mle=1.5942 pcon=5.0630 forget=6.6433 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=13.3710 mle=1.6816 pcon=5.0600 forget=6.6295 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=13.3247 mle=1.6310 pcon=5.0576 forget=6.6361 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=13.5425 mle=1.7990 pcon=5.0546 forget=6.6889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=13.3507 mle=1.5471 pcon=5.0521 forget=6.7515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=13.4034 mle=1.6703 pcon=5.0498 forget=6.6833 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 7 it 20 total=13.4032 mle=1.6510 pcon=5.0474 forget=6.7049 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=13.5263 mle=1.7659 pcon=5.0453 forget=6.7151 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=13.4399 mle=1.6176 pcon=5.0432 forget=6.7790 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=13.4468 mle=1.5965 pcon=5.0415 forget=6.8089 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=13.5854 mle=1.6445 pcon=5.0401 forget=6.9008 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=13.7965 mle=1.7800 pcon=5.0388 forget=6.9777 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=13.7837 mle=1.7537 pcon=5.0372 forget=6.9927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=13.6791 mle=1.6352 pcon=5.0361 forget=7.0077 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 30 total=13.7225 mle=1.5611 pcon=5.0352 forget=7.1262 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=13.7310 mle=1.5985 pcon=5.0343 forget=7.0982 nr=64 nf=64 protos=540 fproto_sim=NA
 45%|████▌     | 9/20 [02:18<02:46, 15.16s/it] 50%|█████     | 10/20 [02:33<02:31, 15.18s/it] 55%|█████▌    | 11/20 [02:49<02:17, 15.23s/it] 60%|██████    | 12/20 [03:04<02:01, 15.22s/it] 65%|██████▌   | 13/20 [03:19<01:46, 15.24s/it] 70%|███████   | 14/20 [03:35<01:31, 15.28s/it] 75%|███████▌  | 15/20 [03:50<01:16, 15.26s/it] 80%|████████  | 16/20 [04:05<01:01, 15.27s/it] 85%|████████▌ | 17/20 [04:20<00:45, 15.26s/it][loss] ep 8 it 130 total=13.6543 mle=1.4968 pcon=5.0331 forget=7.1244 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=13.8711 mle=1.7137 pcon=5.0322 forget=7.1252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=13.6506 mle=1.5297 pcon=5.0315 forget=7.0894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=13.7763 mle=1.7034 pcon=5.0302 forget=7.0427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=13.8859 mle=1.7307 pcon=5.0286 forget=7.1266 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=13.4632 mle=1.4731 pcon=5.0270 forget=6.9631 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 40 total=13.4101 mle=1.6204 pcon=5.0252 forget=6.7645 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=13.3207 mle=1.6540 pcon=5.0228 forget=6.6439 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=13.2958 mle=1.8051 pcon=5.0206 forget=6.4701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=12.7674 mle=1.5773 pcon=5.0179 forget=6.1721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=12.6481 mle=1.5922 pcon=5.0147 forget=6.0411 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=12.5617 mle=1.5531 pcon=5.0109 forget=5.9977 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=12.5520 mle=1.5802 pcon=5.0074 forget=5.9644 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 10 it 0 total=12.8018 mle=1.6823 pcon=5.0037 forget=6.1158 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=12.8264 mle=1.6227 pcon=5.0000 forget=6.2037 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=12.9458 mle=1.7456 pcon=4.9967 forget=6.2035 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=13.1366 mle=1.8062 pcon=4.9936 forget=6.3368 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=13.3377 mle=1.8842 pcon=4.9906 forget=6.4629 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=13.2327 mle=1.6571 pcon=4.9880 forget=6.5877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=13.3262 mle=1.6402 pcon=4.9857 forget=6.7003 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=13.4490 mle=1.6387 pcon=4.9835 forget=6.8269 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 10 total=13.5507 mle=1.5990 pcon=4.9815 forget=6.9702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=13.7404 mle=1.7202 pcon=4.9797 forget=7.0406 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=13.7618 mle=1.6309 pcon=4.9783 forget=7.1526 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=13.9556 mle=1.6970 pcon=4.9766 forget=7.2820 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=14.0704 mle=1.7385 pcon=4.9756 forget=7.3562 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=13.9819 mle=1.6106 pcon=4.9751 forget=7.3961 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=14.3541 mle=1.9732 pcon=4.9741 forget=7.4068 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=14.2351 mle=1.7617 pcon=4.9734 forget=7.5000 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 20 total=14.0207 mle=1.5777 pcon=4.9726 forget=7.4704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=14.1914 mle=1.7527 pcon=4.9719 forget=7.4668 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=13.8954 mle=1.6183 pcon=4.9711 forget=7.3060 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=13.8862 mle=1.7430 pcon=4.9703 forget=7.1729 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=13.7453 mle=1.5935 pcon=4.9692 forget=7.1826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=13.6312 mle=1.5690 pcon=4.9683 forget=7.0938 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=13.6286 mle=1.5225 pcon=4.9668 forget=7.1394 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=13.6764 mle=1.8437 pcon=4.9649 forget=6.8677 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 30 total=13.4087 mle=1.4763 pcon=4.9630 forget=6.9694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=13.2879 mle=1.4676 pcon=4.9608 forget=6.8595 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=13.1603 mle=1.5212 pcon=4.9582 forget=6.6810 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=13.2375 mle=1.6233 pcon=4.9556 forget=6.6586 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=13.2113 mle=1.6663 pcon=4.9527 forget=6.5923 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=13.0827 mle=1.5381 pcon=4.9496 forget=6.5949 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=12.9866 mle=1.5918 pcon=4.9462 forget=6.4486 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=12.9372 mle=1.6007 pcon=4.9426 forget=6.3939 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 40 total=13.0407 mle=1.8053 pcon=4.9387 forget=6.2967 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=13.0134 mle=1.7810 pcon=4.9350 forget=6.2973 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=12.7601 mle=1.5867 pcon=4.9313 forget=6.2421 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=12.9360 mle=1.8102 pcon=4.9271 forget=6.1987 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=12.8403 mle=1.6761 pcon=4.9233 forget=6.2409 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=12.8019 mle=1.6479 pcon=4.9192 forget=6.2347 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=12.8790 mle=1.6967 pcon=4.9152 forget=6.2671 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 15 it 0 total=12.9668 mle=1.7506 pcon=4.9113 forget=6.3050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=12.7478 mle=1.5171 pcon=4.9071 forget=6.3235 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=12.8570 mle=1.5978 pcon=4.9034 forget=6.3558 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=13.0278 mle=1.7143 pcon=4.8997 forget=6.4138 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=12.9896 mle=1.6507 pcon=4.8962 forget=6.4426 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=13.4073 mle=2.0026 pcon=4.8929 forget=6.5119 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=13.0456 mle=1.5787 pcon=4.8897 forget=6.5771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=13.1108 mle=1.5611 pcon=4.8867 forget=6.6630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 10 total=13.1545 mle=1.6268 pcon=4.8836 forget=6.6440 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=13.1195 mle=1.5388 pcon=4.8809 forget=6.6997 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=13.2101 mle=1.5820 pcon=4.8783 forget=6.7497 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=13.2898 mle=1.6038 pcon=4.8755 forget=6.8105 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=13.2894 mle=1.5982 pcon=4.8728 forget=6.8185 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=13.6041 mle=1.8650 pcon=4.8701 forget=6.8689 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=13.5359 mle=1.7653 pcon=4.8677 forget=6.9029 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=13.4139 mle=1.6169 pcon=4.8654 forget=6.9316 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 20 total=13.6477 mle=1.7873 pcon=4.8629 forget=6.9975 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=13.5966 mle=1.7701 pcon=4.8608 forget=6.9658 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=13.5470 mle=1.6218 pcon=4.8586 forget=7.0666 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=13.5172 mle=1.5890 pcon=4.8562 forget=7.0719 nr=64 nf=64 protos=540 fproto_sim=NA
 90%|█████████ | 18/20 [04:35<00:30, 15.20s/it] 95%|█████████▌| 19/20 [04:51<00:15, 15.26s/it]100%|██████████| 20/20 [05:06<00:00, 15.23s/it]100%|██████████| 20/20 [05:06<00:00, 15.32s/it]
[loss] ep 17 it 220 total=13.5929 mle=1.6369 pcon=4.8541 forget=7.1019 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=13.4737 mle=1.5220 pcon=4.8524 forget=7.0993 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=13.8240 mle=1.8281 pcon=4.8505 forget=7.1453 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=13.7796 mle=1.7583 pcon=4.8483 forget=7.1730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=13.5916 mle=1.5424 pcon=4.8468 forget=7.2025 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=13.7171 mle=1.6117 pcon=4.8449 forget=7.2604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=13.7555 mle=1.6634 pcon=4.8432 forget=7.2490 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=13.6773 mle=1.5516 pcon=4.8414 forget=7.2843 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=13.8188 mle=1.6745 pcon=4.8397 forget=7.3046 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=13.8026 mle=1.6251 pcon=4.8381 forget=7.3394 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=13.8738 mle=1.7289 pcon=4.8366 forget=7.3083 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=13.9124 mle=1.6847 pcon=4.8352 forget=7.3924 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 40 total=13.8191 mle=1.5345 pcon=4.8339 forget=7.4508 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=14.0605 mle=1.6650 pcon=4.8326 forget=7.5630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=13.8673 mle=1.5673 pcon=4.8311 forget=7.4689 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=13.9736 mle=1.6694 pcon=4.8296 forget=7.4745 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=14.2353 mle=1.8788 pcon=4.8284 forget=7.5281 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=14.1609 mle=1.6752 pcon=4.8273 forget=7.6584 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=14.0024 mle=1.5506 pcon=4.8262 forget=7.6256 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:43,  3.76it/s]  3%|▎         | 11/391 [00:00<00:10, 36.44it/s]  5%|▌         | 21/391 [00:00<00:06, 56.58it/s]  8%|▊         | 30/391 [00:00<00:05, 67.08it/s] 10%|█         | 40/391 [00:00<00:04, 76.44it/s] 13%|█▎        | 50/391 [00:00<00:04, 82.18it/s] 15%|█▌        | 60/391 [00:00<00:03, 86.48it/s] 18%|█▊        | 70/391 [00:00<00:03, 89.28it/s] 20%|██        | 80/391 [00:01<00:03, 91.34it/s] 23%|██▎       | 90/391 [00:01<00:03, 91.99it/s] 26%|██▌       | 100/391 [00:01<00:03, 93.03it/s] 28%|██▊       | 110/391 [00:01<00:02, 93.79it/s] 31%|███       | 120/391 [00:01<00:02, 94.33it/s] 33%|███▎      | 130/391 [00:01<00:02, 93.78it/s] 36%|███▌      | 140/391 [00:01<00:02, 94.64it/s] 38%|███▊      | 150/391 [00:01<00:02, 95.15it/s] 41%|████      | 160/391 [00:01<00:02, 95.35it/s] 43%|████▎     | 170/391 [00:02<00:02, 94.24it/s] 46%|████▌     | 180/391 [00:02<00:02, 92.51it/s] 49%|████▊     | 190/391 [00:02<00:02, 93.67it/s] 51%|█████     | 200/391 [00:02<00:02, 94.35it/s] 54%|█████▎    | 210/391 [00:02<00:01, 93.54it/s] 56%|█████▋    | 220/391 [00:02<00:01, 93.71it/s] 59%|█████▉    | 230/391 [00:02<00:01, 93.46it/s] 61%|██████▏   | 240/391 [00:02<00:01, 93.60it/s] 64%|██████▍   | 250/391 [00:02<00:01, 92.78it/s] 66%|██████▋   | 260/391 [00:03<00:01, 93.27it/s] 69%|██████▉   | 270/391 [00:03<00:01, 92.28it/s] 72%|███████▏  | 280/391 [00:03<00:01, 93.04it/s] 74%|███████▍  | 290/391 [00:03<00:01, 93.78it/s] 77%|███████▋  | 300/391 [00:03<00:00, 94.60it/s] 79%|███████▉  | 310/391 [00:03<00:00, 95.02it/s] 82%|████████▏ | 320/391 [00:03<00:00, 95.40it/s] 84%|████████▍ | 330/391 [00:03<00:00, 95.55it/s] 87%|████████▋ | 340/391 [00:03<00:00, 95.85it/s] 90%|████████▉ | 350/391 [00:03<00:00, 95.98it/s] 92%|█████████▏| 360/391 [00:04<00:00, 95.98it/s] 95%|█████████▍| 370/391 [00:04<00:00, 95.48it/s] 97%|█████████▋| 380/391 [00:04<00:00, 95.45it/s]100%|█████████▉| 390/391 [00:04<00:00, 96.06it/s]100%|██████████| 391/391 [00:04<00:00, 88.89it/s]
50000 images processed, 4.712776184082031 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:24,  3.14it/s] 14%|█▍        | 11/79 [00:00<00:02, 32.35it/s] 27%|██▋       | 21/79 [00:00<00:01, 51.79it/s] 39%|███▉      | 31/79 [00:00<00:00, 63.48it/s] 52%|█████▏    | 41/79 [00:00<00:00, 72.74it/s] 65%|██████▍   | 51/79 [00:00<00:00, 79.08it/s] 77%|███████▋  | 61/79 [00:00<00:00, 83.91it/s] 90%|████████▉ | 71/79 [00:01<00:00, 87.55it/s]100%|██████████| 79/79 [00:01<00:00, 68.15it/s]
10000 images processed, 1.2005195617675781 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:09,  2.92it/s]  5%|▌         | 11/204 [00:00<00:06, 31.00it/s] 10%|█         | 21/204 [00:00<00:03, 50.39it/s] 15%|█▌        | 31/204 [00:00<00:02, 62.85it/s] 20%|██        | 41/204 [00:00<00:02, 72.31it/s] 25%|██▌       | 51/204 [00:00<00:01, 78.44it/s] 30%|██▉       | 61/204 [00:00<00:01, 82.75it/s] 34%|███▍      | 70/204 [00:01<00:01, 84.75it/s] 39%|███▉      | 80/204 [00:01<00:01, 87.60it/s] 44%|████▍     | 90/204 [00:01<00:01, 89.59it/s] 49%|████▉     | 100/204 [00:01<00:01, 90.82it/s] 54%|█████▍    | 110/204 [00:01<00:01, 90.38it/s] 59%|█████▉    | 120/204 [00:01<00:00, 90.24it/s] 64%|██████▎   | 130/204 [00:01<00:00, 91.34it/s] 69%|██████▊   | 140/204 [00:01<00:00, 92.03it/s] 74%|███████▎  | 150/204 [00:01<00:00, 92.27it/s] 78%|███████▊  | 160/204 [00:02<00:00, 92.16it/s] 83%|████████▎ | 170/204 [00:02<00:00, 92.93it/s] 88%|████████▊ | 180/204 [00:02<00:00, 93.52it/s] 93%|█████████▎| 190/204 [00:02<00:00, 93.60it/s] 98%|█████████▊| 200/204 [00:02<00:00, 94.32it/s]100%|██████████| 204/204 [00:02<00:00, 80.78it/s]
26032 images processed, 2.580446243286133 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:33,  2.31it/s] 11%|█▏        | 9/79 [00:00<00:03, 17.94it/s] 22%|██▏       | 17/79 [00:00<00:02, 27.56it/s] 32%|███▏      | 25/79 [00:00<00:01, 33.75it/s] 42%|████▏     | 33/79 [00:01<00:01, 38.45it/s] 48%|████▊     | 38/79 [00:01<00:01, 40.79it/s] 54%|█████▍    | 43/79 [00:01<00:00, 37.33it/s] 65%|██████▍   | 51/79 [00:01<00:00, 38.89it/s] 75%|███████▍  | 59/79 [00:01<00:00, 39.87it/s] 85%|████████▍ | 67/79 [00:01<00:00, 40.71it/s] 95%|█████████▍| 75/79 [00:02<00:00, 41.43it/s]100%|██████████| 79/79 [00:02<00:00, 36.26it/s]
10000 images processed, 2.2133657932281494 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:26,  2.97it/s] 14%|█▍        | 11/79 [00:00<00:02, 31.35it/s] 27%|██▋       | 21/79 [00:00<00:01, 50.94it/s] 39%|███▉      | 31/79 [00:00<00:00, 64.23it/s] 52%|█████▏    | 41/79 [00:00<00:00, 72.85it/s] 65%|██████▍   | 51/79 [00:00<00:00, 79.27it/s] 77%|███████▋  | 61/79 [00:00<00:00, 83.78it/s] 90%|████████▉ | 71/79 [00:01<00:00, 86.71it/s]100%|██████████| 79/79 [00:01<00:00, 68.02it/s]
10000 images processed, 1.1943223476409912 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:27,  2.53it/s] 16%|█▌        | 11/70 [00:00<00:02, 27.92it/s] 30%|███       | 21/70 [00:00<00:01, 46.53it/s] 44%|████▍     | 31/70 [00:00<00:00, 60.31it/s] 59%|█████▊    | 41/70 [00:00<00:00, 70.29it/s] 73%|███████▎  | 51/70 [00:00<00:00, 77.42it/s] 87%|████████▋ | 61/70 [00:01<00:00, 82.78it/s]100%|██████████| 70/70 [00:01<00:00, 61.89it/s]
8925 images processed, 1.165691614151001 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:31,  1.38it/s]  4%|▍         | 2/45 [00:00<00:17,  2.44it/s] 20%|██        | 9/45 [00:01<00:03, 10.90it/s] 24%|██▍       | 11/45 [00:01<00:03, 10.66it/s] 31%|███       | 14/45 [00:01<00:02, 13.65it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.81it/s] 42%|████▏     | 19/45 [00:01<00:01, 13.26it/s] 49%|████▉     | 22/45 [00:02<00:01, 14.14it/s] 56%|█████▌    | 25/45 [00:02<00:01, 13.96it/s] 60%|██████    | 27/45 [00:02<00:01, 14.00it/s] 67%|██████▋   | 30/45 [00:02<00:01, 14.07it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.31it/s] 78%|███████▊  | 35/45 [00:03<00:00, 12.20it/s] 91%|█████████ | 41/45 [00:03<00:00, 13.61it/s] 96%|█████████▌| 43/45 [00:03<00:00, 12.03it/s]100%|██████████| 45/45 [00:03<00:00, 11.83it/s]
5640 images processed, 3.826378583908081 seconds used

18.533953189849854
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.41  99.22
places365     72.26  79.69
LSUN          23.24  95.39
iSUN          73.50  81.09
dtd           41.47  90.44
forget        58.00  88.68
AVG           45.31  89.08
Retain-Acc: 0.7392
Forget-as-OOD (retain known vs forget novel):
  FPR: 58.00 AUROC: 88.68 AUIN: 98.56
53.20500898361206
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3_rf.png
epochs=20 score=-22.657
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:21<17:24, 21.32s/it]  4%|▍         | 2/50 [00:37<14:31, 18.15s/it]  6%|▌         | 3/50 [00:52<13:15, 16.93s/it]  8%|▊         | 4/50 [01:08<12:35, 16.42s/it] 10%|█         | 5/50 [01:23<12:04, 16.10s/it] 12%|█▏        | 6/50 [01:39<11:42, 15.96s/it] 14%|█▍        | 7/50 [01:55<11:27, 15.99s/it] 16%|█▌        | 8/50 [02:11<11:12, 16.01s/it][loss] ep 0 it 0 total=13.7435 mle=1.5710 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8370 mle=1.5424 pcon=5.2879 forget=7.0066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8513 mle=1.7005 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0218 mle=1.8998 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8931 mle=1.7132 pcon=5.2670 forget=6.9130 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6543 mle=1.5020 pcon=5.2603 forget=6.8919 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7233 mle=1.5597 pcon=5.2540 forget=6.9095 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8764 mle=1.6810 pcon=5.2476 forget=6.9478 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 1 it 10 total=13.9620 mle=1.6716 pcon=5.2409 forget=7.0496 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7062 mle=1.6371 pcon=5.2346 forget=6.8344 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6914 mle=1.5165 pcon=5.2284 forget=6.9464 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9483 mle=1.7804 pcon=5.2224 forget=6.9455 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9066 mle=1.7647 pcon=5.2167 forget=6.9252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6561 mle=1.6049 pcon=5.2112 forget=6.8400 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8182 mle=1.7237 pcon=5.2056 forget=6.8889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9233 mle=1.7941 pcon=5.2003 forget=6.9289 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 2 it 20 total=13.6296 mle=1.5528 pcon=5.1950 forget=6.8817 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8627 mle=1.8881 pcon=5.1899 forget=6.7848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8039 mle=1.7422 pcon=5.1846 forget=6.8771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5266 mle=1.5261 pcon=5.1796 forget=6.8209 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6860 mle=1.5859 pcon=5.1745 forget=6.9256 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9022 mle=1.8840 pcon=5.1698 forget=6.8483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5394 mle=1.5912 pcon=5.1651 forget=6.7831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7884 mle=1.8110 pcon=5.1604 forget=6.8170 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 3 it 30 total=13.7067 mle=1.7050 pcon=5.1559 forget=6.8458 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7120 mle=1.6712 pcon=5.1518 forget=6.8890 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7348 mle=1.7665 pcon=5.1474 forget=6.8210 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8673 mle=1.9230 pcon=5.1434 forget=6.8009 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.4987 mle=1.5695 pcon=5.1393 forget=6.7899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7425 mle=1.8358 pcon=5.1351 forget=6.7715 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.6962 mle=1.7028 pcon=5.1308 forget=6.8626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.6913 mle=1.6955 pcon=5.1272 forget=6.8686 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 4 it 40 total=13.4700 mle=1.6521 pcon=5.1233 forget=6.6946 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6124 mle=1.7318 pcon=5.1192 forget=6.7614 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5720 mle=1.6521 pcon=5.1155 forget=6.8044 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3640 mle=1.4534 pcon=5.1119 forget=6.7987 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6084 mle=1.7665 pcon=5.1079 forget=6.7340 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3133 mle=1.4893 pcon=5.1042 forget=6.7198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7347 mle=1.8760 pcon=5.1006 forget=6.7581 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 5 it 0 total=13.6934 mle=1.8652 pcon=5.0970 forget=6.7311 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=13.5490 mle=1.8060 pcon=5.0931 forget=6.6498 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=13.4056 mle=1.5903 pcon=5.0897 forget=6.7257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=13.3916 mle=1.5923 pcon=5.0858 forget=6.7135 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=13.4120 mle=1.6921 pcon=5.0826 forget=6.6374 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=13.4547 mle=1.7801 pcon=5.0792 forget=6.5954 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=13.2879 mle=1.6036 pcon=5.0757 forget=6.6086 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=13.4637 mle=1.7399 pcon=5.0724 forget=6.6514 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 6 it 10 total=13.2318 mle=1.4103 pcon=5.0691 forget=6.7524 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=13.1496 mle=1.5272 pcon=5.0660 forget=6.5564 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=13.3005 mle=1.5942 pcon=5.0630 forget=6.6433 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=13.3711 mle=1.6817 pcon=5.0600 forget=6.6295 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=13.3248 mle=1.6311 pcon=5.0575 forget=6.6361 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=13.5427 mle=1.7993 pcon=5.0545 forget=6.6889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=13.3505 mle=1.5469 pcon=5.0521 forget=6.7515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=13.4035 mle=1.6704 pcon=5.0498 forget=6.6833 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 7 it 20 total=13.4034 mle=1.6512 pcon=5.0474 forget=6.7049 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=13.5264 mle=1.7660 pcon=5.0453 forget=6.7151 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=13.4398 mle=1.6175 pcon=5.0432 forget=6.7790 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=13.4469 mle=1.5965 pcon=5.0414 forget=6.8089 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=13.5856 mle=1.6446 pcon=5.0401 forget=6.9009 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=13.7964 mle=1.7799 pcon=5.0388 forget=6.9777 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=13.7836 mle=1.7537 pcon=5.0372 forget=6.9927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=13.6789 mle=1.6351 pcon=5.0361 forget=7.0076 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 30 total=13.7228 mle=1.5613 pcon=5.0352 forget=7.1262 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=13.7310 mle=1.5986 pcon=5.0343 forget=7.0982 nr=64 nf=64 protos=540 fproto_sim=NA
 18%|█▊        | 9/50 [02:27<10:52, 15.93s/it] 20%|██        | 10/50 [02:43<10:38, 15.96s/it] 22%|██▏       | 11/50 [02:58<10:16, 15.80s/it] 24%|██▍       | 12/50 [03:15<10:04, 15.90s/it] 26%|██▌       | 13/50 [03:30<09:48, 15.90s/it] 28%|██▊       | 14/50 [03:47<09:38, 16.06s/it] 30%|███       | 15/50 [04:03<09:25, 16.17s/it] 32%|███▏      | 16/50 [04:20<09:20, 16.47s/it] 34%|███▍      | 17/50 [04:37<09:01, 16.41s/it][loss] ep 8 it 130 total=13.6543 mle=1.4969 pcon=5.0331 forget=7.1244 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=13.8711 mle=1.7138 pcon=5.0322 forget=7.1252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=13.6503 mle=1.5294 pcon=5.0315 forget=7.0894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=13.7776 mle=1.7046 pcon=5.0302 forget=7.0427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=13.8855 mle=1.7304 pcon=5.0286 forget=7.1265 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=13.4633 mle=1.4733 pcon=5.0270 forget=6.9630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 40 total=13.4101 mle=1.6204 pcon=5.0252 forget=6.7645 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=13.3206 mle=1.6540 pcon=5.0228 forget=6.6438 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=13.2959 mle=1.8051 pcon=5.0206 forget=6.4701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=12.7674 mle=1.5775 pcon=5.0179 forget=6.1721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=12.6483 mle=1.5924 pcon=5.0147 forget=6.0412 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=12.5617 mle=1.5531 pcon=5.0109 forget=5.9977 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=12.5522 mle=1.5804 pcon=5.0074 forget=5.9645 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 10 it 0 total=12.8019 mle=1.6822 pcon=5.0037 forget=6.1159 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=12.8266 mle=1.6229 pcon=5.0000 forget=6.2038 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=12.9458 mle=1.7455 pcon=4.9967 forget=6.2036 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=13.1367 mle=1.8062 pcon=4.9936 forget=6.3369 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=13.3378 mle=1.8844 pcon=4.9906 forget=6.4628 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=13.2326 mle=1.6571 pcon=4.9880 forget=6.5875 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=13.3245 mle=1.6388 pcon=4.9857 forget=6.7000 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=13.4485 mle=1.6385 pcon=4.9835 forget=6.8265 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 10 total=13.5495 mle=1.5987 pcon=4.9815 forget=6.9693 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=13.7391 mle=1.7201 pcon=4.9797 forget=7.0393 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=13.7590 mle=1.6307 pcon=4.9783 forget=7.1500 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=13.9514 mle=1.6967 pcon=4.9766 forget=7.2782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=14.0640 mle=1.7384 pcon=4.9756 forget=7.3499 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=13.9690 mle=1.6103 pcon=4.9750 forget=7.3836 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=14.3321 mle=1.9729 pcon=4.9741 forget=7.3851 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=14.2091 mle=1.7615 pcon=4.9733 forget=7.4743 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 20 total=13.9792 mle=1.5781 pcon=4.9725 forget=7.4286 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=14.1343 mle=1.7512 pcon=4.9718 forget=7.4113 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=13.8211 mle=1.6192 pcon=4.9709 forget=7.2310 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=13.8050 mle=1.7464 pcon=4.9700 forget=7.0886 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=13.6528 mle=1.6004 pcon=4.9686 forget=7.0838 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=13.5371 mle=1.5756 pcon=4.9675 forget=6.9940 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=13.5107 mle=1.5278 pcon=4.9657 forget=7.0171 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=13.5788 mle=1.8443 pcon=4.9636 forget=6.7709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 30 total=13.2969 mle=1.4808 pcon=4.9613 forget=6.8548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=13.1716 mle=1.4619 pcon=4.9587 forget=6.7510 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=13.0291 mle=1.5113 pcon=4.9556 forget=6.5622 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=13.0820 mle=1.6142 pcon=4.9525 forget=6.5153 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=13.0487 mle=1.6725 pcon=4.9490 forget=6.4272 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=12.8510 mle=1.5179 pcon=4.9452 forget=6.3879 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=12.7763 mle=1.5893 pcon=4.9410 forget=6.2461 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=12.7379 mle=1.6245 pcon=4.9366 forget=6.1768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 40 total=12.8705 mle=1.8181 pcon=4.9318 forget=6.1207 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=12.8480 mle=1.7901 pcon=4.9273 forget=6.1306 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=12.6773 mle=1.6150 pcon=4.9228 forget=6.1395 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=12.8674 mle=1.8050 pcon=4.9181 forget=6.1443 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=12.8470 mle=1.7067 pcon=4.9137 forget=6.2266 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=12.8581 mle=1.6683 pcon=4.9091 forget=6.2807 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=12.9883 mle=1.7345 pcon=4.9047 forget=6.3491 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 15 it 0 total=13.0764 mle=1.7561 pcon=4.9006 forget=6.4197 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=12.9301 mle=1.5490 pcon=4.8962 forget=6.4849 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=13.0449 mle=1.6035 pcon=4.8922 forget=6.5492 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=13.2285 mle=1.7102 pcon=4.8885 forget=6.6298 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=13.2140 mle=1.6586 pcon=4.8849 forget=6.6705 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=13.6084 mle=1.9905 pcon=4.8814 forget=6.7365 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=13.2655 mle=1.5989 pcon=4.8782 forget=6.7884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=13.2841 mle=1.5775 pcon=4.8749 forget=6.8317 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 10 total=13.2445 mle=1.6126 pcon=4.8716 forget=6.7602 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=13.1914 mle=1.5490 pcon=4.8684 forget=6.7739 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=13.1665 mle=1.5611 pcon=4.8654 forget=6.7400 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=13.1397 mle=1.5397 pcon=4.8619 forget=6.7381 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=13.0709 mle=1.5725 pcon=4.8584 forget=6.6400 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=13.2780 mle=1.8242 pcon=4.8547 forget=6.5990 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=13.1940 mle=1.7843 pcon=4.8513 forget=6.5584 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=12.9483 mle=1.5980 pcon=4.8478 forget=6.5026 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 20 total=13.1332 mle=1.7946 pcon=4.8440 forget=6.4946 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=13.0010 mle=1.7591 pcon=4.8405 forget=6.4014 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=12.9076 mle=1.6388 pcon=4.8369 forget=6.4319 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=12.8133 mle=1.5822 pcon=4.8331 forget=6.3981 nr=64 nf=64 protos=540 fproto_sim=NA
 36%|███▌      | 18/50 [04:52<08:36, 16.14s/it] 38%|███▊      | 19/50 [05:08<08:15, 15.97s/it] 40%|████      | 20/50 [05:23<07:56, 15.87s/it] 42%|████▏     | 21/50 [05:39<07:38, 15.82s/it] 44%|████▍     | 22/50 [05:55<07:19, 15.71s/it] 46%|████▌     | 23/50 [06:10<07:05, 15.74s/it] 48%|████▊     | 24/50 [06:26<06:49, 15.76s/it] 50%|█████     | 25/50 [06:42<06:34, 15.76s/it] 52%|█████▏    | 26/50 [06:59<06:26, 16.10s/it][loss] ep 17 it 220 total=12.8346 mle=1.6166 pcon=4.8295 forget=6.3885 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=12.6863 mle=1.5434 pcon=4.8263 forget=6.3166 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=12.9712 mle=1.8496 pcon=4.8229 forget=6.2986 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=12.9051 mle=1.7861 pcon=4.8193 forget=6.2997 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=12.6659 mle=1.5765 pcon=4.8164 forget=6.2729 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=12.7995 mle=1.6484 pcon=4.8133 forget=6.3378 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=12.8751 mle=1.6840 pcon=4.8104 forget=6.3808 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=12.7155 mle=1.5739 pcon=4.8075 forget=6.3341 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=12.8673 mle=1.6916 pcon=4.8048 forget=6.3709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=12.9002 mle=1.6873 pcon=4.8022 forget=6.4106 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=13.0241 mle=1.7754 pcon=4.7999 forget=6.4488 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=13.0664 mle=1.7504 pcon=4.7978 forget=6.5182 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 40 total=13.0110 mle=1.6530 pcon=4.7958 forget=6.5622 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=13.1042 mle=1.6672 pcon=4.7939 forget=6.6431 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=13.0376 mle=1.5948 pcon=4.7920 forget=6.6508 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=13.1275 mle=1.6927 pcon=4.7901 forget=6.6447 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=13.3325 mle=1.8517 pcon=4.7884 forget=6.6924 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=13.2648 mle=1.7414 pcon=4.7870 forget=6.7363 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=13.1761 mle=1.5999 pcon=4.7855 forget=6.7907 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 0 total=13.2423 mle=1.6750 pcon=4.7839 forget=6.7834 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=13.2611 mle=1.6981 pcon=4.7823 forget=6.7807 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=13.2740 mle=1.6981 pcon=4.7808 forget=6.7950 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=13.3531 mle=1.8126 pcon=4.7790 forget=6.7615 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=13.0870 mle=1.5797 pcon=4.7772 forget=6.7301 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=13.3917 mle=1.9074 pcon=4.7754 forget=6.7089 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=13.3325 mle=1.8299 pcon=4.7734 forget=6.7291 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=12.9662 mle=1.5463 pcon=4.7715 forget=6.6484 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 10 total=12.9285 mle=1.5274 pcon=4.7696 forget=6.6315 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=13.0642 mle=1.6723 pcon=4.7675 forget=6.6244 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=12.9042 mle=1.5715 pcon=4.7657 forget=6.5671 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=12.9127 mle=1.6258 pcon=4.7638 forget=6.5232 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=13.0019 mle=1.7311 pcon=4.7618 forget=6.5090 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=12.8791 mle=1.6610 pcon=4.7598 forget=6.4583 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=12.8293 mle=1.6456 pcon=4.7576 forget=6.4261 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=12.7276 mle=1.5482 pcon=4.7555 forget=6.4240 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 20 total=12.7646 mle=1.6667 pcon=4.7534 forget=6.3445 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=12.7517 mle=1.7123 pcon=4.7513 forget=6.2881 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=12.6317 mle=1.6047 pcon=4.7490 forget=6.2780 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=12.6331 mle=1.6236 pcon=4.7469 forget=6.2625 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=12.5822 mle=1.6016 pcon=4.7449 forget=6.2357 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=12.4809 mle=1.5344 pcon=4.7428 forget=6.2037 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=12.6347 mle=1.6999 pcon=4.7408 forget=6.1941 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=12.8502 mle=1.9085 pcon=4.7386 forget=6.2031 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 23 it 30 total=12.4803 mle=1.5567 pcon=4.7368 forget=6.1868 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=12.6007 mle=1.6577 pcon=4.7348 forget=6.2082 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=12.5336 mle=1.6033 pcon=4.7328 forget=6.1974 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=12.6821 mle=1.7386 pcon=4.7309 forget=6.2127 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=12.6091 mle=1.6314 pcon=4.7292 forget=6.2486 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=12.6598 mle=1.6448 pcon=4.7273 forget=6.2877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=12.6991 mle=1.6723 pcon=4.7256 forget=6.3012 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=12.6908 mle=1.6312 pcon=4.7240 forget=6.3356 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
[loss] ep 24 it 40 total=12.6701 mle=1.5723 pcon=4.7226 forget=6.3751 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=12.8163 mle=1.6499 pcon=4.7213 forget=6.4451 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=12.8337 mle=1.6307 pcon=4.7199 forget=6.4830 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=12.8747 mle=1.6423 pcon=4.7188 forget=6.5137 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 240 total=12.8006 mle=1.5406 pcon=4.7176 forget=6.5424 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=13.0056 mle=1.7261 pcon=4.7166 forget=6.5630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=12.9370 mle=1.6254 pcon=4.7156 forget=6.5960 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 0 total=12.9665 mle=1.6262 pcon=4.7146 forget=6.6256 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=13.1373 mle=1.7596 pcon=4.7139 forget=6.6638 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=13.0199 mle=1.6532 pcon=4.7130 forget=6.6537 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 150 total=13.1773 mle=1.7885 pcon=4.7123 forget=6.6764 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 200 total=13.2112 mle=1.8101 pcon=4.7115 forget=6.6896 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=13.1949 mle=1.7384 pcon=4.7107 forget=6.7459 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=13.0444 mle=1.6377 pcon=4.7099 forget=6.6968 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=12.9852 mle=1.5708 pcon=4.7094 forget=6.7051 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 10 total=13.0341 mle=1.6361 pcon=4.7087 forget=6.6893 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=13.0109 mle=1.6008 pcon=4.7080 forget=6.7020 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=13.1105 mle=1.7258 pcon=4.7073 forget=6.6773 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=12.9206 mle=1.5658 pcon=4.7066 forget=6.6483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=13.0352 mle=1.6725 pcon=4.7057 forget=6.6570 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=12.9551 mle=1.5643 pcon=4.7050 forget=6.6858 nr=64 nf=64 protos=540 fproto_sim=NA
 54%|█████▍    | 27/50 [07:16<06:16, 16.37s/it] 56%|█████▌    | 28/50 [07:32<06:00, 16.40s/it] 58%|█████▊    | 29/50 [07:49<05:46, 16.51s/it] 60%|██████    | 30/50 [08:05<05:27, 16.35s/it] 62%|██████▏   | 31/50 [08:22<05:11, 16.38s/it] 64%|██████▍   | 32/50 [08:38<04:57, 16.52s/it] 66%|██████▌   | 33/50 [08:56<04:45, 16.78s/it] 68%|██████▊   | 34/50 [09:14<04:33, 17.07s/it] 70%|███████   | 35/50 [09:31<04:16, 17.08s/it] 72%|███████▏  | 36/50 [09:48<03:58, 17.04s/it][loss] ep 26 it 310 total=13.2531 mle=1.9216 pcon=4.7042 forget=6.6273 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=12.9295 mle=1.6183 pcon=4.7033 forget=6.6078 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 20 total=12.9576 mle=1.6521 pcon=4.7026 forget=6.6029 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=12.8567 mle=1.5787 pcon=4.7016 forget=6.5764 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=12.8674 mle=1.5763 pcon=4.7007 forget=6.5904 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=12.8402 mle=1.5549 pcon=4.6998 forget=6.5855 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=12.7828 mle=1.5240 pcon=4.6989 forget=6.5599 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=12.8597 mle=1.6083 pcon=4.6979 forget=6.5535 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=12.9925 mle=1.7246 pcon=4.6971 forget=6.5708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=12.9800 mle=1.7244 pcon=4.6963 forget=6.5593 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 30 total=12.8966 mle=1.6702 pcon=4.6956 forget=6.5309 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=12.9629 mle=1.7054 pcon=4.6949 forget=6.5626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=12.8275 mle=1.6012 pcon=4.6942 forget=6.5321 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=12.8140 mle=1.5927 pcon=4.6934 forget=6.5279 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=12.6854 mle=1.4635 pcon=4.6924 forget=6.5294 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=12.8363 mle=1.5848 pcon=4.6915 forget=6.5600 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=13.0366 mle=1.8066 pcon=4.6906 forget=6.5394 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=12.9591 mle=1.7289 pcon=4.6897 forget=6.5405 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 40 total=12.8859 mle=1.6834 pcon=4.6887 forget=6.5138 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=12.8041 mle=1.5792 pcon=4.6878 forget=6.5372 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=12.8988 mle=1.6652 pcon=4.6870 forget=6.5466 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=12.8755 mle=1.6293 pcon=4.6863 forget=6.5600 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=12.8328 mle=1.5784 pcon=4.6856 forget=6.5688 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=12.9683 mle=1.7313 pcon=4.6850 forget=6.5520 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=12.8332 mle=1.5807 pcon=4.6844 forget=6.5681 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 0 total=12.9078 mle=1.6373 pcon=4.6837 forget=6.5868 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=12.9496 mle=1.6586 pcon=4.6832 forget=6.6078 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=12.8608 mle=1.5681 pcon=4.6826 forget=6.6101 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=13.1360 mle=1.8491 pcon=4.6819 forget=6.6050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=12.8505 mle=1.5066 pcon=4.6813 forget=6.6626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=13.1720 mle=1.8049 pcon=4.6806 forget=6.6865 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=12.8480 mle=1.5105 pcon=4.6801 forget=6.6574 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=13.0583 mle=1.7214 pcon=4.6796 forget=6.6573 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 10 total=12.9998 mle=1.6606 pcon=4.6791 forget=6.6601 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=12.9131 mle=1.5657 pcon=4.6786 forget=6.6688 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=12.9797 mle=1.6066 pcon=4.6781 forget=6.6950 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=12.9396 mle=1.5697 pcon=4.6775 forget=6.6924 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=12.8991 mle=1.5776 pcon=4.6770 forget=6.6445 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 260 total=12.8986 mle=1.5145 pcon=4.6764 forget=6.7077 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=13.0357 mle=1.6177 pcon=4.6758 forget=6.7422 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=12.9402 mle=1.5455 pcon=4.6751 forget=6.7196 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 20 total=13.0933 mle=1.7055 pcon=4.6746 forget=6.7132 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=12.9437 mle=1.5851 pcon=4.6739 forget=6.6846 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=13.0315 mle=1.6061 pcon=4.6734 forget=6.7520 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 170 total=12.9394 mle=1.5573 pcon=4.6730 forget=6.7091 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=13.0596 mle=1.6849 pcon=4.6725 forget=6.7023 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=12.9258 mle=1.5562 pcon=4.6720 forget=6.6976 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=12.8713 mle=1.5081 pcon=4.6715 forget=6.6917 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=12.8410 mle=1.4800 pcon=4.6712 forget=6.6899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 30 total=13.0297 mle=1.6414 pcon=4.6707 forget=6.7176 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=13.1821 mle=1.8814 pcon=4.6703 forget=6.6304 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=12.8682 mle=1.5251 pcon=4.6698 forget=6.6733 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=13.1105 mle=1.6940 pcon=4.6693 forget=6.7472 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=13.0225 mle=1.6376 pcon=4.6689 forget=6.7160 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=13.0177 mle=1.6660 pcon=4.6683 forget=6.6833 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=13.0265 mle=1.6982 pcon=4.6680 forget=6.6603 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 380 total=12.9947 mle=1.6207 pcon=4.6674 forget=6.7065 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 40 total=13.0679 mle=1.7311 pcon=4.6668 forget=6.6699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=12.8446 mle=1.5344 pcon=4.6663 forget=6.6439 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 140 total=12.9179 mle=1.6338 pcon=4.6659 forget=6.6182 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=13.0005 mle=1.6681 pcon=4.6654 forget=6.6669 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 240 total=12.9804 mle=1.6819 pcon=4.6649 forget=6.6336 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=12.9960 mle=1.6464 pcon=4.6644 forget=6.6852 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=12.9732 mle=1.5814 pcon=4.6640 forget=6.7278 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 0 total=12.9180 mle=1.5989 pcon=4.6635 forget=6.6555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 50 total=12.9614 mle=1.6266 pcon=4.6631 forget=6.6718 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 100 total=13.2096 mle=1.8270 pcon=4.6626 forget=6.7201 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 150 total=12.9772 mle=1.6596 pcon=4.6621 forget=6.6554 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 200 total=12.8985 mle=1.5488 pcon=4.6618 forget=6.6880 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 250 total=13.0200 mle=1.7141 pcon=4.6613 forget=6.6446 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 300 total=12.8512 mle=1.5701 pcon=4.6609 forget=6.6202 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 350 total=12.9997 mle=1.7267 pcon=4.6606 forget=6.6124 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 10 total=13.0305 mle=1.7195 pcon=4.6603 forget=6.6507 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 60 total=12.8959 mle=1.5736 pcon=4.6601 forget=6.6622 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 110 total=12.8822 mle=1.5815 pcon=4.6597 forget=6.6409 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 160 total=12.9620 mle=1.6479 pcon=4.6594 forget=6.6548 nr=64 nf=64 protos=540 fproto_sim=NA
 74%|███████▍  | 37/50 [10:05<03:43, 17.15s/it] 76%|███████▌  | 38/50 [10:23<03:27, 17.27s/it] 78%|███████▊  | 39/50 [10:40<03:10, 17.30s/it] 80%|████████  | 40/50 [10:57<02:52, 17.22s/it] 82%|████████▏ | 41/50 [11:14<02:35, 17.25s/it] 84%|████████▍ | 42/50 [11:32<02:18, 17.36s/it] 86%|████████▌ | 43/50 [11:49<02:01, 17.37s/it] 88%|████████▊ | 44/50 [12:07<01:44, 17.46s/it] 90%|█████████ | 45/50 [12:25<01:27, 17.54s/it] 92%|█████████▏| 46/50 [12:43<01:10, 17.63s/it][loss] ep 36 it 210 total=12.8635 mle=1.5390 pcon=4.6591 forget=6.6653 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 260 total=12.9300 mle=1.5823 pcon=4.6590 forget=6.6887 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 310 total=13.0495 mle=1.6775 pcon=4.6586 forget=6.7134 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 360 total=12.9622 mle=1.6617 pcon=4.6583 forget=6.6422 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 20 total=12.8215 mle=1.5124 pcon=4.6579 forget=6.6512 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 70 total=13.1122 mle=1.7465 pcon=4.6577 forget=6.7080 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 120 total=12.9690 mle=1.6220 pcon=4.6575 forget=6.6896 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 170 total=12.9117 mle=1.6001 pcon=4.6572 forget=6.6544 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 220 total=13.0816 mle=1.7283 pcon=4.6568 forget=6.6965 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 270 total=12.9248 mle=1.6005 pcon=4.6565 forget=6.6679 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 320 total=12.9972 mle=1.6491 pcon=4.6562 forget=6.6919 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 370 total=13.0461 mle=1.6383 pcon=4.6561 forget=6.7517 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 30 total=13.0564 mle=1.6778 pcon=4.6560 forget=6.7226 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 80 total=12.8648 mle=1.4939 pcon=4.6559 forget=6.7150 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 130 total=12.9764 mle=1.6205 pcon=4.6557 forget=6.7002 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 180 total=12.8970 mle=1.5779 pcon=4.6555 forget=6.6637 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 230 total=12.8717 mle=1.5254 pcon=4.6553 forget=6.6910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 280 total=13.0277 mle=1.7069 pcon=4.6552 forget=6.6657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 330 total=12.9327 mle=1.6147 pcon=4.6549 forget=6.6631 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 380 total=12.8413 mle=1.5072 pcon=4.6549 forget=6.6793 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 40 total=13.0167 mle=1.6977 pcon=4.6549 forget=6.6641 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 90 total=13.0139 mle=1.6389 pcon=4.6548 forget=6.7201 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 140 total=12.9982 mle=1.6155 pcon=4.6547 forget=6.7280 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 190 total=12.7749 mle=1.4650 pcon=4.6546 forget=6.6552 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 240 total=13.0305 mle=1.6570 pcon=4.6545 forget=6.7190 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 290 total=12.8466 mle=1.5074 pcon=4.6545 forget=6.6848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 340 total=12.9588 mle=1.5857 pcon=4.6545 forget=6.7187 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 0 total=12.9140 mle=1.5726 pcon=4.6542 forget=6.6872 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 50 total=13.0104 mle=1.6121 pcon=4.6541 forget=6.7442 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 100 total=12.9484 mle=1.6276 pcon=4.6539 forget=6.6669 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 150 total=12.8269 mle=1.4734 pcon=4.6538 forget=6.6996 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 200 total=13.1104 mle=1.7443 pcon=4.6538 forget=6.7123 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 250 total=13.2178 mle=1.8022 pcon=4.6536 forget=6.7620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 300 total=12.9224 mle=1.5648 pcon=4.6536 forget=6.7040 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 350 total=12.8382 mle=1.4632 pcon=4.6536 forget=6.7214 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 10 total=12.8841 mle=1.5123 pcon=4.6535 forget=6.7183 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 60 total=13.2610 mle=1.8495 pcon=4.6535 forget=6.7580 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 110 total=13.0324 mle=1.6221 pcon=4.6534 forget=6.7569 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 160 total=13.0539 mle=1.6116 pcon=4.6535 forget=6.7889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 210 total=13.0887 mle=1.6873 pcon=4.6534 forget=6.7481 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 260 total=12.9346 mle=1.5421 pcon=4.6536 forget=6.7389 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 310 total=12.9579 mle=1.5340 pcon=4.6537 forget=6.7702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 360 total=13.1295 mle=1.7122 pcon=4.6537 forget=6.7636 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 20 total=13.0726 mle=1.6483 pcon=4.6537 forget=6.7706 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 70 total=13.0218 mle=1.5861 pcon=4.6536 forget=6.7821 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 120 total=13.3030 mle=1.8522 pcon=4.6535 forget=6.7973 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 170 total=12.9927 mle=1.5855 pcon=4.6535 forget=6.7536 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 220 total=13.0951 mle=1.6502 pcon=4.6535 forget=6.7914 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 270 total=13.0985 mle=1.6720 pcon=4.6535 forget=6.7730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 320 total=13.1402 mle=1.6818 pcon=4.6536 forget=6.8048 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 370 total=12.9991 mle=1.5838 pcon=4.6536 forget=6.7616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 30 total=13.0775 mle=1.6013 pcon=4.6536 forget=6.8226 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 80 total=13.0689 mle=1.6068 pcon=4.6535 forget=6.8086 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 130 total=13.1261 mle=1.6308 pcon=4.6535 forget=6.8418 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 180 total=13.0213 mle=1.5275 pcon=4.6535 forget=6.8403 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 230 total=13.0090 mle=1.5429 pcon=4.6534 forget=6.8127 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 280 total=13.0218 mle=1.5395 pcon=4.6534 forget=6.8288 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 330 total=13.0837 mle=1.5369 pcon=4.6533 forget=6.8935 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 380 total=12.9850 mle=1.4946 pcon=4.6532 forget=6.8372 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 40 total=13.2321 mle=1.7130 pcon=4.6532 forget=6.8659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 90 total=13.0807 mle=1.5609 pcon=4.6531 forget=6.8666 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 140 total=13.1796 mle=1.6570 pcon=4.6531 forget=6.8695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 190 total=13.0495 mle=1.5409 pcon=4.6531 forget=6.8555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 240 total=13.1474 mle=1.5307 pcon=4.6531 forget=6.9637 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 290 total=13.3537 mle=1.8033 pcon=4.6531 forget=6.8973 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 340 total=13.4008 mle=1.8164 pcon=4.6532 forget=6.9313 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 0 total=13.1826 mle=1.5653 pcon=4.6531 forget=6.9642 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 50 total=13.1371 mle=1.5406 pcon=4.6531 forget=6.9434 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 100 total=13.1713 mle=1.5924 pcon=4.6530 forget=6.9258 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 150 total=13.1288 mle=1.5423 pcon=4.6530 forget=6.9335 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 200 total=13.1709 mle=1.5751 pcon=4.6530 forget=6.9428 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 250 total=13.2144 mle=1.6158 pcon=4.6529 forget=6.9456 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 300 total=13.1222 mle=1.6290 pcon=4.6528 forget=6.8404 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 350 total=13.0706 mle=1.5369 pcon=4.6528 forget=6.8810 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 10 total=13.1349 mle=1.5139 pcon=4.6528 forget=6.9682 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 60 total=13.1564 mle=1.5105 pcon=4.6528 forget=6.9931 nr=64 nf=64 protos=540 fproto_sim=NA
 94%|█████████▍| 47/50 [13:00<00:53, 17.70s/it] 96%|█████████▌| 48/50 [13:18<00:35, 17.75s/it] 98%|█████████▊| 49/50 [13:36<00:17, 17.84s/it]100%|██████████| 50/50 [13:54<00:00, 17.89s/it]100%|██████████| 50/50 [13:54<00:00, 16.70s/it]
[loss] ep 46 it 110 total=13.2524 mle=1.6441 pcon=4.6528 forget=6.9555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 160 total=13.0648 mle=1.4687 pcon=4.6528 forget=6.9433 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 210 total=13.3178 mle=1.7087 pcon=4.6527 forget=6.9565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 260 total=13.2809 mle=1.6899 pcon=4.6526 forget=6.9384 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 310 total=13.1426 mle=1.5040 pcon=4.6527 forget=6.9859 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 360 total=13.3767 mle=1.7114 pcon=4.6526 forget=7.0126 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 20 total=13.1852 mle=1.5333 pcon=4.6525 forget=6.9995 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 70 total=13.0962 mle=1.5343 pcon=4.6525 forget=6.9093 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 120 total=13.3164 mle=1.6499 pcon=4.6523 forget=7.0142 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 170 total=13.2929 mle=1.5468 pcon=4.6522 forget=7.0939 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 220 total=13.0804 mle=1.4630 pcon=4.6522 forget=6.9651 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 270 total=13.3528 mle=1.5675 pcon=4.6523 forget=7.1330 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 320 total=13.3688 mle=1.5686 pcon=4.6522 forget=7.1480 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 370 total=13.3282 mle=1.6142 pcon=4.6522 forget=7.0618 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 30 total=13.3824 mle=1.5976 pcon=4.6523 forget=7.1325 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 80 total=13.3558 mle=1.5910 pcon=4.6522 forget=7.1125 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 130 total=13.3462 mle=1.6551 pcon=4.6523 forget=7.0389 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 180 total=13.4960 mle=1.6531 pcon=4.6523 forget=7.1905 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 230 total=13.4292 mle=1.6591 pcon=4.6523 forget=7.1177 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 280 total=13.2450 mle=1.5967 pcon=4.6523 forget=6.9960 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 330 total=13.3110 mle=1.5749 pcon=4.6523 forget=7.0838 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 380 total=13.4110 mle=1.5293 pcon=4.6522 forget=7.2295 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 40 total=13.4185 mle=1.5480 pcon=4.6522 forget=7.2183 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 90 total=13.4427 mle=1.5890 pcon=4.6522 forget=7.2015 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 140 total=13.4328 mle=1.6137 pcon=4.6522 forget=7.1668 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 190 total=13.5721 mle=1.7735 pcon=4.6521 forget=7.1465 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 240 total=13.4611 mle=1.6601 pcon=4.6520 forget=7.1490 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 290 total=13.4179 mle=1.6294 pcon=4.6519 forget=7.1366 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 340 total=13.3375 mle=1.5263 pcon=4.6520 forget=7.1593 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:30,  2.60it/s]  3%|▎         | 11/391 [00:00<00:13, 28.61it/s]  5%|▌         | 21/391 [00:00<00:07, 47.19it/s]  8%|▊         | 31/391 [00:00<00:05, 60.78it/s] 10%|█         | 41/391 [00:00<00:04, 70.78it/s] 13%|█▎        | 51/391 [00:00<00:04, 78.03it/s] 16%|█▌        | 61/391 [00:01<00:04, 82.31it/s] 18%|█▊        | 71/391 [00:01<00:03, 85.98it/s] 21%|██        | 81/391 [00:01<00:03, 88.79it/s] 23%|██▎       | 91/391 [00:01<00:03, 90.42it/s] 26%|██▌       | 101/391 [00:01<00:03, 91.88it/s] 28%|██▊       | 111/391 [00:01<00:03, 92.61it/s] 31%|███       | 121/391 [00:01<00:02, 93.28it/s] 34%|███▎      | 131/391 [00:01<00:02, 93.94it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.47it/s] 39%|███▊      | 151/391 [00:01<00:02, 94.51it/s] 41%|████      | 161/391 [00:02<00:02, 94.12it/s] 44%|████▎     | 171/391 [00:02<00:02, 93.82it/s] 46%|████▋     | 181/391 [00:02<00:02, 93.95it/s] 49%|████▉     | 191/391 [00:02<00:02, 94.23it/s] 51%|█████▏    | 201/391 [00:02<00:02, 94.29it/s] 54%|█████▍    | 211/391 [00:02<00:01, 94.45it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.53it/s] 59%|█████▉    | 231/391 [00:02<00:01, 94.59it/s] 62%|██████▏   | 241/391 [00:02<00:01, 93.89it/s] 64%|██████▍   | 251/391 [00:03<00:01, 93.53it/s] 67%|██████▋   | 261/391 [00:03<00:01, 93.28it/s] 69%|██████▉   | 271/391 [00:03<00:01, 93.70it/s] 72%|███████▏  | 281/391 [00:03<00:01, 94.18it/s] 74%|███████▍  | 291/391 [00:03<00:01, 94.53it/s] 77%|███████▋  | 301/391 [00:03<00:00, 94.60it/s] 80%|███████▉  | 311/391 [00:03<00:00, 93.74it/s] 82%|████████▏ | 321/391 [00:03<00:00, 94.00it/s] 85%|████████▍ | 331/391 [00:03<00:00, 93.81it/s] 87%|████████▋ | 341/391 [00:03<00:00, 93.96it/s] 90%|████████▉ | 351/391 [00:04<00:00, 93.93it/s] 92%|█████████▏| 361/391 [00:04<00:00, 92.44it/s] 95%|█████████▍| 371/391 [00:04<00:00, 93.00it/s] 97%|█████████▋| 381/391 [00:04<00:00, 93.18it/s]100%|██████████| 391/391 [00:04<00:00, 91.29it/s]100%|██████████| 391/391 [00:04<00:00, 86.14it/s]
50000 images processed, 4.668738603591919 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:24,  3.24it/s] 14%|█▍        | 11/79 [00:00<00:02, 33.17it/s] 27%|██▋       | 21/79 [00:00<00:01, 52.63it/s] 39%|███▉      | 31/79 [00:00<00:00, 65.27it/s] 52%|█████▏    | 41/79 [00:00<00:00, 73.42it/s] 63%|██████▎   | 50/79 [00:00<00:00, 77.83it/s] 76%|███████▌  | 60/79 [00:00<00:00, 82.53it/s] 89%|████████▊ | 70/79 [00:01<00:00, 86.34it/s]100%|██████████| 79/79 [00:01<00:00, 68.27it/s]
10000 images processed, 1.1836469173431396 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<00:56,  3.62it/s]  5%|▌         | 11/204 [00:00<00:05, 35.70it/s] 10%|█         | 21/204 [00:00<00:03, 55.08it/s] 15%|█▌        | 31/204 [00:00<00:02, 67.63it/s] 20%|█▉        | 40/204 [00:00<00:02, 73.40it/s] 25%|██▍       | 50/204 [00:00<00:01, 79.70it/s] 29%|██▉       | 60/204 [00:00<00:01, 83.84it/s] 34%|███▍      | 70/204 [00:01<00:01, 86.38it/s] 39%|███▉      | 80/204 [00:01<00:01, 88.38it/s] 44%|████▍     | 90/204 [00:01<00:01, 88.76it/s] 49%|████▉     | 100/204 [00:01<00:01, 89.78it/s] 54%|█████▍    | 110/204 [00:01<00:01, 90.42it/s] 59%|█████▉    | 120/204 [00:01<00:00, 90.89it/s] 64%|██████▎   | 130/204 [00:01<00:00, 91.30it/s] 69%|██████▊   | 140/204 [00:01<00:00, 91.79it/s] 74%|███████▎  | 150/204 [00:01<00:00, 91.49it/s] 78%|███████▊  | 160/204 [00:02<00:00, 91.45it/s] 83%|████████▎ | 170/204 [00:02<00:00, 91.82it/s] 88%|████████▊ | 180/204 [00:02<00:00, 91.50it/s] 93%|█████████▎| 190/204 [00:02<00:00, 92.06it/s] 98%|█████████▊| 200/204 [00:02<00:00, 92.25it/s]100%|██████████| 204/204 [00:02<00:00, 82.00it/s]
26032 images processed, 2.544991970062256 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:39,  1.95it/s] 11%|█▏        | 9/79 [00:00<00:04, 17.07it/s] 22%|██▏       | 17/79 [00:00<00:02, 26.26it/s] 32%|███▏      | 25/79 [00:01<00:01, 32.35it/s] 42%|████▏     | 33/79 [00:01<00:01, 36.33it/s] 52%|█████▏    | 41/79 [00:01<00:00, 38.87it/s] 62%|██████▏   | 49/79 [00:01<00:00, 41.72it/s] 72%|███████▏  | 57/79 [00:01<00:00, 43.57it/s] 82%|████████▏ | 65/79 [00:01<00:00, 44.92it/s] 92%|█████████▏| 73/79 [00:02<00:00, 46.79it/s]100%|██████████| 79/79 [00:02<00:00, 37.85it/s]
10000 images processed, 2.1334047317504883 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:32,  2.40it/s] 13%|█▎        | 10/79 [00:00<00:02, 24.58it/s] 25%|██▌       | 20/79 [00:00<00:01, 43.77it/s] 38%|███▊      | 30/79 [00:00<00:00, 57.78it/s] 51%|█████     | 40/79 [00:00<00:00, 68.00it/s] 62%|██████▏   | 49/79 [00:00<00:00, 74.09it/s] 75%|███████▍  | 59/79 [00:01<00:00, 79.58it/s] 87%|████████▋ | 69/79 [00:01<00:00, 83.88it/s]100%|██████████| 79/79 [00:01<00:00, 62.92it/s]
10000 images processed, 1.2836434841156006 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:30,  2.30it/s] 16%|█▌        | 11/70 [00:00<00:02, 25.99it/s] 30%|███       | 21/70 [00:00<00:01, 44.10it/s] 44%|████▍     | 31/70 [00:00<00:00, 57.54it/s] 59%|█████▊    | 41/70 [00:00<00:00, 67.33it/s] 73%|███████▎  | 51/70 [00:00<00:00, 74.69it/s] 87%|████████▋ | 61/70 [00:01<00:00, 80.25it/s]100%|██████████| 70/70 [00:01<00:00, 82.39it/s]100%|██████████| 70/70 [00:01<00:00, 59.02it/s]
8925 images processed, 1.223677158355713 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:43,  1.01it/s]  4%|▍         | 2/45 [00:01<00:21,  2.02it/s] 20%|██        | 9/45 [00:01<00:04,  8.96it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.68it/s] 36%|███▌      | 16/45 [00:01<00:02, 14.27it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.63it/s] 49%|████▉     | 22/45 [00:02<00:01, 12.71it/s] 56%|█████▌    | 25/45 [00:02<00:01, 12.71it/s] 60%|██████    | 27/45 [00:02<00:01, 12.69it/s] 67%|██████▋   | 30/45 [00:02<00:01, 14.26it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.16it/s] 78%|███████▊  | 35/45 [00:03<00:00, 11.48it/s] 91%|█████████ | 41/45 [00:03<00:00, 14.60it/s] 96%|█████████▌| 43/45 [00:04<00:00, 11.49it/s]100%|██████████| 45/45 [00:04<00:00, 10.86it/s]
5640 images processed, 4.164028882980347 seconds used

19.081978797912598
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           5.29  98.85
places365     76.35  78.84
LSUN          28.51  94.80
iSUN          76.93  80.04
dtd           46.95  89.39
forget        46.20  91.24
AVG           46.70  88.86
Retain-Acc: 0.7361
Forget-as-OOD (retain known vs forget novel):
  FPR: 46.20 AUROC: 91.24 AUIN: 98.86
97.24562573432922
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-phase3_rf.png
epochs=50 score=-23.3522
[Phase 3] best epochs= (score=-1)
Final best config: lambda=1.0, lr=0.001, epochs=5
