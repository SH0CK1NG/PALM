nohup: ignoring input
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=25, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10,64,22,42,9,90', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/25 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  4%|▍         | 1/25 [00:14<05:56, 14.85s/it]  8%|▊         | 2/25 [00:23<04:15, 11.13s/it] 12%|█▏        | 3/25 [00:32<03:46, 10.31s/it] 16%|█▌        | 4/25 [00:41<03:26,  9.84s/it] 20%|██        | 5/25 [00:51<03:17,  9.86s/it] 24%|██▍       | 6/25 [01:01<03:06,  9.83s/it][loss] ep 0 it 0 total=9.1659 mle=1.4545 pcon=5.2950 forget=2.4164 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1513 mle=1.3818 pcon=5.2879 forget=2.4816 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.1817 mle=1.4046 pcon=5.2806 forget=2.4965 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.1707 mle=1.4324 pcon=5.2742 forget=2.4641 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.1089 mle=1.3351 pcon=5.2674 forget=2.5064 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1393 mle=1.3778 pcon=5.2610 forget=2.5005 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.1560 mle=1.4542 pcon=5.2549 forget=2.4469 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1562 mle=1.3976 pcon=5.2492 forget=2.5094 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 1 it 10 total=9.1847 mle=1.4428 pcon=5.2433 forget=2.4986 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.1518 mle=1.4008 pcon=5.2375 forget=2.5136 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=9.1460 mle=1.4108 pcon=5.2319 forget=2.5033 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=9.0577 mle=1.3385 pcon=5.2261 forget=2.4931 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.1702 mle=1.4542 pcon=5.2207 forget=2.4953 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=9.0748 mle=1.3681 pcon=5.2159 forget=2.4907 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=9.0632 mle=1.4028 pcon=5.2107 forget=2.4498 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.1426 mle=1.4447 pcon=5.2059 forget=2.4920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 2 it 20 total=8.9847 mle=1.3069 pcon=5.2011 forget=2.4767 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.1231 mle=1.4082 pcon=5.1966 forget=2.5184 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=9.1397 mle=1.4713 pcon=5.1920 forget=2.4764 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=9.0204 mle=1.3309 pcon=5.1876 forget=2.5019 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=9.0654 mle=1.3887 pcon=5.1832 forget=2.4936 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=8.9873 mle=1.3173 pcon=5.1792 forget=2.4908 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.9049 mle=1.2468 pcon=5.1748 forget=2.4833 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.0479 mle=1.3643 pcon=5.1709 forget=2.5127 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 3 it 30 total=9.0995 mle=1.4265 pcon=5.1666 forget=2.5064 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9912 mle=1.3460 pcon=5.1627 forget=2.4824 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=9.0738 mle=1.3964 pcon=5.1587 forget=2.5187 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.0138 mle=1.3691 pcon=5.1552 forget=2.4895 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9866 mle=1.3178 pcon=5.1515 forget=2.5173 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.1210 mle=1.4954 pcon=5.1480 forget=2.4776 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0267 mle=1.3651 pcon=5.1444 forget=2.5171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=9.1603 mle=1.5321 pcon=5.1409 forget=2.4873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 4 it 40 total=9.0359 mle=1.4041 pcon=5.1376 forget=2.4942 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=9.0214 mle=1.3588 pcon=5.1342 forget=2.5284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=8.9400 mle=1.2900 pcon=5.1312 forget=2.5189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.0265 mle=1.3673 pcon=5.1282 forget=2.5311 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0329 mle=1.4037 pcon=5.1252 forget=2.5040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=8.9693 mle=1.3412 pcon=5.1222 forget=2.5059 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.0611 mle=1.4308 pcon=5.1193 forget=2.5111 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 5 it 0 total=8.9834 mle=1.3640 pcon=5.1166 forget=2.5028 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=9.0239 mle=1.4154 pcon=5.1139 forget=2.4946 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=8.8391 mle=1.2602 pcon=5.1116 forget=2.4674 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=8.9728 mle=1.3669 pcon=5.1089 forget=2.4970 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.0329 mle=1.4472 pcon=5.1065 forget=2.4792 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=8.9318 mle=1.3184 pcon=5.1038 forget=2.5097 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=8.9697 mle=1.3851 pcon=5.1014 forget=2.4832 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=9.0307 mle=1.3737 pcon=5.0991 forget=2.5579 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 6 it 10 total=8.9048 mle=1.2775 pcon=5.0965 forget=2.5308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=8.9812 mle=1.3858 pcon=5.0947 forget=2.5007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=9.0613 mle=1.4776 pcon=5.0926 forget=2.4910 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.9928 mle=1.3782 pcon=5.0904 forget=2.5242 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9332 mle=1.3354 pcon=5.0881 forget=2.5097 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=9.0364 mle=1.4729 pcon=5.0862 forget=2.4773 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=9.0884 mle=1.5022 pcon=5.0842 forget=2.5020 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 28%|██▊       | 7/25 [01:10<02:50,  9.49s/it] 32%|███▏      | 8/25 [01:19<02:39,  9.41s/it] 36%|███▌      | 9/25 [01:29<02:32,  9.50s/it] 40%|████      | 10/25 [01:38<02:23,  9.57s/it] 44%|████▍     | 11/25 [01:49<02:17,  9.84s/it] 48%|████▊     | 12/25 [02:01<02:15, 10.44s/it] 52%|█████▏    | 13/25 [02:11<02:04, 10.38s/it][loss] ep 6 it 360 total=9.0245 mle=1.4255 pcon=5.0823 forget=2.5167 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 7 it 20 total=8.8790 mle=1.3351 pcon=5.0803 forget=2.4636 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=8.8830 mle=1.2932 pcon=5.0781 forget=2.5117 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.9511 mle=1.3751 pcon=5.0764 forget=2.4995 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.8879 mle=1.3406 pcon=5.0743 forget=2.4729 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=8.9888 mle=1.4415 pcon=5.0730 forget=2.4743 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=8.8781 mle=1.3267 pcon=5.0714 forget=2.4800 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=8.9535 mle=1.4199 pcon=5.0700 forget=2.4636 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=9.0313 mle=1.4517 pcon=5.0685 forget=2.5111 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 8 it 30 total=9.0837 mle=1.5239 pcon=5.0670 forget=2.4928 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=8.9158 mle=1.3429 pcon=5.0655 forget=2.5075 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=8.8543 mle=1.3074 pcon=5.0639 forget=2.4829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=8.9571 mle=1.4020 pcon=5.0624 forget=2.4927 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=8.8295 mle=1.2948 pcon=5.0608 forget=2.4739 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.9722 mle=1.4475 pcon=5.0593 forget=2.4654 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=8.8753 mle=1.3266 pcon=5.0577 forget=2.4910 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.9388 mle=1.3990 pcon=5.0566 forget=2.4832 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 9 it 40 total=8.9037 mle=1.3993 pcon=5.0554 forget=2.4491 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=9.0227 mle=1.4858 pcon=5.0542 forget=2.4827 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=8.9324 mle=1.3853 pcon=5.0529 forget=2.4942 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.8800 mle=1.3415 pcon=5.0518 forget=2.4867 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=8.8682 mle=1.3385 pcon=5.0508 forget=2.4788 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=8.9597 mle=1.4328 pcon=5.0498 forget=2.4771 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.9389 mle=1.4219 pcon=5.0487 forget=2.4683 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 10 it 0 total=8.9330 mle=1.3606 pcon=5.0474 forget=2.5250 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.8841 mle=1.3638 pcon=5.0464 forget=2.4738 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=8.9188 mle=1.3613 pcon=5.0454 forget=2.5120 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=8.9062 mle=1.3450 pcon=5.0441 forget=2.5171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=9.0056 mle=1.4897 pcon=5.0431 forget=2.4728 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=8.8494 mle=1.3184 pcon=5.0419 forget=2.4892 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=8.9516 mle=1.4464 pcon=5.0408 forget=2.4645 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=9.0206 mle=1.4587 pcon=5.0398 forget=2.5220 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 11 it 10 total=9.0060 mle=1.4481 pcon=5.0390 forget=2.5189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.9035 mle=1.3414 pcon=5.0382 forget=2.5238 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.9943 mle=1.4369 pcon=5.0373 forget=2.5201 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.8423 mle=1.2864 pcon=5.0365 forget=2.5193 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.9847 mle=1.4621 pcon=5.0356 forget=2.4870 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.8654 mle=1.3947 pcon=5.0352 forget=2.4355 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.9125 mle=1.3477 pcon=5.0344 forget=2.5304 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.8007 mle=1.2643 pcon=5.0335 forget=2.5029 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 12 it 20 total=8.8759 mle=1.3182 pcon=5.0325 forget=2.5252 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.8502 mle=1.3232 pcon=5.0317 forget=2.4954 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.9018 mle=1.3849 pcon=5.0307 forget=2.4862 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.9089 mle=1.4165 pcon=5.0301 forget=2.4623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.9836 mle=1.4642 pcon=5.0295 forget=2.4899 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.8705 mle=1.3767 pcon=5.0290 forget=2.4648 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.8349 mle=1.3429 pcon=5.0283 forget=2.4637 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.8996 mle=1.3692 pcon=5.0272 forget=2.5033 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 13 it 30 total=8.9011 mle=1.3606 pcon=5.0265 forget=2.5140 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.9797 mle=1.4597 pcon=5.0263 forget=2.4937 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.8011 mle=1.2904 pcon=5.0256 forget=2.4851 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=8.9313 mle=1.3850 pcon=5.0250 forget=2.5214 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 56%|█████▌    | 14/25 [02:21<01:52, 10.19s/it] 60%|██████    | 15/25 [02:31<01:41, 10.18s/it] 64%|██████▍   | 16/25 [02:41<01:32, 10.31s/it] 68%|██████▊   | 17/25 [02:52<01:23, 10.49s/it] 72%|███████▏  | 18/25 [03:01<01:09, 10.00s/it] 76%|███████▌  | 19/25 [03:10<00:58,  9.71s/it] 80%|████████  | 20/25 [03:21<00:49,  9.89s/it][loss] ep 13 it 230 total=8.8826 mle=1.3553 pcon=5.0246 forget=2.5028 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.9104 mle=1.3553 pcon=5.0241 forget=2.5310 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.9725 mle=1.4180 pcon=5.0233 forget=2.5311 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.8149 mle=1.3329 pcon=5.0229 forget=2.4591 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 14 it 40 total=8.8996 mle=1.3723 pcon=5.0224 forget=2.5049 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.8185 mle=1.3057 pcon=5.0219 forget=2.4910 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.7814 mle=1.2727 pcon=5.0212 forget=2.4875 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.9056 mle=1.3833 pcon=5.0205 forget=2.5018 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.8126 mle=1.2677 pcon=5.0202 forget=2.5247 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=9.0513 mle=1.5349 pcon=5.0197 forget=2.4967 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.9541 mle=1.4411 pcon=5.0192 forget=2.4938 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 15 it 0 total=9.0756 mle=1.5611 pcon=5.0185 forget=2.4961 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=9.0560 mle=1.5207 pcon=5.0182 forget=2.5171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=9.0218 mle=1.5240 pcon=5.0182 forget=2.4796 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.7520 mle=1.2171 pcon=5.0174 forget=2.5175 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.9263 mle=1.4058 pcon=5.0167 forget=2.5039 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=9.0374 mle=1.5262 pcon=5.0161 forget=2.4950 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.8076 mle=1.3056 pcon=5.0156 forget=2.4864 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.9016 mle=1.3896 pcon=5.0151 forget=2.4970 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 16 it 10 total=8.8354 mle=1.3252 pcon=5.0146 forget=2.4956 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.8701 mle=1.3357 pcon=5.0142 forget=2.5202 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.7726 mle=1.3230 pcon=5.0137 forget=2.4359 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.9151 mle=1.4048 pcon=5.0133 forget=2.4971 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.9546 mle=1.4149 pcon=5.0127 forget=2.5270 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.8248 mle=1.3174 pcon=5.0121 forget=2.4953 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.7354 mle=1.2347 pcon=5.0121 forget=2.4887 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.8453 mle=1.3132 pcon=5.0117 forget=2.5204 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 17 it 20 total=8.8423 mle=1.3517 pcon=5.0109 forget=2.4798 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.8942 mle=1.3992 pcon=5.0108 forget=2.4843 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.8166 mle=1.2587 pcon=5.0103 forget=2.5476 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.8442 mle=1.3228 pcon=5.0104 forget=2.5110 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.7318 mle=1.2450 pcon=5.0100 forget=2.4768 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.8516 mle=1.3563 pcon=5.0093 forget=2.4860 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=9.0415 mle=1.5379 pcon=5.0094 forget=2.4941 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.7563 mle=1.3221 pcon=5.0091 forget=2.4252 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 18 it 30 total=8.8527 mle=1.3490 pcon=5.0088 forget=2.4949 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.8288 mle=1.3377 pcon=5.0087 forget=2.4825 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.7974 mle=1.3465 pcon=5.0081 forget=2.4428 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.9200 mle=1.4199 pcon=5.0079 forget=2.4922 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.8293 mle=1.3240 pcon=5.0075 forget=2.4979 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.8184 mle=1.3121 pcon=5.0069 forget=2.4994 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.9373 mle=1.4627 pcon=5.0064 forget=2.4682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.8616 mle=1.3928 pcon=5.0058 forget=2.4630 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 19 it 40 total=8.8952 mle=1.3861 pcon=5.0058 forget=2.5034 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.8814 mle=1.3881 pcon=5.0055 forget=2.4879 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=9.0807 mle=1.6284 pcon=5.0051 forget=2.4472 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.8098 mle=1.3309 pcon=5.0051 forget=2.4738 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.9183 mle=1.4016 pcon=5.0047 forget=2.5120 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.8230 mle=1.3173 pcon=5.0044 forget=2.5013 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.8591 mle=1.3309 pcon=5.0047 forget=2.5235 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 20 it 0 total=8.9104 mle=1.4304 pcon=5.0044 forget=2.4756 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.8781 mle=1.3594 pcon=5.0044 forget=2.5143 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 84%|████████▍ | 21/25 [03:29<00:38,  9.59s/it] 88%|████████▊ | 22/25 [03:39<00:29,  9.68s/it] 92%|█████████▏| 23/25 [03:49<00:19,  9.81s/it] 96%|█████████▌| 24/25 [04:00<00:10, 10.15s/it]100%|██████████| 25/25 [04:10<00:00,  9.90s/it]100%|██████████| 25/25 [04:10<00:00, 10.01s/it]
[loss] ep 20 it 100 total=8.9127 mle=1.4313 pcon=5.0041 forget=2.4774 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.9241 mle=1.4210 pcon=5.0041 forget=2.4989 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.8729 mle=1.3544 pcon=5.0039 forget=2.5146 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=9.0158 mle=1.5056 pcon=5.0033 forget=2.5068 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=8.7955 mle=1.3209 pcon=5.0034 forget=2.4712 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=9.0465 mle=1.5401 pcon=5.0033 forget=2.5031 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 21 it 10 total=8.8957 mle=1.3953 pcon=5.0030 forget=2.4975 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=8.9032 mle=1.4072 pcon=5.0028 forget=2.4932 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.8742 mle=1.3627 pcon=5.0026 forget=2.5089 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.8467 mle=1.3434 pcon=5.0024 forget=2.5010 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=8.7942 mle=1.3030 pcon=5.0024 forget=2.4888 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.6872 mle=1.2266 pcon=5.0022 forget=2.4584 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.8414 mle=1.3076 pcon=5.0021 forget=2.5317 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.7941 mle=1.2559 pcon=5.0017 forget=2.5365 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 20 total=8.8779 mle=1.3760 pcon=5.0020 forget=2.4999 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.8004 mle=1.3053 pcon=5.0017 forget=2.4934 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.8224 mle=1.3508 pcon=5.0015 forget=2.4701 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.9279 mle=1.3951 pcon=5.0013 forget=2.5316 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.8167 mle=1.3372 pcon=5.0013 forget=2.4782 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.8620 mle=1.3936 pcon=5.0015 forget=2.4670 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.9988 mle=1.5113 pcon=5.0017 forget=2.4858 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=8.8442 mle=1.3579 pcon=5.0014 forget=2.4850 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[loss] ep 23 it 30 total=8.8024 mle=1.3099 pcon=5.0014 forget=2.4911 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.8290 mle=1.3366 pcon=5.0013 forget=2.4910 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=8.9629 mle=1.4995 pcon=5.0011 forget=2.4623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.9559 mle=1.4533 pcon=5.0008 forget=2.5018 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.9241 mle=1.4418 pcon=5.0007 forget=2.4817 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=8.8489 mle=1.3691 pcon=5.0005 forget=2.4793 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=8.8468 mle=1.3213 pcon=5.0003 forget=2.5252 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.8123 mle=1.2772 pcon=5.0004 forget=2.5348 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 40 total=8.7695 mle=1.3081 pcon=5.0005 forget=2.4609 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=8.7815 mle=1.3000 pcon=5.0000 forget=2.4815 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.7191 mle=1.2628 pcon=4.9996 forget=2.4568 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.9149 mle=1.4525 pcon=4.9998 forget=2.4626 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=8.7733 mle=1.2780 pcon=5.0001 forget=2.4953 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.8949 mle=1.3873 pcon=5.0003 forget=2.5073 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.8075 mle=1.2767 pcon=5.0002 forget=2.5306 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:45,  1.73it/s]  2%|▏         | 9/391 [00:00<00:22, 17.24it/s]  5%|▍         | 19/391 [00:00<00:10, 35.10it/s]  7%|▋         | 28/391 [00:00<00:07, 48.04it/s]  9%|▉         | 36/391 [00:00<00:06, 55.55it/s] 11%|█▏        | 44/391 [00:01<00:05, 61.53it/s] 13%|█▎        | 52/391 [00:01<00:05, 64.71it/s] 15%|█▌        | 60/391 [00:01<00:05, 65.07it/s] 17%|█▋        | 68/391 [00:01<00:04, 68.68it/s] 19%|█▉        | 76/391 [00:01<00:04, 63.00it/s] 21%|██▏       | 84/391 [00:01<00:04, 67.23it/s] 24%|██▍       | 94/391 [00:01<00:03, 75.35it/s] 27%|██▋       | 104/391 [00:01<00:03, 81.28it/s] 29%|██▉       | 114/391 [00:01<00:03, 85.15it/s] 32%|███▏      | 124/391 [00:02<00:03, 88.58it/s] 34%|███▍      | 134/391 [00:02<00:02, 90.95it/s] 37%|███▋      | 144/391 [00:02<00:02, 92.23it/s] 39%|███▉      | 154/391 [00:02<00:02, 93.23it/s] 42%|████▏     | 164/391 [00:02<00:02, 94.38it/s] 45%|████▍     | 174/391 [00:02<00:02, 95.00it/s] 47%|████▋     | 184/391 [00:02<00:02, 95.35it/s] 50%|████▉     | 194/391 [00:02<00:02, 95.55it/s] 52%|█████▏    | 204/391 [00:02<00:01, 95.24it/s] 55%|█████▍    | 214/391 [00:03<00:01, 95.78it/s] 57%|█████▋    | 224/391 [00:03<00:01, 96.04it/s] 60%|█████▉    | 234/391 [00:03<00:01, 94.12it/s] 62%|██████▏   | 244/391 [00:03<00:01, 94.51it/s] 65%|██████▍   | 254/391 [00:03<00:01, 94.72it/s] 68%|██████▊   | 264/391 [00:03<00:01, 93.77it/s] 70%|███████   | 274/391 [00:03<00:01, 93.36it/s] 73%|███████▎  | 284/391 [00:03<00:01, 94.10it/s] 75%|███████▌  | 294/391 [00:03<00:01, 94.53it/s] 78%|███████▊  | 304/391 [00:03<00:00, 92.16it/s] 80%|████████  | 314/391 [00:04<00:00, 93.09it/s] 83%|████████▎ | 324/391 [00:04<00:00, 93.89it/s] 85%|████████▌ | 334/391 [00:04<00:00, 94.60it/s] 88%|████████▊ | 344/391 [00:04<00:00, 92.86it/s] 91%|█████████ | 354/391 [00:04<00:00, 93.54it/s] 93%|█████████▎| 364/391 [00:04<00:00, 94.16it/s] 96%|█████████▌| 374/391 [00:04<00:00, 95.22it/s] 98%|█████████▊| 384/391 [00:04<00:00, 96.04it/s]100%|██████████| 391/391 [00:04<00:00, 79.55it/s]
50000 images processed, 4.9965620040893555 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:37,  2.09it/s] 14%|█▍        | 11/79 [00:00<00:02, 23.98it/s] 27%|██▋       | 21/79 [00:00<00:01, 41.44it/s] 38%|███▊      | 30/79 [00:00<00:00, 52.98it/s] 48%|████▊     | 38/79 [00:00<00:00, 56.81it/s] 58%|█████▊    | 46/79 [00:01<00:00, 62.34it/s] 71%|███████   | 56/79 [00:01<00:00, 71.05it/s] 82%|████████▏ | 65/79 [00:01<00:00, 70.39it/s] 92%|█████████▏| 73/79 [00:01<00:00, 70.37it/s]100%|██████████| 79/79 [00:01<00:00, 47.29it/s]
10000 images processed, 1.7025585174560547 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:43,  1.96it/s]  3%|▎         | 7/204 [00:00<00:13, 14.60it/s]  8%|▊         | 16/204 [00:00<00:05, 31.93it/s] 12%|█▏        | 24/204 [00:00<00:04, 43.41it/s] 17%|█▋        | 34/204 [00:00<00:02, 56.85it/s] 22%|██▏       | 44/204 [00:01<00:02, 67.16it/s] 26%|██▌       | 53/204 [00:01<00:02, 73.23it/s] 31%|███       | 63/204 [00:01<00:01, 79.20it/s] 36%|███▌      | 73/204 [00:01<00:01, 83.39it/s] 41%|████      | 83/204 [00:01<00:01, 86.62it/s] 46%|████▌     | 93/204 [00:01<00:01, 88.86it/s] 50%|█████     | 103/204 [00:01<00:01, 90.32it/s] 55%|█████▌    | 113/204 [00:01<00:01, 90.20it/s] 60%|██████    | 123/204 [00:01<00:00, 84.68it/s] 65%|██████▌   | 133/204 [00:02<00:00, 87.79it/s] 70%|███████   | 143/204 [00:02<00:00, 89.56it/s] 75%|███████▌  | 153/204 [00:02<00:00, 91.24it/s] 80%|███████▉  | 163/204 [00:02<00:00, 91.84it/s] 85%|████████▍ | 173/204 [00:02<00:00, 92.23it/s] 90%|████████▉ | 183/204 [00:02<00:00, 93.80it/s] 95%|█████████▍| 193/204 [00:02<00:00, 94.84it/s]100%|█████████▉| 203/204 [00:02<00:00, 95.61it/s]100%|██████████| 204/204 [00:02<00:00, 73.62it/s]
26032 images processed, 2.8177342414855957 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:55,  1.42it/s] 13%|█▎        | 10/79 [00:00<00:04, 16.28it/s] 25%|██▌       | 20/79 [00:00<00:01, 31.83it/s] 38%|███▊      | 30/79 [00:01<00:01, 45.47it/s] 51%|█████     | 40/79 [00:01<00:00, 56.74it/s] 63%|██████▎   | 50/79 [00:01<00:00, 65.85it/s] 75%|███████▍  | 59/79 [00:01<00:00, 68.23it/s] 87%|████████▋ | 69/79 [00:01<00:00, 75.74it/s]100%|██████████| 79/79 [00:01<00:00, 50.43it/s]
10000 images processed, 1.5985713005065918 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:35,  2.17it/s]  6%|▋         | 5/79 [00:00<00:06, 10.86it/s] 19%|█▉        | 15/79 [00:00<00:01, 32.19it/s] 32%|███▏      | 25/79 [00:00<00:01, 48.01it/s] 44%|████▍     | 35/79 [00:00<00:00, 60.30it/s] 57%|█████▋    | 45/79 [00:01<00:00, 69.98it/s] 70%|██████▉   | 55/79 [00:01<00:00, 77.38it/s] 82%|████████▏ | 65/79 [00:01<00:00, 82.91it/s] 95%|█████████▍| 75/79 [00:01<00:00, 86.98it/s]100%|██████████| 79/79 [00:01<00:00, 58.41it/s]
10000 images processed, 1.3748972415924072 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:37,  1.82it/s]  7%|▋         | 5/70 [00:00<00:06,  9.61it/s] 21%|██▏       | 15/70 [00:00<00:01, 29.56it/s] 36%|███▌      | 25/70 [00:00<00:00, 45.42it/s] 50%|█████     | 35/70 [00:00<00:00, 58.28it/s] 64%|██████▍   | 45/70 [00:01<00:00, 68.18it/s] 79%|███████▊  | 55/70 [00:01<00:00, 76.08it/s] 93%|█████████▎| 65/70 [00:01<00:00, 81.66it/s]100%|██████████| 70/70 [00:01<00:00, 51.99it/s]
8925 images processed, 1.380202054977417 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:54,  1.24s/it]  4%|▍         | 2/45 [00:01<00:24,  1.75it/s] 16%|█▌        | 7/45 [00:01<00:04,  7.62it/s] 38%|███▊      | 17/45 [00:01<00:01, 18.06it/s] 47%|████▋     | 21/45 [00:01<00:01, 20.74it/s] 56%|█████▌    | 25/45 [00:02<00:01, 17.51it/s] 73%|███████▎  | 33/45 [00:02<00:00, 24.99it/s] 87%|████████▋ | 39/45 [00:02<00:00, 28.60it/s] 96%|█████████▌| 43/45 [00:02<00:00, 18.68it/s]100%|██████████| 45/45 [00:02<00:00, 15.35it/s]
5640 images processed, 2.9584288597106934 seconds used

18.67753028869629
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.45  99.38  97.86
places365     67.64  81.64  77.74
LSUN          18.37  95.96  95.57
iSUN          62.60  86.70  87.39
dtd           38.81  91.29  93.68
AVG           37.98  90.99  90.45
Retain-Acc: 0.7614
Forget-as-OOD (retain known vs forget novel):
  FPR: 57.65 AUROC: 88.36 AUIN: 96.81
7.300142526626587
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget20-temp0.08_rf.png
