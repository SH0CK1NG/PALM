nohup: ignoring input
[Full Grid] Searching over: lambdas(0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0) × lrs(0.001) × epochs(5 10 15 20 25 30 35 40 45 50)
[Run] lambda=0.1 lr=0.001 epochs=5
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.1, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:17<01:09, 17.48s/it] 40%|████      | 2/5 [00:31<00:47, 15.69s/it] 60%|██████    | 3/5 [00:46<00:29, 14.98s/it] 80%|████████  | 4/5 [01:00<00:14, 14.71s/it]100%|██████████| 5/5 [01:14<00:00, 14.47s/it]100%|██████████| 5/5 [01:14<00:00, 14.88s/it]
[loss] ep 0 it 0 total=7.8078 mle=1.2804 pcon=5.2951 forget=1.2324 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=7.8924 mle=1.3921 pcon=5.2887 forget=1.2116 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=8.0159 mle=1.5079 pcon=5.2827 forget=1.2253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=8.0598 mle=1.5645 pcon=5.2770 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=8.0045 mle=1.5330 pcon=5.2708 forget=1.2007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=7.8785 mle=1.3804 pcon=5.2650 forget=1.2331 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=7.7785 mle=1.2921 pcon=5.2596 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=7.9325 mle=1.4378 pcon=5.2543 forget=1.2405 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 1 it 10 total=7.7858 mle=1.3065 pcon=5.2487 forget=1.2305 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=7.7981 mle=1.3159 pcon=5.2433 forget=1.2389 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=7.7469 mle=1.2903 pcon=5.2382 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=7.7127 mle=1.2381 pcon=5.2331 forget=1.2415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=7.8318 mle=1.3723 pcon=5.2283 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=7.7791 mle=1.3467 pcon=5.2235 forget=1.2089 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=7.7558 mle=1.3081 pcon=5.2185 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=7.8229 mle=1.3767 pcon=5.2136 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 2 it 20 total=7.7062 mle=1.2745 pcon=5.2089 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=7.8501 mle=1.3989 pcon=5.2044 forget=1.2468 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=7.7729 mle=1.3589 pcon=5.1998 forget=1.2142 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=7.7724 mle=1.3579 pcon=5.1954 forget=1.2190 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=7.7515 mle=1.3298 pcon=5.1908 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=7.7868 mle=1.3657 pcon=5.1865 forget=1.2345 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=7.6577 mle=1.2606 pcon=5.1824 forget=1.2146 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=7.7850 mle=1.3836 pcon=5.1787 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 3 it 30 total=7.7683 mle=1.3638 pcon=5.1748 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=7.6956 mle=1.2956 pcon=5.1709 forget=1.2290 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=7.6785 mle=1.2905 pcon=5.1672 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=7.8326 mle=1.4601 pcon=5.1635 forget=1.2091 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=7.7463 mle=1.3524 pcon=5.1601 forget=1.2338 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=7.8180 mle=1.4268 pcon=5.1568 forget=1.2344 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=7.8237 mle=1.4480 pcon=5.1531 forget=1.2225 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=7.6896 mle=1.3140 pcon=5.1497 forget=1.2259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 4 it 40 total=7.7404 mle=1.3769 pcon=5.1465 forget=1.2171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=7.7166 mle=1.3336 pcon=5.1434 forget=1.2396 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=7.9298 mle=1.5852 pcon=5.1406 forget=1.2041 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=7.7628 mle=1.3942 pcon=5.1374 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=7.7865 mle=1.4142 pcon=5.1344 forget=1.2379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=7.5439 mle=1.1861 pcon=5.1316 forget=1.2262 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=7.8146 mle=1.4610 pcon=5.1290 forget=1.2246 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:01<08:32,  1.32s/it]  3%|▎         | 11/391 [00:01<00:36, 10.42it/s]  5%|▌         | 21/391 [00:01<00:17, 21.27it/s]  8%|▊         | 31/391 [00:01<00:11, 32.63it/s] 10%|█         | 41/391 [00:01<00:07, 43.99it/s] 13%|█▎        | 51/391 [00:01<00:06, 54.55it/s] 16%|█▌        | 61/391 [00:01<00:05, 63.80it/s] 18%|█▊        | 71/391 [00:02<00:04, 71.59it/s] 21%|██        | 81/391 [00:02<00:03, 77.72it/s] 23%|██▎       | 91/391 [00:02<00:03, 81.34it/s] 26%|██▌       | 101/391 [00:02<00:03, 82.55it/s] 28%|██▊       | 111/391 [00:02<00:03, 85.94it/s] 31%|███       | 121/391 [00:02<00:03, 88.26it/s] 34%|███▎      | 131/391 [00:02<00:02, 89.92it/s] 36%|███▌      | 141/391 [00:02<00:02, 91.30it/s] 39%|███▊      | 151/391 [00:02<00:02, 91.77it/s] 41%|████      | 161/391 [00:03<00:02, 92.59it/s] 44%|████▎     | 171/391 [00:03<00:02, 92.71it/s] 46%|████▋     | 181/391 [00:03<00:02, 93.16it/s] 49%|████▉     | 191/391 [00:03<00:02, 93.39it/s] 51%|█████▏    | 201/391 [00:03<00:02, 94.03it/s] 54%|█████▍    | 211/391 [00:03<00:01, 94.55it/s] 57%|█████▋    | 221/391 [00:03<00:01, 94.91it/s] 59%|█████▉    | 231/391 [00:03<00:01, 94.82it/s] 62%|██████▏   | 241/391 [00:03<00:01, 95.15it/s] 64%|██████▍   | 251/391 [00:03<00:01, 95.33it/s] 67%|██████▋   | 261/391 [00:04<00:01, 95.53it/s] 69%|██████▉   | 271/391 [00:04<00:01, 95.65it/s] 72%|███████▏  | 281/391 [00:04<00:01, 95.41it/s] 74%|███████▍  | 291/391 [00:04<00:01, 93.91it/s] 77%|███████▋  | 301/391 [00:04<00:00, 94.50it/s] 80%|███████▉  | 311/391 [00:04<00:00, 94.93it/s] 82%|████████▏ | 321/391 [00:04<00:00, 93.99it/s] 85%|████████▍ | 331/391 [00:04<00:00, 94.00it/s] 87%|████████▋ | 341/391 [00:04<00:00, 94.30it/s] 90%|████████▉ | 351/391 [00:05<00:00, 94.70it/s] 92%|█████████▏| 361/391 [00:05<00:00, 94.58it/s] 95%|█████████▍| 371/391 [00:05<00:00, 95.20it/s] 97%|█████████▋| 381/391 [00:05<00:00, 95.80it/s]100%|██████████| 391/391 [00:05<00:00, 92.88it/s]100%|██████████| 391/391 [00:05<00:00, 71.71it/s]
50000 images processed, 5.54006814956665 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:33,  2.33it/s] 11%|█▏        | 9/79 [00:00<00:03, 21.54it/s] 23%|██▎       | 18/79 [00:00<00:01, 39.37it/s] 34%|███▍      | 27/79 [00:00<00:00, 52.81it/s] 46%|████▌     | 36/79 [00:00<00:00, 63.02it/s] 58%|█████▊    | 46/79 [00:00<00:00, 71.54it/s] 71%|███████   | 56/79 [00:01<00:00, 77.61it/s] 84%|████████▎ | 66/79 [00:01<00:00, 82.15it/s] 96%|█████████▌| 76/79 [00:01<00:00, 85.41it/s]100%|██████████| 79/79 [00:01<00:00, 60.24it/s]
10000 images processed, 1.34779691696167 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:34,  2.15it/s]  4%|▍         | 9/204 [00:00<00:09, 20.33it/s]  9%|▉         | 19/204 [00:00<00:04, 39.64it/s] 14%|█▍        | 29/204 [00:00<00:03, 54.05it/s] 19%|█▉        | 39/204 [00:00<00:02, 64.42it/s] 24%|██▍       | 49/204 [00:01<00:02, 72.23it/s] 29%|██▉       | 59/204 [00:01<00:01, 78.25it/s] 34%|███▍      | 69/204 [00:01<00:01, 82.27it/s] 39%|███▊      | 79/204 [00:01<00:01, 84.88it/s] 43%|████▎     | 88/204 [00:01<00:01, 86.16it/s] 48%|████▊     | 98/204 [00:01<00:01, 88.26it/s] 53%|█████▎    | 108/204 [00:01<00:01, 87.71it/s] 58%|█████▊    | 118/204 [00:01<00:00, 88.89it/s] 63%|██████▎   | 128/204 [00:01<00:00, 90.05it/s] 68%|██████▊   | 138/204 [00:01<00:00, 91.34it/s] 73%|███████▎  | 148/204 [00:02<00:00, 92.03it/s] 77%|███████▋  | 158/204 [00:02<00:00, 92.72it/s] 82%|████████▏ | 168/204 [00:02<00:00, 93.23it/s] 87%|████████▋ | 178/204 [00:02<00:00, 93.85it/s] 92%|█████████▏| 188/204 [00:02<00:00, 94.50it/s] 97%|█████████▋| 198/204 [00:02<00:00, 94.93it/s]100%|██████████| 204/204 [00:02<00:00, 76.25it/s]
26032 images processed, 2.7154252529144287 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:45,  1.73it/s] 10%|█         | 8/79 [00:00<00:04, 15.20it/s] 20%|██        | 16/79 [00:00<00:02, 29.41it/s] 32%|███▏      | 25/79 [00:00<00:01, 43.57it/s] 44%|████▍     | 35/79 [00:00<00:00, 56.31it/s] 56%|█████▌    | 44/79 [00:01<00:00, 64.68it/s] 68%|██████▊   | 54/79 [00:01<00:00, 72.59it/s] 81%|████████  | 64/79 [00:01<00:00, 78.90it/s] 94%|█████████▎| 74/79 [00:01<00:00, 83.50it/s]100%|██████████| 79/79 [00:01<00:00, 53.87it/s]
10000 images processed, 1.4965214729309082 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:37,  2.07it/s]  9%|▉         | 7/79 [00:00<00:04, 14.96it/s] 20%|██        | 16/79 [00:00<00:01, 32.63it/s] 32%|███▏      | 25/79 [00:00<00:01, 45.98it/s] 43%|████▎     | 34/79 [00:00<00:00, 56.16it/s] 54%|█████▍    | 43/79 [00:01<00:00, 64.53it/s] 67%|██████▋   | 53/79 [00:01<00:00, 72.85it/s] 80%|███████▉  | 63/79 [00:01<00:00, 79.40it/s] 92%|█████████▏| 73/79 [00:01<00:00, 84.12it/s]100%|██████████| 79/79 [00:01<00:00, 56.68it/s]
10000 images processed, 1.421196460723877 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:35,  1.96it/s] 14%|█▍        | 10/70 [00:00<00:02, 20.74it/s] 27%|██▋       | 19/70 [00:00<00:01, 35.97it/s] 40%|████      | 28/70 [00:00<00:00, 48.40it/s] 53%|█████▎    | 37/70 [00:00<00:00, 58.13it/s] 67%|██████▋   | 47/70 [00:01<00:00, 68.11it/s] 81%|████████▏ | 57/70 [00:01<00:00, 75.85it/s] 96%|█████████▌| 67/70 [00:01<00:00, 81.41it/s]100%|██████████| 70/70 [00:01<00:00, 53.85it/s]
8925 images processed, 1.331937313079834 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:39,  1.12it/s]  4%|▍         | 2/45 [00:01<00:19,  2.26it/s] 16%|█▌        | 7/45 [00:01<00:04,  8.08it/s] 36%|███▌      | 16/45 [00:01<00:01, 20.80it/s] 47%|████▋     | 21/45 [00:01<00:00, 25.84it/s] 58%|█████▊    | 26/45 [00:01<00:00, 19.81it/s] 73%|███████▎  | 33/45 [00:01<00:00, 27.02it/s] 84%|████████▍ | 38/45 [00:02<00:00, 29.36it/s] 96%|█████████▌| 43/45 [00:02<00:00, 29.13it/s]100%|██████████| 45/45 [00:02<00:00, 19.54it/s]
5640 images processed, 2.3324201107025146 seconds used

17.698988437652588
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.58  99.36  98.01
places365     68.03  81.18  78.69
LSUN          17.81  96.08  96.09
iSUN          72.30  81.80  84.19
dtd           38.49  91.37  94.31
AVG           39.84  89.96  90.26
Retain-Acc: 0.7482
Forget-as-OOD (retain known vs forget novel):
  FPR: 54.70 AUROC: 89.29 AUIN: 98.64
7.633149147033691
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_rf.png
[Run] lambda=0.1 lr=0.001 epochs=10
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.1, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:19<02:54, 19.35s/it] 20%|██        | 2/10 [00:32<02:06, 15.77s/it] 30%|███       | 3/10 [00:45<01:41, 14.47s/it] 40%|████      | 4/10 [00:59<01:25, 14.27s/it] 50%|█████     | 5/10 [01:12<01:09, 13.89s/it] 60%|██████    | 6/10 [01:26<00:55, 13.93s/it][loss] ep 0 it 0 total=7.8079 mle=1.2805 pcon=5.2951 forget=1.2324 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=7.8927 mle=1.3924 pcon=5.2887 forget=1.2116 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=8.0160 mle=1.5080 pcon=5.2827 forget=1.2253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=8.0598 mle=1.5645 pcon=5.2770 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=8.0045 mle=1.5330 pcon=5.2708 forget=1.2007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=7.8785 mle=1.3804 pcon=5.2650 forget=1.2331 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=7.7786 mle=1.2922 pcon=5.2596 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=7.9325 mle=1.4378 pcon=5.2542 forget=1.2404 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 1 it 10 total=7.7858 mle=1.3066 pcon=5.2487 forget=1.2305 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=7.7982 mle=1.3160 pcon=5.2433 forget=1.2389 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=7.7466 mle=1.2901 pcon=5.2382 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=7.7126 mle=1.2381 pcon=5.2331 forget=1.2415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=7.8318 mle=1.3722 pcon=5.2283 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=7.7791 mle=1.3468 pcon=5.2235 forget=1.2088 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=7.7558 mle=1.3082 pcon=5.2185 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=7.8229 mle=1.3766 pcon=5.2136 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 2 it 20 total=7.7062 mle=1.2745 pcon=5.2089 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=7.8500 mle=1.3988 pcon=5.2044 forget=1.2468 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=7.7727 mle=1.3588 pcon=5.1998 forget=1.2142 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=7.7723 mle=1.3579 pcon=5.1954 forget=1.2190 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=7.7516 mle=1.3299 pcon=5.1909 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=7.7869 mle=1.3658 pcon=5.1865 forget=1.2346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=7.6577 mle=1.2607 pcon=5.1824 forget=1.2146 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=7.7849 mle=1.3835 pcon=5.1787 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 3 it 30 total=7.7685 mle=1.3640 pcon=5.1748 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=7.6956 mle=1.2956 pcon=5.1710 forget=1.2290 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=7.6785 mle=1.2906 pcon=5.1672 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=7.8327 mle=1.4601 pcon=5.1635 forget=1.2091 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=7.7463 mle=1.3524 pcon=5.1601 forget=1.2338 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=7.8179 mle=1.4268 pcon=5.1568 forget=1.2344 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=7.8238 mle=1.4482 pcon=5.1531 forget=1.2225 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=7.6897 mle=1.3141 pcon=5.1497 forget=1.2258 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 4 it 40 total=7.7404 mle=1.3769 pcon=5.1465 forget=1.2170 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=7.7166 mle=1.3336 pcon=5.1434 forget=1.2396 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=7.9298 mle=1.5852 pcon=5.1406 forget=1.2041 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=7.7627 mle=1.3941 pcon=5.1374 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=7.7866 mle=1.4144 pcon=5.1344 forget=1.2379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=7.5440 mle=1.1862 pcon=5.1316 forget=1.2262 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=7.8148 mle=1.4612 pcon=5.1290 forget=1.2246 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 5 it 0 total=7.5815 mle=1.2047 pcon=5.1261 forget=1.2507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=7.7229 mle=1.3647 pcon=5.1234 forget=1.2348 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=7.7650 mle=1.4006 pcon=5.1206 forget=1.2438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=7.8337 mle=1.4809 pcon=5.1178 forget=1.2350 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=7.7883 mle=1.4523 pcon=5.1152 forget=1.2208 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=7.6862 mle=1.3482 pcon=5.1124 forget=1.2256 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=7.7565 mle=1.4295 pcon=5.1098 forget=1.2172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=7.7335 mle=1.4044 pcon=5.1071 forget=1.2219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 6 it 10 total=7.6346 mle=1.2783 pcon=5.1048 forget=1.2516 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=7.7001 mle=1.3787 pcon=5.1024 forget=1.2189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=7.6562 mle=1.3184 pcon=5.1002 forget=1.2376 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=7.6599 mle=1.3322 pcon=5.0979 forget=1.2298 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=7.6925 mle=1.3654 pcon=5.0954 forget=1.2317 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=7.7161 mle=1.3835 pcon=5.0933 forget=1.2393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=7.6565 mle=1.3538 pcon=5.0913 forget=1.2115 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 70%|███████   | 7/10 [01:40<00:41, 13.89s/it] 80%|████████  | 8/10 [01:53<00:27, 13.75s/it] 90%|█████████ | 9/10 [02:07<00:13, 13.53s/it]100%|██████████| 10/10 [02:20<00:00, 13.41s/it]100%|██████████| 10/10 [02:20<00:00, 14.02s/it]
[loss] ep 6 it 360 total=7.7497 mle=1.4567 pcon=5.0891 forget=1.2040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 7 it 20 total=7.5654 mle=1.2501 pcon=5.0870 forget=1.2283 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=7.5608 mle=1.2464 pcon=5.0847 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=7.6923 mle=1.3880 pcon=5.0827 forget=1.2216 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=7.7177 mle=1.4331 pcon=5.0808 forget=1.2038 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=7.6182 mle=1.3207 pcon=5.0791 forget=1.2184 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=7.7541 mle=1.4545 pcon=5.0775 forget=1.2221 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=7.6235 mle=1.3171 pcon=5.0753 forget=1.2311 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=7.6912 mle=1.3713 pcon=5.0733 forget=1.2467 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 8 it 30 total=7.7577 mle=1.4810 pcon=5.0716 forget=1.2051 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=7.5762 mle=1.2754 pcon=5.0699 forget=1.2309 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=7.6953 mle=1.3991 pcon=5.0682 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=7.6397 mle=1.3457 pcon=5.0665 forget=1.2276 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=7.6604 mle=1.3664 pcon=5.0648 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=7.5536 mle=1.2578 pcon=5.0632 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=7.8330 mle=1.5801 pcon=5.0614 forget=1.1915 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=7.6516 mle=1.3648 pcon=5.0600 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 9 it 40 total=7.6994 mle=1.4116 pcon=5.0585 forget=1.2293 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=7.7618 mle=1.4931 pcon=5.0570 forget=1.2117 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=7.6463 mle=1.3548 pcon=5.0558 forget=1.2356 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=7.5182 mle=1.2291 pcon=5.0546 forget=1.2346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=7.7623 mle=1.4987 pcon=5.0535 forget=1.2100 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=7.6413 mle=1.3595 pcon=5.0520 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=7.5896 mle=1.3080 pcon=5.0508 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:09,  2.06it/s]  2%|▏         | 6/391 [00:00<00:29, 12.88it/s]  3%|▎         | 11/391 [00:00<00:17, 21.40it/s]  4%|▍         | 17/391 [00:00<00:12, 30.37it/s]  7%|▋         | 26/391 [00:00<00:08, 45.53it/s]  9%|▉         | 36/391 [00:01<00:05, 59.66it/s] 12%|█▏        | 45/391 [00:01<00:05, 68.01it/s] 14%|█▍        | 55/391 [00:01<00:04, 75.03it/s] 17%|█▋        | 65/391 [00:01<00:04, 80.64it/s] 19%|█▉        | 74/391 [00:01<00:03, 81.07it/s] 21%|██        | 83/391 [00:01<00:04, 76.91it/s] 23%|██▎       | 91/391 [00:01<00:03, 75.24it/s] 25%|██▌       | 99/391 [00:01<00:04, 70.48it/s] 28%|██▊       | 108/391 [00:01<00:03, 74.38it/s] 30%|██▉       | 117/391 [00:02<00:03, 78.52it/s] 32%|███▏      | 126/391 [00:02<00:03, 78.62it/s] 34%|███▍      | 134/391 [00:02<00:03, 76.69it/s] 36%|███▋      | 142/391 [00:02<00:03, 73.40it/s] 38%|███▊      | 150/391 [00:02<00:03, 74.28it/s] 41%|████      | 159/391 [00:02<00:03, 76.41it/s] 43%|████▎     | 168/391 [00:02<00:02, 79.06it/s] 45%|████▌     | 176/391 [00:02<00:02, 78.03it/s] 47%|████▋     | 184/391 [00:02<00:02, 77.53it/s] 50%|████▉     | 194/391 [00:03<00:02, 83.14it/s] 52%|█████▏    | 204/391 [00:03<00:02, 86.58it/s] 55%|█████▍    | 214/391 [00:03<00:02, 88.38it/s] 57%|█████▋    | 223/391 [00:03<00:01, 88.45it/s] 60%|█████▉    | 233/391 [00:03<00:01, 90.40it/s] 62%|██████▏   | 243/391 [00:03<00:01, 91.39it/s] 65%|██████▍   | 253/391 [00:03<00:01, 92.60it/s] 67%|██████▋   | 263/391 [00:03<00:01, 92.78it/s] 70%|██████▉   | 273/391 [00:03<00:01, 92.87it/s] 72%|███████▏  | 283/391 [00:03<00:01, 93.45it/s] 75%|███████▍  | 293/391 [00:04<00:01, 92.61it/s] 77%|███████▋  | 303/391 [00:04<00:00, 92.87it/s] 80%|████████  | 313/391 [00:04<00:00, 92.69it/s] 83%|████████▎ | 323/391 [00:04<00:00, 92.18it/s] 85%|████████▌ | 333/391 [00:04<00:00, 91.12it/s] 88%|████████▊ | 343/391 [00:04<00:00, 91.24it/s] 90%|█████████ | 353/391 [00:04<00:00, 92.00it/s] 93%|█████████▎| 363/391 [00:04<00:00, 92.40it/s] 95%|█████████▌| 373/391 [00:04<00:00, 93.59it/s] 98%|█████████▊| 383/391 [00:05<00:00, 94.65it/s]100%|██████████| 391/391 [00:05<00:00, 76.24it/s]
50000 images processed, 5.217191219329834 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:38,  2.01it/s] 11%|█▏        | 9/79 [00:00<00:03, 18.96it/s] 22%|██▏       | 17/79 [00:00<00:01, 32.81it/s] 33%|███▎      | 26/79 [00:00<00:01, 46.16it/s] 44%|████▍     | 35/79 [00:00<00:00, 57.43it/s] 56%|█████▌    | 44/79 [00:01<00:00, 65.59it/s] 67%|██████▋   | 53/79 [00:01<00:00, 70.24it/s] 80%|███████▉  | 63/79 [00:01<00:00, 76.74it/s] 92%|█████████▏| 73/79 [00:01<00:00, 81.47it/s]100%|██████████| 79/79 [00:01<00:00, 43.48it/s]
10000 images processed, 1.8747410774230957 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:54,  1.77it/s]  4%|▍         | 9/204 [00:00<00:11, 17.38it/s]  9%|▉         | 18/204 [00:00<00:05, 33.32it/s] 12%|█▏        | 25/204 [00:00<00:04, 41.89it/s] 16%|█▌        | 32/204 [00:01<00:03, 43.96it/s] 19%|█▉        | 39/204 [00:01<00:03, 49.15it/s] 24%|██▎       | 48/204 [00:01<00:02, 58.93it/s] 28%|██▊       | 58/204 [00:01<00:02, 68.70it/s] 33%|███▎      | 68/204 [00:01<00:01, 76.16it/s] 38%|███▊      | 78/204 [00:01<00:01, 81.00it/s] 43%|████▎     | 88/204 [00:01<00:01, 83.85it/s] 48%|████▊     | 98/204 [00:01<00:01, 86.09it/s] 53%|█████▎    | 108/204 [00:01<00:01, 87.88it/s] 57%|█████▋    | 117/204 [00:01<00:01, 86.41it/s] 62%|██████▏   | 126/204 [00:02<00:00, 86.46it/s] 66%|██████▌   | 135/204 [00:02<00:00, 86.11it/s] 71%|███████   | 144/204 [00:02<00:00, 83.08it/s] 75%|███████▌  | 153/204 [00:02<00:00, 73.06it/s] 79%|███████▉  | 161/204 [00:02<00:00, 67.71it/s] 83%|████████▎ | 169/204 [00:02<00:00, 69.81it/s] 88%|████████▊ | 179/204 [00:02<00:00, 75.86it/s] 93%|█████████▎| 189/204 [00:02<00:00, 81.36it/s] 98%|█████████▊| 199/204 [00:03<00:00, 85.42it/s]100%|██████████| 204/204 [00:03<00:00, 65.78it/s]
26032 images processed, 3.1481683254241943 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:58,  1.34it/s] 13%|█▎        | 10/79 [00:00<00:04, 15.42it/s] 24%|██▍       | 19/79 [00:00<00:02, 29.12it/s] 35%|███▌      | 28/79 [00:01<00:01, 41.55it/s] 48%|████▊     | 38/79 [00:01<00:00, 53.97it/s] 61%|██████    | 48/79 [00:01<00:00, 63.56it/s] 73%|███████▎  | 58/79 [00:01<00:00, 71.50it/s] 86%|████████▌ | 68/79 [00:01<00:00, 77.81it/s] 99%|█████████▊| 78/79 [00:01<00:00, 82.88it/s]100%|██████████| 79/79 [00:01<00:00, 49.28it/s]
10000 images processed, 1.6388001441955566 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:48,  1.60it/s]  9%|▉         | 7/79 [00:00<00:05, 12.35it/s] 18%|█▊        | 14/79 [00:00<00:02, 24.25it/s] 28%|██▊       | 22/79 [00:00<00:01, 36.72it/s] 39%|███▉      | 31/79 [00:01<00:00, 49.70it/s] 51%|█████     | 40/79 [00:01<00:00, 59.64it/s] 63%|██████▎   | 50/79 [00:01<00:00, 69.20it/s] 76%|███████▌  | 60/79 [00:01<00:00, 76.64it/s] 89%|████████▊ | 70/79 [00:01<00:00, 81.97it/s]100%|██████████| 79/79 [00:01<00:00, 51.00it/s]
10000 images processed, 1.5757362842559814 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:56,  1.22it/s] 11%|█▏        | 8/70 [00:00<00:05, 11.33it/s] 26%|██▌       | 18/70 [00:01<00:01, 26.35it/s] 40%|████      | 28/70 [00:01<00:01, 40.11it/s] 54%|█████▍    | 38/70 [00:01<00:00, 51.94it/s] 69%|██████▊   | 48/70 [00:01<00:00, 62.27it/s] 83%|████████▎ | 58/70 [00:01<00:00, 70.71it/s] 97%|█████████▋| 68/70 [00:01<00:00, 77.33it/s]100%|██████████| 70/70 [00:01<00:00, 43.93it/s]
8925 images processed, 1.6247920989990234 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:42,  1.04it/s]  7%|▋         | 3/45 [00:01<00:14,  2.80it/s] 29%|██▉       | 13/45 [00:01<00:02, 15.21it/s] 40%|████      | 18/45 [00:01<00:01, 17.72it/s] 49%|████▉     | 22/45 [00:01<00:01, 15.79it/s] 64%|██████▍   | 29/45 [00:01<00:00, 23.46it/s] 76%|███████▌  | 34/45 [00:02<00:00, 22.48it/s] 84%|████████▍ | 38/45 [00:02<00:00, 23.91it/s]100%|██████████| 45/45 [00:02<00:00, 18.35it/s]
5640 images processed, 2.4766318798065186 seconds used

19.210301399230957
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.57  99.36  98.00
places365     68.09  81.18  78.70
LSUN          17.67  96.11  96.11
iSUN          72.28  81.81  84.21
dtd           38.63  91.35  94.29
AVG           39.85  89.96  90.26
Retain-Acc: 0.7483
Forget-as-OOD (retain known vs forget novel):
  FPR: 54.60 AUROC: 89.29 AUIN: 98.64
7.272969961166382
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_rf.png
[Run] lambda=0.1 lr=0.001 epochs=15
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=15, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.1, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/15 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  7%|▋         | 1/15 [00:19<04:36, 19.78s/it] 13%|█▎        | 2/15 [00:34<03:38, 16.81s/it] 20%|██        | 3/15 [00:48<03:07, 15.61s/it] 27%|██▋       | 4/15 [01:03<02:47, 15.19s/it] 33%|███▎      | 5/15 [01:17<02:27, 14.78s/it] 40%|████      | 6/15 [01:31<02:12, 14.74s/it][loss] ep 0 it 0 total=7.8078 mle=1.2804 pcon=5.2951 forget=1.2324 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=7.8925 mle=1.3921 pcon=5.2887 forget=1.2116 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=8.0161 mle=1.5080 pcon=5.2827 forget=1.2253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=8.0597 mle=1.5644 pcon=5.2770 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=8.0044 mle=1.5329 pcon=5.2708 forget=1.2007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=7.8785 mle=1.3804 pcon=5.2650 forget=1.2331 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=7.7784 mle=1.2921 pcon=5.2596 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=7.9323 mle=1.4376 pcon=5.2543 forget=1.2404 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 1 it 10 total=7.7858 mle=1.3065 pcon=5.2487 forget=1.2306 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=7.7982 mle=1.3160 pcon=5.2433 forget=1.2389 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=7.7467 mle=1.2901 pcon=5.2382 forget=1.2184 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=7.7123 mle=1.2378 pcon=5.2331 forget=1.2415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=7.8318 mle=1.3723 pcon=5.2283 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=7.7792 mle=1.3469 pcon=5.2235 forget=1.2088 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=7.7558 mle=1.3081 pcon=5.2185 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=7.8230 mle=1.3767 pcon=5.2136 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 2 it 20 total=7.7060 mle=1.2744 pcon=5.2089 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=7.8500 mle=1.3988 pcon=5.2044 forget=1.2468 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=7.7729 mle=1.3589 pcon=5.1998 forget=1.2142 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=7.7724 mle=1.3580 pcon=5.1954 forget=1.2190 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=7.7515 mle=1.3299 pcon=5.1909 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=7.7867 mle=1.3656 pcon=5.1865 forget=1.2345 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=7.6577 mle=1.2606 pcon=5.1824 forget=1.2147 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=7.7847 mle=1.3833 pcon=5.1787 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 3 it 30 total=7.7685 mle=1.3639 pcon=5.1749 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=7.6955 mle=1.2955 pcon=5.1710 forget=1.2290 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=7.6783 mle=1.2904 pcon=5.1672 forget=1.2206 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=7.8326 mle=1.4600 pcon=5.1635 forget=1.2091 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=7.7461 mle=1.3522 pcon=5.1601 forget=1.2338 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=7.8178 mle=1.4267 pcon=5.1568 forget=1.2344 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=7.8238 mle=1.4482 pcon=5.1531 forget=1.2225 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=7.6897 mle=1.3141 pcon=5.1497 forget=1.2258 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 4 it 40 total=7.7403 mle=1.3768 pcon=5.1464 forget=1.2170 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=7.7167 mle=1.3337 pcon=5.1434 forget=1.2396 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=7.9300 mle=1.5853 pcon=5.1406 forget=1.2041 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=7.7626 mle=1.3940 pcon=5.1374 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=7.7864 mle=1.4142 pcon=5.1344 forget=1.2379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=7.5441 mle=1.1864 pcon=5.1316 forget=1.2261 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=7.8149 mle=1.4613 pcon=5.1290 forget=1.2246 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 5 it 0 total=7.5816 mle=1.2048 pcon=5.1261 forget=1.2507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=7.7229 mle=1.3648 pcon=5.1234 forget=1.2348 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=7.7651 mle=1.4007 pcon=5.1206 forget=1.2438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=7.8326 mle=1.4798 pcon=5.1178 forget=1.2350 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=7.7880 mle=1.4521 pcon=5.1151 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=7.6864 mle=1.3484 pcon=5.1124 forget=1.2256 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=7.7564 mle=1.4294 pcon=5.1098 forget=1.2172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=7.7336 mle=1.4046 pcon=5.1071 forget=1.2219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 6 it 10 total=7.6346 mle=1.2782 pcon=5.1047 forget=1.2516 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=7.7001 mle=1.3787 pcon=5.1024 forget=1.2189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=7.6564 mle=1.3186 pcon=5.1002 forget=1.2376 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=7.6599 mle=1.3322 pcon=5.0979 forget=1.2298 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=7.6925 mle=1.3653 pcon=5.0954 forget=1.2318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=7.7162 mle=1.3836 pcon=5.0933 forget=1.2393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=7.6565 mle=1.3538 pcon=5.0913 forget=1.2114 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 47%|████▋     | 7/15 [01:46<01:57, 14.71s/it] 53%|█████▎    | 8/15 [02:01<01:43, 14.81s/it] 60%|██████    | 9/15 [02:16<01:28, 14.73s/it] 67%|██████▋   | 10/15 [02:29<01:11, 14.23s/it] 73%|███████▎  | 11/15 [02:42<00:55, 13.88s/it] 80%|████████  | 12/15 [02:55<00:41, 13.73s/it] 87%|████████▋ | 13/15 [03:08<00:27, 13.53s/it][loss] ep 6 it 360 total=7.7497 mle=1.4566 pcon=5.0891 forget=1.2040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 7 it 20 total=7.5655 mle=1.2502 pcon=5.0870 forget=1.2283 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=7.5609 mle=1.2465 pcon=5.0847 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=7.6921 mle=1.3878 pcon=5.0827 forget=1.2216 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=7.7178 mle=1.4333 pcon=5.0808 forget=1.2038 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=7.6181 mle=1.3206 pcon=5.0791 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=7.7540 mle=1.4544 pcon=5.0775 forget=1.2221 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=7.6234 mle=1.3171 pcon=5.0753 forget=1.2311 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=7.6912 mle=1.3712 pcon=5.0733 forget=1.2467 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 8 it 30 total=7.7575 mle=1.4808 pcon=5.0716 forget=1.2052 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=7.5762 mle=1.2754 pcon=5.0699 forget=1.2309 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=7.6954 mle=1.3992 pcon=5.0682 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=7.6394 mle=1.3454 pcon=5.0665 forget=1.2276 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=7.6604 mle=1.3664 pcon=5.0648 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=7.5536 mle=1.2578 pcon=5.0632 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=7.8330 mle=1.5801 pcon=5.0614 forget=1.1915 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=7.6515 mle=1.3647 pcon=5.0600 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 9 it 40 total=7.6993 mle=1.4115 pcon=5.0585 forget=1.2293 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=7.7618 mle=1.4931 pcon=5.0570 forget=1.2117 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=7.6463 mle=1.3549 pcon=5.0559 forget=1.2356 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=7.5189 mle=1.2297 pcon=5.0546 forget=1.2346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=7.7623 mle=1.4988 pcon=5.0535 forget=1.2100 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=7.6415 mle=1.3597 pcon=5.0520 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=7.5896 mle=1.3080 pcon=5.0508 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 10 it 0 total=7.5544 mle=1.2683 pcon=5.0491 forget=1.2369 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=7.5333 mle=1.2586 pcon=5.0482 forget=1.2265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=7.5939 mle=1.3283 pcon=5.0469 forget=1.2187 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=7.6345 mle=1.3450 pcon=5.0457 forget=1.2439 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=7.6851 mle=1.4102 pcon=5.0446 forget=1.2303 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=7.6989 mle=1.4289 pcon=5.0436 forget=1.2265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=7.7520 mle=1.5151 pcon=5.0423 forget=1.1946 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=7.5336 mle=1.2627 pcon=5.0415 forget=1.2294 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 11 it 10 total=7.7023 mle=1.4431 pcon=5.0403 forget=1.2189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=7.6332 mle=1.3737 pcon=5.0394 forget=1.2201 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=7.6153 mle=1.3488 pcon=5.0385 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=7.5509 mle=1.2741 pcon=5.0374 forget=1.2394 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=7.5319 mle=1.2652 pcon=5.0365 forget=1.2302 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=7.5278 mle=1.2698 pcon=5.0355 forget=1.2224 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=7.6544 mle=1.3938 pcon=5.0347 forget=1.2259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=7.6010 mle=1.3227 pcon=5.0340 forget=1.2443 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 12 it 20 total=7.5826 mle=1.3172 pcon=5.0327 forget=1.2327 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=7.6209 mle=1.3573 pcon=5.0317 forget=1.2319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=7.6063 mle=1.3671 pcon=5.0307 forget=1.2086 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=7.6147 mle=1.3453 pcon=5.0296 forget=1.2398 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=7.5009 mle=1.2278 pcon=5.0290 forget=1.2441 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=7.5490 mle=1.3000 pcon=5.0285 forget=1.2206 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=7.6775 mle=1.4195 pcon=5.0278 forget=1.2301 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=7.6454 mle=1.3897 pcon=5.0271 forget=1.2286 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 13 it 30 total=7.5906 mle=1.3193 pcon=5.0266 forget=1.2447 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=7.7530 mle=1.5131 pcon=5.0260 forget=1.2139 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=7.6175 mle=1.3673 pcon=5.0251 forget=1.2251 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=7.6322 mle=1.3598 pcon=5.0240 forget=1.2484 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 93%|█████████▎| 14/15 [03:22<00:13, 13.47s/it]100%|██████████| 15/15 [03:35<00:00, 13.47s/it]100%|██████████| 15/15 [03:35<00:00, 14.38s/it]
[loss] ep 13 it 230 total=7.5561 mle=1.3091 pcon=5.0233 forget=1.2237 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=7.5091 mle=1.2306 pcon=5.0226 forget=1.2560 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=7.6541 mle=1.4095 pcon=5.0221 forget=1.2225 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=7.5789 mle=1.3359 pcon=5.0215 forget=1.2215 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 14 it 40 total=7.5337 mle=1.2885 pcon=5.0208 forget=1.2244 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=7.5970 mle=1.3414 pcon=5.0203 forget=1.2353 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=7.5754 mle=1.3142 pcon=5.0197 forget=1.2414 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=7.5276 mle=1.2710 pcon=5.0192 forget=1.2373 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=7.5963 mle=1.3425 pcon=5.0183 forget=1.2355 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=7.5428 mle=1.3098 pcon=5.0178 forget=1.2152 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=7.5006 mle=1.2610 pcon=5.0173 forget=1.2222 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:54,  2.24it/s]  2%|▏         | 8/391 [00:00<00:20, 18.61it/s]  4%|▍         | 17/391 [00:00<00:10, 35.89it/s]  7%|▋         | 26/391 [00:00<00:07, 48.98it/s]  9%|▉         | 35/391 [00:00<00:05, 59.76it/s] 11%|█▏        | 44/391 [00:00<00:05, 68.08it/s] 14%|█▎        | 53/391 [00:01<00:04, 73.61it/s] 16%|█▌        | 63/391 [00:01<00:04, 78.78it/s] 19%|█▊        | 73/391 [00:01<00:03, 82.88it/s] 21%|██        | 83/391 [00:01<00:03, 86.56it/s] 24%|██▍       | 93/391 [00:01<00:03, 88.50it/s] 26%|██▋       | 103/391 [00:01<00:03, 89.95it/s] 29%|██▉       | 113/391 [00:01<00:03, 90.99it/s] 31%|███▏      | 123/391 [00:01<00:02, 92.33it/s] 34%|███▍      | 133/391 [00:01<00:02, 93.38it/s] 37%|███▋      | 143/391 [00:02<00:02, 94.03it/s] 39%|███▉      | 153/391 [00:02<00:02, 93.62it/s] 42%|████▏     | 163/391 [00:02<00:02, 94.21it/s] 44%|████▍     | 173/391 [00:02<00:02, 93.90it/s] 47%|████▋     | 183/391 [00:02<00:02, 94.37it/s] 49%|████▉     | 193/391 [00:02<00:02, 94.15it/s] 52%|█████▏    | 203/391 [00:02<00:01, 94.05it/s] 54%|█████▍    | 213/391 [00:02<00:01, 94.24it/s] 57%|█████▋    | 223/391 [00:02<00:01, 94.29it/s] 60%|█████▉    | 233/391 [00:02<00:01, 94.29it/s] 62%|██████▏   | 243/391 [00:03<00:01, 85.85it/s] 64%|██████▍   | 252/391 [00:03<00:01, 86.35it/s] 67%|██████▋   | 261/391 [00:03<00:01, 85.58it/s] 69%|██████▉   | 270/391 [00:03<00:01, 86.50it/s] 71%|███████▏  | 279/391 [00:03<00:01, 84.74it/s] 74%|███████▎  | 288/391 [00:03<00:01, 73.15it/s] 76%|███████▌  | 296/391 [00:03<00:01, 70.89it/s] 78%|███████▊  | 304/391 [00:03<00:01, 66.98it/s] 80%|████████  | 313/391 [00:04<00:01, 71.58it/s] 83%|████████▎ | 323/391 [00:04<00:00, 78.07it/s] 85%|████████▌ | 333/391 [00:04<00:00, 82.73it/s] 88%|████████▊ | 343/391 [00:04<00:00, 85.95it/s] 90%|█████████ | 353/391 [00:04<00:00, 87.99it/s] 93%|█████████▎| 362/391 [00:04<00:00, 88.39it/s] 95%|█████████▌| 372/391 [00:04<00:00, 90.80it/s] 98%|█████████▊| 382/391 [00:04<00:00, 92.57it/s]100%|██████████| 391/391 [00:05<00:00, 78.15it/s]
50000 images processed, 5.189836025238037 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:47,  1.63it/s] 11%|█▏        | 9/79 [00:00<00:04, 16.35it/s] 22%|██▏       | 17/79 [00:00<00:02, 29.17it/s] 33%|███▎      | 26/79 [00:00<00:01, 42.63it/s] 44%|████▍     | 35/79 [00:01<00:00, 54.10it/s] 57%|█████▋    | 45/79 [00:01<00:00, 64.25it/s] 70%|██████▉   | 55/79 [00:01<00:00, 72.15it/s] 82%|████████▏ | 65/79 [00:01<00:00, 78.12it/s] 95%|█████████▍| 75/79 [00:01<00:00, 82.55it/s]100%|██████████| 79/79 [00:01<00:00, 51.85it/s]
10000 images processed, 1.5554759502410889 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:47,  1.89it/s]  3%|▎         | 7/204 [00:00<00:14, 13.90it/s]  6%|▋         | 13/204 [00:00<00:07, 24.41it/s] 11%|█▏        | 23/204 [00:00<00:04, 42.29it/s] 16%|█▌        | 33/204 [00:00<00:03, 55.80it/s] 21%|██        | 43/204 [00:01<00:02, 65.98it/s] 26%|██▌       | 53/204 [00:01<00:02, 73.75it/s] 31%|███       | 63/204 [00:01<00:01, 79.61it/s] 36%|███▌      | 73/204 [00:01<00:01, 83.38it/s] 41%|████      | 83/204 [00:01<00:01, 85.55it/s] 45%|████▌     | 92/204 [00:01<00:01, 86.44it/s] 50%|█████     | 102/204 [00:01<00:01, 87.88it/s] 55%|█████▍    | 112/204 [00:01<00:01, 88.67it/s] 60%|█████▉    | 122/204 [00:01<00:00, 89.99it/s] 65%|██████▍   | 132/204 [00:02<00:00, 91.20it/s] 70%|██████▉   | 142/204 [00:02<00:00, 91.04it/s] 75%|███████▍  | 152/204 [00:02<00:00, 90.78it/s] 79%|███████▉  | 162/204 [00:02<00:00, 91.84it/s] 84%|████████▍ | 172/204 [00:02<00:00, 92.29it/s] 89%|████████▉ | 182/204 [00:02<00:00, 93.32it/s] 94%|█████████▍| 192/204 [00:02<00:00, 94.21it/s] 99%|█████████▉| 202/204 [00:02<00:00, 94.84it/s]100%|██████████| 204/204 [00:02<00:00, 72.71it/s]
26032 images processed, 2.8456013202667236 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:53,  1.47it/s] 13%|█▎        | 10/79 [00:00<00:04, 16.58it/s] 24%|██▍       | 19/79 [00:00<00:01, 30.68it/s] 33%|███▎      | 26/79 [00:01<00:01, 38.43it/s] 44%|████▍     | 35/79 [00:01<00:00, 50.04it/s] 54%|█████▍    | 43/79 [00:01<00:00, 48.30it/s] 67%|██████▋   | 53/79 [00:01<00:00, 58.85it/s] 77%|███████▋  | 61/79 [00:01<00:00, 57.26it/s] 90%|████████▉ | 71/79 [00:01<00:00, 66.45it/s]100%|██████████| 79/79 [00:01<00:00, 45.80it/s]
10000 images processed, 1.7935595512390137 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:55,  1.39it/s] 11%|█▏        | 9/79 [00:00<00:04, 14.14it/s] 23%|██▎       | 18/79 [00:00<00:02, 28.20it/s] 35%|███▌      | 28/79 [00:01<00:01, 42.49it/s] 47%|████▋     | 37/79 [00:01<00:00, 53.04it/s] 59%|█████▉    | 47/79 [00:01<00:00, 63.18it/s] 72%|███████▏  | 57/79 [00:01<00:00, 71.56it/s] 85%|████████▍ | 67/79 [00:01<00:00, 78.18it/s] 97%|█████████▋| 77/79 [00:01<00:00, 83.03it/s]100%|██████████| 79/79 [00:01<00:00, 49.73it/s]
10000 images processed, 1.6147172451019287 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:36,  1.89it/s] 16%|█▌        | 11/70 [00:00<00:02, 22.25it/s] 30%|███       | 21/70 [00:00<00:01, 39.34it/s] 44%|████▍     | 31/70 [00:00<00:00, 53.04it/s] 59%|█████▊    | 41/70 [00:00<00:00, 63.74it/s] 73%|███████▎  | 51/70 [00:01<00:00, 72.39it/s] 87%|████████▋ | 61/70 [00:01<00:00, 78.82it/s]100%|██████████| 70/70 [00:01<00:00, 54.95it/s]
8925 images processed, 1.3042352199554443 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:43,  1.02it/s]  9%|▉         | 4/45 [00:01<00:08,  4.67it/s] 13%|█▎        | 6/45 [00:01<00:05,  6.63it/s] 36%|███▌      | 16/45 [00:01<00:01, 22.35it/s] 47%|████▋     | 21/45 [00:01<00:01, 15.86it/s] 69%|██████▉   | 31/45 [00:01<00:00, 27.22it/s] 82%|████████▏ | 37/45 [00:02<00:00, 20.94it/s] 98%|█████████▊| 44/45 [00:02<00:00, 27.05it/s]100%|██████████| 45/45 [00:02<00:00, 17.98it/s]
5640 images processed, 2.5619397163391113 seconds used

18.566171646118164
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.60  99.36  97.99
places365     68.29  81.17  78.71
LSUN          17.68  96.12  96.12
iSUN          72.20  81.87  84.27
dtd           38.76  91.33  94.28
AVG           39.91  89.97  90.28
Retain-Acc: 0.7484
Forget-as-OOD (retain known vs forget novel):
  FPR: 54.50 AUROC: 89.29 AUIN: 98.64
8.894391775131226
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_rf.png
[Run] lambda=0.1 lr=0.001 epochs=20
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.1, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:20<06:29, 20.51s/it] 10%|█         | 2/20 [00:34<05:02, 16.82s/it] 15%|█▌        | 3/20 [00:47<04:17, 15.16s/it] 20%|██        | 4/20 [01:00<03:48, 14.29s/it] 25%|██▌       | 5/20 [01:13<03:27, 13.86s/it] 30%|███       | 6/20 [01:27<03:11, 13.69s/it][loss] ep 0 it 0 total=7.8079 mle=1.2805 pcon=5.2951 forget=1.2323 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=7.8925 mle=1.3921 pcon=5.2887 forget=1.2116 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=8.0159 mle=1.5079 pcon=5.2827 forget=1.2253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=8.0596 mle=1.5643 pcon=5.2770 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=8.0045 mle=1.5330 pcon=5.2708 forget=1.2007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=7.8786 mle=1.3805 pcon=5.2650 forget=1.2331 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=7.7784 mle=1.2919 pcon=5.2596 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=7.9327 mle=1.4379 pcon=5.2543 forget=1.2405 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 1 it 10 total=7.7858 mle=1.3066 pcon=5.2487 forget=1.2305 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=7.7984 mle=1.3162 pcon=5.2433 forget=1.2389 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=7.7468 mle=1.2903 pcon=5.2382 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=7.7125 mle=1.2380 pcon=5.2331 forget=1.2415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=7.8320 mle=1.3724 pcon=5.2283 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=7.7791 mle=1.3468 pcon=5.2235 forget=1.2088 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=7.7558 mle=1.3081 pcon=5.2185 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=7.8229 mle=1.3767 pcon=5.2136 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 2 it 20 total=7.7062 mle=1.2745 pcon=5.2089 forget=1.2228 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=7.8500 mle=1.3988 pcon=5.2044 forget=1.2468 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=7.7729 mle=1.3589 pcon=5.1998 forget=1.2142 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=7.7726 mle=1.3582 pcon=5.1954 forget=1.2190 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=7.7517 mle=1.3301 pcon=5.1908 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=7.7870 mle=1.3659 pcon=5.1865 forget=1.2346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=7.6575 mle=1.2605 pcon=5.1824 forget=1.2147 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=7.7850 mle=1.3836 pcon=5.1787 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 3 it 30 total=7.7684 mle=1.3638 pcon=5.1748 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=7.6957 mle=1.2957 pcon=5.1709 forget=1.2290 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=7.6783 mle=1.2904 pcon=5.1672 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=7.8326 mle=1.4601 pcon=5.1635 forget=1.2091 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=7.7462 mle=1.3523 pcon=5.1601 forget=1.2338 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=7.8179 mle=1.4267 pcon=5.1568 forget=1.2344 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=7.8239 mle=1.4483 pcon=5.1531 forget=1.2225 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=7.6897 mle=1.3141 pcon=5.1497 forget=1.2259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 4 it 40 total=7.7403 mle=1.3768 pcon=5.1464 forget=1.2171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=7.7165 mle=1.3335 pcon=5.1434 forget=1.2396 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=7.9300 mle=1.5854 pcon=5.1406 forget=1.2040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=7.7626 mle=1.3940 pcon=5.1374 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=7.7863 mle=1.4141 pcon=5.1344 forget=1.2379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=7.5440 mle=1.1862 pcon=5.1316 forget=1.2262 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=7.8147 mle=1.4611 pcon=5.1290 forget=1.2246 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 5 it 0 total=7.5815 mle=1.2048 pcon=5.1261 forget=1.2507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=7.7232 mle=1.3650 pcon=5.1234 forget=1.2348 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=7.7651 mle=1.4008 pcon=5.1206 forget=1.2438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=7.8336 mle=1.4808 pcon=5.1178 forget=1.2350 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=7.7882 mle=1.4524 pcon=5.1151 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=7.6863 mle=1.3483 pcon=5.1124 forget=1.2256 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=7.7566 mle=1.4296 pcon=5.1098 forget=1.2172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=7.7334 mle=1.4044 pcon=5.1071 forget=1.2220 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 6 it 10 total=7.6346 mle=1.2783 pcon=5.1047 forget=1.2516 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=7.7001 mle=1.3787 pcon=5.1024 forget=1.2189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=7.6561 mle=1.3183 pcon=5.1002 forget=1.2376 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=7.6599 mle=1.3322 pcon=5.0979 forget=1.2298 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=7.6924 mle=1.3652 pcon=5.0954 forget=1.2318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=7.7160 mle=1.3834 pcon=5.0933 forget=1.2393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=7.6566 mle=1.3539 pcon=5.0913 forget=1.2115 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 35%|███▌      | 7/20 [01:40<02:56, 13.57s/it] 40%|████      | 8/20 [01:54<02:43, 13.59s/it] 45%|████▌     | 9/20 [02:06<02:26, 13.30s/it] 50%|█████     | 10/20 [02:19<02:10, 13.05s/it] 55%|█████▌    | 11/20 [02:32<01:56, 12.94s/it] 60%|██████    | 12/20 [02:44<01:42, 12.84s/it] 65%|██████▌   | 13/20 [02:57<01:30, 12.93s/it][loss] ep 6 it 360 total=7.7497 mle=1.4566 pcon=5.0891 forget=1.2040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 7 it 20 total=7.5654 mle=1.2500 pcon=5.0870 forget=1.2283 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=7.5610 mle=1.2466 pcon=5.0847 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=7.6923 mle=1.3880 pcon=5.0827 forget=1.2216 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=7.7176 mle=1.4330 pcon=5.0808 forget=1.2038 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=7.6182 mle=1.3207 pcon=5.0791 forget=1.2184 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=7.7541 mle=1.4545 pcon=5.0775 forget=1.2221 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=7.6235 mle=1.3171 pcon=5.0753 forget=1.2311 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=7.6912 mle=1.3712 pcon=5.0733 forget=1.2467 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 8 it 30 total=7.7576 mle=1.4809 pcon=5.0716 forget=1.2052 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=7.5762 mle=1.2754 pcon=5.0699 forget=1.2309 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=7.6953 mle=1.3991 pcon=5.0682 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=7.6396 mle=1.3455 pcon=5.0665 forget=1.2276 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=7.6603 mle=1.3662 pcon=5.0648 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=7.5535 mle=1.2577 pcon=5.0632 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=7.8328 mle=1.5798 pcon=5.0614 forget=1.1915 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=7.6516 mle=1.3648 pcon=5.0600 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 9 it 40 total=7.6996 mle=1.4118 pcon=5.0585 forget=1.2293 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=7.7620 mle=1.4932 pcon=5.0570 forget=1.2117 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=7.6461 mle=1.3547 pcon=5.0559 forget=1.2356 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=7.5183 mle=1.2291 pcon=5.0546 forget=1.2346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=7.7623 mle=1.4987 pcon=5.0535 forget=1.2101 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=7.6413 mle=1.3595 pcon=5.0520 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=7.5895 mle=1.3079 pcon=5.0508 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 10 it 0 total=7.5544 mle=1.2683 pcon=5.0491 forget=1.2369 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=7.5332 mle=1.2585 pcon=5.0482 forget=1.2265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=7.5938 mle=1.3283 pcon=5.0468 forget=1.2187 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=7.6345 mle=1.3450 pcon=5.0457 forget=1.2438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=7.6850 mle=1.4101 pcon=5.0446 forget=1.2303 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=7.6989 mle=1.4289 pcon=5.0436 forget=1.2264 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=7.7516 mle=1.5147 pcon=5.0424 forget=1.1946 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=7.5337 mle=1.2628 pcon=5.0415 forget=1.2294 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 11 it 10 total=7.7024 mle=1.4432 pcon=5.0403 forget=1.2189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=7.6332 mle=1.3738 pcon=5.0394 forget=1.2200 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=7.6150 mle=1.3485 pcon=5.0385 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=7.5510 mle=1.2743 pcon=5.0374 forget=1.2393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=7.5317 mle=1.2649 pcon=5.0365 forget=1.2302 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=7.5274 mle=1.2695 pcon=5.0355 forget=1.2224 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=7.6542 mle=1.3936 pcon=5.0347 forget=1.2259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=7.6003 mle=1.3222 pcon=5.0340 forget=1.2441 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 12 it 20 total=7.5822 mle=1.3168 pcon=5.0327 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=7.6199 mle=1.3563 pcon=5.0317 forget=1.2319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=7.6048 mle=1.3657 pcon=5.0307 forget=1.2084 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=7.6130 mle=1.3437 pcon=5.0296 forget=1.2397 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=7.5003 mle=1.2271 pcon=5.0290 forget=1.2442 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=7.5474 mle=1.2984 pcon=5.0285 forget=1.2206 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=7.6756 mle=1.4176 pcon=5.0279 forget=1.2300 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=7.6435 mle=1.3879 pcon=5.0272 forget=1.2283 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 13 it 30 total=7.5891 mle=1.3179 pcon=5.0268 forget=1.2444 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=7.7464 mle=1.5067 pcon=5.0261 forget=1.2135 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=7.6143 mle=1.3642 pcon=5.0253 forget=1.2248 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=7.6299 mle=1.3579 pcon=5.0242 forget=1.2479 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 70%|███████   | 14/20 [03:10<01:16, 12.74s/it] 75%|███████▌  | 15/20 [03:23<01:04, 12.84s/it] 80%|████████  | 16/20 [03:36<00:51, 12.82s/it] 85%|████████▌ | 17/20 [03:48<00:37, 12.67s/it] 90%|█████████ | 18/20 [04:01<00:25, 12.86s/it] 95%|█████████▌| 19/20 [04:16<00:13, 13.54s/it]100%|██████████| 20/20 [04:30<00:00, 13.58s/it]100%|██████████| 20/20 [04:30<00:00, 13.52s/it]
[loss] ep 13 it 230 total=7.5526 mle=1.3063 pcon=5.0235 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=7.5063 mle=1.2280 pcon=5.0229 forget=1.2554 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=7.6482 mle=1.4037 pcon=5.0224 forget=1.2222 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=7.5755 mle=1.3328 pcon=5.0218 forget=1.2209 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 14 it 40 total=7.5251 mle=1.2803 pcon=5.0211 forget=1.2238 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=7.5879 mle=1.3329 pcon=5.0207 forget=1.2343 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=7.5687 mle=1.3078 pcon=5.0202 forget=1.2406 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=7.5246 mle=1.2681 pcon=5.0198 forget=1.2367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=7.5867 mle=1.3326 pcon=5.0189 forget=1.2352 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=7.5323 mle=1.3003 pcon=5.0185 forget=1.2136 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=7.4920 mle=1.2523 pcon=5.0180 forget=1.2216 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 15 it 0 total=7.5726 mle=1.3432 pcon=5.0174 forget=1.2121 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=7.4848 mle=1.2515 pcon=5.0170 forget=1.2163 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=7.5993 mle=1.3759 pcon=5.0168 forget=1.2066 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=7.5399 mle=1.2964 pcon=5.0164 forget=1.2271 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=7.5135 mle=1.2623 pcon=5.0160 forget=1.2351 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=7.5357 mle=1.2917 pcon=5.0156 forget=1.2284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=7.6135 mle=1.4041 pcon=5.0151 forget=1.1943 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=7.6398 mle=1.4155 pcon=5.0147 forget=1.2095 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 16 it 10 total=7.4313 mle=1.1804 pcon=5.0143 forget=1.2366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=7.4750 mle=1.2543 pcon=5.0141 forget=1.2066 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=7.4492 mle=1.2118 pcon=5.0137 forget=1.2237 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=7.5249 mle=1.2696 pcon=5.0135 forget=1.2418 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=7.5302 mle=1.2682 pcon=5.0132 forget=1.2487 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=7.6760 mle=1.4308 pcon=5.0126 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=7.7039 mle=1.4749 pcon=5.0120 forget=1.2169 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=7.5643 mle=1.3264 pcon=5.0116 forget=1.2263 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 17 it 20 total=7.6255 mle=1.4126 pcon=5.0116 forget=1.2014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=7.6423 mle=1.3877 pcon=5.0116 forget=1.2431 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=7.5021 mle=1.2645 pcon=5.0110 forget=1.2266 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=7.5980 mle=1.3577 pcon=5.0106 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=7.5849 mle=1.3775 pcon=5.0102 forget=1.1972 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=7.6151 mle=1.3767 pcon=5.0100 forget=1.2283 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=7.4956 mle=1.2656 pcon=5.0097 forget=1.2203 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=7.5695 mle=1.3687 pcon=5.0092 forget=1.1916 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 18 it 30 total=7.4377 mle=1.2035 pcon=5.0090 forget=1.2252 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=7.6689 mle=1.4505 pcon=5.0088 forget=1.2096 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=7.6543 mle=1.4240 pcon=5.0085 forget=1.2218 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=7.5291 mle=1.3078 pcon=5.0084 forget=1.2130 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=7.6090 mle=1.3808 pcon=5.0080 forget=1.2203 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=7.6440 mle=1.4045 pcon=5.0076 forget=1.2319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=7.6139 mle=1.3741 pcon=5.0073 forget=1.2325 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=7.4941 mle=1.2734 pcon=5.0073 forget=1.2133 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 19 it 40 total=7.4680 mle=1.2370 pcon=5.0070 forget=1.2239 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=7.5715 mle=1.3466 pcon=5.0066 forget=1.2182 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=7.4807 mle=1.2573 pcon=5.0063 forget=1.2171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=7.5435 mle=1.3404 pcon=5.0060 forget=1.1971 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=7.5244 mle=1.2855 pcon=5.0056 forget=1.2333 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=7.5480 mle=1.3336 pcon=5.0059 forget=1.2085 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=7.5508 mle=1.3032 pcon=5.0061 forget=1.2415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:59,  2.17it/s]  3%|▎         | 11/391 [00:00<00:15, 25.00it/s]  5%|▌         | 21/391 [00:00<00:08, 42.89it/s]  8%|▊         | 31/391 [00:00<00:06, 56.84it/s] 10%|█         | 41/391 [00:00<00:05, 67.09it/s] 13%|█▎        | 51/391 [00:00<00:04, 75.02it/s] 16%|█▌        | 61/391 [00:01<00:04, 79.83it/s] 18%|█▊        | 70/391 [00:01<00:04, 79.31it/s] 20%|██        | 79/391 [00:01<00:04, 74.99it/s] 23%|██▎       | 89/391 [00:01<00:03, 80.33it/s] 25%|██▌       | 99/391 [00:01<00:03, 84.36it/s] 28%|██▊       | 108/391 [00:01<00:03, 85.68it/s] 30%|██▉       | 117/391 [00:01<00:03, 86.40it/s] 32%|███▏      | 127/391 [00:01<00:03, 87.85it/s] 35%|███▍      | 136/391 [00:01<00:02, 88.19it/s] 37%|███▋      | 146/391 [00:02<00:02, 90.73it/s] 40%|███▉      | 156/391 [00:02<00:02, 91.40it/s] 42%|████▏     | 166/391 [00:02<00:02, 92.70it/s] 45%|████▌     | 176/391 [00:02<00:02, 93.62it/s] 48%|████▊     | 186/391 [00:02<00:02, 92.98it/s] 50%|█████     | 196/391 [00:02<00:02, 93.44it/s] 53%|█████▎    | 206/391 [00:02<00:01, 94.29it/s] 55%|█████▌    | 216/391 [00:02<00:01, 94.62it/s] 58%|█████▊    | 226/391 [00:02<00:01, 94.45it/s] 60%|██████    | 236/391 [00:03<00:01, 94.76it/s] 63%|██████▎   | 246/391 [00:03<00:01, 94.73it/s] 65%|██████▌   | 256/391 [00:03<00:01, 76.12it/s] 68%|██████▊   | 265/391 [00:03<00:01, 75.13it/s] 70%|███████   | 275/391 [00:03<00:01, 80.33it/s] 73%|███████▎  | 284/391 [00:03<00:01, 79.16it/s] 75%|███████▌  | 294/391 [00:03<00:01, 83.23it/s] 78%|███████▊  | 304/391 [00:03<00:01, 86.12it/s] 80%|████████  | 314/391 [00:03<00:00, 88.60it/s] 83%|████████▎ | 324/391 [00:04<00:00, 90.29it/s] 85%|████████▌ | 334/391 [00:04<00:00, 86.54it/s] 88%|████████▊ | 344/391 [00:04<00:00, 89.04it/s] 91%|█████████ | 354/391 [00:04<00:00, 90.65it/s] 93%|█████████▎| 364/391 [00:04<00:00, 90.27it/s] 96%|█████████▌| 374/391 [00:04<00:00, 92.16it/s] 98%|█████████▊| 384/391 [00:04<00:00, 93.58it/s]100%|██████████| 391/391 [00:04<00:00, 80.77it/s]
50000 images processed, 4.923324346542358 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:32,  2.42it/s]  9%|▉         | 7/79 [00:00<00:04, 17.15it/s] 20%|██        | 16/79 [00:00<00:01, 36.48it/s] 32%|███▏      | 25/79 [00:00<00:01, 51.17it/s] 44%|████▍     | 35/79 [00:00<00:00, 63.15it/s] 56%|█████▌    | 44/79 [00:00<00:00, 70.57it/s] 67%|██████▋   | 53/79 [00:01<00:00, 75.82it/s] 80%|███████▉  | 63/79 [00:01<00:00, 81.32it/s] 92%|█████████▏| 73/79 [00:01<00:00, 85.08it/s]100%|██████████| 79/79 [00:02<00:00, 29.50it/s]
10000 images processed, 2.7389070987701416 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:05,  1.62it/s]  4%|▍         | 9/204 [00:00<00:12, 16.07it/s]  8%|▊         | 17/204 [00:00<00:06, 29.38it/s] 13%|█▎        | 26/204 [00:00<00:04, 43.05it/s] 17%|█▋        | 35/204 [00:01<00:03, 54.34it/s] 22%|██▏       | 45/204 [00:01<00:02, 64.95it/s] 27%|██▋       | 55/204 [00:01<00:02, 72.80it/s] 32%|███▏      | 65/204 [00:01<00:01, 78.62it/s] 37%|███▋      | 75/204 [00:01<00:01, 82.10it/s] 41%|████      | 84/204 [00:01<00:01, 83.91it/s] 46%|████▌     | 94/204 [00:01<00:01, 86.56it/s] 51%|█████     | 104/204 [00:01<00:01, 88.00it/s] 56%|█████▌    | 114/204 [00:01<00:01, 88.91it/s] 61%|██████    | 124/204 [00:01<00:00, 90.58it/s] 66%|██████▌   | 134/204 [00:02<00:00, 91.32it/s] 71%|███████   | 144/204 [00:02<00:00, 92.06it/s] 75%|███████▌  | 154/204 [00:02<00:00, 92.45it/s] 80%|████████  | 164/204 [00:02<00:00, 92.92it/s] 85%|████████▌ | 174/204 [00:02<00:00, 93.39it/s] 90%|█████████ | 184/204 [00:02<00:00, 94.09it/s] 95%|█████████▌| 194/204 [00:02<00:00, 94.77it/s]100%|██████████| 204/204 [00:02<00:00, 93.99it/s]100%|██████████| 204/204 [00:02<00:00, 71.61it/s]
26032 images processed, 2.889068841934204 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:49,  1.58it/s] 11%|█▏        | 9/79 [00:00<00:04, 14.19it/s] 23%|██▎       | 18/79 [00:00<00:02, 28.16it/s] 32%|███▏      | 25/79 [00:01<00:01, 34.01it/s] 43%|████▎     | 34/79 [00:01<00:01, 44.97it/s] 54%|█████▍    | 43/79 [00:01<00:00, 55.13it/s] 67%|██████▋   | 53/79 [00:01<00:00, 65.19it/s] 80%|███████▉  | 63/79 [00:01<00:00, 73.16it/s] 92%|█████████▏| 73/79 [00:01<00:00, 79.21it/s]100%|██████████| 79/79 [00:01<00:00, 48.05it/s]
10000 images processed, 1.6746437549591064 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.94it/s] 13%|█▎        | 10/79 [00:00<00:03, 20.85it/s] 24%|██▍       | 19/79 [00:00<00:01, 37.06it/s] 35%|███▌      | 28/79 [00:00<00:01, 49.38it/s] 47%|████▋     | 37/79 [00:00<00:00, 58.93it/s] 59%|█████▉    | 47/79 [00:01<00:00, 68.91it/s] 72%|███████▏  | 57/79 [00:01<00:00, 76.41it/s] 84%|████████▎ | 66/79 [00:01<00:00, 79.40it/s] 95%|█████████▍| 75/79 [00:01<00:00, 75.89it/s]100%|██████████| 79/79 [00:01<00:00, 55.62it/s]
10000 images processed, 1.4432542324066162 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:34,  2.01it/s] 11%|█▏        | 8/70 [00:00<00:03, 17.14it/s] 24%|██▍       | 17/70 [00:00<00:01, 34.25it/s] 36%|███▌      | 25/70 [00:00<00:00, 45.76it/s] 49%|████▊     | 34/70 [00:00<00:00, 56.82it/s] 63%|██████▎   | 44/70 [00:01<00:00, 67.27it/s] 77%|███████▋  | 54/70 [00:01<00:00, 75.41it/s] 91%|█████████▏| 64/70 [00:01<00:00, 81.24it/s]100%|██████████| 70/70 [00:01<00:00, 54.12it/s]
8925 images processed, 1.3278865814208984 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:42,  1.03it/s]  4%|▍         | 2/45 [00:01<00:20,  2.14it/s] 20%|██        | 9/45 [00:01<00:02, 12.25it/s] 31%|███       | 14/45 [00:01<00:01, 18.77it/s] 40%|████      | 18/45 [00:01<00:01, 19.25it/s] 49%|████▉     | 22/45 [00:01<00:01, 17.74it/s] 67%|██████▋   | 30/45 [00:01<00:00, 28.01it/s] 78%|███████▊  | 35/45 [00:02<00:00, 22.72it/s]100%|██████████| 45/45 [00:02<00:00, 34.52it/s]100%|██████████| 45/45 [00:02<00:00, 19.55it/s]
5640 images processed, 2.3271024227142334 seconds used

18.92630434036255
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.61  99.35  97.97
places365     68.57  81.09  78.71
LSUN          17.60  96.13  96.13
iSUN          72.03  82.04  84.45
dtd           38.67  91.31  94.27
AVG           39.90  89.98  90.31
Retain-Acc: 0.7478
Forget-as-OOD (retain known vs forget novel):
  FPR: 54.70 AUROC: 89.28 AUIN: 98.64
8.56633186340332
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_rf.png
[Run] lambda=0.1 lr=0.001 epochs=25
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=25, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.1, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/25 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  4%|▍         | 1/25 [00:18<07:19, 18.29s/it]  8%|▊         | 2/25 [00:32<06:06, 15.92s/it] 12%|█▏        | 3/25 [00:47<05:40, 15.49s/it] 16%|█▌        | 4/25 [01:02<05:19, 15.20s/it] 20%|██        | 5/25 [01:16<05:00, 15.00s/it] 24%|██▍       | 6/25 [01:31<04:41, 14.84s/it][loss] ep 0 it 0 total=7.8079 mle=1.2805 pcon=5.2951 forget=1.2323 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=7.8924 mle=1.3921 pcon=5.2887 forget=1.2116 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=8.0160 mle=1.5079 pcon=5.2827 forget=1.2253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=8.0597 mle=1.5644 pcon=5.2770 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=8.0047 mle=1.5332 pcon=5.2708 forget=1.2007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=7.8786 mle=1.3805 pcon=5.2650 forget=1.2331 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=7.7785 mle=1.2921 pcon=5.2596 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=7.9327 mle=1.4379 pcon=5.2543 forget=1.2405 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 1 it 10 total=7.7859 mle=1.3066 pcon=5.2487 forget=1.2305 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=7.7985 mle=1.3163 pcon=5.2433 forget=1.2389 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=7.7466 mle=1.2901 pcon=5.2382 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=7.7125 mle=1.2379 pcon=5.2331 forget=1.2415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=7.8321 mle=1.3726 pcon=5.2283 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=7.7792 mle=1.3469 pcon=5.2235 forget=1.2088 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=7.7558 mle=1.3081 pcon=5.2185 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=7.8229 mle=1.3767 pcon=5.2136 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 2 it 20 total=7.7061 mle=1.2744 pcon=5.2089 forget=1.2228 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=7.8501 mle=1.3989 pcon=5.2044 forget=1.2468 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=7.7728 mle=1.3588 pcon=5.1998 forget=1.2142 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=7.7725 mle=1.3581 pcon=5.1954 forget=1.2190 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=7.7517 mle=1.3301 pcon=5.1908 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=7.7868 mle=1.3658 pcon=5.1865 forget=1.2345 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=7.6576 mle=1.2606 pcon=5.1824 forget=1.2146 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=7.7850 mle=1.3836 pcon=5.1787 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 3 it 30 total=7.7685 mle=1.3640 pcon=5.1748 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=7.6956 mle=1.2956 pcon=5.1709 forget=1.2290 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=7.6785 mle=1.2905 pcon=5.1672 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=7.8326 mle=1.4600 pcon=5.1635 forget=1.2090 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=7.7463 mle=1.3524 pcon=5.1601 forget=1.2338 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=7.8178 mle=1.4266 pcon=5.1568 forget=1.2344 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=7.8238 mle=1.4482 pcon=5.1531 forget=1.2225 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=7.6897 mle=1.3142 pcon=5.1497 forget=1.2259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 4 it 40 total=7.7404 mle=1.3770 pcon=5.1465 forget=1.2170 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=7.7165 mle=1.3335 pcon=5.1434 forget=1.2396 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=7.9301 mle=1.5854 pcon=5.1406 forget=1.2041 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=7.7627 mle=1.3941 pcon=5.1374 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=7.7865 mle=1.4142 pcon=5.1344 forget=1.2379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=7.5440 mle=1.1863 pcon=5.1316 forget=1.2262 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=7.8148 mle=1.4612 pcon=5.1290 forget=1.2246 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 5 it 0 total=7.5816 mle=1.2049 pcon=5.1261 forget=1.2507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=7.7231 mle=1.3649 pcon=5.1234 forget=1.2348 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=7.7652 mle=1.4008 pcon=5.1206 forget=1.2438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=7.8336 mle=1.4808 pcon=5.1178 forget=1.2350 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=7.7882 mle=1.4524 pcon=5.1151 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=7.6865 mle=1.3485 pcon=5.1124 forget=1.2256 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=7.7566 mle=1.4295 pcon=5.1098 forget=1.2172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=7.7334 mle=1.4043 pcon=5.1071 forget=1.2220 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 6 it 10 total=7.6346 mle=1.2783 pcon=5.1047 forget=1.2515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=7.7002 mle=1.3788 pcon=5.1024 forget=1.2189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=7.6562 mle=1.3184 pcon=5.1002 forget=1.2376 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=7.6599 mle=1.3322 pcon=5.0979 forget=1.2298 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=7.6925 mle=1.3653 pcon=5.0954 forget=1.2318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=7.7162 mle=1.3836 pcon=5.0933 forget=1.2393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=7.6565 mle=1.3538 pcon=5.0913 forget=1.2114 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 28%|██▊       | 7/25 [01:46<04:28, 14.91s/it] 32%|███▏      | 8/25 [02:00<04:10, 14.71s/it] 36%|███▌      | 9/25 [02:14<03:52, 14.55s/it] 40%|████      | 10/25 [02:29<03:38, 14.59s/it] 44%|████▍     | 11/25 [02:43<03:22, 14.47s/it] 48%|████▊     | 12/25 [02:58<03:06, 14.37s/it] 52%|█████▏    | 13/25 [03:12<02:53, 14.49s/it][loss] ep 6 it 360 total=7.7497 mle=1.4566 pcon=5.0891 forget=1.2040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 7 it 20 total=7.5655 mle=1.2502 pcon=5.0870 forget=1.2283 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=7.5610 mle=1.2466 pcon=5.0847 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=7.6923 mle=1.3880 pcon=5.0827 forget=1.2216 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=7.7177 mle=1.4331 pcon=5.0808 forget=1.2038 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=7.6182 mle=1.3207 pcon=5.0791 forget=1.2184 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=7.7542 mle=1.4546 pcon=5.0775 forget=1.2221 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=7.6235 mle=1.3171 pcon=5.0753 forget=1.2311 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=7.6912 mle=1.3712 pcon=5.0733 forget=1.2467 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 8 it 30 total=7.7576 mle=1.4809 pcon=5.0716 forget=1.2051 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=7.5762 mle=1.2754 pcon=5.0699 forget=1.2309 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=7.6953 mle=1.3991 pcon=5.0682 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=7.6396 mle=1.3456 pcon=5.0665 forget=1.2275 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=7.6600 mle=1.3660 pcon=5.0648 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=7.5533 mle=1.2575 pcon=5.0632 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=7.8328 mle=1.5798 pcon=5.0614 forget=1.1915 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=7.6517 mle=1.3648 pcon=5.0600 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 9 it 40 total=7.6995 mle=1.4117 pcon=5.0585 forget=1.2293 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=7.7619 mle=1.4932 pcon=5.0570 forget=1.2117 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=7.6461 mle=1.3547 pcon=5.0559 forget=1.2356 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=7.5184 mle=1.2292 pcon=5.0546 forget=1.2346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=7.7623 mle=1.4987 pcon=5.0535 forget=1.2101 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=7.6412 mle=1.3595 pcon=5.0520 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=7.5895 mle=1.3079 pcon=5.0508 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 10 it 0 total=7.5543 mle=1.2682 pcon=5.0491 forget=1.2369 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=7.5333 mle=1.2585 pcon=5.0482 forget=1.2265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=7.5938 mle=1.3283 pcon=5.0469 forget=1.2187 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=7.6345 mle=1.3451 pcon=5.0457 forget=1.2438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=7.6850 mle=1.4101 pcon=5.0446 forget=1.2303 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=7.6989 mle=1.4289 pcon=5.0436 forget=1.2264 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=7.7518 mle=1.5148 pcon=5.0424 forget=1.1946 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=7.5336 mle=1.2627 pcon=5.0415 forget=1.2294 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 11 it 10 total=7.7023 mle=1.4432 pcon=5.0403 forget=1.2189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=7.6332 mle=1.3737 pcon=5.0394 forget=1.2200 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=7.6150 mle=1.3484 pcon=5.0385 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=7.5509 mle=1.2742 pcon=5.0374 forget=1.2393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=7.5316 mle=1.2648 pcon=5.0365 forget=1.2302 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=7.5274 mle=1.2695 pcon=5.0355 forget=1.2224 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=7.6543 mle=1.3938 pcon=5.0347 forget=1.2259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=7.6003 mle=1.3222 pcon=5.0340 forget=1.2441 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 12 it 20 total=7.5820 mle=1.3167 pcon=5.0327 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=7.6196 mle=1.3560 pcon=5.0317 forget=1.2319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=7.6047 mle=1.3656 pcon=5.0307 forget=1.2084 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=7.6128 mle=1.3435 pcon=5.0296 forget=1.2397 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=7.5000 mle=1.2268 pcon=5.0291 forget=1.2441 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=7.5470 mle=1.2980 pcon=5.0285 forget=1.2205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=7.6749 mle=1.4170 pcon=5.0279 forget=1.2300 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=7.6427 mle=1.3872 pcon=5.0272 forget=1.2283 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 13 it 30 total=7.5889 mle=1.3178 pcon=5.0268 forget=1.2444 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=7.7446 mle=1.5051 pcon=5.0261 forget=1.2135 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=7.6136 mle=1.3636 pcon=5.0253 forget=1.2248 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=7.6294 mle=1.3576 pcon=5.0242 forget=1.2477 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 56%|█████▌    | 14/25 [03:27<02:39, 14.47s/it] 60%|██████    | 15/25 [03:41<02:22, 14.27s/it] 64%|██████▍   | 16/25 [03:55<02:09, 14.44s/it] 68%|██████▊   | 17/25 [04:11<01:58, 14.84s/it] 72%|███████▏  | 18/25 [04:26<01:44, 14.88s/it] 76%|███████▌  | 19/25 [04:41<01:29, 14.87s/it] 80%|████████  | 20/25 [04:56<01:14, 14.85s/it][loss] ep 13 it 230 total=7.5516 mle=1.3055 pcon=5.0236 forget=1.2224 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=7.5054 mle=1.2272 pcon=5.0229 forget=1.2553 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=7.6462 mle=1.4017 pcon=5.0224 forget=1.2221 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=7.5719 mle=1.3294 pcon=5.0219 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 14 it 40 total=7.5237 mle=1.2790 pcon=5.0212 forget=1.2235 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=7.5846 mle=1.3299 pcon=5.0208 forget=1.2339 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=7.5663 mle=1.3057 pcon=5.0203 forget=1.2403 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=7.5236 mle=1.2673 pcon=5.0199 forget=1.2364 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=7.5827 mle=1.3285 pcon=5.0191 forget=1.2350 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=7.5284 mle=1.2967 pcon=5.0187 forget=1.2131 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=7.4883 mle=1.2488 pcon=5.0183 forget=1.2212 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 15 it 0 total=7.5689 mle=1.3395 pcon=5.0176 forget=1.2117 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=7.4789 mle=1.2462 pcon=5.0174 forget=1.2154 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=7.5936 mle=1.3707 pcon=5.0171 forget=1.2057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=7.5355 mle=1.2925 pcon=5.0168 forget=1.2262 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=7.5094 mle=1.2592 pcon=5.0165 forget=1.2338 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=7.5303 mle=1.2869 pcon=5.0161 forget=1.2274 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=7.6049 mle=1.3964 pcon=5.0156 forget=1.1929 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=7.6311 mle=1.4081 pcon=5.0153 forget=1.2077 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 16 it 10 total=7.4300 mle=1.1806 pcon=5.0149 forget=1.2345 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=7.4595 mle=1.2413 pcon=5.0148 forget=1.2034 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=7.4418 mle=1.2070 pcon=5.0144 forget=1.2204 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=7.5246 mle=1.2717 pcon=5.0143 forget=1.2387 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=7.5230 mle=1.2615 pcon=5.0141 forget=1.2475 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=7.6648 mle=1.4198 pcon=5.0135 forget=1.2315 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=7.6861 mle=1.4586 pcon=5.0130 forget=1.2145 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=7.5581 mle=1.3207 pcon=5.0127 forget=1.2248 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 17 it 20 total=7.6081 mle=1.3964 pcon=5.0127 forget=1.1990 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=7.6289 mle=1.3755 pcon=5.0128 forget=1.2406 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=7.4877 mle=1.2513 pcon=5.0123 forget=1.2240 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=7.5826 mle=1.3448 pcon=5.0120 forget=1.2258 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=7.5759 mle=1.3705 pcon=5.0117 forget=1.1937 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=7.5929 mle=1.3580 pcon=5.0116 forget=1.2234 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=7.4859 mle=1.2600 pcon=5.0113 forget=1.2145 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=7.5470 mle=1.3484 pcon=5.0110 forget=1.1876 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 18 it 30 total=7.4255 mle=1.1959 pcon=5.0108 forget=1.2187 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=7.6574 mle=1.4385 pcon=5.0107 forget=1.2082 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=7.6290 mle=1.3994 pcon=5.0106 forget=1.2190 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=7.5002 mle=1.2824 pcon=5.0106 forget=1.2073 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=7.5882 mle=1.3628 pcon=5.0103 forget=1.2151 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=7.6116 mle=1.3731 pcon=5.0100 forget=1.2285 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=7.5982 mle=1.3602 pcon=5.0098 forget=1.2282 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=7.4620 mle=1.2445 pcon=5.0099 forget=1.2077 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 19 it 40 total=7.4486 mle=1.2202 pcon=5.0097 forget=1.2187 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=7.5348 mle=1.3143 pcon=5.0094 forget=1.2111 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=7.4581 mle=1.2383 pcon=5.0093 forget=1.2105 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=7.5117 mle=1.3124 pcon=5.0090 forget=1.1902 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=7.4986 mle=1.2633 pcon=5.0088 forget=1.2265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=7.5335 mle=1.3236 pcon=5.0092 forget=1.2007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=7.5226 mle=1.2779 pcon=5.0096 forget=1.2352 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 20 it 0 total=7.5214 mle=1.3058 pcon=5.0096 forget=1.2060 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=7.5475 mle=1.2970 pcon=5.0096 forget=1.2408 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 84%|████████▍ | 21/25 [05:11<00:59, 14.93s/it] 88%|████████▊ | 22/25 [05:25<00:44, 14.78s/it] 92%|█████████▏| 23/25 [05:39<00:29, 14.59s/it] 96%|█████████▌| 24/25 [05:54<00:14, 14.48s/it]100%|██████████| 25/25 [06:08<00:00, 14.45s/it]100%|██████████| 25/25 [06:08<00:00, 14.74s/it]
[loss] ep 20 it 100 total=7.4566 mle=1.2373 pcon=5.0095 forget=1.2099 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=7.5075 mle=1.2939 pcon=5.0095 forget=1.2041 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=7.6141 mle=1.3894 pcon=5.0095 forget=1.2151 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=7.5819 mle=1.3467 pcon=5.0096 forget=1.2257 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=7.5919 mle=1.3720 pcon=5.0098 forget=1.2101 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=7.5839 mle=1.3549 pcon=5.0097 forget=1.2193 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 10 total=7.3548 mle=1.1422 pcon=5.0094 forget=1.2032 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=7.4910 mle=1.2577 pcon=5.0094 forget=1.2239 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=7.5417 mle=1.3234 pcon=5.0093 forget=1.2090 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=7.5481 mle=1.3340 pcon=5.0091 forget=1.2049 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=7.4446 mle=1.2203 pcon=5.0090 forget=1.2153 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=7.5981 mle=1.4052 pcon=5.0092 forget=1.1837 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=7.6114 mle=1.3825 pcon=5.0090 forget=1.2199 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=7.5524 mle=1.3210 pcon=5.0090 forget=1.2224 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 22 it 20 total=7.5347 mle=1.3142 pcon=5.0092 forget=1.2114 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=7.4838 mle=1.2664 pcon=5.0089 forget=1.2085 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=7.5859 mle=1.3647 pcon=5.0090 forget=1.2122 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=7.4903 mle=1.2698 pcon=5.0088 forget=1.2117 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=7.5783 mle=1.3539 pcon=5.0089 forget=1.2155 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=7.5306 mle=1.2962 pcon=5.0091 forget=1.2253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=7.4617 mle=1.2632 pcon=5.0093 forget=1.1892 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=7.6222 mle=1.4114 pcon=5.0091 forget=1.2017 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 23 it 30 total=7.5038 mle=1.2714 pcon=5.0092 forget=1.2232 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=7.5102 mle=1.2647 pcon=5.0092 forget=1.2363 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=7.6030 mle=1.3841 pcon=5.0091 forget=1.2097 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=7.5099 mle=1.2955 pcon=5.0090 forget=1.2054 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=7.4807 mle=1.2745 pcon=5.0090 forget=1.1972 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=7.5977 mle=1.3605 pcon=5.0092 forget=1.2281 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=7.5921 mle=1.3798 pcon=5.0089 forget=1.2035 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=7.4113 mle=1.1774 pcon=5.0089 forget=1.2250 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 24 it 40 total=7.5466 mle=1.3187 pcon=5.0091 forget=1.2188 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=7.5636 mle=1.3460 pcon=5.0090 forget=1.2086 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=7.4166 mle=1.1934 pcon=5.0089 forget=1.2144 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=7.4375 mle=1.2160 pcon=5.0091 forget=1.2124 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=7.5378 mle=1.3169 pcon=5.0092 forget=1.2117 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=7.4851 mle=1.2758 pcon=5.0092 forget=1.2002 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=7.4674 mle=1.2410 pcon=5.0089 forget=1.2174 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:18,  1.96it/s]  2%|▏         | 9/391 [00:00<00:20, 18.77it/s]  5%|▍         | 18/391 [00:00<00:10, 35.50it/s]  7%|▋         | 27/391 [00:00<00:07, 49.11it/s]  9%|▉         | 35/391 [00:00<00:06, 57.22it/s] 11%|█         | 43/391 [00:01<00:05, 62.66it/s] 14%|█▎        | 53/391 [00:01<00:04, 71.05it/s] 16%|█▌        | 63/391 [00:01<00:04, 77.91it/s] 18%|█▊        | 72/391 [00:01<00:04, 79.75it/s] 21%|██        | 81/391 [00:01<00:03, 80.92it/s] 23%|██▎       | 91/391 [00:01<00:03, 85.05it/s] 26%|██▌       | 101/391 [00:01<00:03, 87.73it/s] 28%|██▊       | 111/391 [00:01<00:03, 89.42it/s] 31%|███       | 121/391 [00:01<00:02, 90.98it/s] 34%|███▎      | 131/391 [00:01<00:02, 90.63it/s] 36%|███▌      | 141/391 [00:02<00:02, 91.99it/s] 39%|███▊      | 151/391 [00:02<00:02, 90.93it/s] 41%|████      | 161/391 [00:02<00:02, 91.46it/s] 44%|████▎     | 171/391 [00:02<00:02, 92.19it/s] 46%|████▋     | 181/391 [00:02<00:02, 93.00it/s] 49%|████▉     | 191/391 [00:02<00:02, 93.56it/s] 51%|█████▏    | 201/391 [00:02<00:02, 94.05it/s] 54%|█████▍    | 211/391 [00:02<00:01, 94.05it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.31it/s] 59%|█████▉    | 231/391 [00:03<00:01, 94.03it/s] 62%|██████▏   | 241/391 [00:03<00:01, 94.56it/s] 64%|██████▍   | 251/391 [00:03<00:01, 93.61it/s] 67%|██████▋   | 261/391 [00:03<00:01, 93.67it/s] 69%|██████▉   | 271/391 [00:03<00:01, 92.18it/s] 72%|███████▏  | 281/391 [00:03<00:01, 90.74it/s] 74%|███████▍  | 291/391 [00:03<00:01, 91.14it/s] 77%|███████▋  | 301/391 [00:03<00:00, 91.64it/s] 80%|███████▉  | 311/391 [00:03<00:00, 92.43it/s] 82%|████████▏ | 321/391 [00:04<00:00, 92.02it/s] 85%|████████▍ | 331/391 [00:04<00:00, 92.39it/s] 87%|████████▋ | 341/391 [00:04<00:00, 93.14it/s] 90%|████████▉ | 351/391 [00:04<00:00, 92.74it/s] 92%|█████████▏| 361/391 [00:04<00:00, 86.94it/s] 95%|█████████▍| 371/391 [00:04<00:00, 89.82it/s] 97%|█████████▋| 381/391 [00:04<00:00, 91.85it/s]100%|██████████| 391/391 [00:04<00:00, 75.41it/s]100%|██████████| 391/391 [00:04<00:00, 80.08it/s]
50000 images processed, 5.0907883644104 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:31,  2.45it/s]  9%|▉         | 7/79 [00:00<00:04, 17.08it/s] 20%|██        | 16/79 [00:00<00:01, 36.08it/s] 32%|███▏      | 25/79 [00:00<00:01, 50.53it/s] 43%|████▎     | 34/79 [00:00<00:00, 60.09it/s] 56%|█████▌    | 44/79 [00:00<00:00, 69.13it/s] 68%|██████▊   | 54/79 [00:01<00:00, 75.80it/s] 81%|████████  | 64/79 [00:01<00:00, 80.86it/s] 94%|█████████▎| 74/79 [00:01<00:00, 84.61it/s]100%|██████████| 79/79 [00:01<00:00, 51.76it/s]
10000 images processed, 1.5800960063934326 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:43,  1.95it/s]  5%|▍         | 10/204 [00:00<00:09, 20.94it/s]  9%|▉         | 19/204 [00:00<00:05, 36.96it/s] 14%|█▍        | 29/204 [00:00<00:03, 51.80it/s] 19%|█▉        | 39/204 [00:00<00:02, 63.09it/s] 24%|██▎       | 48/204 [00:01<00:02, 70.19it/s] 28%|██▊       | 57/204 [00:01<00:01, 74.98it/s] 33%|███▎      | 67/204 [00:01<00:01, 79.39it/s] 38%|███▊      | 77/204 [00:01<00:01, 83.27it/s] 42%|████▏     | 86/204 [00:01<00:01, 84.86it/s] 47%|████▋     | 96/204 [00:01<00:01, 86.56it/s] 52%|█████▏    | 106/204 [00:01<00:01, 88.41it/s] 57%|█████▋    | 116/204 [00:01<00:00, 88.68it/s] 62%|██████▏   | 126/204 [00:01<00:00, 89.53it/s] 67%|██████▋   | 136/204 [00:02<00:00, 91.22it/s] 72%|███████▏  | 146/204 [00:02<00:00, 90.31it/s] 76%|███████▋  | 156/204 [00:02<00:00, 90.81it/s] 81%|████████▏ | 166/204 [00:02<00:00, 90.89it/s] 86%|████████▋ | 176/204 [00:02<00:00, 91.97it/s] 91%|█████████ | 186/204 [00:02<00:00, 93.14it/s] 96%|█████████▌| 196/204 [00:02<00:00, 93.88it/s]100%|██████████| 204/204 [00:02<00:00, 74.42it/s]
26032 images processed, 2.779860734939575 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:02,  1.26it/s] 11%|█▏        | 9/79 [00:00<00:05, 13.22it/s] 23%|██▎       | 18/79 [00:01<00:02, 25.84it/s] 34%|███▍      | 27/79 [00:01<00:01, 38.31it/s] 44%|████▍     | 35/79 [00:01<00:01, 40.77it/s] 57%|█████▋    | 45/79 [00:01<00:00, 52.45it/s] 67%|██████▋   | 53/79 [00:01<00:00, 52.75it/s] 80%|███████▉  | 63/79 [00:01<00:00, 62.51it/s] 90%|████████▉ | 71/79 [00:01<00:00, 59.59it/s]100%|██████████| 79/79 [00:01<00:00, 41.75it/s]
10000 images processed, 1.9449758529663086 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:34,  2.29it/s] 10%|█         | 8/79 [00:00<00:03, 18.81it/s] 23%|██▎       | 18/79 [00:00<00:01, 38.91it/s] 33%|███▎      | 26/79 [00:00<00:01, 48.76it/s] 42%|████▏     | 33/79 [00:00<00:00, 51.11it/s] 52%|█████▏    | 41/79 [00:00<00:00, 56.94it/s] 61%|██████    | 48/79 [00:01<00:00, 57.89it/s] 71%|███████   | 56/79 [00:01<00:00, 61.84it/s] 84%|████████▎ | 66/79 [00:01<00:00, 71.16it/s] 96%|█████████▌| 76/79 [00:01<00:00, 78.11it/s]100%|██████████| 79/79 [00:01<00:00, 54.01it/s]
10000 images processed, 1.4924983978271484 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:31,  2.16it/s]  6%|▌         | 4/70 [00:00<00:07,  8.60it/s] 16%|█▌        | 11/70 [00:00<00:02, 23.29it/s] 27%|██▋       | 19/70 [00:00<00:01, 37.88it/s] 37%|███▋      | 26/70 [00:00<00:00, 45.84it/s] 51%|█████▏    | 36/70 [00:00<00:00, 59.30it/s] 66%|██████▌   | 46/70 [00:01<00:00, 69.43it/s] 80%|████████  | 56/70 [00:01<00:00, 77.02it/s] 94%|█████████▍| 66/70 [00:01<00:00, 82.39it/s]100%|██████████| 70/70 [00:01<00:00, 51.55it/s]
8925 images processed, 1.391545057296753 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:49,  1.13s/it]  4%|▍         | 2/45 [00:01<00:22,  1.87it/s] 16%|█▌        | 7/45 [00:01<00:04,  8.21it/s] 36%|███▌      | 16/45 [00:01<00:01, 21.14it/s] 47%|████▋     | 21/45 [00:01<00:01, 20.63it/s] 56%|█████▌    | 25/45 [00:01<00:00, 20.54it/s] 73%|███████▎  | 33/45 [00:02<00:00, 15.87it/s] 96%|█████████▌| 43/45 [00:02<00:00, 25.07it/s]100%|██████████| 45/45 [00:02<00:00, 16.66it/s]
5640 images processed, 2.722513198852539 seconds used

18.628592014312744
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.64  99.34  97.94
places365     69.15  80.92  78.66
LSUN          17.52  96.14  96.15
iSUN          72.03  82.19  84.65
dtd           38.87  91.28  94.25
AVG           40.04  89.98  90.33
Retain-Acc: 0.7482
Forget-as-OOD (retain known vs forget novel):
  FPR: 55.90 AUROC: 89.09 AUIN: 98.62
7.632638216018677
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_rf.png
[Run] lambda=0.1 lr=0.001 epochs=30
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=30, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.1, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/30 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  3%|▎         | 1/30 [00:19<09:25, 19.49s/it]  7%|▋         | 2/30 [00:33<07:30, 16.09s/it] 10%|█         | 3/30 [00:48<06:59, 15.55s/it] 13%|█▎        | 4/30 [01:02<06:34, 15.18s/it] 17%|█▋        | 5/30 [01:16<06:09, 14.78s/it] 20%|██        | 6/30 [01:31<05:52, 14.67s/it][loss] ep 0 it 0 total=7.8079 mle=1.2805 pcon=5.2951 forget=1.2323 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=7.8925 mle=1.3921 pcon=5.2887 forget=1.2116 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=8.0159 mle=1.5078 pcon=5.2827 forget=1.2253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=8.0597 mle=1.5644 pcon=5.2770 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=8.0047 mle=1.5331 pcon=5.2708 forget=1.2007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=7.8786 mle=1.3805 pcon=5.2650 forget=1.2331 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=7.7785 mle=1.2921 pcon=5.2596 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=7.9326 mle=1.4379 pcon=5.2543 forget=1.2405 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 1 it 10 total=7.7859 mle=1.3067 pcon=5.2487 forget=1.2305 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=7.7984 mle=1.3162 pcon=5.2433 forget=1.2389 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=7.7468 mle=1.2903 pcon=5.2382 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=7.7126 mle=1.2380 pcon=5.2331 forget=1.2415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=7.8319 mle=1.3723 pcon=5.2283 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=7.7791 mle=1.3468 pcon=5.2235 forget=1.2088 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=7.7558 mle=1.3081 pcon=5.2185 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=7.8228 mle=1.3766 pcon=5.2136 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 2 it 20 total=7.7062 mle=1.2745 pcon=5.2089 forget=1.2228 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=7.8500 mle=1.3988 pcon=5.2044 forget=1.2469 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=7.7728 mle=1.3588 pcon=5.1998 forget=1.2142 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=7.7725 mle=1.3581 pcon=5.1954 forget=1.2190 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=7.7516 mle=1.3300 pcon=5.1908 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=7.7868 mle=1.3657 pcon=5.1865 forget=1.2346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=7.6575 mle=1.2605 pcon=5.1824 forget=1.2147 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=7.7849 mle=1.3836 pcon=5.1787 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 3 it 30 total=7.7683 mle=1.3638 pcon=5.1748 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=7.6958 mle=1.2958 pcon=5.1709 forget=1.2290 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=7.6784 mle=1.2905 pcon=5.1672 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=7.8326 mle=1.4600 pcon=5.1635 forget=1.2090 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=7.7462 mle=1.3523 pcon=5.1601 forget=1.2338 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=7.8178 mle=1.4266 pcon=5.1568 forget=1.2344 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=7.8238 mle=1.4482 pcon=5.1531 forget=1.2225 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=7.6896 mle=1.3140 pcon=5.1497 forget=1.2259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 4 it 40 total=7.7404 mle=1.3769 pcon=5.1464 forget=1.2171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=7.7165 mle=1.3336 pcon=5.1434 forget=1.2396 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=7.9300 mle=1.5854 pcon=5.1406 forget=1.2041 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=7.7626 mle=1.3940 pcon=5.1374 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=7.7864 mle=1.4141 pcon=5.1344 forget=1.2379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=7.5440 mle=1.1863 pcon=5.1316 forget=1.2262 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=7.8148 mle=1.4612 pcon=5.1290 forget=1.2246 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 5 it 0 total=7.5815 mle=1.2047 pcon=5.1261 forget=1.2507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=7.7232 mle=1.3650 pcon=5.1234 forget=1.2348 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=7.7651 mle=1.4007 pcon=5.1206 forget=1.2438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=7.8336 mle=1.4808 pcon=5.1178 forget=1.2350 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=7.7881 mle=1.4523 pcon=5.1151 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=7.6863 mle=1.3483 pcon=5.1124 forget=1.2256 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=7.7566 mle=1.4295 pcon=5.1098 forget=1.2173 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=7.7334 mle=1.4043 pcon=5.1071 forget=1.2220 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 6 it 10 total=7.6346 mle=1.2783 pcon=5.1047 forget=1.2516 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=7.7001 mle=1.3787 pcon=5.1024 forget=1.2189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=7.6564 mle=1.3186 pcon=5.1002 forget=1.2376 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=7.6599 mle=1.3322 pcon=5.0979 forget=1.2298 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=7.6924 mle=1.3653 pcon=5.0954 forget=1.2317 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=7.7161 mle=1.3835 pcon=5.0933 forget=1.2393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=7.6566 mle=1.3538 pcon=5.0913 forget=1.2114 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 23%|██▎       | 7/30 [01:45<05:35, 14.58s/it] 27%|██▋       | 8/30 [02:00<05:19, 14.52s/it] 30%|███       | 9/30 [02:13<05:00, 14.30s/it] 33%|███▎      | 10/30 [02:27<04:44, 14.20s/it] 37%|███▋      | 11/30 [02:41<04:27, 14.08s/it] 40%|████      | 12/30 [02:54<04:09, 13.84s/it] 43%|████▎     | 13/30 [03:09<04:00, 14.13s/it][loss] ep 6 it 360 total=7.7496 mle=1.4565 pcon=5.0891 forget=1.2040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 7 it 20 total=7.5654 mle=1.2501 pcon=5.0870 forget=1.2283 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=7.5609 mle=1.2465 pcon=5.0847 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=7.6922 mle=1.3880 pcon=5.0827 forget=1.2216 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=7.7176 mle=1.4330 pcon=5.0808 forget=1.2038 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=7.6181 mle=1.3206 pcon=5.0791 forget=1.2184 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=7.7541 mle=1.4545 pcon=5.0775 forget=1.2221 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=7.6235 mle=1.3171 pcon=5.0753 forget=1.2311 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=7.6913 mle=1.3713 pcon=5.0733 forget=1.2467 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 8 it 30 total=7.7576 mle=1.4809 pcon=5.0716 forget=1.2052 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=7.5762 mle=1.2754 pcon=5.0699 forget=1.2310 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=7.6953 mle=1.3991 pcon=5.0682 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=7.6396 mle=1.3456 pcon=5.0665 forget=1.2276 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=7.6602 mle=1.3661 pcon=5.0648 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=7.5535 mle=1.2577 pcon=5.0632 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=7.8329 mle=1.5799 pcon=5.0614 forget=1.1915 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=7.6516 mle=1.3648 pcon=5.0600 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 9 it 40 total=7.6995 mle=1.4116 pcon=5.0585 forget=1.2293 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=7.7619 mle=1.4932 pcon=5.0570 forget=1.2117 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=7.6462 mle=1.3548 pcon=5.0559 forget=1.2355 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=7.5183 mle=1.2292 pcon=5.0546 forget=1.2346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=7.7623 mle=1.4987 pcon=5.0535 forget=1.2101 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=7.6412 mle=1.3594 pcon=5.0520 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=7.5895 mle=1.3079 pcon=5.0508 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 10 it 0 total=7.5543 mle=1.2682 pcon=5.0491 forget=1.2369 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=7.5332 mle=1.2585 pcon=5.0482 forget=1.2265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=7.5939 mle=1.3284 pcon=5.0468 forget=1.2187 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=7.6346 mle=1.3451 pcon=5.0457 forget=1.2438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=7.6850 mle=1.4101 pcon=5.0446 forget=1.2303 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=7.6988 mle=1.4288 pcon=5.0436 forget=1.2265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=7.7516 mle=1.5146 pcon=5.0424 forget=1.1946 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=7.5336 mle=1.2627 pcon=5.0415 forget=1.2294 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 11 it 10 total=7.7022 mle=1.4431 pcon=5.0403 forget=1.2188 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=7.6332 mle=1.3738 pcon=5.0394 forget=1.2201 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=7.6149 mle=1.3484 pcon=5.0385 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=7.5510 mle=1.2742 pcon=5.0374 forget=1.2393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=7.5317 mle=1.2650 pcon=5.0365 forget=1.2302 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=7.5273 mle=1.2694 pcon=5.0355 forget=1.2224 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=7.6542 mle=1.3936 pcon=5.0347 forget=1.2259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=7.6003 mle=1.3223 pcon=5.0340 forget=1.2441 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 12 it 20 total=7.5820 mle=1.3166 pcon=5.0328 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=7.6197 mle=1.3560 pcon=5.0318 forget=1.2319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=7.6045 mle=1.3654 pcon=5.0307 forget=1.2084 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=7.6128 mle=1.3434 pcon=5.0297 forget=1.2397 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=7.5000 mle=1.2268 pcon=5.0291 forget=1.2441 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=7.5469 mle=1.2978 pcon=5.0285 forget=1.2206 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=7.6749 mle=1.4170 pcon=5.0279 forget=1.2300 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=7.6430 mle=1.3875 pcon=5.0273 forget=1.2282 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 13 it 30 total=7.5888 mle=1.3176 pcon=5.0268 forget=1.2443 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=7.7440 mle=1.5044 pcon=5.0261 forget=1.2134 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=7.6135 mle=1.3634 pcon=5.0253 forget=1.2247 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=7.6293 mle=1.3574 pcon=5.0242 forget=1.2476 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 47%|████▋     | 14/30 [03:24<03:47, 14.21s/it] 50%|█████     | 15/30 [03:38<03:31, 14.11s/it] 53%|█████▎    | 16/30 [03:52<03:18, 14.14s/it] 57%|█████▋    | 17/30 [04:06<03:05, 14.25s/it] 60%|██████    | 18/30 [04:21<02:53, 14.45s/it] 63%|██████▎   | 19/30 [04:36<02:40, 14.55s/it] 67%|██████▋   | 20/30 [04:49<02:21, 14.16s/it][loss] ep 13 it 230 total=7.5511 mle=1.3051 pcon=5.0236 forget=1.2223 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=7.5050 mle=1.2269 pcon=5.0229 forget=1.2552 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=7.6455 mle=1.4010 pcon=5.0225 forget=1.2220 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=7.5712 mle=1.3287 pcon=5.0220 forget=1.2206 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 14 it 40 total=7.5232 mle=1.2785 pcon=5.0213 forget=1.2234 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=7.5835 mle=1.3288 pcon=5.0209 forget=1.2338 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=7.5652 mle=1.3047 pcon=5.0204 forget=1.2401 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=7.5232 mle=1.2670 pcon=5.0200 forget=1.2362 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=7.5811 mle=1.3269 pcon=5.0192 forget=1.2350 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=7.5266 mle=1.2950 pcon=5.0188 forget=1.2129 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=7.4869 mle=1.2474 pcon=5.0184 forget=1.2211 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 15 it 0 total=7.5674 mle=1.3381 pcon=5.0177 forget=1.2115 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=7.4766 mle=1.2442 pcon=5.0175 forget=1.2149 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=7.5910 mle=1.3684 pcon=5.0173 forget=1.2053 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=7.5335 mle=1.2907 pcon=5.0169 forget=1.2258 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=7.5078 mle=1.2581 pcon=5.0166 forget=1.2331 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=7.5279 mle=1.2849 pcon=5.0162 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=7.6011 mle=1.3932 pcon=5.0158 forget=1.1921 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=7.6272 mle=1.4049 pcon=5.0156 forget=1.2068 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 16 it 10 total=7.4298 mle=1.1811 pcon=5.0152 forget=1.2335 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=7.4524 mle=1.2356 pcon=5.0151 forget=1.2017 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=7.4388 mle=1.2053 pcon=5.0147 forget=1.2188 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=7.5245 mle=1.2728 pcon=5.0146 forget=1.2371 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=7.5189 mle=1.2577 pcon=5.0144 forget=1.2468 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=7.6601 mle=1.4154 pcon=5.0139 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=7.6786 mle=1.4521 pcon=5.0135 forget=1.2131 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=7.5551 mle=1.3182 pcon=5.0132 forget=1.2237 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 17 it 20 total=7.5998 mle=1.3892 pcon=5.0132 forget=1.1974 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=7.6232 mle=1.3709 pcon=5.0133 forget=1.2390 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=7.4813 mle=1.2459 pcon=5.0129 forget=1.2225 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=7.5741 mle=1.3381 pcon=5.0126 forget=1.2233 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=7.5732 mle=1.3695 pcon=5.0123 forget=1.1913 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=7.5834 mle=1.3509 pcon=5.0122 forget=1.2203 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=7.4819 mle=1.2588 pcon=5.0120 forget=1.2111 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=7.5361 mle=1.3396 pcon=5.0117 forget=1.1848 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 18 it 30 total=7.4214 mle=1.1954 pcon=5.0116 forget=1.2144 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=7.6520 mle=1.4336 pcon=5.0116 forget=1.2068 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=7.6189 mle=1.3904 pcon=5.0114 forget=1.2170 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=7.4882 mle=1.2729 pcon=5.0114 forget=1.2039 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=7.5805 mle=1.3582 pcon=5.0112 forget=1.2111 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=7.5926 mle=1.3561 pcon=5.0110 forget=1.2255 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=7.5875 mle=1.3517 pcon=5.0109 forget=1.2249 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=7.4483 mle=1.2342 pcon=5.0110 forget=1.2031 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 19 it 40 total=7.4385 mle=1.2132 pcon=5.0109 forget=1.2144 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=7.5177 mle=1.3014 pcon=5.0106 forget=1.2056 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=7.4465 mle=1.2306 pcon=5.0105 forget=1.2054 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=7.4918 mle=1.2964 pcon=5.0103 forget=1.1850 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=7.4819 mle=1.2514 pcon=5.0101 forget=1.2204 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=7.5184 mle=1.3148 pcon=5.0105 forget=1.1931 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=7.5087 mle=1.2684 pcon=5.0109 forget=1.2294 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 20 it 0 total=7.5113 mle=1.3005 pcon=5.0111 forget=1.1997 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=7.5400 mle=1.2954 pcon=5.0111 forget=1.2334 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 70%|███████   | 21/30 [05:04<02:08, 14.28s/it] 73%|███████▎  | 22/30 [05:18<01:54, 14.29s/it] 77%|███████▋  | 23/30 [05:32<01:39, 14.18s/it] 80%|████████  | 24/30 [05:47<01:27, 14.50s/it] 83%|████████▎ | 25/30 [06:01<01:11, 14.21s/it] 87%|████████▋ | 26/30 [06:14<00:55, 13.92s/it][loss] ep 20 it 100 total=7.4330 mle=1.2196 pcon=5.0110 forget=1.2024 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=7.4840 mle=1.2749 pcon=5.0111 forget=1.1980 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=7.5944 mle=1.3720 pcon=5.0111 forget=1.2112 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=7.5677 mle=1.3398 pcon=5.0112 forget=1.2167 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=7.5718 mle=1.3569 pcon=5.0114 forget=1.2035 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=7.5580 mle=1.3384 pcon=5.0114 forget=1.2082 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 21 it 10 total=7.3362 mle=1.1304 pcon=5.0112 forget=1.1947 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=7.4608 mle=1.2362 pcon=5.0112 forget=1.2133 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=7.5298 mle=1.3191 pcon=5.0112 forget=1.1996 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=7.5190 mle=1.3158 pcon=5.0110 forget=1.1922 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=7.4206 mle=1.2039 pcon=5.0109 forget=1.2057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=7.5771 mle=1.3929 pcon=5.0111 forget=1.1730 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=7.5877 mle=1.3647 pcon=5.0110 forget=1.2120 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=7.5326 mle=1.3092 pcon=5.0110 forget=1.2124 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 22 it 20 total=7.5169 mle=1.3056 pcon=5.0112 forget=1.2001 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=7.4670 mle=1.2537 pcon=5.0110 forget=1.2023 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=7.5520 mle=1.3402 pcon=5.0110 forget=1.2007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=7.4503 mle=1.2433 pcon=5.0108 forget=1.1961 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=7.5535 mle=1.3401 pcon=5.0110 forget=1.2023 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=7.4992 mle=1.2766 pcon=5.0112 forget=1.2114 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=7.4288 mle=1.2416 pcon=5.0114 forget=1.1758 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=7.5719 mle=1.3705 pcon=5.0112 forget=1.1902 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 23 it 30 total=7.4718 mle=1.2513 pcon=5.0113 forget=1.2092 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=7.4797 mle=1.2434 pcon=5.0113 forget=1.2250 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=7.5651 mle=1.3551 pcon=5.0112 forget=1.1988 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=7.4760 mle=1.2745 pcon=5.0110 forget=1.1905 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=7.4562 mle=1.2635 pcon=5.0112 forget=1.1815 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=7.5563 mle=1.3299 pcon=5.0113 forget=1.2152 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=7.5546 mle=1.3516 pcon=5.0110 forget=1.1920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=7.3797 mle=1.1613 pcon=5.0110 forget=1.2074 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 24 it 40 total=7.5022 mle=1.2866 pcon=5.0112 forget=1.2044 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=7.5189 mle=1.3100 pcon=5.0110 forget=1.1978 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=7.3883 mle=1.1756 pcon=5.0109 forget=1.2018 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=7.4059 mle=1.1976 pcon=5.0111 forget=1.1973 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=7.4910 mle=1.2817 pcon=5.0112 forget=1.1981 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=7.4506 mle=1.2537 pcon=5.0111 forget=1.1859 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=7.4380 mle=1.2217 pcon=5.0108 forget=1.2056 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 25 it 0 total=7.4569 mle=1.2599 pcon=5.0107 forget=1.1862 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 50 total=7.5468 mle=1.3693 pcon=5.0105 forget=1.1671 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 100 total=7.5283 mle=1.3285 pcon=5.0102 forget=1.1895 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 150 total=7.4949 mle=1.3031 pcon=5.0102 forget=1.1816 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 200 total=7.4896 mle=1.2821 pcon=5.0101 forget=1.1974 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 250 total=7.4718 mle=1.2664 pcon=5.0102 forget=1.1951 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 300 total=7.4553 mle=1.2680 pcon=5.0102 forget=1.1771 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 350 total=7.5340 mle=1.3286 pcon=5.0100 forget=1.1955 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 26 it 10 total=7.4342 mle=1.2239 pcon=5.0099 forget=1.2003 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 60 total=7.4374 mle=1.2429 pcon=5.0097 forget=1.1848 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 110 total=7.4806 mle=1.2806 pcon=5.0096 forget=1.1903 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 160 total=7.3896 mle=1.1894 pcon=5.0093 forget=1.1908 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 210 total=7.4948 mle=1.3072 pcon=5.0091 forget=1.1785 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 260 total=7.4106 mle=1.2059 pcon=5.0094 forget=1.1952 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 310 total=7.6140 mle=1.4315 pcon=5.0092 forget=1.1733 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 360 total=7.5716 mle=1.3693 pcon=5.0091 forget=1.1932 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 90%|█████████ | 27/30 [06:28<00:41, 13.83s/it] 93%|█████████▎| 28/30 [06:41<00:27, 13.84s/it] 97%|█████████▋| 29/30 [06:56<00:13, 13.92s/it]100%|██████████| 30/30 [07:09<00:00, 13.87s/it]100%|██████████| 30/30 [07:09<00:00, 14.33s/it]
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 27 it 20 total=7.4402 mle=1.2229 pcon=5.0091 forget=1.2082 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 70 total=7.4580 mle=1.2703 pcon=5.0089 forget=1.1789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 120 total=7.4985 mle=1.2800 pcon=5.0089 forget=1.2095 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 170 total=7.4587 mle=1.2476 pcon=5.0088 forget=1.2023 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 220 total=7.4523 mle=1.2551 pcon=5.0090 forget=1.1882 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 270 total=7.4663 mle=1.2604 pcon=5.0089 forget=1.1970 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 320 total=7.4647 mle=1.2451 pcon=5.0088 forget=1.2107 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 370 total=7.3782 mle=1.1685 pcon=5.0090 forget=1.2007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 30 total=7.4528 mle=1.2208 pcon=5.0090 forget=1.2230 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 80 total=7.5237 mle=1.3218 pcon=5.0089 forget=1.1930 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 130 total=7.3557 mle=1.1638 pcon=5.0091 forget=1.1829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 180 total=7.4060 mle=1.1894 pcon=5.0086 forget=1.2081 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 230 total=7.4178 mle=1.2291 pcon=5.0086 forget=1.1801 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 280 total=7.4800 mle=1.2762 pcon=5.0086 forget=1.1953 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 330 total=7.3908 mle=1.1857 pcon=5.0085 forget=1.1966 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 380 total=7.4390 mle=1.2361 pcon=5.0081 forget=1.1947 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 40 total=7.4806 mle=1.2986 pcon=5.0080 forget=1.1740 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 90 total=7.4895 mle=1.2712 pcon=5.0078 forget=1.2105 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 140 total=7.4121 mle=1.2085 pcon=5.0078 forget=1.1958 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 190 total=7.5875 mle=1.3778 pcon=5.0075 forget=1.2022 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 240 total=7.5491 mle=1.3545 pcon=5.0075 forget=1.1871 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 290 total=7.4092 mle=1.2079 pcon=5.0070 forget=1.1943 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 340 total=7.4345 mle=1.2217 pcon=5.0071 forget=1.2058 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:01<11:16,  1.73s/it]  3%|▎         | 10/391 [00:01<00:51,  7.34it/s]  5%|▍         | 19/391 [00:01<00:24, 15.27it/s]  7%|▋         | 28/391 [00:02<00:14, 24.22it/s] 10%|▉         | 38/391 [00:02<00:10, 34.85it/s] 12%|█▏        | 48/391 [00:02<00:07, 45.28it/s] 15%|█▍        | 58/391 [00:02<00:06, 54.82it/s] 17%|█▋        | 67/391 [00:02<00:05, 62.28it/s] 19%|█▉        | 76/391 [00:02<00:04, 68.54it/s] 22%|██▏       | 85/391 [00:02<00:04, 73.38it/s] 24%|██▍       | 94/391 [00:02<00:03, 76.77it/s] 26%|██▋       | 103/391 [00:02<00:03, 79.56it/s] 29%|██▉       | 113/391 [00:03<00:03, 83.15it/s] 31%|███▏      | 123/391 [00:03<00:03, 86.10it/s] 34%|███▍      | 133/391 [00:03<00:02, 88.62it/s] 37%|███▋      | 143/391 [00:03<00:02, 90.60it/s] 39%|███▉      | 153/391 [00:03<00:02, 91.49it/s] 42%|████▏     | 163/391 [00:03<00:02, 90.23it/s] 44%|████▍     | 173/391 [00:03<00:02, 91.27it/s] 47%|████▋     | 183/391 [00:03<00:02, 92.83it/s] 49%|████▉     | 193/391 [00:03<00:02, 93.67it/s] 52%|█████▏    | 203/391 [00:03<00:02, 93.43it/s] 54%|█████▍    | 213/391 [00:04<00:01, 93.97it/s] 57%|█████▋    | 223/391 [00:04<00:01, 94.40it/s] 60%|█████▉    | 233/391 [00:04<00:01, 94.37it/s] 62%|██████▏   | 243/391 [00:04<00:01, 94.27it/s] 65%|██████▍   | 253/391 [00:04<00:01, 94.55it/s] 67%|██████▋   | 263/391 [00:04<00:01, 94.34it/s] 70%|██████▉   | 273/391 [00:04<00:01, 94.10it/s] 72%|███████▏  | 283/391 [00:04<00:01, 93.12it/s] 75%|███████▍  | 293/391 [00:04<00:01, 93.50it/s] 77%|███████▋  | 303/391 [00:05<00:00, 93.94it/s] 80%|████████  | 313/391 [00:05<00:00, 93.87it/s] 83%|████████▎ | 323/391 [00:05<00:00, 92.61it/s] 85%|████████▌ | 333/391 [00:05<00:00, 93.36it/s] 88%|████████▊ | 343/391 [00:05<00:00, 94.00it/s] 90%|█████████ | 353/391 [00:05<00:00, 92.21it/s] 93%|█████████▎| 363/391 [00:05<00:00, 92.29it/s] 95%|█████████▌| 373/391 [00:05<00:00, 93.81it/s] 98%|█████████▊| 383/391 [00:05<00:00, 94.61it/s]100%|██████████| 391/391 [00:05<00:00, 65.40it/s]
50000 images processed, 6.078869342803955 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:43,  1.80it/s] 13%|█▎        | 10/79 [00:00<00:03, 19.46it/s] 24%|██▍       | 19/79 [00:00<00:01, 34.87it/s] 35%|███▌      | 28/79 [00:00<00:01, 47.82it/s] 48%|████▊     | 38/79 [00:00<00:00, 59.48it/s] 59%|█████▉    | 47/79 [00:01<00:00, 66.27it/s] 72%|███████▏  | 57/79 [00:01<00:00, 73.64it/s] 85%|████████▍ | 67/79 [00:01<00:00, 78.88it/s] 97%|█████████▋| 77/79 [00:01<00:00, 82.88it/s]100%|██████████| 79/79 [00:01<00:00, 54.62it/s]
10000 images processed, 1.4813141822814941 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:56,  1.74it/s]  3%|▎         | 6/204 [00:00<00:17, 11.32it/s]  8%|▊         | 16/204 [00:00<00:06, 30.26it/s] 13%|█▎        | 26/204 [00:00<00:03, 45.77it/s] 18%|█▊        | 36/204 [00:01<00:02, 58.21it/s] 23%|██▎       | 46/204 [00:01<00:02, 67.26it/s] 27%|██▋       | 56/204 [00:01<00:02, 73.84it/s] 32%|███▏      | 66/204 [00:01<00:01, 78.88it/s] 37%|███▋      | 76/204 [00:01<00:01, 82.68it/s] 42%|████▏     | 85/204 [00:01<00:01, 80.84it/s] 47%|████▋     | 95/204 [00:01<00:01, 84.70it/s] 51%|█████▏    | 105/204 [00:01<00:01, 87.00it/s] 56%|█████▌    | 114/204 [00:01<00:01, 84.05it/s] 60%|██████    | 123/204 [00:01<00:00, 83.05it/s] 65%|██████▌   | 133/204 [00:02<00:00, 84.41it/s] 70%|███████   | 143/204 [00:02<00:00, 86.95it/s] 75%|███████▌  | 153/204 [00:02<00:00, 88.08it/s] 80%|███████▉  | 163/204 [00:02<00:00, 89.63it/s] 85%|████████▍ | 173/204 [00:02<00:00, 88.44it/s] 90%|████████▉ | 183/204 [00:02<00:00, 90.57it/s] 95%|█████████▍| 193/204 [00:02<00:00, 91.78it/s]100%|█████████▉| 203/204 [00:02<00:00, 92.95it/s]100%|██████████| 204/204 [00:02<00:00, 70.74it/s]
26032 images processed, 2.9221980571746826 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:06,  1.18it/s] 10%|█         | 8/79 [00:00<00:06, 10.67it/s] 22%|██▏       | 17/79 [00:01<00:02, 23.67it/s] 33%|███▎      | 26/79 [00:01<00:01, 35.97it/s] 44%|████▍     | 35/79 [00:01<00:00, 46.60it/s] 57%|█████▋    | 45/79 [00:01<00:00, 57.56it/s] 70%|██████▉   | 55/79 [00:01<00:00, 66.30it/s] 82%|████████▏ | 65/79 [00:01<00:00, 73.10it/s] 95%|█████████▍| 75/79 [00:01<00:00, 79.03it/s]100%|██████████| 79/79 [00:01<00:00, 44.68it/s]
10000 images processed, 1.80133056640625 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:48,  1.59it/s] 11%|█▏        | 9/79 [00:00<00:04, 16.13it/s] 23%|██▎       | 18/79 [00:00<00:01, 31.01it/s] 34%|███▍      | 27/79 [00:00<00:01, 43.97it/s] 47%|████▋     | 37/79 [00:01<00:00, 56.11it/s] 58%|█████▊    | 46/79 [00:01<00:00, 63.91it/s] 71%|███████   | 56/79 [00:01<00:00, 71.61it/s] 84%|████████▎ | 66/79 [00:01<00:00, 78.11it/s] 96%|█████████▌| 76/79 [00:01<00:00, 83.07it/s]100%|██████████| 79/79 [00:01<00:00, 52.52it/s]
10000 images processed, 1.5238521099090576 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:38,  1.81it/s] 11%|█▏        | 8/70 [00:00<00:03, 15.56it/s] 24%|██▍       | 17/70 [00:00<00:01, 32.22it/s] 37%|███▋      | 26/70 [00:00<00:00, 46.10it/s] 51%|█████▏    | 36/70 [00:00<00:00, 58.63it/s] 64%|██████▍   | 45/70 [00:01<00:00, 66.56it/s] 79%|███████▊  | 55/70 [00:01<00:00, 74.65it/s] 93%|█████████▎| 65/70 [00:01<00:00, 80.64it/s]100%|██████████| 70/70 [00:01<00:00, 52.18it/s]
8925 images processed, 1.3744137287139893 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:44,  1.02s/it]  4%|▍         | 2/45 [00:01<00:21,  2.04it/s] 27%|██▋       | 12/45 [00:01<00:02, 16.21it/s] 38%|███▊      | 17/45 [00:01<00:01, 17.14it/s] 47%|████▋     | 21/45 [00:01<00:01, 20.27it/s] 69%|██████▉   | 31/45 [00:01<00:00, 34.20it/s] 82%|████████▏ | 37/45 [00:02<00:00, 21.61it/s]100%|██████████| 45/45 [00:02<00:00, 19.27it/s]
5640 images processed, 2.360586166381836 seconds used

19.19868564605713
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.75  99.33  97.90
places365     70.00  80.71  78.59
LSUN          18.07  96.11  96.13
iSUN          73.05  81.98  84.54
dtd           39.08  91.19  94.19
AVG           40.59  89.86  90.27
Retain-Acc: 0.7471
Forget-as-OOD (retain known vs forget novel):
  FPR: 60.20 AUROC: 88.60 AUIN: 98.56
7.215521812438965
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_rf.png
[Run] lambda=0.1 lr=0.001 epochs=35
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=35, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.1, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/35 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  3%|▎         | 1/35 [00:19<11:16, 19.89s/it]  6%|▌         | 2/35 [00:33<08:56, 16.25s/it]  9%|▊         | 3/35 [00:47<08:02, 15.07s/it] 11%|█▏        | 4/35 [01:01<07:34, 14.66s/it] 14%|█▍        | 5/35 [01:15<07:19, 14.66s/it] 17%|█▋        | 6/35 [01:28<06:48, 14.07s/it][loss] ep 0 it 0 total=7.8079 mle=1.2805 pcon=5.2951 forget=1.2323 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=7.8927 mle=1.3924 pcon=5.2887 forget=1.2116 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=8.0161 mle=1.5080 pcon=5.2827 forget=1.2253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=8.0597 mle=1.5644 pcon=5.2770 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=8.0044 mle=1.5329 pcon=5.2708 forget=1.2007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=7.8787 mle=1.3807 pcon=5.2650 forget=1.2330 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=7.7782 mle=1.2919 pcon=5.2596 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=7.9325 mle=1.4378 pcon=5.2543 forget=1.2405 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 1 it 10 total=7.7857 mle=1.3065 pcon=5.2487 forget=1.2305 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=7.7981 mle=1.3159 pcon=5.2433 forget=1.2389 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=7.7468 mle=1.2902 pcon=5.2382 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=7.7124 mle=1.2378 pcon=5.2331 forget=1.2415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=7.8319 mle=1.3724 pcon=5.2283 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=7.7792 mle=1.3469 pcon=5.2235 forget=1.2088 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=7.7559 mle=1.3082 pcon=5.2185 forget=1.2291 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=7.8230 mle=1.3768 pcon=5.2136 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 2 it 20 total=7.7064 mle=1.2747 pcon=5.2089 forget=1.2228 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=7.8501 mle=1.3989 pcon=5.2044 forget=1.2468 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=7.7730 mle=1.3590 pcon=5.1998 forget=1.2142 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=7.7722 mle=1.3578 pcon=5.1954 forget=1.2189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=7.7516 mle=1.3299 pcon=5.1909 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=7.7869 mle=1.3658 pcon=5.1866 forget=1.2345 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=7.6578 mle=1.2608 pcon=5.1824 forget=1.2146 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=7.7848 mle=1.3834 pcon=5.1787 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 3 it 30 total=7.7685 mle=1.3639 pcon=5.1748 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=7.6955 mle=1.2955 pcon=5.1709 forget=1.2290 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=7.6784 mle=1.2905 pcon=5.1672 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=7.8326 mle=1.4601 pcon=5.1635 forget=1.2090 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=7.7462 mle=1.3523 pcon=5.1601 forget=1.2338 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=7.8179 mle=1.4267 pcon=5.1568 forget=1.2344 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=7.8238 mle=1.4481 pcon=5.1531 forget=1.2225 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=7.6897 mle=1.3141 pcon=5.1497 forget=1.2259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 4 it 40 total=7.7402 mle=1.3768 pcon=5.1464 forget=1.2170 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=7.7167 mle=1.3337 pcon=5.1434 forget=1.2396 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=7.9298 mle=1.5851 pcon=5.1406 forget=1.2041 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=7.7627 mle=1.3941 pcon=5.1374 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=7.7863 mle=1.4141 pcon=5.1344 forget=1.2379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=7.5439 mle=1.1862 pcon=5.1316 forget=1.2262 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=7.8148 mle=1.4612 pcon=5.1290 forget=1.2246 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 5 it 0 total=7.5815 mle=1.2047 pcon=5.1261 forget=1.2507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=7.7230 mle=1.3648 pcon=5.1234 forget=1.2348 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=7.7651 mle=1.4007 pcon=5.1206 forget=1.2438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=7.8337 mle=1.4810 pcon=5.1178 forget=1.2350 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=7.7881 mle=1.4523 pcon=5.1151 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=7.6861 mle=1.3481 pcon=5.1124 forget=1.2256 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=7.7565 mle=1.4295 pcon=5.1098 forget=1.2172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=7.7337 mle=1.4046 pcon=5.1071 forget=1.2220 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 6 it 10 total=7.6345 mle=1.2782 pcon=5.1047 forget=1.2516 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=7.6999 mle=1.3786 pcon=5.1024 forget=1.2189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=7.6563 mle=1.3185 pcon=5.1002 forget=1.2376 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=7.6600 mle=1.3323 pcon=5.0979 forget=1.2298 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=7.6924 mle=1.3652 pcon=5.0954 forget=1.2318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=7.7159 mle=1.3833 pcon=5.0933 forget=1.2393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=7.6566 mle=1.3538 pcon=5.0913 forget=1.2115 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 20%|██        | 7/35 [01:42<06:27, 13.86s/it] 23%|██▎       | 8/35 [01:55<06:12, 13.78s/it] 26%|██▌       | 9/35 [02:09<05:56, 13.72s/it] 29%|██▊       | 10/35 [02:24<05:49, 13.98s/it] 31%|███▏      | 11/35 [02:38<05:35, 13.99s/it] 34%|███▍      | 12/35 [02:52<05:21, 14.00s/it] 37%|███▋      | 13/35 [03:05<05:01, 13.73s/it][loss] ep 6 it 360 total=7.7498 mle=1.4567 pcon=5.0891 forget=1.2040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 7 it 20 total=7.5654 mle=1.2501 pcon=5.0870 forget=1.2283 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=7.5610 mle=1.2466 pcon=5.0847 forget=1.2296 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=7.6921 mle=1.3878 pcon=5.0827 forget=1.2216 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=7.7177 mle=1.4332 pcon=5.0808 forget=1.2038 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=7.6182 mle=1.3207 pcon=5.0791 forget=1.2184 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=7.7540 mle=1.4545 pcon=5.0775 forget=1.2221 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=7.6235 mle=1.3171 pcon=5.0753 forget=1.2311 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=7.6910 mle=1.3710 pcon=5.0733 forget=1.2467 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 8 it 30 total=7.7576 mle=1.4808 pcon=5.0716 forget=1.2052 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=7.5761 mle=1.2753 pcon=5.0699 forget=1.2309 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=7.6952 mle=1.3989 pcon=5.0682 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=7.6396 mle=1.3456 pcon=5.0665 forget=1.2276 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=7.6602 mle=1.3662 pcon=5.0648 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=7.5536 mle=1.2578 pcon=5.0632 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=7.8330 mle=1.5800 pcon=5.0614 forget=1.1916 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=7.6517 mle=1.3649 pcon=5.0600 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 9 it 40 total=7.6993 mle=1.4115 pcon=5.0585 forget=1.2293 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=7.7620 mle=1.4932 pcon=5.0570 forget=1.2117 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=7.6463 mle=1.3548 pcon=5.0559 forget=1.2356 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=7.5181 mle=1.2290 pcon=5.0546 forget=1.2346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=7.7624 mle=1.4989 pcon=5.0535 forget=1.2100 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=7.6414 mle=1.3596 pcon=5.0520 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=7.5894 mle=1.3078 pcon=5.0508 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 10 it 0 total=7.5542 mle=1.2682 pcon=5.0491 forget=1.2369 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=7.5332 mle=1.2585 pcon=5.0482 forget=1.2265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=7.5938 mle=1.3283 pcon=5.0468 forget=1.2187 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=7.6343 mle=1.3448 pcon=5.0457 forget=1.2438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=7.6850 mle=1.4100 pcon=5.0446 forget=1.2303 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=7.6989 mle=1.4289 pcon=5.0436 forget=1.2264 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=7.7519 mle=1.5150 pcon=5.0424 forget=1.1946 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=7.5334 mle=1.2625 pcon=5.0415 forget=1.2294 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 11 it 10 total=7.7022 mle=1.4430 pcon=5.0403 forget=1.2189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=7.6333 mle=1.3738 pcon=5.0394 forget=1.2200 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=7.6151 mle=1.3486 pcon=5.0385 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=7.5511 mle=1.2743 pcon=5.0374 forget=1.2394 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=7.5319 mle=1.2652 pcon=5.0365 forget=1.2302 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=7.5273 mle=1.2694 pcon=5.0355 forget=1.2224 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=7.6540 mle=1.3934 pcon=5.0347 forget=1.2259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=7.6003 mle=1.3222 pcon=5.0340 forget=1.2441 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 12 it 20 total=7.5820 mle=1.3166 pcon=5.0328 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=7.6191 mle=1.3555 pcon=5.0318 forget=1.2319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=7.6045 mle=1.3654 pcon=5.0307 forget=1.2084 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=7.6127 mle=1.3434 pcon=5.0297 forget=1.2397 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=7.4999 mle=1.2268 pcon=5.0291 forget=1.2441 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=7.5468 mle=1.2977 pcon=5.0285 forget=1.2205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=7.6748 mle=1.4168 pcon=5.0279 forget=1.2300 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=7.6430 mle=1.3876 pcon=5.0273 forget=1.2282 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 13 it 30 total=7.5887 mle=1.3176 pcon=5.0268 forget=1.2443 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=7.7439 mle=1.5044 pcon=5.0261 forget=1.2134 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=7.6133 mle=1.3633 pcon=5.0253 forget=1.2247 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=7.6294 mle=1.3576 pcon=5.0242 forget=1.2476 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 40%|████      | 14/35 [03:19<04:50, 13.82s/it] 43%|████▎     | 15/35 [03:33<04:38, 13.91s/it] 46%|████▌     | 16/35 [03:48<04:28, 14.14s/it] 49%|████▊     | 17/35 [04:02<04:16, 14.23s/it] 51%|█████▏    | 18/35 [04:16<03:59, 14.11s/it] 54%|█████▍    | 19/35 [04:30<03:44, 14.05s/it] 57%|█████▋    | 20/35 [04:44<03:30, 14.04s/it][loss] ep 13 it 230 total=7.5508 mle=1.3049 pcon=5.0236 forget=1.2223 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=7.5049 mle=1.2268 pcon=5.0230 forget=1.2551 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=7.6449 mle=1.4003 pcon=5.0225 forget=1.2220 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=7.5723 mle=1.3298 pcon=5.0220 forget=1.2205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 14 it 40 total=7.5230 mle=1.2783 pcon=5.0213 forget=1.2234 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=7.5829 mle=1.3284 pcon=5.0209 forget=1.2337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=7.5646 mle=1.3040 pcon=5.0204 forget=1.2401 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=7.5232 mle=1.2670 pcon=5.0200 forget=1.2362 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=7.5802 mle=1.3261 pcon=5.0192 forget=1.2349 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=7.5257 mle=1.2942 pcon=5.0188 forget=1.2127 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=7.4862 mle=1.2468 pcon=5.0184 forget=1.2210 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 15 it 0 total=7.5664 mle=1.3372 pcon=5.0178 forget=1.2114 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=7.4754 mle=1.2432 pcon=5.0175 forget=1.2147 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=7.5896 mle=1.3671 pcon=5.0173 forget=1.2051 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=7.5325 mle=1.2899 pcon=5.0170 forget=1.2256 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=7.5071 mle=1.2576 pcon=5.0167 forget=1.2327 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=7.5266 mle=1.2838 pcon=5.0163 forget=1.2265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=7.5991 mle=1.3915 pcon=5.0159 forget=1.1917 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=7.6250 mle=1.4030 pcon=5.0157 forget=1.2063 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 16 it 10 total=7.4295 mle=1.1813 pcon=5.0153 forget=1.2329 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=7.4490 mle=1.2330 pcon=5.0152 forget=1.2008 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=7.4372 mle=1.2045 pcon=5.0149 forget=1.2178 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=7.5243 mle=1.2732 pcon=5.0148 forget=1.2363 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=7.5168 mle=1.2558 pcon=5.0146 forget=1.2464 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=7.6571 mle=1.4126 pcon=5.0141 forget=1.2304 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=7.6748 mle=1.4488 pcon=5.0137 forget=1.2123 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=7.5531 mle=1.3167 pcon=5.0134 forget=1.2231 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 17 it 20 total=7.5960 mle=1.3861 pcon=5.0134 forget=1.1965 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=7.6190 mle=1.3674 pcon=5.0136 forget=1.2380 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=7.4778 mle=1.2431 pcon=5.0132 forget=1.2215 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=7.5700 mle=1.3351 pcon=5.0129 forget=1.2220 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=7.5699 mle=1.3673 pcon=5.0126 forget=1.1900 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=7.5789 mle=1.3480 pcon=5.0126 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=7.4794 mle=1.2579 pcon=5.0124 forget=1.2091 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=7.5305 mle=1.3354 pcon=5.0121 forget=1.1831 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 18 it 30 total=7.4195 mle=1.1956 pcon=5.0120 forget=1.2119 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=7.6493 mle=1.4315 pcon=5.0120 forget=1.2058 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=7.6138 mle=1.3862 pcon=5.0119 forget=1.2157 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=7.4842 mle=1.2708 pcon=5.0119 forget=1.2015 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=7.5747 mle=1.3544 pcon=5.0117 forget=1.2086 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=7.5824 mle=1.3474 pcon=5.0115 forget=1.2235 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=7.5774 mle=1.3434 pcon=5.0114 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=7.4398 mle=1.2283 pcon=5.0115 forget=1.2000 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 19 it 40 total=7.4318 mle=1.2087 pcon=5.0114 forget=1.2117 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=7.5105 mle=1.2974 pcon=5.0112 forget=1.2019 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=7.4404 mle=1.2273 pcon=5.0111 forget=1.2021 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=7.4809 mle=1.2883 pcon=5.0109 forget=1.1817 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=7.4722 mle=1.2456 pcon=5.0107 forget=1.2159 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=7.5096 mle=1.3110 pcon=5.0111 forget=1.1876 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=7.5017 mle=1.2650 pcon=5.0116 forget=1.2251 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 20 it 0 total=7.5048 mle=1.2974 pcon=5.0117 forget=1.1956 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=7.5339 mle=1.2940 pcon=5.0118 forget=1.2281 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 60%|██████    | 21/35 [04:58<03:16, 14.05s/it] 63%|██████▎   | 22/35 [05:11<03:01, 13.95s/it] 66%|██████▌   | 23/35 [05:25<02:44, 13.70s/it] 69%|██████▊   | 24/35 [05:38<02:29, 13.60s/it] 71%|███████▏  | 25/35 [05:51<02:13, 13.35s/it] 74%|███████▍  | 26/35 [06:04<01:59, 13.27s/it][loss] ep 20 it 100 total=7.4197 mle=1.2109 pcon=5.0117 forget=1.1971 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=7.4697 mle=1.2655 pcon=5.0117 forget=1.1925 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=7.5820 mle=1.3621 pcon=5.0117 forget=1.2082 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=7.5542 mle=1.3308 pcon=5.0119 forget=1.2115 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=7.5642 mle=1.3545 pcon=5.0121 forget=1.1976 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=7.5447 mle=1.3325 pcon=5.0120 forget=1.2002 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 21 it 10 total=7.3238 mle=1.1238 pcon=5.0118 forget=1.1882 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=7.4433 mle=1.2259 pcon=5.0118 forget=1.2056 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=7.5240 mle=1.3207 pcon=5.0117 forget=1.1916 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=7.5018 mle=1.3086 pcon=5.0116 forget=1.1817 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=7.4111 mle=1.2013 pcon=5.0114 forget=1.1984 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=7.5610 mle=1.3855 pcon=5.0116 forget=1.1639 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=7.5679 mle=1.3518 pcon=5.0113 forget=1.2048 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=7.5194 mle=1.3039 pcon=5.0113 forget=1.2042 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 22 it 20 total=7.5032 mle=1.3004 pcon=5.0114 forget=1.1913 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=7.4599 mle=1.2518 pcon=5.0112 forget=1.1969 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=7.5311 mle=1.3283 pcon=5.0112 forget=1.1917 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=7.4239 mle=1.2298 pcon=5.0108 forget=1.1833 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=7.5362 mle=1.3352 pcon=5.0109 forget=1.1901 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=7.4764 mle=1.2667 pcon=5.0110 forget=1.1987 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=7.4056 mle=1.2295 pcon=5.0110 forget=1.1650 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=7.5483 mle=1.3587 pcon=5.0107 forget=1.1788 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 23 it 30 total=7.4424 mle=1.2366 pcon=5.0107 forget=1.1951 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=7.4530 mle=1.2270 pcon=5.0106 forget=1.2155 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=7.5430 mle=1.3439 pcon=5.0103 forget=1.1887 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=7.4486 mle=1.2635 pcon=5.0100 forget=1.1751 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=7.4720 mle=1.2935 pcon=5.0099 forget=1.1686 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=7.5332 mle=1.3186 pcon=5.0098 forget=1.2047 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=7.5302 mle=1.3395 pcon=5.0094 forget=1.1813 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=7.3630 mle=1.1604 pcon=5.0092 forget=1.1934 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 24 it 40 total=7.4624 mle=1.2630 pcon=5.0091 forget=1.1903 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=7.4901 mle=1.2963 pcon=5.0087 forget=1.1851 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=7.3619 mle=1.1694 pcon=5.0084 forget=1.1841 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=7.3835 mle=1.1923 pcon=5.0083 forget=1.1829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=7.4652 mle=1.2714 pcon=5.0082 forget=1.1856 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=7.4327 mle=1.2520 pcon=5.0078 forget=1.1729 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=7.4122 mle=1.2128 pcon=5.0073 forget=1.1921 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 25 it 0 total=7.4485 mle=1.2660 pcon=5.0070 forget=1.1755 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 50 total=7.5183 mle=1.3584 pcon=5.0065 forget=1.1533 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 100 total=7.5025 mle=1.3230 pcon=5.0060 forget=1.1736 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 150 total=7.4755 mle=1.3040 pcon=5.0057 forget=1.1658 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 200 total=7.4643 mle=1.2802 pcon=5.0053 forget=1.1788 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 250 total=7.4289 mle=1.2430 pcon=5.0051 forget=1.1808 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 300 total=7.4283 mle=1.2600 pcon=5.0048 forget=1.1635 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 350 total=7.4976 mle=1.3104 pcon=5.0044 forget=1.1828 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 26 it 10 total=7.4179 mle=1.2259 pcon=5.0040 forget=1.1879 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 60 total=7.4266 mle=1.2531 pcon=5.0036 forget=1.1699 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 110 total=7.4486 mle=1.2724 pcon=5.0033 forget=1.1729 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 160 total=7.3713 mle=1.1949 pcon=5.0028 forget=1.1736 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 210 total=7.4712 mle=1.3003 pcon=5.0024 forget=1.1684 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 260 total=7.3927 mle=1.2118 pcon=5.0024 forget=1.1785 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 310 total=7.5939 mle=1.4352 pcon=5.0020 forget=1.1566 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 360 total=7.5426 mle=1.3644 pcon=5.0018 forget=1.1764 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 77%|███████▋  | 27/35 [06:17<01:46, 13.36s/it] 80%|████████  | 28/35 [06:31<01:33, 13.40s/it] 83%|████████▎ | 29/35 [06:44<01:19, 13.29s/it] 86%|████████▌ | 30/35 [06:58<01:07, 13.56s/it] 89%|████████▊ | 31/35 [07:11<00:53, 13.43s/it] 91%|█████████▏| 32/35 [07:25<00:40, 13.46s/it] 94%|█████████▍| 33/35 [07:38<00:26, 13.40s/it] 97%|█████████▋| 34/35 [07:52<00:13, 13.49s/it][peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 27 it 20 total=7.4228 mle=1.2309 pcon=5.0016 forget=1.1903 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 70 total=7.4382 mle=1.2662 pcon=5.0012 forget=1.1708 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 120 total=7.4871 mle=1.2842 pcon=5.0010 forget=1.2018 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 170 total=7.4585 mle=1.2602 pcon=5.0008 forget=1.1975 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 220 total=7.4503 mle=1.2654 pcon=5.0008 forget=1.1841 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 270 total=7.4520 mle=1.2588 pcon=5.0005 forget=1.1927 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 320 total=7.4498 mle=1.2421 pcon=5.0004 forget=1.2074 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 370 total=7.3799 mle=1.1802 pcon=5.0004 forget=1.1993 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 30 total=7.4357 mle=1.2220 pcon=5.0004 forget=1.2133 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 80 total=7.5229 mle=1.3307 pcon=5.0001 forget=1.1921 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 130 total=7.3546 mle=1.1774 pcon=5.0002 forget=1.1770 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 180 total=7.3997 mle=1.1897 pcon=4.9997 forget=1.2103 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 230 total=7.4578 mle=1.2714 pcon=4.9997 forget=1.1867 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 280 total=7.4691 mle=1.2776 pcon=4.9996 forget=1.1919 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 330 total=7.3855 mle=1.1967 pcon=4.9993 forget=1.1895 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 380 total=7.4348 mle=1.2354 pcon=4.9990 forget=1.2004 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 40 total=7.4581 mle=1.2793 pcon=4.9988 forget=1.1800 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 90 total=7.4734 mle=1.2659 pcon=4.9986 forget=1.2089 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 140 total=7.4261 mle=1.2293 pcon=4.9986 forget=1.1982 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 190 total=7.5944 mle=1.3869 pcon=4.9982 forget=1.2092 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 240 total=7.5311 mle=1.3424 pcon=4.9983 forget=1.1904 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 290 total=7.4315 mle=1.2317 pcon=4.9978 forget=1.2021 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 340 total=7.4511 mle=1.2486 pcon=4.9979 forget=1.2046 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 0 total=7.4494 mle=1.2433 pcon=4.9977 forget=1.2084 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 50 total=7.4984 mle=1.3144 pcon=4.9974 forget=1.1865 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 100 total=7.4715 mle=1.2711 pcon=4.9976 forget=1.2028 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 150 total=7.4373 mle=1.2347 pcon=4.9975 forget=1.2051 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 200 total=7.4979 mle=1.3039 pcon=4.9976 forget=1.1964 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 250 total=7.5069 mle=1.3220 pcon=4.9977 forget=1.1871 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 300 total=7.5297 mle=1.3356 pcon=4.9975 forget=1.1966 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 350 total=7.5152 mle=1.2973 pcon=4.9974 forget=1.2205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 10 total=7.5194 mle=1.3207 pcon=4.9972 forget=1.2014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 60 total=7.4391 mle=1.2354 pcon=4.9973 forget=1.2064 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 110 total=7.5767 mle=1.3829 pcon=4.9975 forget=1.1963 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 160 total=7.4704 mle=1.2698 pcon=4.9973 forget=1.2033 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 210 total=7.5726 mle=1.3899 pcon=4.9972 forget=1.1856 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 260 total=7.5147 mle=1.3030 pcon=4.9972 forget=1.2145 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 310 total=7.4787 mle=1.2663 pcon=4.9971 forget=1.2153 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 360 total=7.4324 mle=1.2266 pcon=4.9975 forget=1.2083 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 20 total=7.4123 mle=1.1935 pcon=4.9979 forget=1.2209 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 70 total=7.4309 mle=1.2219 pcon=4.9982 forget=1.2109 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 120 total=7.4235 mle=1.2133 pcon=4.9980 forget=1.2122 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 170 total=7.4498 mle=1.2248 pcon=4.9981 forget=1.2269 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 220 total=7.5408 mle=1.3273 pcon=4.9980 forget=1.2155 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 270 total=7.4103 mle=1.2038 pcon=4.9979 forget=1.2086 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 320 total=7.4859 mle=1.2748 pcon=4.9975 forget=1.2135 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 370 total=7.3812 mle=1.1669 pcon=4.9977 forget=1.2167 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 30 total=7.5467 mle=1.3268 pcon=4.9977 forget=1.2222 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 80 total=7.4945 mle=1.2929 pcon=4.9977 forget=1.2039 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 130 total=7.4670 mle=1.2680 pcon=4.9979 forget=1.2011 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 180 total=7.4756 mle=1.2868 pcon=4.9979 forget=1.1909 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 230 total=7.5132 mle=1.3129 pcon=4.9982 forget=1.2021 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 280 total=7.4734 mle=1.2818 pcon=4.9979 forget=1.1937 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 330 total=7.6042 mle=1.3978 pcon=4.9978 forget=1.2086 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 380 total=7.4442 mle=1.2319 pcon=4.9977 forget=1.2147 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 40 total=7.5021 mle=1.3035 pcon=4.9975 forget=1.2011 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 90 total=7.4624 mle=1.2576 pcon=4.9975 forget=1.2073 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 140 total=7.5339 mle=1.3357 pcon=4.9972 forget=1.2010 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 190 total=7.4338 mle=1.2423 pcon=4.9972 forget=1.1943 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 240 total=7.4735 mle=1.2476 pcon=4.9971 forget=1.2288 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
100%|██████████| 35/35 [08:05<00:00, 13.55s/it]100%|██████████| 35/35 [08:05<00:00, 13.88s/it]
[loss] ep 34 it 290 total=7.4347 mle=1.2469 pcon=4.9973 forget=1.1905 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 340 total=7.4419 mle=1.2092 pcon=4.9970 forget=1.2358 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:55,  2.22it/s]  2%|▏         | 9/391 [00:00<00:18, 20.54it/s]  5%|▍         | 19/391 [00:00<00:09, 40.00it/s]  7%|▋         | 29/391 [00:00<00:06, 54.39it/s] 10%|▉         | 39/391 [00:00<00:05, 65.34it/s] 13%|█▎        | 49/391 [00:00<00:04, 73.75it/s] 15%|█▌        | 59/391 [00:01<00:04, 79.49it/s] 18%|█▊        | 69/391 [00:01<00:03, 83.59it/s] 20%|██        | 79/391 [00:01<00:03, 86.92it/s] 23%|██▎       | 89/391 [00:01<00:03, 89.43it/s] 25%|██▌       | 99/391 [00:01<00:03, 90.56it/s] 28%|██▊       | 109/391 [00:01<00:03, 90.21it/s] 30%|███       | 119/391 [00:01<00:02, 91.54it/s] 33%|███▎      | 129/391 [00:01<00:02, 91.68it/s] 36%|███▌      | 139/391 [00:01<00:02, 92.30it/s] 38%|███▊      | 149/391 [00:02<00:02, 92.51it/s] 41%|████      | 159/391 [00:02<00:02, 92.51it/s] 43%|████▎     | 169/391 [00:02<00:02, 92.68it/s] 46%|████▌     | 179/391 [00:02<00:02, 93.39it/s] 48%|████▊     | 189/391 [00:02<00:02, 88.90it/s] 51%|█████     | 198/391 [00:02<00:02, 88.92it/s] 53%|█████▎    | 208/391 [00:02<00:02, 90.94it/s] 56%|█████▌    | 218/391 [00:02<00:01, 92.19it/s] 58%|█████▊    | 228/391 [00:02<00:01, 92.81it/s] 61%|██████    | 238/391 [00:03<00:01, 93.85it/s] 63%|██████▎   | 248/391 [00:03<00:01, 92.24it/s] 66%|██████▌   | 258/391 [00:03<00:01, 92.51it/s] 69%|██████▊   | 268/391 [00:03<00:01, 93.43it/s] 71%|███████   | 278/391 [00:03<00:01, 94.23it/s] 74%|███████▎  | 288/391 [00:03<00:01, 94.65it/s] 76%|███████▌  | 298/391 [00:03<00:00, 94.27it/s] 79%|███████▉  | 308/391 [00:03<00:00, 94.70it/s] 81%|████████▏ | 318/391 [00:03<00:00, 94.85it/s] 84%|████████▍ | 328/391 [00:03<00:00, 95.26it/s] 86%|████████▋ | 338/391 [00:04<00:00, 95.28it/s] 89%|████████▉ | 348/391 [00:04<00:00, 94.95it/s] 92%|█████████▏| 358/391 [00:04<00:00, 95.14it/s] 94%|█████████▍| 368/391 [00:04<00:00, 95.04it/s] 97%|█████████▋| 378/391 [00:04<00:00, 95.65it/s] 99%|█████████▉| 388/391 [00:04<00:00, 96.01it/s]100%|██████████| 391/391 [00:04<00:00, 84.15it/s]
50000 images processed, 4.740050554275513 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:31,  2.46it/s] 13%|█▎        | 10/79 [00:00<00:02, 24.94it/s] 24%|██▍       | 19/79 [00:00<00:01, 41.91it/s] 37%|███▋      | 29/79 [00:00<00:00, 56.27it/s] 48%|████▊     | 38/79 [00:00<00:00, 65.45it/s] 61%|██████    | 48/79 [00:00<00:00, 72.52it/s] 73%|███████▎  | 58/79 [00:01<00:00, 78.64it/s] 86%|████████▌ | 68/79 [00:01<00:00, 83.07it/s] 99%|█████████▊| 78/79 [00:01<00:00, 86.14it/s]100%|██████████| 79/79 [00:02<00:00, 37.92it/s]
10000 images processed, 2.1405415534973145 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:47,  1.89it/s]  4%|▍         | 9/204 [00:00<00:10, 18.04it/s]  8%|▊         | 17/204 [00:00<00:05, 31.52it/s] 13%|█▎        | 26/204 [00:00<00:04, 44.41it/s] 17%|█▋        | 34/204 [00:00<00:03, 53.08it/s] 21%|██        | 43/204 [00:01<00:02, 61.98it/s] 25%|██▌       | 52/204 [00:01<00:02, 69.01it/s] 30%|███       | 62/204 [00:01<00:01, 76.19it/s] 35%|███▍      | 71/204 [00:01<00:01, 78.41it/s] 39%|███▉      | 80/204 [00:01<00:01, 79.67it/s] 44%|████▍     | 90/204 [00:01<00:01, 84.11it/s] 49%|████▉     | 100/204 [00:01<00:01, 87.08it/s] 53%|█████▎    | 109/204 [00:01<00:01, 87.47it/s] 58%|█████▊    | 119/204 [00:01<00:00, 88.58it/s] 63%|██████▎   | 129/204 [00:02<00:00, 89.83it/s] 68%|██████▊   | 139/204 [00:02<00:00, 90.65it/s] 73%|███████▎  | 149/204 [00:02<00:00, 91.43it/s] 78%|███████▊  | 159/204 [00:02<00:00, 91.98it/s] 83%|████████▎ | 169/204 [00:02<00:00, 92.05it/s] 88%|████████▊ | 179/204 [00:02<00:00, 92.24it/s] 93%|█████████▎| 189/204 [00:02<00:00, 93.49it/s] 98%|█████████▊| 199/204 [00:02<00:00, 94.21it/s]100%|██████████| 204/204 [00:02<00:00, 71.97it/s]
26032 images processed, 2.8747265338897705 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:52,  1.50it/s] 13%|█▎        | 10/79 [00:00<00:04, 16.96it/s] 25%|██▌       | 20/79 [00:00<00:01, 33.07it/s] 37%|███▋      | 29/79 [00:00<00:01, 44.89it/s] 49%|████▉     | 39/79 [00:01<00:00, 56.98it/s] 62%|██████▏   | 49/79 [00:01<00:00, 66.15it/s] 75%|███████▍  | 59/79 [00:01<00:00, 73.14it/s] 87%|████████▋ | 69/79 [00:01<00:00, 79.25it/s]100%|██████████| 79/79 [00:01<00:00, 52.08it/s]
10000 images processed, 1.5463979244232178 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:39,  1.97it/s] 14%|█▍        | 11/79 [00:00<00:02, 23.01it/s] 25%|██▌       | 20/79 [00:00<00:01, 38.74it/s] 38%|███▊      | 30/79 [00:00<00:00, 53.31it/s] 51%|█████     | 40/79 [00:00<00:00, 64.52it/s] 63%|██████▎   | 50/79 [00:01<00:00, 72.81it/s] 76%|███████▌  | 60/79 [00:01<00:00, 79.38it/s] 89%|████████▊ | 70/79 [00:01<00:00, 83.98it/s]100%|██████████| 79/79 [00:01<00:00, 59.08it/s]
10000 images processed, 1.3589539527893066 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:31,  2.22it/s] 13%|█▎        | 9/70 [00:00<00:02, 20.68it/s] 27%|██▋       | 19/70 [00:00<00:01, 39.99it/s] 41%|████▏     | 29/70 [00:00<00:00, 54.58it/s] 56%|█████▌    | 39/70 [00:00<00:00, 65.52it/s] 70%|███████   | 49/70 [00:00<00:00, 74.04it/s] 84%|████████▍ | 59/70 [00:01<00:00, 80.43it/s] 99%|█████████▊| 69/70 [00:01<00:00, 84.83it/s]100%|██████████| 70/70 [00:01<00:00, 57.90it/s]
8925 images processed, 1.2407071590423584 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:39,  1.11it/s]  4%|▍         | 2/45 [00:01<00:19,  2.25it/s] 27%|██▋       | 12/45 [00:01<00:01, 17.48it/s] 38%|███▊      | 17/45 [00:01<00:01, 18.95it/s] 47%|████▋     | 21/45 [00:01<00:01, 22.41it/s] 56%|█████▌    | 25/45 [00:01<00:00, 24.02it/s] 73%|███████▎  | 33/45 [00:01<00:00, 23.31it/s] 82%|████████▏ | 37/45 [00:02<00:00, 25.98it/s]100%|██████████| 45/45 [00:02<00:00, 20.84it/s]
5640 images processed, 2.1801681518554688 seconds used

17.62275528907776
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.88  99.31  97.84
places365     71.60  80.52  78.51
LSUN          19.34  96.00  96.06
iSUN          74.72  81.56  84.25
dtd           39.73  91.07  94.13
AVG           41.66  89.69  90.16
Retain-Acc: 0.7467
Forget-as-OOD (retain known vs forget novel):
  FPR: 72.60 AUROC: 87.87 AUIN: 98.48
7.1037209033966064
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_rf.png
[Run] lambda=0.1 lr=0.001 epochs=40
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=40, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.1, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/40 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▎         | 1/40 [00:20<13:13, 20.34s/it]  5%|▌         | 2/40 [00:35<10:59, 17.35s/it]  8%|▊         | 3/40 [00:50<09:55, 16.11s/it] 10%|█         | 4/40 [01:05<09:26, 15.73s/it] 12%|█▎        | 5/40 [01:20<09:04, 15.56s/it] 15%|█▌        | 6/40 [01:36<08:52, 15.67s/it][loss] ep 0 it 0 total=7.8078 mle=1.2804 pcon=5.2951 forget=1.2323 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=7.8926 mle=1.3923 pcon=5.2887 forget=1.2116 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=8.0161 mle=1.5080 pcon=5.2827 forget=1.2253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=8.0598 mle=1.5645 pcon=5.2770 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=8.0046 mle=1.5331 pcon=5.2708 forget=1.2007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=7.8786 mle=1.3805 pcon=5.2650 forget=1.2330 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=7.7785 mle=1.2921 pcon=5.2596 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=7.9325 mle=1.4378 pcon=5.2543 forget=1.2404 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 1 it 10 total=7.7859 mle=1.3066 pcon=5.2487 forget=1.2305 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=7.7983 mle=1.3161 pcon=5.2433 forget=1.2389 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=7.7468 mle=1.2903 pcon=5.2382 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=7.7125 mle=1.2380 pcon=5.2331 forget=1.2415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=7.8321 mle=1.3725 pcon=5.2284 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=7.7792 mle=1.3468 pcon=5.2235 forget=1.2088 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=7.7559 mle=1.3082 pcon=5.2185 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=7.8230 mle=1.3767 pcon=5.2136 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 2 it 20 total=7.7063 mle=1.2745 pcon=5.2089 forget=1.2228 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=7.8500 mle=1.3988 pcon=5.2044 forget=1.2469 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=7.7730 mle=1.3590 pcon=5.1998 forget=1.2142 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=7.7720 mle=1.3576 pcon=5.1955 forget=1.2190 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=7.7515 mle=1.3299 pcon=5.1909 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=7.7869 mle=1.3658 pcon=5.1865 forget=1.2345 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=7.6577 mle=1.2606 pcon=5.1824 forget=1.2147 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=7.7848 mle=1.3834 pcon=5.1787 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 3 it 30 total=7.7685 mle=1.3639 pcon=5.1749 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=7.6956 mle=1.2956 pcon=5.1710 forget=1.2290 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=7.6785 mle=1.2905 pcon=5.1672 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=7.8325 mle=1.4600 pcon=5.1635 forget=1.2090 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=7.7463 mle=1.3524 pcon=5.1601 forget=1.2338 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=7.8178 mle=1.4266 pcon=5.1568 forget=1.2344 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=7.8237 mle=1.4481 pcon=5.1531 forget=1.2225 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=7.6898 mle=1.3142 pcon=5.1497 forget=1.2259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 4 it 40 total=7.7402 mle=1.3768 pcon=5.1465 forget=1.2170 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=7.7167 mle=1.3337 pcon=5.1434 forget=1.2396 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=7.9299 mle=1.5852 pcon=5.1406 forget=1.2041 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=7.7627 mle=1.3940 pcon=5.1374 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=7.7865 mle=1.4142 pcon=5.1344 forget=1.2379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=7.5440 mle=1.1863 pcon=5.1316 forget=1.2262 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=7.8146 mle=1.4610 pcon=5.1291 forget=1.2246 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 5 it 0 total=7.5815 mle=1.2047 pcon=5.1261 forget=1.2507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=7.7230 mle=1.3647 pcon=5.1234 forget=1.2348 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=7.7653 mle=1.4009 pcon=5.1206 forget=1.2438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=7.8336 mle=1.4808 pcon=5.1178 forget=1.2350 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=7.7882 mle=1.4523 pcon=5.1152 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=7.6865 mle=1.3485 pcon=5.1124 forget=1.2255 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=7.7565 mle=1.4295 pcon=5.1098 forget=1.2172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=7.7336 mle=1.4045 pcon=5.1072 forget=1.2219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 6 it 10 total=7.6347 mle=1.2783 pcon=5.1048 forget=1.2516 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=7.7001 mle=1.3787 pcon=5.1024 forget=1.2189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=7.6563 mle=1.3185 pcon=5.1002 forget=1.2376 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=7.6599 mle=1.3322 pcon=5.0979 forget=1.2298 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=7.6925 mle=1.3653 pcon=5.0954 forget=1.2318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=7.7162 mle=1.3836 pcon=5.0933 forget=1.2393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=7.6565 mle=1.3538 pcon=5.0913 forget=1.2115 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 18%|█▊        | 7/40 [01:50<08:20, 15.18s/it] 20%|██        | 8/40 [02:04<07:51, 14.74s/it] 22%|██▎       | 9/40 [02:18<07:29, 14.49s/it] 25%|██▌       | 10/40 [02:33<07:20, 14.68s/it] 28%|██▊       | 11/40 [02:47<06:57, 14.41s/it] 30%|███       | 12/40 [03:01<06:44, 14.43s/it] 32%|███▎      | 13/40 [03:16<06:30, 14.47s/it][loss] ep 6 it 360 total=7.7498 mle=1.4567 pcon=5.0891 forget=1.2040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 7 it 20 total=7.5655 mle=1.2502 pcon=5.0870 forget=1.2283 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=7.5609 mle=1.2465 pcon=5.0847 forget=1.2296 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=7.6924 mle=1.3881 pcon=5.0827 forget=1.2216 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=7.7176 mle=1.4330 pcon=5.0808 forget=1.2038 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=7.6182 mle=1.3207 pcon=5.0791 forget=1.2184 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=7.7540 mle=1.4544 pcon=5.0775 forget=1.2221 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=7.6235 mle=1.3171 pcon=5.0753 forget=1.2311 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=7.6913 mle=1.3713 pcon=5.0733 forget=1.2467 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 8 it 30 total=7.7578 mle=1.4810 pcon=5.0716 forget=1.2052 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=7.5762 mle=1.2754 pcon=5.0699 forget=1.2309 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=7.6953 mle=1.3991 pcon=5.0682 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=7.6396 mle=1.3455 pcon=5.0665 forget=1.2276 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=7.6603 mle=1.3663 pcon=5.0648 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=7.5535 mle=1.2577 pcon=5.0632 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=7.8330 mle=1.5800 pcon=5.0614 forget=1.1916 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=7.6517 mle=1.3648 pcon=5.0600 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 9 it 40 total=7.6993 mle=1.4115 pcon=5.0585 forget=1.2293 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=7.7620 mle=1.4932 pcon=5.0570 forget=1.2117 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=7.6463 mle=1.3548 pcon=5.0559 forget=1.2356 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=7.5182 mle=1.2290 pcon=5.0546 forget=1.2346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=7.7622 mle=1.4987 pcon=5.0535 forget=1.2101 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=7.6411 mle=1.3593 pcon=5.0520 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=7.5895 mle=1.3078 pcon=5.0508 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 10 it 0 total=7.5544 mle=1.2684 pcon=5.0492 forget=1.2369 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=7.5333 mle=1.2585 pcon=5.0482 forget=1.2265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=7.5939 mle=1.3283 pcon=5.0469 forget=1.2187 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=7.6344 mle=1.3449 pcon=5.0457 forget=1.2438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=7.6851 mle=1.4101 pcon=5.0446 forget=1.2303 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=7.6987 mle=1.4287 pcon=5.0436 forget=1.2265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=7.7519 mle=1.5150 pcon=5.0424 forget=1.1946 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=7.5334 mle=1.2625 pcon=5.0415 forget=1.2294 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 11 it 10 total=7.7022 mle=1.4430 pcon=5.0403 forget=1.2189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=7.6325 mle=1.3731 pcon=5.0394 forget=1.2201 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=7.6151 mle=1.3486 pcon=5.0385 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=7.5508 mle=1.2741 pcon=5.0374 forget=1.2393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=7.5319 mle=1.2652 pcon=5.0365 forget=1.2302 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=7.5273 mle=1.2694 pcon=5.0355 forget=1.2224 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=7.6541 mle=1.3935 pcon=5.0347 forget=1.2259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=7.6002 mle=1.3221 pcon=5.0340 forget=1.2441 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 12 it 20 total=7.5819 mle=1.3165 pcon=5.0328 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=7.6194 mle=1.3557 pcon=5.0318 forget=1.2319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=7.6044 mle=1.3653 pcon=5.0307 forget=1.2084 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=7.6127 mle=1.3434 pcon=5.0297 forget=1.2397 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=7.5000 mle=1.2268 pcon=5.0291 forget=1.2441 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=7.5468 mle=1.2977 pcon=5.0285 forget=1.2205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=7.6746 mle=1.4167 pcon=5.0279 forget=1.2300 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=7.6428 mle=1.3873 pcon=5.0273 forget=1.2282 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 13 it 30 total=7.5885 mle=1.3175 pcon=5.0268 forget=1.2443 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=7.7436 mle=1.5041 pcon=5.0261 forget=1.2134 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=7.6131 mle=1.3631 pcon=5.0253 forget=1.2247 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=7.6289 mle=1.3571 pcon=5.0242 forget=1.2476 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 35%|███▌      | 14/40 [03:30<06:16, 14.47s/it] 38%|███▊      | 15/40 [03:45<06:03, 14.52s/it] 40%|████      | 16/40 [03:59<05:45, 14.38s/it] 42%|████▎     | 17/40 [04:13<05:28, 14.29s/it] 45%|████▌     | 18/40 [04:27<05:14, 14.28s/it] 48%|████▊     | 19/40 [04:44<05:14, 14.99s/it] 50%|█████     | 20/40 [05:01<05:08, 15.45s/it][loss] ep 13 it 230 total=7.5506 mle=1.3048 pcon=5.0236 forget=1.2222 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=7.5049 mle=1.2268 pcon=5.0230 forget=1.2551 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=7.6448 mle=1.4004 pcon=5.0225 forget=1.2220 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=7.5720 mle=1.3295 pcon=5.0220 forget=1.2205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 14 it 40 total=7.5229 mle=1.2782 pcon=5.0213 forget=1.2233 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=7.5825 mle=1.3280 pcon=5.0209 forget=1.2337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=7.5641 mle=1.3037 pcon=5.0204 forget=1.2400 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=7.5230 mle=1.2668 pcon=5.0200 forget=1.2361 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=7.5797 mle=1.3255 pcon=5.0192 forget=1.2349 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=7.5252 mle=1.2937 pcon=5.0188 forget=1.2127 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=7.4858 mle=1.2463 pcon=5.0184 forget=1.2210 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 15 it 0 total=7.5661 mle=1.3369 pcon=5.0178 forget=1.2114 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=7.4747 mle=1.2426 pcon=5.0176 forget=1.2146 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=7.5890 mle=1.3667 pcon=5.0174 forget=1.2050 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=7.5319 mle=1.2894 pcon=5.0170 forget=1.2255 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=7.5066 mle=1.2573 pcon=5.0168 forget=1.2325 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=7.5260 mle=1.2833 pcon=5.0164 forget=1.2263 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=7.5978 mle=1.3904 pcon=5.0160 forget=1.1914 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=7.6217 mle=1.4000 pcon=5.0158 forget=1.2060 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 16 it 10 total=7.4297 mle=1.1818 pcon=5.0154 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=7.4468 mle=1.2313 pcon=5.0153 forget=1.2002 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=7.4363 mle=1.2041 pcon=5.0150 forget=1.2173 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=7.5241 mle=1.2734 pcon=5.0149 forget=1.2358 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=7.5156 mle=1.2546 pcon=5.0147 forget=1.2463 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=7.6557 mle=1.4113 pcon=5.0142 forget=1.2302 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=7.6725 mle=1.4469 pcon=5.0138 forget=1.2118 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=7.5524 mle=1.3162 pcon=5.0135 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 17 it 20 total=7.5935 mle=1.3840 pcon=5.0136 forget=1.1959 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=7.6172 mle=1.3662 pcon=5.0137 forget=1.2374 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=7.4758 mle=1.2415 pcon=5.0133 forget=1.2209 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=7.5676 mle=1.3334 pcon=5.0131 forget=1.2211 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=7.5703 mle=1.3684 pcon=5.0128 forget=1.1892 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=7.5764 mle=1.3465 pcon=5.0127 forget=1.2171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=7.4783 mle=1.2579 pcon=5.0126 forget=1.2078 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=7.5270 mle=1.3329 pcon=5.0122 forget=1.1819 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 18 it 30 total=7.4184 mle=1.1960 pcon=5.0122 forget=1.2103 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=7.6472 mle=1.4300 pcon=5.0122 forget=1.2050 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=7.6108 mle=1.3838 pcon=5.0121 forget=1.2149 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=7.4824 mle=1.2703 pcon=5.0122 forget=1.2000 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=7.5714 mle=1.3526 pcon=5.0119 forget=1.2069 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=7.5768 mle=1.3428 pcon=5.0117 forget=1.2222 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=7.5732 mle=1.3405 pcon=5.0116 forget=1.2211 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=7.4346 mle=1.2251 pcon=5.0118 forget=1.1977 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 19 it 40 total=7.4277 mle=1.2061 pcon=5.0117 forget=1.2099 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=7.5070 mle=1.2961 pcon=5.0114 forget=1.1995 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=7.4370 mle=1.2259 pcon=5.0114 forget=1.1998 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=7.4747 mle=1.2842 pcon=5.0112 forget=1.1794 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=7.4657 mle=1.2421 pcon=5.0110 forget=1.2126 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=7.5045 mle=1.3092 pcon=5.0114 forget=1.1838 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=7.4971 mle=1.2630 pcon=5.0119 forget=1.2222 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 20 it 0 total=7.4986 mle=1.2938 pcon=5.0120 forget=1.1929 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=7.5294 mle=1.2929 pcon=5.0121 forget=1.2244 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 52%|█████▎    | 21/40 [05:16<04:51, 15.34s/it] 55%|█████▌    | 22/40 [05:30<04:31, 15.06s/it] 57%|█████▊    | 23/40 [05:44<04:12, 14.85s/it] 60%|██████    | 24/40 [05:59<03:57, 14.82s/it] 62%|██████▎   | 25/40 [06:15<03:45, 15.06s/it] 65%|██████▌   | 26/40 [06:30<03:32, 15.19s/it] 68%|██████▊   | 27/40 [06:44<03:12, 14.80s/it][loss] ep 20 it 100 total=7.4120 mle=1.2066 pcon=5.0119 forget=1.1936 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=7.4623 mle=1.2620 pcon=5.0120 forget=1.1883 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=7.5727 mle=1.3551 pcon=5.0120 forget=1.2057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=7.5467 mle=1.3267 pcon=5.0121 forget=1.2079 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=7.5571 mle=1.3516 pcon=5.0122 forget=1.1932 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=7.5373 mle=1.3310 pcon=5.0122 forget=1.1941 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 21 it 10 total=7.3141 mle=1.1185 pcon=5.0119 forget=1.1837 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=7.4327 mle=1.2206 pcon=5.0119 forget=1.2002 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=7.5197 mle=1.3232 pcon=5.0117 forget=1.1848 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=7.4893 mle=1.3042 pcon=5.0115 forget=1.1736 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=7.4058 mle=1.2023 pcon=5.0113 forget=1.1923 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=7.5482 mle=1.3809 pcon=5.0113 forget=1.1560 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=7.5518 mle=1.3428 pcon=5.0111 forget=1.1980 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=7.5144 mle=1.3071 pcon=5.0109 forget=1.1964 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 22 it 20 total=7.4846 mle=1.2918 pcon=5.0109 forget=1.1818 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=7.4539 mle=1.2535 pcon=5.0105 forget=1.1899 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=7.5145 mle=1.3214 pcon=5.0104 forget=1.1827 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=7.4068 mle=1.2279 pcon=5.0099 forget=1.1690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=7.5220 mle=1.3376 pcon=5.0098 forget=1.1746 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=7.4499 mle=1.2585 pcon=5.0097 forget=1.1817 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=7.3836 mle=1.2261 pcon=5.0096 forget=1.1478 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=7.5308 mle=1.3609 pcon=5.0091 forget=1.1608 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 23 it 30 total=7.4168 mle=1.2372 pcon=5.0089 forget=1.1707 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=7.4357 mle=1.2301 pcon=5.0086 forget=1.1971 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=7.5267 mle=1.3506 pcon=5.0081 forget=1.1680 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=7.4148 mle=1.2554 pcon=5.0075 forget=1.1519 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=7.4815 mle=1.3252 pcon=5.0073 forget=1.1490 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=7.5016 mle=1.3188 pcon=5.0070 forget=1.1758 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=7.5022 mle=1.3351 pcon=5.0064 forget=1.1607 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=7.3538 mle=1.1856 pcon=5.0060 forget=1.1622 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 24 it 40 total=7.4215 mle=1.2531 pcon=5.0059 forget=1.1626 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=7.4763 mle=1.3169 pcon=5.0053 forget=1.1541 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=7.3557 mle=1.1997 pcon=5.0049 forget=1.1512 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=7.3927 mle=1.2309 pcon=5.0047 forget=1.1570 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=7.4510 mle=1.2879 pcon=5.0046 forget=1.1586 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=7.4331 mle=1.2840 pcon=5.0042 forget=1.1449 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=7.4066 mle=1.2416 pcon=5.0037 forget=1.1613 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 0 total=7.4488 mle=1.2942 pcon=5.0034 forget=1.1512 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 50 total=7.5099 mle=1.3707 pcon=5.0031 forget=1.1361 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 100 total=7.5244 mle=1.3706 pcon=5.0027 forget=1.1512 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 150 total=7.4791 mle=1.3295 pcon=5.0026 forget=1.1470 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 200 total=7.4748 mle=1.3193 pcon=5.0024 forget=1.1531 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 250 total=7.4197 mle=1.2541 pcon=5.0024 forget=1.1631 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 300 total=7.4666 mle=1.3066 pcon=5.0024 forget=1.1575 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 350 total=7.5364 mle=1.3627 pcon=5.0022 forget=1.1715 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 10 total=7.4564 mle=1.2761 pcon=5.0022 forget=1.1781 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 60 total=7.4802 mle=1.3123 pcon=5.0021 forget=1.1658 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 110 total=7.4811 mle=1.3120 pcon=5.0022 forget=1.1670 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 160 total=7.4221 mle=1.2431 pcon=5.0020 forget=1.1769 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 210 total=7.5253 mle=1.3491 pcon=5.0021 forget=1.1740 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 260 total=7.4393 mle=1.2579 pcon=5.0026 forget=1.1789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 310 total=7.6512 mle=1.4857 pcon=5.0026 forget=1.1629 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 360 total=7.5956 mle=1.4192 pcon=5.0028 forget=1.1737 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 20 total=7.4626 mle=1.2667 pcon=5.0030 forget=1.1929 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 70 total=7.4897 mle=1.3046 pcon=5.0030 forget=1.1821 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 120 total=7.5367 mle=1.3269 pcon=5.0034 forget=1.2065 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 70%|███████   | 28/40 [07:00<03:00, 15.03s/it] 72%|███████▎  | 29/40 [07:15<02:45, 15.04s/it] 75%|███████▌  | 30/40 [07:29<02:26, 14.66s/it] 78%|███████▊  | 31/40 [07:42<02:09, 14.44s/it] 80%|████████  | 32/40 [07:58<01:57, 14.68s/it] 82%|████████▎ | 33/40 [08:12<01:42, 14.70s/it] 85%|████████▌ | 34/40 [08:27<01:28, 14.77s/it] 88%|████████▊ | 35/40 [08:42<01:13, 14.63s/it][loss] ep 27 it 170 total=7.5158 mle=1.3066 pcon=5.0036 forget=1.2057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 220 total=7.5065 mle=1.3073 pcon=5.0041 forget=1.1952 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 270 total=7.5156 mle=1.3068 pcon=5.0042 forget=1.2046 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 320 total=7.5161 mle=1.2925 pcon=5.0045 forget=1.2191 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 370 total=7.4626 mle=1.2465 pcon=5.0049 forget=1.2112 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 30 total=7.5013 mle=1.2743 pcon=5.0053 forget=1.2217 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 80 total=7.5911 mle=1.3780 pcon=5.0055 forget=1.2077 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 130 total=7.4336 mle=1.2303 pcon=5.0059 forget=1.1974 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 180 total=7.4687 mle=1.2380 pcon=5.0057 forget=1.2250 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 230 total=7.5204 mle=1.3050 pcon=5.0061 forget=1.2093 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 280 total=7.5385 mle=1.3181 pcon=5.0063 forget=1.2141 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 330 total=7.4786 mle=1.2615 pcon=5.0063 forget=1.2107 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 380 total=7.5159 mle=1.2885 pcon=5.0063 forget=1.2211 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 40 total=7.5336 mle=1.3225 pcon=5.0064 forget=1.2047 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 90 total=7.5323 mle=1.2969 pcon=5.0064 forget=1.2290 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 140 total=7.4983 mle=1.2718 pcon=5.0066 forget=1.2199 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 190 total=7.6856 mle=1.4497 pcon=5.0064 forget=1.2294 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 240 total=7.5976 mle=1.3751 pcon=5.0066 forget=1.2159 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 290 total=7.5145 mle=1.2832 pcon=5.0063 forget=1.2250 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 340 total=7.5290 mle=1.2996 pcon=5.0066 forget=1.2229 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 0 total=7.5201 mle=1.2845 pcon=5.0065 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 50 total=7.5887 mle=1.3675 pcon=5.0062 forget=1.2150 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 100 total=7.5681 mle=1.3369 pcon=5.0064 forget=1.2248 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 150 total=7.5096 mle=1.2807 pcon=5.0064 forget=1.2225 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 200 total=7.5841 mle=1.3589 pcon=5.0064 forget=1.2187 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 250 total=7.5901 mle=1.3702 pcon=5.0066 forget=1.2133 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 300 total=7.6143 mle=1.3883 pcon=5.0062 forget=1.2197 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 350 total=7.5791 mle=1.3287 pcon=5.0061 forget=1.2443 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 10 total=7.6021 mle=1.3680 pcon=5.0058 forget=1.2283 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 60 total=7.5110 mle=1.2769 pcon=5.0058 forget=1.2283 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 110 total=7.6129 mle=1.3878 pcon=5.0058 forget=1.2193 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 160 total=7.5409 mle=1.3100 pcon=5.0054 forget=1.2255 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 210 total=7.6307 mle=1.4162 pcon=5.0051 forget=1.2095 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 260 total=7.5552 mle=1.3172 pcon=5.0049 forget=1.2331 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 310 total=7.5363 mle=1.2896 pcon=5.0045 forget=1.2421 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 360 total=7.4927 mle=1.2559 pcon=5.0047 forget=1.2322 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 20 total=7.4667 mle=1.2265 pcon=5.0048 forget=1.2354 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 70 total=7.4899 mle=1.2548 pcon=5.0048 forget=1.2303 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 120 total=7.5003 mle=1.2661 pcon=5.0043 forget=1.2299 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 170 total=7.5223 mle=1.2710 pcon=5.0041 forget=1.2473 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 220 total=7.6014 mle=1.3651 pcon=5.0036 forget=1.2327 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 270 total=7.4867 mle=1.2505 pcon=5.0032 forget=1.2331 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 320 total=7.5364 mle=1.2991 pcon=5.0025 forget=1.2347 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 370 total=7.4472 mle=1.2107 pcon=5.0022 forget=1.2343 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 30 total=7.6107 mle=1.3658 pcon=5.0019 forget=1.2430 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 80 total=7.5467 mle=1.3219 pcon=5.0014 forget=1.2234 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 130 total=7.5334 mle=1.3152 pcon=5.0012 forget=1.2169 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 180 total=7.5304 mle=1.3184 pcon=5.0008 forget=1.2112 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 230 total=7.5587 mle=1.3370 pcon=5.0007 forget=1.2210 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 280 total=7.5180 mle=1.3061 pcon=5.0000 forget=1.2120 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 330 total=7.6377 mle=1.4112 pcon=4.9994 forget=1.2271 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 380 total=7.4912 mle=1.2588 pcon=4.9989 forget=1.2335 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 40 total=7.5669 mle=1.3483 pcon=4.9983 forget=1.2203 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 90 total=7.5181 mle=1.2972 pcon=4.9978 forget=1.2232 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 140 total=7.5619 mle=1.3502 pcon=4.9970 forget=1.2147 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 190 total=7.4771 mle=1.2661 pcon=4.9964 forget=1.2146 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 240 total=7.4992 mle=1.2618 pcon=4.9960 forget=1.2415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 290 total=7.4757 mle=1.2746 pcon=4.9955 forget=1.2056 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 340 total=7.4762 mle=1.2333 pcon=4.9949 forget=1.2480 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 0 total=7.4894 mle=1.2616 pcon=4.9945 forget=1.2333 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 50 total=7.5714 mle=1.3549 pcon=4.9943 forget=1.2222 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 100 total=7.5306 mle=1.3003 pcon=4.9940 forget=1.2363 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 90%|█████████ | 36/40 [08:57<00:59, 14.76s/it] 92%|█████████▎| 37/40 [09:11<00:43, 14.64s/it] 95%|█████████▌| 38/40 [09:26<00:29, 14.74s/it] 98%|█████████▊| 39/40 [09:41<00:14, 14.92s/it]100%|██████████| 40/40 [09:56<00:00, 14.94s/it]100%|██████████| 40/40 [09:56<00:00, 14.92s/it]
[loss] ep 35 it 150 total=7.4028 mle=1.1797 pcon=4.9933 forget=1.2298 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 200 total=7.5129 mle=1.2995 pcon=4.9928 forget=1.2206 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 250 total=7.5237 mle=1.3019 pcon=4.9925 forget=1.2293 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 300 total=7.5258 mle=1.3048 pcon=4.9919 forget=1.2291 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 350 total=7.5498 mle=1.3243 pcon=4.9914 forget=1.2341 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 10 total=7.5507 mle=1.3426 pcon=4.9909 forget=1.2173 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 60 total=7.5526 mle=1.3355 pcon=4.9903 forget=1.2267 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 110 total=7.6720 mle=1.4523 pcon=4.9900 forget=1.2296 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 160 total=7.5906 mle=1.3720 pcon=4.9897 forget=1.2289 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 210 total=7.6302 mle=1.4137 pcon=4.9890 forget=1.2275 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 260 total=7.5554 mle=1.3648 pcon=4.9886 forget=1.2021 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 310 total=7.4316 mle=1.2132 pcon=4.9883 forget=1.2302 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 360 total=7.4832 mle=1.2699 pcon=4.9883 forget=1.2250 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 20 total=7.5459 mle=1.3107 pcon=4.9879 forget=1.2474 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 70 total=7.5950 mle=1.3724 pcon=4.9876 forget=1.2351 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 120 total=7.4866 mle=1.2976 pcon=4.9870 forget=1.2020 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 170 total=7.4431 mle=1.2246 pcon=4.9869 forget=1.2316 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 220 total=7.4794 mle=1.2854 pcon=4.9863 forget=1.2078 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 270 total=7.4648 mle=1.2747 pcon=4.9861 forget=1.2040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 320 total=7.5220 mle=1.2916 pcon=4.9859 forget=1.2446 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 370 total=7.4835 mle=1.2807 pcon=4.9857 forget=1.2171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 30 total=7.4292 mle=1.2086 pcon=4.9854 forget=1.2351 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 80 total=7.5377 mle=1.3195 pcon=4.9850 forget=1.2332 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 130 total=7.4905 mle=1.2880 pcon=4.9846 forget=1.2179 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 180 total=7.4484 mle=1.2336 pcon=4.9841 forget=1.2307 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 230 total=7.4946 mle=1.2945 pcon=4.9839 forget=1.2162 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 280 total=7.5936 mle=1.3875 pcon=4.9837 forget=1.2224 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 330 total=7.7021 mle=1.4928 pcon=4.9833 forget=1.2260 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 380 total=7.4775 mle=1.2723 pcon=4.9828 forget=1.2225 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 40 total=7.4963 mle=1.3028 pcon=4.9824 forget=1.2110 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 90 total=7.5405 mle=1.3164 pcon=4.9822 forget=1.2419 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 140 total=7.5280 mle=1.3403 pcon=4.9820 forget=1.2057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 190 total=7.4356 mle=1.2378 pcon=4.9817 forget=1.2161 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 240 total=7.5358 mle=1.3093 pcon=4.9814 forget=1.2451 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 290 total=7.4771 mle=1.2642 pcon=4.9812 forget=1.2317 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 340 total=7.5314 mle=1.3048 pcon=4.9811 forget=1.2456 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:24,  2.70it/s]  2%|▏         | 6/391 [00:00<00:25, 15.32it/s]  4%|▍         | 15/391 [00:00<00:10, 35.06it/s]  6%|▌         | 24/391 [00:00<00:07, 50.20it/s]  9%|▊         | 34/391 [00:00<00:05, 63.19it/s] 11%|█▏        | 44/391 [00:00<00:04, 71.78it/s] 14%|█▍        | 54/391 [00:01<00:04, 78.86it/s] 16%|█▋        | 64/391 [00:01<00:03, 83.85it/s] 19%|█▉        | 74/391 [00:01<00:03, 87.11it/s] 21%|██▏       | 84/391 [00:01<00:03, 89.03it/s] 24%|██▍       | 94/391 [00:01<00:03, 90.98it/s] 27%|██▋       | 104/391 [00:01<00:03, 92.37it/s] 29%|██▉       | 114/391 [00:01<00:02, 93.48it/s] 32%|███▏      | 124/391 [00:01<00:02, 93.97it/s] 34%|███▍      | 134/391 [00:01<00:02, 94.61it/s] 37%|███▋      | 144/391 [00:01<00:02, 94.97it/s] 39%|███▉      | 154/391 [00:02<00:02, 95.07it/s] 42%|████▏     | 164/391 [00:02<00:02, 95.33it/s] 45%|████▍     | 174/391 [00:02<00:02, 93.98it/s] 47%|████▋     | 184/391 [00:02<00:02, 93.69it/s] 50%|████▉     | 194/391 [00:02<00:02, 93.98it/s] 52%|█████▏    | 204/391 [00:02<00:01, 94.46it/s] 55%|█████▍    | 214/391 [00:02<00:01, 94.71it/s] 57%|█████▋    | 224/391 [00:02<00:01, 94.92it/s] 60%|█████▉    | 234/391 [00:02<00:01, 93.90it/s] 62%|██████▏   | 244/391 [00:03<00:01, 94.28it/s] 65%|██████▍   | 254/391 [00:03<00:01, 94.94it/s] 68%|██████▊   | 264/391 [00:03<00:01, 94.94it/s] 70%|███████   | 274/391 [00:03<00:01, 94.79it/s] 73%|███████▎  | 284/391 [00:03<00:01, 95.10it/s] 75%|███████▌  | 294/391 [00:03<00:01, 95.22it/s] 78%|███████▊  | 304/391 [00:03<00:00, 95.58it/s] 80%|████████  | 314/391 [00:03<00:00, 95.86it/s] 83%|████████▎ | 324/391 [00:03<00:00, 95.51it/s] 85%|████████▌ | 334/391 [00:03<00:00, 95.70it/s] 88%|████████▊ | 344/391 [00:04<00:00, 95.59it/s] 91%|█████████ | 354/391 [00:04<00:00, 95.69it/s] 93%|█████████▎| 364/391 [00:04<00:00, 95.39it/s] 96%|█████████▌| 374/391 [00:04<00:00, 95.88it/s] 98%|█████████▊| 384/391 [00:04<00:00, 96.22it/s]100%|██████████| 391/391 [00:04<00:00, 85.69it/s]
50000 images processed, 4.6586384773254395 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:30,  2.57it/s] 10%|█         | 8/79 [00:00<00:03, 20.47it/s] 22%|██▏       | 17/79 [00:00<00:01, 38.62it/s] 33%|███▎      | 26/79 [00:00<00:01, 52.81it/s] 44%|████▍     | 35/79 [00:00<00:00, 62.71it/s] 56%|█████▌    | 44/79 [00:00<00:00, 69.31it/s] 68%|██████▊   | 54/79 [00:01<00:00, 76.19it/s] 81%|████████  | 64/79 [00:01<00:00, 81.24it/s] 94%|█████████▎| 74/79 [00:01<00:00, 84.74it/s]100%|██████████| 79/79 [00:02<00:00, 32.26it/s]
10000 images processed, 2.5119283199310303 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:39,  2.05it/s]  4%|▍         | 8/204 [00:00<00:11, 17.06it/s]  8%|▊         | 17/204 [00:00<00:05, 33.86it/s] 13%|█▎        | 26/204 [00:00<00:03, 47.09it/s] 17%|█▋        | 35/204 [00:00<00:02, 58.06it/s] 22%|██▏       | 45/204 [00:01<00:02, 67.87it/s] 27%|██▋       | 55/204 [00:01<00:01, 75.26it/s] 31%|███▏      | 64/204 [00:01<00:01, 79.26it/s] 36%|███▋      | 74/204 [00:01<00:01, 83.93it/s] 41%|████      | 84/204 [00:01<00:01, 86.85it/s] 46%|████▌     | 94/204 [00:01<00:01, 88.07it/s] 51%|█████     | 104/204 [00:01<00:01, 85.77it/s] 55%|█████▌    | 113/204 [00:01<00:01, 83.90it/s] 60%|█████▉    | 122/204 [00:01<00:01, 78.68it/s] 64%|██████▍   | 131/204 [00:02<00:00, 80.87it/s] 69%|██████▊   | 140/204 [00:02<00:00, 82.08it/s] 74%|███████▎  | 150/204 [00:02<00:00, 84.61it/s] 78%|███████▊  | 159/204 [00:02<00:00, 86.05it/s] 83%|████████▎ | 169/204 [00:02<00:00, 88.67it/s] 88%|████████▊ | 179/204 [00:02<00:00, 90.57it/s] 93%|█████████▎| 189/204 [00:02<00:00, 92.10it/s] 98%|█████████▊| 199/204 [00:02<00:00, 93.27it/s]100%|██████████| 204/204 [00:02<00:00, 72.39it/s]
26032 images processed, 2.8569345474243164 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:08,  1.14it/s]  9%|▉         | 7/79 [00:00<00:07,  9.33it/s] 22%|██▏       | 17/79 [00:01<00:02, 23.94it/s] 34%|███▍      | 27/79 [00:01<00:01, 37.61it/s] 46%|████▌     | 36/79 [00:01<00:00, 48.57it/s] 57%|█████▋    | 45/79 [00:01<00:00, 57.74it/s] 68%|██████▊   | 54/79 [00:01<00:00, 64.69it/s] 81%|████████  | 64/79 [00:01<00:00, 72.85it/s] 94%|█████████▎| 74/79 [00:01<00:00, 78.40it/s]100%|██████████| 79/79 [00:01<00:00, 44.55it/s]
10000 images processed, 1.8084824085235596 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:38,  2.01it/s] 11%|█▏        | 9/79 [00:00<00:03, 19.21it/s] 24%|██▍       | 19/79 [00:00<00:01, 37.70it/s] 37%|███▋      | 29/79 [00:00<00:00, 52.45it/s] 48%|████▊     | 38/79 [00:00<00:00, 61.76it/s] 61%|██████    | 48/79 [00:01<00:00, 69.94it/s] 73%|███████▎  | 58/79 [00:01<00:00, 77.08it/s] 86%|████████▌ | 68/79 [00:01<00:00, 82.41it/s] 99%|█████████▊| 78/79 [00:01<00:00, 86.23it/s]100%|██████████| 79/79 [00:01<00:00, 58.26it/s]
10000 images processed, 1.3784658908843994 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:49,  1.41it/s] 11%|█▏        | 8/70 [00:00<00:04, 12.76it/s] 21%|██▏       | 15/70 [00:00<00:02, 23.13it/s] 29%|██▊       | 20/70 [00:01<00:01, 28.36it/s] 40%|████      | 28/70 [00:01<00:01, 38.38it/s] 51%|█████▏    | 36/70 [00:01<00:00, 48.18it/s] 64%|██████▍   | 45/70 [00:01<00:00, 58.75it/s] 79%|███████▊  | 55/70 [00:01<00:00, 68.81it/s] 93%|█████████▎| 65/70 [00:01<00:00, 76.42it/s]100%|██████████| 70/70 [00:01<00:00, 43.04it/s]
8925 images processed, 1.6592307090759277 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:41,  1.07it/s] 11%|█         | 5/45 [00:01<00:07,  5.49it/s] 16%|█▌        | 7/45 [00:01<00:05,  6.71it/s] 36%|███▌      | 16/45 [00:01<00:01, 19.26it/s] 47%|████▋     | 21/45 [00:01<00:01, 18.32it/s] 56%|█████▌    | 25/45 [00:01<00:00, 20.28it/s] 73%|███████▎  | 33/45 [00:02<00:00, 26.18it/s] 87%|████████▋ | 39/45 [00:02<00:00, 27.28it/s]100%|██████████| 45/45 [00:02<00:00, 31.96it/s]100%|██████████| 45/45 [00:02<00:00, 18.78it/s]
5640 images processed, 2.426650285720825 seconds used

19.050232410430908
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.86  99.31  97.84
places365     71.46  80.55  78.53
LSUN          19.20  96.01  96.06
iSUN          74.63  81.59  84.26
dtd           39.49  91.10  94.14
AVG           41.53  89.71  90.17
Retain-Acc: 0.7466
Forget-as-OOD (retain known vs forget novel):
  FPR: 71.90 AUROC: 87.89 AUIN: 98.48
7.865838050842285
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_rf.png
[Run] lambda=0.1 lr=0.001 epochs=45
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=45, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.1, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/45 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/45 [00:19<14:09, 19.32s/it]  4%|▍         | 2/45 [00:34<12:09, 16.97s/it]  7%|▋         | 3/45 [00:49<11:04, 15.83s/it]  9%|▉         | 4/45 [01:03<10:26, 15.27s/it] 11%|█         | 5/45 [01:18<09:59, 14.98s/it] 13%|█▎        | 6/45 [01:32<09:42, 14.95s/it][loss] ep 0 it 0 total=7.8076 mle=1.2802 pcon=5.2951 forget=1.2324 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=7.8925 mle=1.3922 pcon=5.2887 forget=1.2116 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=8.0160 mle=1.5079 pcon=5.2827 forget=1.2253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=8.0596 mle=1.5643 pcon=5.2770 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=8.0045 mle=1.5330 pcon=5.2708 forget=1.2007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=7.8786 mle=1.3805 pcon=5.2650 forget=1.2331 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=7.7786 mle=1.2922 pcon=5.2596 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=7.9326 mle=1.4379 pcon=5.2543 forget=1.2405 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 1 it 10 total=7.7857 mle=1.3065 pcon=5.2487 forget=1.2305 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=7.7983 mle=1.3161 pcon=5.2433 forget=1.2389 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=7.7468 mle=1.2902 pcon=5.2382 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=7.7126 mle=1.2381 pcon=5.2331 forget=1.2415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=7.8319 mle=1.3724 pcon=5.2283 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=7.7791 mle=1.3468 pcon=5.2235 forget=1.2088 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=7.7558 mle=1.3081 pcon=5.2185 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=7.8229 mle=1.3766 pcon=5.2136 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 2 it 20 total=7.7062 mle=1.2744 pcon=5.2089 forget=1.2228 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=7.8501 mle=1.3989 pcon=5.2044 forget=1.2468 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=7.7728 mle=1.3588 pcon=5.1998 forget=1.2142 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=7.7724 mle=1.3579 pcon=5.1954 forget=1.2190 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=7.7516 mle=1.3300 pcon=5.1908 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=7.7870 mle=1.3659 pcon=5.1865 forget=1.2346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=7.6577 mle=1.2606 pcon=5.1824 forget=1.2146 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=7.7848 mle=1.3834 pcon=5.1787 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 3 it 30 total=7.7684 mle=1.3638 pcon=5.1748 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=7.6956 mle=1.2957 pcon=5.1709 forget=1.2290 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=7.6784 mle=1.2905 pcon=5.1672 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=7.8326 mle=1.4601 pcon=5.1635 forget=1.2090 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=7.7463 mle=1.3524 pcon=5.1601 forget=1.2338 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=7.8179 mle=1.4268 pcon=5.1568 forget=1.2344 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=7.8238 mle=1.4482 pcon=5.1531 forget=1.2225 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=7.6898 mle=1.3142 pcon=5.1497 forget=1.2259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 4 it 40 total=7.7406 mle=1.3771 pcon=5.1464 forget=1.2171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=7.7165 mle=1.3335 pcon=5.1434 forget=1.2396 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=7.9300 mle=1.5854 pcon=5.1406 forget=1.2041 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=7.7627 mle=1.3941 pcon=5.1374 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=7.7866 mle=1.4143 pcon=5.1344 forget=1.2379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=7.5438 mle=1.1861 pcon=5.1316 forget=1.2262 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=7.8147 mle=1.4611 pcon=5.1290 forget=1.2246 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 5 it 0 total=7.5816 mle=1.2048 pcon=5.1261 forget=1.2507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=7.7230 mle=1.3648 pcon=5.1234 forget=1.2348 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=7.7646 mle=1.4002 pcon=5.1206 forget=1.2438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=7.8338 mle=1.4810 pcon=5.1178 forget=1.2350 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=7.7883 mle=1.4524 pcon=5.1151 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=7.6862 mle=1.3483 pcon=5.1124 forget=1.2256 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=7.7564 mle=1.4294 pcon=5.1098 forget=1.2172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=7.7336 mle=1.4045 pcon=5.1071 forget=1.2220 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 6 it 10 total=7.6346 mle=1.2782 pcon=5.1047 forget=1.2516 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=7.7001 mle=1.3788 pcon=5.1024 forget=1.2189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=7.6562 mle=1.3184 pcon=5.1002 forget=1.2376 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=7.6598 mle=1.3321 pcon=5.0979 forget=1.2298 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=7.6925 mle=1.3653 pcon=5.0954 forget=1.2318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=7.7162 mle=1.3836 pcon=5.0933 forget=1.2393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=7.6565 mle=1.3538 pcon=5.0913 forget=1.2115 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 16%|█▌        | 7/45 [01:47<09:26, 14.92s/it] 18%|█▊        | 8/45 [02:02<09:15, 15.01s/it] 20%|██        | 9/45 [02:17<08:51, 14.76s/it] 22%|██▏       | 10/45 [02:31<08:34, 14.70s/it] 24%|██▍       | 11/45 [02:46<08:17, 14.62s/it] 27%|██▋       | 12/45 [03:01<08:06, 14.74s/it] 29%|██▉       | 13/45 [03:15<07:48, 14.65s/it][loss] ep 6 it 360 total=7.7497 mle=1.4566 pcon=5.0891 forget=1.2040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 7 it 20 total=7.5653 mle=1.2500 pcon=5.0870 forget=1.2283 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=7.5608 mle=1.2464 pcon=5.0847 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=7.6923 mle=1.3880 pcon=5.0827 forget=1.2216 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=7.7175 mle=1.4329 pcon=5.0808 forget=1.2038 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=7.6182 mle=1.3207 pcon=5.0791 forget=1.2184 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=7.7539 mle=1.4543 pcon=5.0775 forget=1.2221 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=7.6236 mle=1.3172 pcon=5.0753 forget=1.2311 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=7.6912 mle=1.3713 pcon=5.0733 forget=1.2467 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 8 it 30 total=7.7576 mle=1.4809 pcon=5.0716 forget=1.2052 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=7.5763 mle=1.2755 pcon=5.0699 forget=1.2309 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=7.6954 mle=1.3992 pcon=5.0682 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=7.6397 mle=1.3457 pcon=5.0665 forget=1.2275 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=7.6602 mle=1.3662 pcon=5.0648 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=7.5536 mle=1.2578 pcon=5.0632 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=7.8331 mle=1.5802 pcon=5.0614 forget=1.1915 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=7.6516 mle=1.3648 pcon=5.0600 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 9 it 40 total=7.6995 mle=1.4117 pcon=5.0585 forget=1.2293 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=7.7619 mle=1.4933 pcon=5.0570 forget=1.2117 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=7.6462 mle=1.3547 pcon=5.0559 forget=1.2356 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=7.5182 mle=1.2290 pcon=5.0546 forget=1.2346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=7.7622 mle=1.4986 pcon=5.0535 forget=1.2101 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=7.6412 mle=1.3594 pcon=5.0520 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=7.5895 mle=1.3079 pcon=5.0508 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 10 it 0 total=7.5544 mle=1.2683 pcon=5.0491 forget=1.2369 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=7.5333 mle=1.2585 pcon=5.0482 forget=1.2265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=7.5937 mle=1.3282 pcon=5.0468 forget=1.2187 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=7.6346 mle=1.3451 pcon=5.0456 forget=1.2438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=7.6848 mle=1.4099 pcon=5.0446 forget=1.2303 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=7.6988 mle=1.4288 pcon=5.0436 forget=1.2265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=7.7519 mle=1.5150 pcon=5.0424 forget=1.1946 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=7.5336 mle=1.2627 pcon=5.0415 forget=1.2294 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 11 it 10 total=7.7022 mle=1.4430 pcon=5.0403 forget=1.2188 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=7.6331 mle=1.3737 pcon=5.0394 forget=1.2200 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=7.6151 mle=1.3486 pcon=5.0385 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=7.5511 mle=1.2743 pcon=5.0374 forget=1.2394 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=7.5317 mle=1.2650 pcon=5.0365 forget=1.2302 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=7.5273 mle=1.2694 pcon=5.0355 forget=1.2224 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=7.6543 mle=1.3937 pcon=5.0347 forget=1.2259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=7.6001 mle=1.3220 pcon=5.0340 forget=1.2441 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 12 it 20 total=7.5818 mle=1.3164 pcon=5.0328 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=7.6195 mle=1.3558 pcon=5.0318 forget=1.2319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=7.6046 mle=1.3655 pcon=5.0307 forget=1.2084 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=7.6127 mle=1.3434 pcon=5.0297 forget=1.2397 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=7.4998 mle=1.2266 pcon=5.0291 forget=1.2441 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=7.5466 mle=1.2976 pcon=5.0285 forget=1.2205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=7.6746 mle=1.4167 pcon=5.0279 forget=1.2300 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=7.6427 mle=1.3872 pcon=5.0273 forget=1.2282 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 13 it 30 total=7.5886 mle=1.3175 pcon=5.0268 forget=1.2443 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=7.7436 mle=1.5041 pcon=5.0261 forget=1.2134 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=7.6131 mle=1.3631 pcon=5.0253 forget=1.2247 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=7.6319 mle=1.3600 pcon=5.0242 forget=1.2476 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 31%|███       | 14/45 [03:30<07:36, 14.73s/it] 33%|███▎      | 15/45 [03:44<07:19, 14.63s/it] 36%|███▌      | 16/45 [04:00<07:11, 14.89s/it] 38%|███▊      | 17/45 [04:15<06:57, 14.93s/it] 40%|████      | 18/45 [04:30<06:44, 14.99s/it] 42%|████▏     | 19/45 [04:45<06:26, 14.86s/it] 44%|████▍     | 20/45 [04:59<06:04, 14.56s/it][loss] ep 13 it 230 total=7.5505 mle=1.3047 pcon=5.0236 forget=1.2222 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=7.5047 mle=1.2266 pcon=5.0230 forget=1.2551 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=7.6448 mle=1.4003 pcon=5.0225 forget=1.2220 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=7.5709 mle=1.3284 pcon=5.0220 forget=1.2205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 14 it 40 total=7.5227 mle=1.2781 pcon=5.0213 forget=1.2233 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=7.5821 mle=1.3276 pcon=5.0209 forget=1.2336 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=7.5643 mle=1.3039 pcon=5.0204 forget=1.2400 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=7.5229 mle=1.2667 pcon=5.0200 forget=1.2361 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=7.5796 mle=1.3255 pcon=5.0192 forget=1.2349 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=7.5251 mle=1.2936 pcon=5.0188 forget=1.2127 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=7.4855 mle=1.2461 pcon=5.0185 forget=1.2209 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 15 it 0 total=7.5658 mle=1.3366 pcon=5.0178 forget=1.2114 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=7.4742 mle=1.2422 pcon=5.0176 forget=1.2145 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=7.5885 mle=1.3662 pcon=5.0174 forget=1.2049 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=7.5316 mle=1.2891 pcon=5.0170 forget=1.2254 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=7.5064 mle=1.2571 pcon=5.0168 forget=1.2324 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=7.5255 mle=1.2830 pcon=5.0164 forget=1.2262 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=7.5971 mle=1.3897 pcon=5.0160 forget=1.1913 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=7.6233 mle=1.4016 pcon=5.0158 forget=1.2059 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 16 it 10 total=7.4295 mle=1.1818 pcon=5.0154 forget=1.2323 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=7.4455 mle=1.2303 pcon=5.0153 forget=1.1999 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=7.4358 mle=1.2038 pcon=5.0150 forget=1.2170 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=7.5240 mle=1.2735 pcon=5.0149 forget=1.2356 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=7.5150 mle=1.2541 pcon=5.0147 forget=1.2461 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=7.6549 mle=1.4107 pcon=5.0142 forget=1.2301 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=7.6713 mle=1.4460 pcon=5.0138 forget=1.2116 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=7.5517 mle=1.3156 pcon=5.0136 forget=1.2225 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 17 it 20 total=7.5916 mle=1.3824 pcon=5.0137 forget=1.1956 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=7.6163 mle=1.3656 pcon=5.0138 forget=1.2370 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=7.4767 mle=1.2427 pcon=5.0134 forget=1.2206 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=7.5661 mle=1.3324 pcon=5.0131 forget=1.2206 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=7.5706 mle=1.3690 pcon=5.0129 forget=1.1886 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=7.5749 mle=1.3458 pcon=5.0128 forget=1.2163 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=7.4775 mle=1.2578 pcon=5.0127 forget=1.2070 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=7.5248 mle=1.3313 pcon=5.0124 forget=1.1812 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 18 it 30 total=7.4174 mle=1.1959 pcon=5.0123 forget=1.2092 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=7.6460 mle=1.4292 pcon=5.0123 forget=1.2044 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=7.6088 mle=1.3822 pcon=5.0122 forget=1.2144 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=7.4811 mle=1.2698 pcon=5.0123 forget=1.1990 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=7.5693 mle=1.3515 pcon=5.0121 forget=1.2057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=7.5733 mle=1.3402 pcon=5.0119 forget=1.2212 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=7.5704 mle=1.3385 pcon=5.0118 forget=1.2201 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=7.4296 mle=1.2215 pcon=5.0119 forget=1.1962 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 19 it 40 total=7.4255 mle=1.2048 pcon=5.0119 forget=1.2088 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=7.5047 mle=1.2952 pcon=5.0116 forget=1.1978 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=7.4346 mle=1.2249 pcon=5.0115 forget=1.1982 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=7.4704 mle=1.2813 pcon=5.0113 forget=1.1777 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=7.4613 mle=1.2400 pcon=5.0112 forget=1.2102 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=7.5013 mle=1.3085 pcon=5.0116 forget=1.1812 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=7.4940 mle=1.2620 pcon=5.0120 forget=1.2200 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 20 it 0 total=7.4939 mle=1.2908 pcon=5.0121 forget=1.1910 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=7.5256 mle=1.2917 pcon=5.0122 forget=1.2216 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 47%|████▋     | 21/45 [05:13<05:46, 14.43s/it] 49%|████▉     | 22/45 [05:26<05:27, 14.23s/it] 51%|█████     | 23/45 [05:42<05:19, 14.52s/it] 53%|█████▎    | 24/45 [05:55<05:00, 14.29s/it] 56%|█████▌    | 25/45 [06:10<04:47, 14.36s/it] 58%|█████▊    | 26/45 [06:24<04:31, 14.28s/it] 60%|██████    | 27/45 [06:37<04:12, 14.04s/it][loss] ep 20 it 100 total=7.4076 mle=1.2046 pcon=5.0120 forget=1.1910 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=7.4571 mle=1.2599 pcon=5.0121 forget=1.1851 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=7.5619 mle=1.3464 pcon=5.0120 forget=1.2035 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=7.5414 mle=1.3242 pcon=5.0121 forget=1.2051 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=7.5546 mle=1.3527 pcon=5.0122 forget=1.1897 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=7.5314 mle=1.3301 pcon=5.0121 forget=1.1893 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 21 it 10 total=7.3067 mle=1.1149 pcon=5.0118 forget=1.1800 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=7.4246 mle=1.2175 pcon=5.0117 forget=1.1953 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=7.5150 mle=1.3249 pcon=5.0115 forget=1.1785 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=7.4801 mle=1.3022 pcon=5.0113 forget=1.1666 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=7.4021 mle=1.2052 pcon=5.0110 forget=1.1859 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=7.5378 mle=1.3787 pcon=5.0109 forget=1.1481 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=7.5377 mle=1.3372 pcon=5.0106 forget=1.1899 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=7.5054 mle=1.3081 pcon=5.0104 forget=1.1869 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 22 it 20 total=7.4667 mle=1.2865 pcon=5.0103 forget=1.1700 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=7.4456 mle=1.2567 pcon=5.0098 forget=1.1792 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=7.4990 mle=1.3186 pcon=5.0094 forget=1.1710 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=7.3913 mle=1.2311 pcon=5.0089 forget=1.1513 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=7.5060 mle=1.3426 pcon=5.0087 forget=1.1547 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=7.4261 mle=1.2578 pcon=5.0085 forget=1.1599 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=7.3632 mle=1.2319 pcon=5.0082 forget=1.1231 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=7.5155 mle=1.3719 pcon=5.0076 forget=1.1360 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 23 it 30 total=7.3967 mle=1.2495 pcon=5.0073 forget=1.1399 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=7.4276 mle=1.2506 pcon=5.0069 forget=1.1701 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=7.5132 mle=1.3661 pcon=5.0064 forget=1.1407 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=7.3962 mle=1.2641 pcon=5.0058 forget=1.1262 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=7.4854 mle=1.3526 pcon=5.0056 forget=1.1272 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=7.4786 mle=1.3294 pcon=5.0054 forget=1.1439 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=7.4898 mle=1.3422 pcon=5.0048 forget=1.1428 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=7.3659 mle=1.2215 pcon=5.0046 forget=1.1398 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 24 it 40 total=7.4121 mle=1.2637 pcon=5.0046 forget=1.1438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=7.4931 mle=1.3534 pcon=5.0043 forget=1.1353 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=7.3940 mle=1.2514 pcon=5.0040 forget=1.1385 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=7.4195 mle=1.2665 pcon=5.0041 forget=1.1488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=7.4690 mle=1.3139 pcon=5.0042 forget=1.1508 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=7.4543 mle=1.3119 pcon=5.0041 forget=1.1383 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=7.4309 mle=1.2722 pcon=5.0039 forget=1.1548 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 0 total=7.4630 mle=1.3119 pcon=5.0039 forget=1.1472 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 50 total=7.5354 mle=1.3917 pcon=5.0039 forget=1.1398 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 100 total=7.5452 mle=1.3914 pcon=5.0038 forget=1.1500 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 150 total=7.4989 mle=1.3471 pcon=5.0039 forget=1.1479 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 200 total=7.5002 mle=1.3421 pcon=5.0040 forget=1.1541 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 250 total=7.4383 mle=1.2685 pcon=5.0043 forget=1.1654 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 300 total=7.5030 mle=1.3358 pcon=5.0046 forget=1.1626 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 350 total=7.5697 mle=1.3934 pcon=5.0046 forget=1.1717 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 10 total=7.4897 mle=1.3072 pcon=5.0048 forget=1.1776 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 60 total=7.5072 mle=1.3341 pcon=5.0049 forget=1.1681 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 110 total=7.5086 mle=1.3356 pcon=5.0052 forget=1.1678 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 160 total=7.4577 mle=1.2751 pcon=5.0053 forget=1.1773 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 210 total=7.5617 mle=1.3802 pcon=5.0055 forget=1.1760 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 260 total=7.4763 mle=1.2919 pcon=5.0061 forget=1.1783 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 310 total=7.6723 mle=1.5033 pcon=5.0063 forget=1.1626 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 360 total=7.6261 mle=1.4490 pcon=5.0067 forget=1.1704 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 20 total=7.4853 mle=1.2918 pcon=5.0070 forget=1.1865 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 70 total=7.5131 mle=1.3291 pcon=5.0071 forget=1.1769 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 120 total=7.5560 mle=1.3483 pcon=5.0076 forget=1.2001 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 62%|██████▏   | 28/45 [06:50<03:51, 13.65s/it] 64%|██████▍   | 29/45 [07:04<03:38, 13.64s/it] 67%|██████▋   | 30/45 [07:18<03:26, 13.77s/it] 69%|██████▉   | 31/45 [07:31<03:09, 13.56s/it] 71%|███████   | 32/45 [07:44<02:54, 13.43s/it] 73%|███████▎  | 33/45 [07:58<02:41, 13.48s/it] 76%|███████▌  | 34/45 [08:11<02:28, 13.46s/it] 78%|███████▊  | 35/45 [08:24<02:12, 13.26s/it][loss] ep 27 it 170 total=7.5239 mle=1.3217 pcon=5.0079 forget=1.1943 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 220 total=7.5330 mle=1.3398 pcon=5.0085 forget=1.1847 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 270 total=7.5343 mle=1.3317 pcon=5.0087 forget=1.1939 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 320 total=7.5319 mle=1.3184 pcon=5.0091 forget=1.2044 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 370 total=7.4769 mle=1.2729 pcon=5.0094 forget=1.1945 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 30 total=7.5124 mle=1.2989 pcon=5.0099 forget=1.2036 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 80 total=7.5969 mle=1.3965 pcon=5.0100 forget=1.1903 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 130 total=7.4473 mle=1.2574 pcon=5.0105 forget=1.1794 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 180 total=7.4769 mle=1.2659 pcon=5.0104 forget=1.2006 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 230 total=7.5266 mle=1.3255 pcon=5.0107 forget=1.1904 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 280 total=7.5360 mle=1.3332 pcon=5.0108 forget=1.1920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 330 total=7.4832 mle=1.2874 pcon=5.0109 forget=1.1849 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 380 total=7.5154 mle=1.3128 pcon=5.0108 forget=1.1918 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 40 total=7.5293 mle=1.3381 pcon=5.0108 forget=1.1804 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 90 total=7.5214 mle=1.3119 pcon=5.0108 forget=1.1987 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 140 total=7.4821 mle=1.2847 pcon=5.0109 forget=1.1865 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 190 total=7.6694 mle=1.4620 pcon=5.0106 forget=1.1967 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 240 total=7.5934 mle=1.3935 pcon=5.0107 forget=1.1892 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 290 total=7.4883 mle=1.2931 pcon=5.0103 forget=1.1849 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 340 total=7.5203 mle=1.3254 pcon=5.0104 forget=1.1845 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 0 total=7.4967 mle=1.2948 pcon=5.0102 forget=1.1917 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 50 total=7.5649 mle=1.3769 pcon=5.0098 forget=1.1781 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 100 total=7.5182 mle=1.3275 pcon=5.0099 forget=1.1807 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 150 total=7.4892 mle=1.2988 pcon=5.0098 forget=1.1807 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 200 total=7.5462 mle=1.3641 pcon=5.0097 forget=1.1723 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 250 total=7.5737 mle=1.3955 pcon=5.0097 forget=1.1685 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 300 total=7.5882 mle=1.4081 pcon=5.0092 forget=1.1709 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 350 total=7.5696 mle=1.3662 pcon=5.0089 forget=1.1945 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 10 total=7.5940 mle=1.4062 pcon=5.0085 forget=1.1793 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 60 total=7.4735 mle=1.2878 pcon=5.0082 forget=1.1774 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 110 total=7.5948 mle=1.4193 pcon=5.0081 forget=1.1673 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 160 total=7.4976 mle=1.3102 pcon=5.0076 forget=1.1799 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 210 total=7.6026 mle=1.4334 pcon=5.0070 forget=1.1621 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 260 total=7.5383 mle=1.3516 pcon=5.0067 forget=1.1800 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 310 total=7.5020 mle=1.3092 pcon=5.0062 forget=1.1867 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 360 total=7.4602 mle=1.2718 pcon=5.0061 forget=1.1824 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 20 total=7.4326 mle=1.2384 pcon=5.0060 forget=1.1883 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 70 total=7.4539 mle=1.2744 pcon=5.0058 forget=1.1738 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 120 total=7.4528 mle=1.2685 pcon=5.0050 forget=1.1793 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 170 total=7.4791 mle=1.2824 pcon=5.0047 forget=1.1921 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 220 total=7.5569 mle=1.3738 pcon=5.0039 forget=1.1791 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 270 total=7.4708 mle=1.2852 pcon=5.0033 forget=1.1823 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 320 total=7.5095 mle=1.3164 pcon=5.0024 forget=1.1908 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 370 total=7.4070 mle=1.2298 pcon=5.0019 forget=1.1753 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 30 total=7.5718 mle=1.3854 pcon=5.0013 forget=1.1851 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 80 total=7.5166 mle=1.3348 pcon=5.0006 forget=1.1812 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 130 total=7.5143 mle=1.3459 pcon=5.0001 forget=1.1683 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 180 total=7.4991 mle=1.3290 pcon=4.9994 forget=1.1707 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 230 total=7.5196 mle=1.3515 pcon=4.9990 forget=1.1691 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 280 total=7.4637 mle=1.3005 pcon=4.9980 forget=1.1653 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 330 total=7.5911 mle=1.4061 pcon=4.9971 forget=1.1878 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 380 total=7.4423 mle=1.2602 pcon=4.9963 forget=1.1857 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 40 total=7.5200 mle=1.3485 pcon=4.9954 forget=1.1760 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 90 total=7.4804 mle=1.3096 pcon=4.9946 forget=1.1762 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 140 total=7.5318 mle=1.3598 pcon=4.9936 forget=1.1784 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 190 total=7.4409 mle=1.2760 pcon=4.9927 forget=1.1722 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 240 total=7.4644 mle=1.2686 pcon=4.9919 forget=1.2039 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 290 total=7.4455 mle=1.2861 pcon=4.9911 forget=1.1682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 340 total=7.4443 mle=1.2474 pcon=4.9901 forget=1.2067 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 0 total=7.4497 mle=1.2682 pcon=4.9894 forget=1.1920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 50 total=7.5225 mle=1.3458 pcon=4.9889 forget=1.1878 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 100 total=7.4930 mle=1.3055 pcon=4.9882 forget=1.1994 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 80%|████████  | 36/45 [08:37<02:00, 13.34s/it] 82%|████████▏ | 37/45 [08:50<01:45, 13.23s/it] 84%|████████▍ | 38/45 [09:03<01:30, 12.93s/it] 87%|████████▋ | 39/45 [09:16<01:17, 12.99s/it] 89%|████████▉ | 40/45 [09:28<01:04, 12.87s/it] 91%|█████████ | 41/45 [09:41<00:51, 12.95s/it] 93%|█████████▎| 42/45 [09:54<00:38, 12.93s/it] 96%|█████████▌| 43/45 [10:07<00:25, 12.99s/it][loss] ep 35 it 150 total=7.3896 mle=1.2022 pcon=4.9871 forget=1.2002 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 200 total=7.4853 mle=1.3138 pcon=4.9863 forget=1.1852 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 250 total=7.5214 mle=1.3301 pcon=4.9856 forget=1.2056 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 300 total=7.5010 mle=1.3112 pcon=4.9846 forget=1.2052 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 350 total=7.5207 mle=1.3295 pcon=4.9838 forget=1.2073 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 10 total=7.5217 mle=1.3487 pcon=4.9830 forget=1.1900 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 60 total=7.5181 mle=1.3365 pcon=4.9820 forget=1.1996 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 110 total=7.6529 mle=1.4671 pcon=4.9813 forget=1.2044 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 160 total=7.5613 mle=1.3780 pcon=4.9806 forget=1.2028 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 210 total=7.6046 mle=1.4285 pcon=4.9796 forget=1.1965 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 260 total=7.5289 mle=1.3652 pcon=4.9787 forget=1.1850 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 310 total=7.4276 mle=1.2420 pcon=4.9780 forget=1.2075 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 360 total=7.4681 mle=1.2879 pcon=4.9777 forget=1.2026 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 20 total=7.5051 mle=1.3024 pcon=4.9768 forget=1.2258 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 70 total=7.5726 mle=1.3837 pcon=4.9762 forget=1.2127 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 120 total=7.4678 mle=1.3073 pcon=4.9752 forget=1.1854 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 170 total=7.4373 mle=1.2466 pcon=4.9747 forget=1.2160 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 220 total=7.4737 mle=1.3041 pcon=4.9737 forget=1.1958 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 270 total=7.4388 mle=1.2803 pcon=4.9731 forget=1.1854 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 320 total=7.4855 mle=1.2835 pcon=4.9725 forget=1.2295 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 370 total=7.4557 mle=1.2792 pcon=4.9719 forget=1.2046 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 30 total=7.4181 mle=1.2262 pcon=4.9713 forget=1.2205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 80 total=7.5220 mle=1.3335 pcon=4.9705 forget=1.2180 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 130 total=7.4739 mle=1.2965 pcon=4.9697 forget=1.2076 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 180 total=7.4119 mle=1.2244 pcon=4.9689 forget=1.2186 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 230 total=7.4880 mle=1.3110 pcon=4.9683 forget=1.2087 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 280 total=7.5669 mle=1.3841 pcon=4.9677 forget=1.2151 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 330 total=7.6717 mle=1.4855 pcon=4.9670 forget=1.2192 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 380 total=7.4565 mle=1.2772 pcon=4.9662 forget=1.2131 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 40 total=7.4613 mle=1.2916 pcon=4.9654 forget=1.2042 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 90 total=7.5280 mle=1.3321 pcon=4.9648 forget=1.2310 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 140 total=7.5233 mle=1.3590 pcon=4.9643 forget=1.2000 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 190 total=7.3973 mle=1.2201 pcon=4.9637 forget=1.2134 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 240 total=7.5206 mle=1.3158 pcon=4.9632 forget=1.2416 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 290 total=7.4613 mle=1.2691 pcon=4.9625 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 340 total=7.5063 mle=1.3035 pcon=4.9621 forget=1.2406 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 0 total=7.4405 mle=1.2484 pcon=4.9615 forget=1.2306 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 50 total=7.5142 mle=1.3254 pcon=4.9609 forget=1.2279 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 100 total=7.4399 mle=1.2520 pcon=4.9604 forget=1.2275 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 150 total=7.4877 mle=1.2877 pcon=4.9599 forget=1.2401 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 200 total=7.4649 mle=1.2838 pcon=4.9594 forget=1.2217 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 250 total=7.5660 mle=1.3817 pcon=4.9587 forget=1.2256 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 300 total=7.5754 mle=1.3765 pcon=4.9583 forget=1.2407 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 350 total=7.4565 mle=1.2782 pcon=4.9577 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 10 total=7.5812 mle=1.4045 pcon=4.9571 forget=1.2195 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 60 total=7.4723 mle=1.2878 pcon=4.9570 forget=1.2275 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 110 total=7.5087 mle=1.3129 pcon=4.9565 forget=1.2394 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 160 total=7.3966 mle=1.2297 pcon=4.9559 forget=1.2110 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 210 total=7.4517 mle=1.2760 pcon=4.9553 forget=1.2205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 260 total=7.4115 mle=1.2289 pcon=4.9549 forget=1.2278 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 310 total=7.4966 mle=1.2994 pcon=4.9546 forget=1.2425 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 360 total=7.4012 mle=1.2117 pcon=4.9544 forget=1.2352 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 20 total=7.5112 mle=1.3274 pcon=4.9541 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 70 total=7.4414 mle=1.2578 pcon=4.9537 forget=1.2300 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 120 total=7.4493 mle=1.2910 pcon=4.9532 forget=1.2050 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 170 total=7.4906 mle=1.3015 pcon=4.9530 forget=1.2361 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 220 total=7.3902 mle=1.2157 pcon=4.9526 forget=1.2219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 270 total=7.5011 mle=1.3150 pcon=4.9520 forget=1.2341 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 320 total=7.5580 mle=1.3851 pcon=4.9516 forget=1.2213 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 370 total=7.5439 mle=1.3899 pcon=4.9512 forget=1.2028 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 30 total=7.5666 mle=1.4115 pcon=4.9509 forget=1.2042 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 80 total=7.5273 mle=1.3378 pcon=4.9505 forget=1.2391 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 98%|█████████▊| 44/45 [10:21<00:13, 13.05s/it]100%|██████████| 45/45 [10:34<00:00, 13.04s/it]100%|██████████| 45/45 [10:34<00:00, 14.09s/it]
[loss] ep 43 it 130 total=7.5599 mle=1.3872 pcon=4.9502 forget=1.2225 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 180 total=7.4137 mle=1.2225 pcon=4.9498 forget=1.2415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 230 total=7.4061 mle=1.2254 pcon=4.9496 forget=1.2310 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 280 total=7.4626 mle=1.2713 pcon=4.9494 forget=1.2419 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 330 total=7.5695 mle=1.3936 pcon=4.9491 forget=1.2269 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 380 total=7.4745 mle=1.2986 pcon=4.9489 forget=1.2271 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 40 total=7.4879 mle=1.3120 pcon=4.9486 forget=1.2273 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 90 total=7.4456 mle=1.2524 pcon=4.9483 forget=1.2448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 140 total=7.5105 mle=1.3296 pcon=4.9481 forget=1.2327 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 190 total=7.5261 mle=1.3890 pcon=4.9479 forget=1.1893 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 240 total=7.5468 mle=1.3792 pcon=4.9476 forget=1.2200 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 290 total=7.5374 mle=1.3680 pcon=4.9474 forget=1.2219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 340 total=7.4925 mle=1.3177 pcon=4.9473 forget=1.2274 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:18,  1.96it/s]  3%|▎         | 10/391 [00:00<00:18, 21.13it/s]  5%|▌         | 20/391 [00:00<00:09, 39.36it/s]  8%|▊         | 30/391 [00:00<00:06, 53.14it/s] 10%|█         | 40/391 [00:00<00:05, 63.65it/s] 13%|█▎        | 50/391 [00:01<00:04, 71.46it/s] 15%|█▌        | 60/391 [00:01<00:04, 77.89it/s] 18%|█▊        | 70/391 [00:01<00:03, 82.69it/s] 20%|██        | 80/391 [00:01<00:03, 83.14it/s] 23%|██▎       | 90/391 [00:01<00:03, 86.41it/s] 26%|██▌       | 100/391 [00:01<00:03, 87.94it/s] 28%|██▊       | 110/391 [00:01<00:03, 89.67it/s] 31%|███       | 120/391 [00:01<00:02, 91.61it/s] 33%|███▎      | 130/391 [00:01<00:02, 92.63it/s] 36%|███▌      | 140/391 [00:02<00:02, 92.50it/s] 38%|███▊      | 150/391 [00:02<00:02, 93.38it/s] 41%|████      | 160/391 [00:02<00:02, 93.43it/s] 43%|████▎     | 170/391 [00:02<00:02, 92.87it/s] 46%|████▌     | 180/391 [00:02<00:02, 91.81it/s] 49%|████▊     | 190/391 [00:02<00:02, 92.17it/s] 51%|█████     | 200/391 [00:02<00:02, 92.95it/s] 54%|█████▎    | 210/391 [00:02<00:01, 92.88it/s] 56%|█████▋    | 220/391 [00:02<00:01, 93.40it/s] 59%|█████▉    | 230/391 [00:02<00:01, 92.85it/s] 61%|██████▏   | 240/391 [00:03<00:01, 92.72it/s] 64%|██████▍   | 250/391 [00:03<00:01, 93.42it/s] 66%|██████▋   | 260/391 [00:03<00:01, 93.15it/s] 69%|██████▉   | 270/391 [00:03<00:01, 93.66it/s] 72%|███████▏  | 280/391 [00:03<00:01, 94.08it/s] 74%|███████▍  | 290/391 [00:03<00:01, 93.86it/s] 77%|███████▋  | 300/391 [00:03<00:00, 93.98it/s] 79%|███████▉  | 310/391 [00:03<00:00, 92.87it/s] 82%|████████▏ | 320/391 [00:03<00:00, 92.92it/s] 84%|████████▍ | 330/391 [00:04<00:00, 93.49it/s] 87%|████████▋ | 340/391 [00:04<00:00, 94.05it/s] 90%|████████▉ | 350/391 [00:04<00:00, 89.60it/s] 92%|█████████▏| 360/391 [00:04<00:00, 90.18it/s] 95%|█████████▍| 370/391 [00:04<00:00, 91.84it/s] 97%|█████████▋| 380/391 [00:04<00:00, 93.30it/s]100%|█████████▉| 390/391 [00:04<00:00, 94.34it/s]100%|██████████| 391/391 [00:04<00:00, 82.75it/s]
50000 images processed, 4.809167385101318 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:33,  2.33it/s]  8%|▊         | 6/79 [00:00<00:05, 14.09it/s] 16%|█▋        | 13/79 [00:00<00:02, 27.69it/s] 24%|██▍       | 19/79 [00:00<00:01, 35.20it/s] 35%|███▌      | 28/79 [00:00<00:01, 49.33it/s] 47%|████▋     | 37/79 [00:00<00:00, 59.40it/s] 58%|█████▊    | 46/79 [00:01<00:00, 67.61it/s] 71%|███████   | 56/79 [00:01<00:00, 74.91it/s] 84%|████████▎ | 66/79 [00:01<00:00, 80.17it/s] 96%|█████████▌| 76/79 [00:01<00:00, 84.06it/s]100%|██████████| 79/79 [00:02<00:00, 35.97it/s]
10000 images processed, 2.2581980228424072 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<03:16,  1.04it/s]  5%|▌         | 11/204 [00:01<00:14, 13.63it/s] 10%|█         | 21/204 [00:01<00:06, 26.51it/s] 15%|█▌        | 31/204 [00:01<00:04, 39.05it/s] 20%|██        | 41/204 [00:01<00:03, 49.13it/s] 25%|██▌       | 51/204 [00:01<00:02, 58.94it/s] 29%|██▉       | 60/204 [00:01<00:02, 65.12it/s] 34%|███▍      | 70/204 [00:01<00:01, 72.19it/s] 39%|███▉      | 80/204 [00:01<00:01, 77.11it/s] 44%|████▎     | 89/204 [00:01<00:01, 78.82it/s] 49%|████▊     | 99/204 [00:02<00:01, 82.52it/s] 53%|█████▎    | 108/204 [00:02<00:01, 83.57it/s] 57%|█████▋    | 117/204 [00:02<00:01, 83.77it/s] 62%|██████▏   | 126/204 [00:02<00:00, 85.12it/s] 67%|██████▋   | 136/204 [00:02<00:00, 87.61it/s] 71%|███████   | 145/204 [00:02<00:00, 87.94it/s] 76%|███████▌  | 155/204 [00:02<00:00, 89.28it/s] 81%|████████  | 165/204 [00:02<00:00, 90.50it/s] 86%|████████▌ | 175/204 [00:02<00:00, 89.06it/s] 91%|█████████ | 185/204 [00:03<00:00, 91.02it/s] 96%|█████████▌| 195/204 [00:03<00:00, 92.52it/s]100%|██████████| 204/204 [00:03<00:00, 63.13it/s]
26032 images processed, 3.272608518600464 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:51,  1.50it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.14it/s] 20%|██        | 16/79 [00:00<00:02, 25.46it/s] 28%|██▊       | 22/79 [00:01<00:01, 31.58it/s] 35%|███▌      | 28/79 [00:01<00:01, 36.84it/s] 46%|████▌     | 36/79 [00:01<00:00, 46.82it/s] 58%|█████▊    | 46/79 [00:01<00:00, 59.13it/s] 71%|███████   | 56/79 [00:01<00:00, 68.23it/s] 84%|████████▎ | 66/79 [00:01<00:00, 75.34it/s] 95%|█████████▍| 75/79 [00:01<00:00, 68.18it/s]100%|██████████| 79/79 [00:01<00:00, 44.57it/s]
10000 images processed, 1.8359315395355225 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:36,  2.14it/s] 10%|█         | 8/79 [00:00<00:03, 17.75it/s] 20%|██        | 16/79 [00:00<00:01, 31.95it/s] 30%|███       | 24/79 [00:00<00:01, 43.23it/s] 43%|████▎     | 34/79 [00:00<00:00, 56.81it/s] 56%|█████▌    | 44/79 [00:01<00:00, 67.21it/s] 68%|██████▊   | 54/79 [00:01<00:00, 74.94it/s] 81%|████████  | 64/79 [00:01<00:00, 80.69it/s] 94%|█████████▎| 74/79 [00:01<00:00, 84.76it/s]100%|██████████| 79/79 [00:01<00:00, 57.11it/s]
10000 images processed, 1.4057295322418213 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:49,  1.38it/s] 13%|█▎        | 9/70 [00:00<00:04, 14.31it/s] 26%|██▌       | 18/70 [00:00<00:01, 28.16it/s] 39%|███▊      | 27/70 [00:01<00:01, 41.04it/s] 53%|█████▎    | 37/70 [00:01<00:00, 53.65it/s] 66%|██████▌   | 46/70 [00:01<00:00, 60.76it/s] 79%|███████▊  | 55/70 [00:01<00:00, 61.71it/s] 91%|█████████▏| 64/70 [00:01<00:00, 67.02it/s]100%|██████████| 70/70 [00:01<00:00, 44.40it/s]
8925 images processed, 1.6308095455169678 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:53,  1.22s/it] 18%|█▊        | 8/45 [00:01<00:04,  8.05it/s] 38%|███▊      | 17/45 [00:01<00:02, 13.88it/s] 51%|█████     | 23/45 [00:01<00:01, 18.08it/s] 73%|███████▎  | 33/45 [00:02<00:00, 18.63it/s] 93%|█████████▎| 42/45 [00:02<00:00, 26.50it/s]100%|██████████| 45/45 [00:02<00:00, 17.91it/s]
5640 images processed, 2.532198905944824 seconds used

19.40501117706299
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.87  99.30  97.83
places365     71.39  80.52  78.52
LSUN          19.00  95.99  96.05
iSUN          74.42  81.53  84.21
dtd           39.45  91.06  94.12
AVG           41.43  89.68  90.15
Retain-Acc: 0.7461
Forget-as-OOD (retain known vs forget novel):
  FPR: 73.70 AUROC: 87.64 AUIN: 98.45
8.017113208770752
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_rf.png
[Run] lambda=0.1 lr=0.001 epochs=50
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.1, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:20<16:54, 20.70s/it]  4%|▍         | 2/50 [00:34<13:25, 16.78s/it]  6%|▌         | 3/50 [00:48<12:08, 15.49s/it]  8%|▊         | 4/50 [01:04<11:56, 15.57s/it] 10%|█         | 5/50 [01:18<11:17, 15.05s/it] 12%|█▏        | 6/50 [01:32<10:39, 14.53s/it][loss] ep 0 it 0 total=7.8079 mle=1.2805 pcon=5.2951 forget=1.2324 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=7.8927 mle=1.3924 pcon=5.2887 forget=1.2116 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=8.0161 mle=1.5080 pcon=5.2827 forget=1.2253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=8.0598 mle=1.5645 pcon=5.2770 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=8.0045 mle=1.5329 pcon=5.2708 forget=1.2007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=7.8785 mle=1.3804 pcon=5.2650 forget=1.2331 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=7.7786 mle=1.2921 pcon=5.2596 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=7.9325 mle=1.4378 pcon=5.2543 forget=1.2405 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 1 it 10 total=7.7859 mle=1.3067 pcon=5.2487 forget=1.2306 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=7.7980 mle=1.3159 pcon=5.2433 forget=1.2389 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=7.7468 mle=1.2902 pcon=5.2382 forget=1.2183 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=7.7124 mle=1.2379 pcon=5.2331 forget=1.2415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=7.8319 mle=1.3724 pcon=5.2283 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=7.7791 mle=1.3468 pcon=5.2235 forget=1.2088 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=7.7559 mle=1.3082 pcon=5.2185 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=7.8229 mle=1.3766 pcon=5.2136 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 2 it 20 total=7.7062 mle=1.2745 pcon=5.2089 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=7.8501 mle=1.3989 pcon=5.2044 forget=1.2468 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=7.7728 mle=1.3588 pcon=5.1998 forget=1.2142 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=7.7722 mle=1.3578 pcon=5.1954 forget=1.2190 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=7.7516 mle=1.3300 pcon=5.1908 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=7.7869 mle=1.3658 pcon=5.1865 forget=1.2346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=7.6577 mle=1.2607 pcon=5.1824 forget=1.2147 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=7.7849 mle=1.3835 pcon=5.1787 forget=1.2227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 3 it 30 total=7.7685 mle=1.3639 pcon=5.1748 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=7.6956 mle=1.2956 pcon=5.1710 forget=1.2290 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=7.6783 mle=1.2904 pcon=5.1672 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=7.8325 mle=1.4599 pcon=5.1635 forget=1.2090 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=7.7462 mle=1.3523 pcon=5.1601 forget=1.2338 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=7.8179 mle=1.4267 pcon=5.1568 forget=1.2344 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=7.8238 mle=1.4481 pcon=5.1531 forget=1.2226 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=7.6896 mle=1.3140 pcon=5.1497 forget=1.2259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 4 it 40 total=7.7403 mle=1.3767 pcon=5.1464 forget=1.2171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=7.7169 mle=1.3339 pcon=5.1434 forget=1.2396 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=7.9299 mle=1.5853 pcon=5.1406 forget=1.2041 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=7.7626 mle=1.3941 pcon=5.1374 forget=1.2312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=7.7864 mle=1.4141 pcon=5.1344 forget=1.2379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=7.5439 mle=1.1861 pcon=5.1316 forget=1.2262 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=7.8147 mle=1.4610 pcon=5.1290 forget=1.2246 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 5 it 0 total=7.5816 mle=1.2048 pcon=5.1261 forget=1.2507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=7.7231 mle=1.3650 pcon=5.1234 forget=1.2348 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=7.7653 mle=1.4009 pcon=5.1206 forget=1.2438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=7.8337 mle=1.4809 pcon=5.1178 forget=1.2350 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=7.7882 mle=1.4524 pcon=5.1151 forget=1.2207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=7.6863 mle=1.3483 pcon=5.1124 forget=1.2255 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=7.7565 mle=1.4294 pcon=5.1098 forget=1.2172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=7.7335 mle=1.4044 pcon=5.1071 forget=1.2219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 6 it 10 total=7.6347 mle=1.2784 pcon=5.1047 forget=1.2516 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=7.7000 mle=1.3787 pcon=5.1024 forget=1.2189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=7.6563 mle=1.3185 pcon=5.1002 forget=1.2376 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=7.6599 mle=1.3321 pcon=5.0979 forget=1.2298 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=7.6925 mle=1.3653 pcon=5.0954 forget=1.2318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=7.7161 mle=1.3835 pcon=5.0933 forget=1.2393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=7.6565 mle=1.3537 pcon=5.0913 forget=1.2115 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 14%|█▍        | 7/50 [01:46<10:27, 14.59s/it] 16%|█▌        | 8/50 [02:01<10:13, 14.60s/it] 18%|█▊        | 9/50 [02:16<10:05, 14.77s/it] 20%|██        | 10/50 [02:31<09:48, 14.72s/it] 22%|██▏       | 11/50 [02:45<09:27, 14.56s/it] 24%|██▍       | 12/50 [03:00<09:19, 14.72s/it] 26%|██▌       | 13/50 [03:15<09:09, 14.85s/it][loss] ep 6 it 360 total=7.7498 mle=1.4567 pcon=5.0891 forget=1.2040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 7 it 20 total=7.5655 mle=1.2502 pcon=5.0870 forget=1.2283 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=7.5608 mle=1.2464 pcon=5.0847 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=7.6922 mle=1.3880 pcon=5.0827 forget=1.2216 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=7.7176 mle=1.4331 pcon=5.0808 forget=1.2038 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=7.6181 mle=1.3206 pcon=5.0791 forget=1.2184 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=7.7541 mle=1.4546 pcon=5.0775 forget=1.2221 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=7.6235 mle=1.3171 pcon=5.0753 forget=1.2311 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=7.6912 mle=1.3713 pcon=5.0733 forget=1.2467 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 8 it 30 total=7.7576 mle=1.4809 pcon=5.0716 forget=1.2052 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=7.5762 mle=1.2754 pcon=5.0699 forget=1.2309 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=7.6952 mle=1.3990 pcon=5.0682 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=7.6396 mle=1.3456 pcon=5.0665 forget=1.2276 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=7.6602 mle=1.3662 pcon=5.0648 forget=1.2292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=7.5535 mle=1.2577 pcon=5.0632 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=7.8329 mle=1.5799 pcon=5.0614 forget=1.1915 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=7.6516 mle=1.3648 pcon=5.0600 forget=1.2268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 9 it 40 total=7.6994 mle=1.4116 pcon=5.0585 forget=1.2293 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=7.7620 mle=1.4933 pcon=5.0570 forget=1.2117 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=7.6464 mle=1.3549 pcon=5.0559 forget=1.2356 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=7.5182 mle=1.2290 pcon=5.0546 forget=1.2346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=7.7621 mle=1.4985 pcon=5.0535 forget=1.2101 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=7.6412 mle=1.3594 pcon=5.0520 forget=1.2297 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=7.5897 mle=1.3080 pcon=5.0509 forget=1.2308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 10 it 0 total=7.5545 mle=1.2684 pcon=5.0492 forget=1.2369 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=7.5333 mle=1.2586 pcon=5.0482 forget=1.2265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=7.5939 mle=1.3283 pcon=5.0468 forget=1.2187 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=7.6345 mle=1.3450 pcon=5.0457 forget=1.2438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=7.6850 mle=1.4101 pcon=5.0446 forget=1.2303 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=7.6989 mle=1.4288 pcon=5.0436 forget=1.2264 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=7.7519 mle=1.5150 pcon=5.0424 forget=1.1946 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=7.5335 mle=1.2625 pcon=5.0415 forget=1.2294 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 11 it 10 total=7.7023 mle=1.4431 pcon=5.0403 forget=1.2189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=7.6332 mle=1.3737 pcon=5.0394 forget=1.2200 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=7.6150 mle=1.3485 pcon=5.0385 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=7.5510 mle=1.2742 pcon=5.0374 forget=1.2393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=7.5316 mle=1.2649 pcon=5.0365 forget=1.2302 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=7.5274 mle=1.2694 pcon=5.0355 forget=1.2224 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=7.6541 mle=1.3935 pcon=5.0347 forget=1.2259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=7.6002 mle=1.3221 pcon=5.0340 forget=1.2441 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 12 it 20 total=7.5818 mle=1.3165 pcon=5.0328 forget=1.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=7.6197 mle=1.3560 pcon=5.0318 forget=1.2319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=7.6045 mle=1.3654 pcon=5.0307 forget=1.2084 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=7.6125 mle=1.3431 pcon=5.0297 forget=1.2397 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=7.5000 mle=1.2268 pcon=5.0291 forget=1.2441 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=7.5467 mle=1.2976 pcon=5.0285 forget=1.2205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=7.6746 mle=1.4167 pcon=5.0279 forget=1.2300 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=7.6428 mle=1.3873 pcon=5.0273 forget=1.2282 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 13 it 30 total=7.5885 mle=1.3174 pcon=5.0268 forget=1.2443 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=7.7435 mle=1.5040 pcon=5.0262 forget=1.2133 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=7.6131 mle=1.3631 pcon=5.0253 forget=1.2247 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=7.6290 mle=1.3572 pcon=5.0242 forget=1.2476 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 28%|██▊       | 14/50 [03:30<08:57, 14.92s/it] 30%|███       | 15/50 [03:45<08:38, 14.81s/it] 32%|███▏      | 16/50 [04:01<08:41, 15.35s/it] 34%|███▍      | 17/50 [04:16<08:17, 15.08s/it] 36%|███▌      | 18/50 [04:30<07:53, 14.79s/it] 38%|███▊      | 19/50 [04:44<07:33, 14.62s/it] 40%|████      | 20/50 [04:59<07:20, 14.69s/it][loss] ep 13 it 230 total=7.5506 mle=1.3048 pcon=5.0236 forget=1.2222 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=7.5048 mle=1.2268 pcon=5.0230 forget=1.2551 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=7.6448 mle=1.4003 pcon=5.0225 forget=1.2220 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=7.5709 mle=1.3284 pcon=5.0220 forget=1.2205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 14 it 40 total=7.5228 mle=1.2782 pcon=5.0213 forget=1.2233 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=7.5822 mle=1.3277 pcon=5.0209 forget=1.2336 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=7.5640 mle=1.3036 pcon=5.0204 forget=1.2400 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=7.5230 mle=1.2669 pcon=5.0200 forget=1.2361 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=7.5794 mle=1.3253 pcon=5.0192 forget=1.2349 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=7.5248 mle=1.2933 pcon=5.0188 forget=1.2126 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=7.4853 mle=1.2459 pcon=5.0185 forget=1.2209 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 15 it 0 total=7.5657 mle=1.3365 pcon=5.0178 forget=1.2113 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=7.4739 mle=1.2419 pcon=5.0176 forget=1.2145 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=7.5883 mle=1.3660 pcon=5.0174 forget=1.2049 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=7.5314 mle=1.2890 pcon=5.0171 forget=1.2253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=7.5061 mle=1.2570 pcon=5.0168 forget=1.2323 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=7.5254 mle=1.2828 pcon=5.0164 forget=1.2261 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=7.5966 mle=1.3894 pcon=5.0160 forget=1.1912 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=7.6228 mle=1.4012 pcon=5.0158 forget=1.2058 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 16 it 10 total=7.4296 mle=1.1819 pcon=5.0154 forget=1.2322 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=7.4447 mle=1.2296 pcon=5.0154 forget=1.1997 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=7.4353 mle=1.2034 pcon=5.0150 forget=1.2168 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=7.5235 mle=1.2732 pcon=5.0149 forget=1.2354 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=7.5145 mle=1.2538 pcon=5.0148 forget=1.2460 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=7.6541 mle=1.4100 pcon=5.0143 forget=1.2299 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=7.6706 mle=1.4454 pcon=5.0139 forget=1.2114 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=7.5514 mle=1.3154 pcon=5.0136 forget=1.2224 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 17 it 20 total=7.5907 mle=1.3816 pcon=5.0137 forget=1.1954 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=7.6157 mle=1.3651 pcon=5.0138 forget=1.2367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=7.4758 mle=1.2420 pcon=5.0135 forget=1.2203 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=7.5651 mle=1.3317 pcon=5.0132 forget=1.2202 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=7.5704 mle=1.3692 pcon=5.0129 forget=1.1882 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=7.5740 mle=1.3454 pcon=5.0129 forget=1.2158 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=7.4769 mle=1.2577 pcon=5.0127 forget=1.2064 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=7.5234 mle=1.3303 pcon=5.0124 forget=1.1806 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 18 it 30 total=7.4169 mle=1.1960 pcon=5.0124 forget=1.2085 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=7.6450 mle=1.4286 pcon=5.0124 forget=1.2040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=7.6076 mle=1.3813 pcon=5.0123 forget=1.2139 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=7.4801 mle=1.2694 pcon=5.0124 forget=1.1984 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=7.5675 mle=1.3504 pcon=5.0122 forget=1.2050 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=7.5712 mle=1.3386 pcon=5.0120 forget=1.2206 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=7.5681 mle=1.3369 pcon=5.0119 forget=1.2193 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=7.4274 mle=1.2203 pcon=5.0120 forget=1.1951 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 19 it 40 total=7.4235 mle=1.2037 pcon=5.0119 forget=1.2079 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=7.5030 mle=1.2947 pcon=5.0117 forget=1.1967 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=7.4328 mle=1.2242 pcon=5.0116 forget=1.1970 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=7.4677 mle=1.2797 pcon=5.0114 forget=1.1766 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=7.4586 mle=1.2389 pcon=5.0113 forget=1.2085 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=7.4995 mle=1.3084 pcon=5.0117 forget=1.1795 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=7.4921 mle=1.2615 pcon=5.0121 forget=1.2185 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 20 it 0 total=7.4902 mle=1.2885 pcon=5.0122 forget=1.1895 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=7.5226 mle=1.2907 pcon=5.0123 forget=1.2196 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 42%|████▏     | 21/50 [05:13<06:57, 14.39s/it] 44%|████▍     | 22/50 [05:27<06:38, 14.24s/it] 46%|████▌     | 23/50 [05:41<06:30, 14.45s/it] 48%|████▊     | 24/50 [05:55<06:10, 14.26s/it] 50%|█████     | 25/50 [06:10<05:57, 14.31s/it] 52%|█████▏    | 26/50 [06:23<05:33, 13.91s/it] 54%|█████▍    | 27/50 [06:38<05:27, 14.25s/it][loss] ep 20 it 100 total=7.4047 mle=1.2036 pcon=5.0121 forget=1.1891 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=7.4538 mle=1.2588 pcon=5.0121 forget=1.1829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=7.5575 mle=1.3437 pcon=5.0121 forget=1.2017 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=7.5373 mle=1.3225 pcon=5.0121 forget=1.2027 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=7.5524 mle=1.3535 pcon=5.0122 forget=1.1867 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=7.5269 mle=1.3298 pcon=5.0120 forget=1.1851 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 21 it 10 total=7.3018 mle=1.1135 pcon=5.0117 forget=1.1767 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=7.4173 mle=1.2150 pcon=5.0116 forget=1.1908 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=7.5105 mle=1.3265 pcon=5.0113 forget=1.1728 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=7.4731 mle=1.3016 pcon=5.0110 forget=1.1605 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=7.3989 mle=1.2088 pcon=5.0106 forget=1.1795 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=7.5296 mle=1.3786 pcon=5.0106 forget=1.1405 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=7.5248 mle=1.3331 pcon=5.0101 forget=1.1815 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=7.4955 mle=1.3092 pcon=5.0098 forget=1.1766 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 22 it 20 total=7.4529 mle=1.2862 pcon=5.0096 forget=1.1570 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=7.4377 mle=1.2625 pcon=5.0091 forget=1.1662 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=7.4870 mle=1.3201 pcon=5.0087 forget=1.1582 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=7.3810 mle=1.2384 pcon=5.0080 forget=1.1346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=7.4956 mle=1.3506 pcon=5.0077 forget=1.1373 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=7.4131 mle=1.2636 pcon=5.0075 forget=1.1420 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=7.3525 mle=1.2409 pcon=5.0073 forget=1.1044 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=7.5197 mle=1.3941 pcon=5.0067 forget=1.1189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 23 it 30 total=7.3973 mle=1.2696 pcon=5.0063 forget=1.1213 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=7.4277 mle=1.2683 pcon=5.0060 forget=1.1534 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=7.5162 mle=1.3845 pcon=5.0056 forget=1.1261 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=7.3954 mle=1.2750 pcon=5.0051 forget=1.1152 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=7.4887 mle=1.3649 pcon=5.0050 forget=1.1187 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=7.4762 mle=1.3392 pcon=5.0049 forget=1.1321 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=7.4947 mle=1.3528 pcon=5.0046 forget=1.1373 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=7.3783 mle=1.2402 pcon=5.0044 forget=1.1336 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 40 total=7.4147 mle=1.2712 pcon=5.0046 forget=1.1389 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=7.5113 mle=1.3767 pcon=5.0044 forget=1.1301 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=7.4174 mle=1.2772 pcon=5.0043 forget=1.1358 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=7.4376 mle=1.2844 pcon=5.0045 forget=1.1487 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=7.4826 mle=1.3300 pcon=5.0048 forget=1.1478 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=7.4675 mle=1.3258 pcon=5.0048 forget=1.1369 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=7.4421 mle=1.2854 pcon=5.0047 forget=1.1520 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 0 total=7.4750 mle=1.3253 pcon=5.0049 forget=1.1449 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 50 total=7.5479 mle=1.4039 pcon=5.0050 forget=1.1390 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 100 total=7.5505 mle=1.3982 pcon=5.0050 forget=1.1474 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 150 total=7.5062 mle=1.3552 pcon=5.0053 forget=1.1457 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 200 total=7.5084 mle=1.3514 pcon=5.0055 forget=1.1515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 250 total=7.4491 mle=1.2815 pcon=5.0059 forget=1.1617 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 300 total=7.5180 mle=1.3531 pcon=5.0062 forget=1.1587 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 350 total=7.5788 mle=1.4080 pcon=5.0064 forget=1.1644 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 10 total=7.5078 mle=1.3307 pcon=5.0067 forget=1.1705 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 60 total=7.5114 mle=1.3439 pcon=5.0068 forget=1.1607 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 110 total=7.5181 mle=1.3514 pcon=5.0072 forget=1.1595 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 160 total=7.4636 mle=1.2898 pcon=5.0073 forget=1.1665 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 210 total=7.5727 mle=1.4001 pcon=5.0076 forget=1.1650 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 260 total=7.4852 mle=1.3127 pcon=5.0082 forget=1.1643 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 310 total=7.6708 mle=1.5124 pcon=5.0085 forget=1.1499 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 360 total=7.6261 mle=1.4626 pcon=5.0088 forget=1.1548 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 20 total=7.4840 mle=1.3076 pcon=5.0091 forget=1.1673 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 70 total=7.5135 mle=1.3474 pcon=5.0093 forget=1.1569 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 120 total=7.5479 mle=1.3596 pcon=5.0097 forget=1.1786 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 170 total=7.5040 mle=1.3257 pcon=5.0101 forget=1.1682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 56%|█████▌    | 28/50 [06:52<05:16, 14.40s/it] 58%|█████▊    | 29/50 [07:07<05:00, 14.29s/it] 60%|██████    | 30/50 [07:21<04:48, 14.41s/it] 62%|██████▏   | 31/50 [07:36<04:35, 14.49s/it] 64%|██████▍   | 32/50 [07:50<04:21, 14.53s/it] 66%|██████▌   | 33/50 [08:05<04:05, 14.42s/it] 68%|██████▊   | 34/50 [08:19<03:52, 14.55s/it] 70%|███████   | 35/50 [08:34<03:38, 14.54s/it][loss] ep 27 it 220 total=7.5316 mle=1.3622 pcon=5.0106 forget=1.1588 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 270 total=7.5270 mle=1.3489 pcon=5.0108 forget=1.1673 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 320 total=7.5218 mle=1.3375 pcon=5.0111 forget=1.1732 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 370 total=7.4564 mle=1.2838 pcon=5.0114 forget=1.1612 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 30 total=7.5019 mle=1.3227 pcon=5.0118 forget=1.1675 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 80 total=7.5720 mle=1.4081 pcon=5.0118 forget=1.1522 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 130 total=7.4274 mle=1.2752 pcon=5.0121 forget=1.1400 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 180 total=7.4520 mle=1.2841 pcon=5.0119 forget=1.1561 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 230 total=7.5051 mle=1.3437 pcon=5.0121 forget=1.1493 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 280 total=7.5095 mle=1.3505 pcon=5.0121 forget=1.1468 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 330 total=7.4479 mle=1.3011 pcon=5.0120 forget=1.1348 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 380 total=7.4785 mle=1.3284 pcon=5.0117 forget=1.1383 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 40 total=7.4938 mle=1.3519 pcon=5.0115 forget=1.1303 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 90 total=7.4786 mle=1.3257 pcon=5.0113 forget=1.1416 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 140 total=7.4316 mle=1.2938 pcon=5.0112 forget=1.1265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 190 total=7.6188 mle=1.4698 pcon=5.0108 forget=1.1382 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 240 total=7.5526 mle=1.4087 pcon=5.0106 forget=1.1333 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 290 total=7.4364 mle=1.3081 pcon=5.0099 forget=1.1184 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 340 total=7.4760 mle=1.3468 pcon=5.0097 forget=1.1194 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 0 total=7.4426 mle=1.3062 pcon=5.0092 forget=1.1272 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 50 total=7.5231 mle=1.4039 pcon=5.0086 forget=1.1106 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 100 total=7.4480 mle=1.3277 pcon=5.0083 forget=1.1121 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 150 total=7.4355 mle=1.3139 pcon=5.0078 forget=1.1139 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 200 total=7.4788 mle=1.3701 pcon=5.0073 forget=1.1014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 250 total=7.5181 mle=1.4138 pcon=5.0069 forget=1.0974 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 300 total=7.5226 mle=1.4215 pcon=5.0060 forget=1.0951 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 350 total=7.5237 mle=1.4000 pcon=5.0051 forget=1.1187 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 10 total=7.5356 mle=1.4290 pcon=5.0043 forget=1.1024 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 60 total=7.4064 mle=1.3005 pcon=5.0036 forget=1.1024 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 110 total=7.5497 mle=1.4541 pcon=5.0029 forget=1.0927 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 160 total=7.4293 mle=1.3231 pcon=5.0018 forget=1.1044 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 210 total=7.5477 mle=1.4565 pcon=5.0008 forget=1.0904 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 260 total=7.4910 mle=1.3873 pcon=4.9999 forget=1.1038 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 310 total=7.4344 mle=1.3317 pcon=4.9987 forget=1.1040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 360 total=7.3942 mle=1.2948 pcon=4.9980 forget=1.1015 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 20 total=7.3701 mle=1.2590 pcon=4.9972 forget=1.1139 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 70 total=7.3949 mle=1.3047 pcon=4.9963 forget=1.0940 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 120 total=7.3809 mle=1.2848 pcon=4.9949 forget=1.1012 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 170 total=7.4101 mle=1.3075 pcon=4.9938 forget=1.1088 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 220 total=7.4783 mle=1.3849 pcon=4.9924 forget=1.1009 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 270 total=7.4128 mle=1.3191 pcon=4.9910 forget=1.1028 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 320 total=7.4576 mle=1.3558 pcon=4.9893 forget=1.1125 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 370 total=7.3448 mle=1.2652 pcon=4.9880 forget=1.0916 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 30 total=7.5108 mle=1.4239 pcon=4.9866 forget=1.1003 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 80 total=7.4453 mle=1.3541 pcon=4.9850 forget=1.1062 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 130 total=7.4426 mle=1.3662 pcon=4.9837 forget=1.0927 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 180 total=7.4270 mle=1.3474 pcon=4.9821 forget=1.0975 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 230 total=7.4616 mle=1.3857 pcon=4.9808 forget=1.0951 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 280 total=7.4029 mle=1.3326 pcon=4.9790 forget=1.0913 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 330 total=7.5079 mle=1.4159 pcon=4.9772 forget=1.1148 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 380 total=7.3682 mle=1.2829 pcon=4.9755 forget=1.1098 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 40 total=7.4565 mle=1.3758 pcon=4.9737 forget=1.1070 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 90 total=7.4206 mle=1.3461 pcon=4.9719 forget=1.1025 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 140 total=7.4633 mle=1.3796 pcon=4.9700 forget=1.1137 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 190 total=7.3720 mle=1.2995 pcon=4.9682 forget=1.1044 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 240 total=7.4126 mle=1.3077 pcon=4.9664 forget=1.1385 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 290 total=7.3947 mle=1.3235 pcon=4.9647 forget=1.1064 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 340 total=7.4020 mle=1.3030 pcon=4.9628 forget=1.1362 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 0 total=7.3756 mle=1.2898 pcon=4.9612 forget=1.1247 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 50 total=7.4586 mle=1.3715 pcon=4.9597 forget=1.1274 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 100 total=7.4268 mle=1.3304 pcon=4.9581 forget=1.1383 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 150 total=7.3364 mle=1.2348 pcon=4.9560 forget=1.1455 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 72%|███████▏  | 36/50 [08:50<03:29, 14.97s/it] 74%|███████▍  | 37/50 [09:06<03:16, 15.14s/it] 76%|███████▌  | 38/50 [09:20<02:57, 14.82s/it] 78%|███████▊  | 39/50 [09:34<02:43, 14.83s/it] 80%|████████  | 40/50 [09:50<02:30, 15.00s/it] 82%|████████▏ | 41/50 [10:05<02:14, 14.94s/it] 84%|████████▍ | 42/50 [10:20<01:59, 14.92s/it] 86%|████████▌ | 43/50 [10:34<01:44, 14.87s/it][loss] ep 35 it 200 total=7.4198 mle=1.3370 pcon=4.9543 forget=1.1285 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 250 total=7.4724 mle=1.3647 pcon=4.9527 forget=1.1550 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 300 total=7.4475 mle=1.3452 pcon=4.9509 forget=1.1515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 350 total=7.4618 mle=1.3612 pcon=4.9492 forget=1.1514 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 10 total=7.4833 mle=1.3949 pcon=4.9473 forget=1.1410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 60 total=7.4681 mle=1.3687 pcon=4.9455 forget=1.1539 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 110 total=7.5829 mle=1.4848 pcon=4.9439 forget=1.1541 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 160 total=7.4963 mle=1.4023 pcon=4.9422 forget=1.1517 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 210 total=7.5804 mle=1.4901 pcon=4.9404 forget=1.1498 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 260 total=7.4792 mle=1.3888 pcon=4.9387 forget=1.1517 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 310 total=7.3921 mle=1.2811 pcon=4.9371 forget=1.1738 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 360 total=7.4253 mle=1.3237 pcon=4.9359 forget=1.1656 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 20 total=7.4707 mle=1.3476 pcon=4.9343 forget=1.1889 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 70 total=7.5422 mle=1.4311 pcon=4.9328 forget=1.1784 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 120 total=7.4274 mle=1.3352 pcon=4.9309 forget=1.1613 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 170 total=7.4143 mle=1.2987 pcon=4.9296 forget=1.1860 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 220 total=7.4289 mle=1.3326 pcon=4.9278 forget=1.1684 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 270 total=7.4171 mle=1.3303 pcon=4.9264 forget=1.1604 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 320 total=7.4511 mle=1.3229 pcon=4.9250 forget=1.2032 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 370 total=7.4319 mle=1.3273 pcon=4.9236 forget=1.1810 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 30 total=7.3949 mle=1.2714 pcon=4.9223 forget=1.2012 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 80 total=7.4902 mle=1.3685 pcon=4.9208 forget=1.2009 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 130 total=7.4254 mle=1.3156 pcon=4.9192 forget=1.1906 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 180 total=7.3816 mle=1.2632 pcon=4.9176 forget=1.2008 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 230 total=7.4340 mle=1.3228 pcon=4.9164 forget=1.1948 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 280 total=7.5361 mle=1.4183 pcon=4.9151 forget=1.2028 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 330 total=7.6253 mle=1.5063 pcon=4.9137 forget=1.2053 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 380 total=7.4338 mle=1.3194 pcon=4.9121 forget=1.2022 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 40 total=7.4332 mle=1.3229 pcon=4.9107 forget=1.1996 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 90 total=7.4956 mle=1.3699 pcon=4.9095 forget=1.2163 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 140 total=7.5151 mle=1.4086 pcon=4.9083 forget=1.1982 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 190 total=7.3500 mle=1.2354 pcon=4.9070 forget=1.2076 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 240 total=7.4977 mle=1.3579 pcon=4.9058 forget=1.2341 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 290 total=7.4342 mle=1.3044 pcon=4.9045 forget=1.2253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 340 total=7.4784 mle=1.3408 pcon=4.9036 forget=1.2340 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 0 total=7.4017 mle=1.2726 pcon=4.9022 forget=1.2269 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 50 total=7.4702 mle=1.3455 pcon=4.9010 forget=1.2237 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 100 total=7.4011 mle=1.2750 pcon=4.9000 forget=1.2260 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 150 total=7.4758 mle=1.3406 pcon=4.8989 forget=1.2364 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 200 total=7.4180 mle=1.2996 pcon=4.8978 forget=1.2206 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 250 total=7.5117 mle=1.3928 pcon=4.8965 forget=1.2224 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 300 total=7.5320 mle=1.4013 pcon=4.8954 forget=1.2353 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 350 total=7.4161 mle=1.2972 pcon=4.8943 forget=1.2246 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 10 total=7.5436 mle=1.4284 pcon=4.8932 forget=1.2220 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 60 total=7.4321 mle=1.3132 pcon=4.8926 forget=1.2263 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 110 total=7.4600 mle=1.3289 pcon=4.8916 forget=1.2396 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 160 total=7.3695 mle=1.2650 pcon=4.8904 forget=1.2141 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 210 total=7.4255 mle=1.3149 pcon=4.8893 forget=1.2213 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 260 total=7.3792 mle=1.2616 pcon=4.8883 forget=1.2293 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 310 total=7.4345 mle=1.3005 pcon=4.8876 forget=1.2465 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 360 total=7.3823 mle=1.2588 pcon=4.8868 forget=1.2367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 20 total=7.4552 mle=1.3347 pcon=4.8860 forget=1.2345 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 70 total=7.3784 mle=1.2609 pcon=4.8852 forget=1.2322 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 120 total=7.3961 mle=1.3048 pcon=4.8843 forget=1.2071 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 170 total=7.4448 mle=1.3224 pcon=4.8836 forget=1.2388 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 220 total=7.3354 mle=1.2281 pcon=4.8827 forget=1.2246 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 270 total=7.4596 mle=1.3411 pcon=4.8817 forget=1.2368 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 320 total=7.5250 mle=1.4155 pcon=4.8809 forget=1.2286 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 370 total=7.4792 mle=1.3930 pcon=4.8800 forget=1.2062 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 30 total=7.5109 mle=1.4325 pcon=4.8794 forget=1.1990 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 80 total=7.4863 mle=1.3642 pcon=4.8785 forget=1.2436 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 130 total=7.5014 mle=1.3988 pcon=4.8779 forget=1.2248 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 88%|████████▊ | 44/50 [10:49<01:28, 14.81s/it] 90%|█████████ | 45/50 [11:03<01:12, 14.52s/it] 92%|█████████▏| 46/50 [11:18<00:58, 14.64s/it] 94%|█████████▍| 47/50 [11:32<00:43, 14.52s/it] 96%|█████████▌| 48/50 [11:47<00:29, 14.63s/it] 98%|█████████▊| 49/50 [12:02<00:14, 14.90s/it]100%|██████████| 50/50 [12:17<00:00, 14.85s/it]100%|██████████| 50/50 [12:17<00:00, 14.75s/it]
[loss] ep 43 it 180 total=7.3398 mle=1.2217 pcon=4.8771 forget=1.2410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 230 total=7.3773 mle=1.2644 pcon=4.8764 forget=1.2364 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 280 total=7.3938 mle=1.2737 pcon=4.8758 forget=1.2443 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 330 total=7.5143 mle=1.4149 pcon=4.8751 forget=1.2243 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 380 total=7.4250 mle=1.3191 pcon=4.8745 forget=1.2314 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 44 it 40 total=7.4418 mle=1.3357 pcon=4.8738 forget=1.2323 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 90 total=7.3895 mle=1.2715 pcon=4.8732 forget=1.2448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 140 total=7.4227 mle=1.3133 pcon=4.8726 forget=1.2367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 190 total=7.4621 mle=1.3988 pcon=4.8720 forget=1.1913 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 240 total=7.4917 mle=1.3963 pcon=4.8714 forget=1.2240 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 290 total=7.4927 mle=1.3938 pcon=4.8708 forget=1.2281 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 340 total=7.4283 mle=1.3300 pcon=4.8703 forget=1.2280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 45 it 0 total=7.3834 mle=1.2991 pcon=4.8698 forget=1.2144 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 50 total=7.3743 mle=1.2658 pcon=4.8690 forget=1.2395 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 100 total=7.4544 mle=1.3569 pcon=4.8687 forget=1.2288 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 150 total=7.4691 mle=1.3739 pcon=4.8682 forget=1.2270 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 200 total=7.4032 mle=1.2964 pcon=4.8675 forget=1.2392 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 250 total=7.4129 mle=1.3163 pcon=4.8670 forget=1.2296 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 300 total=7.4049 mle=1.3358 pcon=4.8665 forget=1.2026 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 350 total=7.3775 mle=1.2873 pcon=4.8664 forget=1.2238 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 46 it 10 total=7.3134 mle=1.2332 pcon=4.8660 forget=1.2142 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 60 total=7.4502 mle=1.3416 pcon=4.8657 forget=1.2430 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 110 total=7.3874 mle=1.3086 pcon=4.8652 forget=1.2136 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 160 total=7.5006 mle=1.4320 pcon=4.8646 forget=1.2040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 210 total=7.4176 mle=1.3246 pcon=4.8639 forget=1.2290 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 260 total=7.5001 mle=1.4156 pcon=4.8635 forget=1.2211 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 310 total=7.3670 mle=1.2805 pcon=4.8629 forget=1.2236 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 360 total=7.3977 mle=1.3037 pcon=4.8627 forget=1.2314 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 47 it 20 total=7.4395 mle=1.3683 pcon=4.8623 forget=1.2090 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 70 total=7.3891 mle=1.3050 pcon=4.8618 forget=1.2223 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 120 total=7.4901 mle=1.3910 pcon=4.8614 forget=1.2377 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 170 total=7.4873 mle=1.3943 pcon=4.8610 forget=1.2320 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 220 total=7.3801 mle=1.2992 pcon=4.8605 forget=1.2204 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 270 total=7.5513 mle=1.4680 pcon=4.8601 forget=1.2232 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 320 total=7.3910 mle=1.3094 pcon=4.8599 forget=1.2217 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 370 total=7.3228 mle=1.2376 pcon=4.8594 forget=1.2258 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 48 it 30 total=7.4711 mle=1.3915 pcon=4.8591 forget=1.2205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 80 total=7.4970 mle=1.4050 pcon=4.8589 forget=1.2330 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 130 total=7.3880 mle=1.3027 pcon=4.8588 forget=1.2266 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 180 total=7.3983 mle=1.3126 pcon=4.8583 forget=1.2274 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 230 total=7.4181 mle=1.3243 pcon=4.8581 forget=1.2356 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 280 total=7.3322 mle=1.2347 pcon=4.8583 forget=1.2391 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 330 total=7.3857 mle=1.2863 pcon=4.8580 forget=1.2414 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 380 total=7.4772 mle=1.3968 pcon=4.8578 forget=1.2226 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 49 it 40 total=7.3045 mle=1.2093 pcon=4.8576 forget=1.2376 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 90 total=7.3599 mle=1.2819 pcon=4.8574 forget=1.2206 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 140 total=7.3030 mle=1.2275 pcon=4.8571 forget=1.2185 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 190 total=7.3765 mle=1.2939 pcon=4.8569 forget=1.2257 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 240 total=7.4150 mle=1.3347 pcon=4.8566 forget=1.2237 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 290 total=7.3863 mle=1.3236 pcon=4.8564 forget=1.2063 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 340 total=7.3873 mle=1.3105 pcon=4.8566 forget=1.2201 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:23,  1.92it/s]  3%|▎         | 10/391 [00:00<00:18, 20.77it/s]  5%|▍         | 18/391 [00:00<00:11, 33.75it/s]  7%|▋         | 28/391 [00:00<00:07, 49.18it/s] 10%|▉         | 38/391 [00:00<00:05, 61.24it/s] 12%|█▏        | 48/391 [00:01<00:04, 70.23it/s] 15%|█▍        | 58/391 [00:01<00:04, 76.70it/s] 17%|█▋        | 67/391 [00:01<00:04, 79.72it/s] 19%|█▉        | 76/391 [00:01<00:04, 76.40it/s] 22%|██▏       | 85/391 [00:01<00:03, 76.89it/s] 24%|██▍       | 95/391 [00:01<00:03, 81.02it/s] 27%|██▋       | 105/391 [00:01<00:03, 84.91it/s] 29%|██▉       | 115/391 [00:01<00:03, 86.85it/s] 32%|███▏      | 125/391 [00:01<00:02, 89.25it/s] 35%|███▍      | 135/391 [00:02<00:02, 91.11it/s] 37%|███▋      | 145/391 [00:02<00:02, 92.46it/s] 40%|███▉      | 155/391 [00:02<00:02, 93.23it/s] 42%|████▏     | 165/391 [00:02<00:02, 94.04it/s] 45%|████▍     | 175/391 [00:02<00:02, 94.16it/s] 47%|████▋     | 185/391 [00:02<00:02, 94.50it/s] 50%|████▉     | 195/391 [00:02<00:02, 95.09it/s] 52%|█████▏    | 205/391 [00:02<00:01, 95.30it/s] 55%|█████▍    | 215/391 [00:02<00:01, 94.71it/s] 58%|█████▊    | 225/391 [00:02<00:01, 94.94it/s] 60%|██████    | 235/391 [00:03<00:01, 93.87it/s] 63%|██████▎   | 245/391 [00:03<00:01, 94.53it/s] 65%|██████▌   | 255/391 [00:03<00:01, 93.14it/s] 68%|██████▊   | 265/391 [00:03<00:01, 91.80it/s] 70%|███████   | 275/391 [00:03<00:01, 92.62it/s] 73%|███████▎  | 285/391 [00:03<00:01, 92.60it/s] 75%|███████▌  | 295/391 [00:03<00:01, 93.53it/s] 78%|███████▊  | 305/391 [00:03<00:00, 92.35it/s] 81%|████████  | 315/391 [00:03<00:00, 92.85it/s] 83%|████████▎ | 325/391 [00:04<00:00, 92.84it/s] 86%|████████▌ | 335/391 [00:04<00:00, 92.75it/s] 88%|████████▊ | 345/391 [00:04<00:00, 92.57it/s] 91%|█████████ | 355/391 [00:04<00:00, 92.60it/s] 93%|█████████▎| 365/391 [00:04<00:00, 92.08it/s] 96%|█████████▌| 375/391 [00:04<00:00, 93.64it/s] 98%|█████████▊| 385/391 [00:04<00:00, 94.82it/s]100%|██████████| 391/391 [00:04<00:00, 81.79it/s]
50000 images processed, 4.8723368644714355 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:35,  2.20it/s] 11%|█▏        | 9/79 [00:00<00:03, 20.63it/s] 23%|██▎       | 18/79 [00:00<00:01, 37.21it/s] 34%|███▍      | 27/79 [00:00<00:01, 50.48it/s] 46%|████▌     | 36/79 [00:00<00:00, 60.99it/s] 57%|█████▋    | 45/79 [00:00<00:00, 67.94it/s] 68%|██████▊   | 54/79 [00:01<00:00, 73.65it/s] 81%|████████  | 64/79 [00:01<00:00, 79.48it/s] 94%|█████████▎| 74/79 [00:01<00:00, 83.72it/s]100%|██████████| 79/79 [00:01<00:00, 43.78it/s]
10000 images processed, 1.857578992843628 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:42,  1.97it/s]  5%|▍         | 10/204 [00:00<00:09, 21.01it/s]  9%|▉         | 19/204 [00:00<00:04, 37.16it/s] 14%|█▍        | 29/204 [00:00<00:03, 52.15it/s] 19%|█▉        | 39/204 [00:00<00:02, 63.60it/s] 24%|██▍       | 49/204 [00:01<00:02, 71.50it/s] 29%|██▉       | 59/204 [00:01<00:01, 77.80it/s] 34%|███▍      | 69/204 [00:01<00:01, 82.06it/s] 39%|███▊      | 79/204 [00:01<00:01, 84.63it/s] 44%|████▎     | 89/204 [00:01<00:01, 87.27it/s] 49%|████▊     | 99/204 [00:01<00:01, 87.34it/s] 53%|█████▎    | 109/204 [00:01<00:01, 85.34it/s] 58%|█████▊    | 118/204 [00:01<00:01, 84.58it/s] 62%|██████▏   | 127/204 [00:01<00:00, 85.18it/s] 67%|██████▋   | 137/204 [00:02<00:00, 87.16it/s] 72%|███████▏  | 147/204 [00:02<00:00, 88.91it/s] 77%|███████▋  | 157/204 [00:02<00:00, 90.05it/s] 82%|████████▏ | 167/204 [00:02<00:00, 89.87it/s] 87%|████████▋ | 177/204 [00:02<00:00, 88.45it/s] 92%|█████████▏| 187/204 [00:02<00:00, 90.61it/s] 97%|█████████▋| 197/204 [00:02<00:00, 92.23it/s]100%|██████████| 204/204 [00:02<00:00, 73.92it/s]
26032 images processed, 2.7967424392700195 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:47,  1.64it/s]  9%|▉         | 7/79 [00:00<00:05, 12.59it/s] 18%|█▊        | 14/79 [00:00<00:02, 24.60it/s] 27%|██▋       | 21/79 [00:00<00:01, 35.02it/s] 38%|███▊      | 30/79 [00:01<00:01, 47.82it/s] 51%|█████     | 40/79 [00:01<00:00, 60.10it/s] 63%|██████▎   | 50/79 [00:01<00:00, 69.39it/s] 76%|███████▌  | 60/79 [00:01<00:00, 76.50it/s] 89%|████████▊ | 70/79 [00:01<00:00, 81.82it/s]100%|██████████| 79/79 [00:01<00:00, 51.16it/s]
10000 images processed, 1.5703210830688477 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:44,  1.75it/s] 11%|█▏        | 9/79 [00:00<00:04, 17.27it/s] 24%|██▍       | 19/79 [00:00<00:01, 34.95it/s] 37%|███▋      | 29/79 [00:00<00:01, 49.33it/s] 48%|████▊     | 38/79 [00:00<00:00, 58.40it/s] 59%|█████▉    | 47/79 [00:01<00:00, 65.22it/s] 72%|███████▏  | 57/79 [00:01<00:00, 73.46it/s] 85%|████████▍ | 67/79 [00:01<00:00, 79.73it/s] 97%|█████████▋| 77/79 [00:01<00:00, 84.27it/s]100%|██████████| 79/79 [00:01<00:00, 54.99it/s]
10000 images processed, 1.4584333896636963 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:33,  2.04it/s] 11%|█▏        | 8/70 [00:00<00:03, 17.39it/s] 26%|██▌       | 18/70 [00:00<00:01, 36.75it/s] 40%|████      | 28/70 [00:00<00:00, 51.78it/s] 54%|█████▍    | 38/70 [00:00<00:00, 63.06it/s] 69%|██████▊   | 48/70 [00:01<00:00, 71.78it/s] 83%|████████▎ | 58/70 [00:01<00:00, 78.53it/s] 97%|█████████▋| 68/70 [00:01<00:00, 83.39it/s]100%|██████████| 70/70 [00:01<00:00, 55.58it/s]
8925 images processed, 1.2883358001708984 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:42,  1.04it/s]  9%|▉         | 4/45 [00:01<00:08,  4.77it/s] 31%|███       | 14/45 [00:01<00:01, 19.47it/s] 44%|████▍     | 20/45 [00:01<00:01, 19.05it/s] 53%|█████▎    | 24/45 [00:01<00:01, 20.35it/s] 73%|███████▎  | 33/45 [00:02<00:00, 22.62it/s] 82%|████████▏ | 37/45 [00:02<00:00, 22.34it/s]100%|██████████| 45/45 [00:02<00:00, 31.03it/s]100%|██████████| 45/45 [00:02<00:00, 19.61it/s]
5640 images processed, 2.315541982650757 seconds used

17.783965826034546
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.41  99.19  97.54
places365     74.66  79.96  78.16
LSUN          22.95  95.58  95.81
iSUN          78.15  80.59  83.61
dtd           42.13  90.62  93.87
AVG           44.26  89.19  89.80
Retain-Acc: 0.7446
Forget-as-OOD (retain known vs forget novel):
  FPR: 82.90 AUROC: 86.22 AUIN: 98.29
7.5399229526519775
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_reruns_rf.png
[Run] lambda=0.2 lr=0.001 epochs=5
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:18<01:13, 18.32s/it] 40%|████      | 2/5 [00:34<00:50, 16.86s/it] 60%|██████    | 3/5 [00:49<00:32, 16.04s/it] 80%|████████  | 4/5 [01:03<00:15, 15.49s/it]100%|██████████| 5/5 [01:18<00:00, 15.25s/it]100%|██████████| 5/5 [01:18<00:00, 15.74s/it]
[loss] ep 0 it 0 total=9.0400 mle=1.2802 pcon=5.2951 forget=2.4648 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1040 mle=1.3921 pcon=5.2887 forget=2.4232 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.2413 mle=1.5078 pcon=5.2827 forget=2.4507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2780 mle=1.5644 pcon=5.2770 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2052 mle=1.5330 pcon=5.2708 forget=2.4014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1118 mle=1.3806 pcon=5.2650 forget=2.4662 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.0052 mle=1.2920 pcon=5.2596 forget=2.4537 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1730 mle=1.4379 pcon=5.2543 forget=2.4809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 1 it 10 total=9.0163 mle=1.3065 pcon=5.2487 forget=2.4610 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0372 mle=1.3161 pcon=5.2433 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.9650 mle=1.2901 pcon=5.2382 forget=2.4367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.9540 mle=1.2380 pcon=5.2331 forget=2.4829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0630 mle=1.3723 pcon=5.2283 forget=2.4623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9880 mle=1.3468 pcon=5.2235 forget=2.4177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9850 mle=1.3082 pcon=5.2185 forget=2.4583 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0556 mle=1.3768 pcon=5.2136 forget=2.4651 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 2 it 20 total=8.9288 mle=1.2745 pcon=5.2089 forget=2.4454 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0969 mle=1.3989 pcon=5.2044 forget=2.4936 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.9871 mle=1.3590 pcon=5.1998 forget=2.4284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9911 mle=1.3578 pcon=5.1954 forget=2.4379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.9821 mle=1.3297 pcon=5.1909 forget=2.4615 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0215 mle=1.3659 pcon=5.1866 forget=2.4690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.8723 mle=1.2607 pcon=5.1824 forget=2.4292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.0074 mle=1.3835 pcon=5.1787 forget=2.4452 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 3 it 30 total=8.9980 mle=1.3639 pcon=5.1749 forget=2.4592 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9245 mle=1.2957 pcon=5.1710 forget=2.4578 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.8990 mle=1.2906 pcon=5.1672 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.0415 mle=1.4600 pcon=5.1635 forget=2.4179 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9798 mle=1.3523 pcon=5.1601 forget=2.4674 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0521 mle=1.4267 pcon=5.1568 forget=2.4687 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0462 mle=1.4482 pcon=5.1531 forget=2.4448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9152 mle=1.3139 pcon=5.1498 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 4 it 40 total=8.9573 mle=1.3770 pcon=5.1465 forget=2.4338 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9559 mle=1.3335 pcon=5.1435 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.1338 mle=1.5852 pcon=5.1406 forget=2.4079 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=8.9933 mle=1.3938 pcon=5.1375 forget=2.4620 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0241 mle=1.4143 pcon=5.1344 forget=2.4754 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=8.7696 mle=1.1862 pcon=5.1317 forget=2.4518 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.0392 mle=1.4613 pcon=5.1291 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:13,  2.02it/s]  2%|▏         | 9/391 [00:00<00:19, 19.47it/s]  5%|▍         | 18/391 [00:00<00:10, 36.26it/s]  7%|▋         | 28/391 [00:00<00:07, 51.44it/s] 10%|▉         | 38/391 [00:00<00:05, 62.92it/s] 12%|█▏        | 48/391 [00:01<00:04, 71.74it/s] 15%|█▍        | 58/391 [00:01<00:04, 75.67it/s] 17%|█▋        | 67/391 [00:01<00:04, 74.18it/s] 20%|█▉        | 77/391 [00:01<00:03, 79.70it/s] 22%|██▏       | 87/391 [00:01<00:03, 84.25it/s] 25%|██▍       | 97/391 [00:01<00:03, 87.60it/s] 27%|██▋       | 107/391 [00:01<00:03, 86.85it/s] 30%|██▉       | 117/391 [00:01<00:03, 89.18it/s] 32%|███▏      | 127/391 [00:01<00:02, 90.96it/s] 35%|███▌      | 137/391 [00:02<00:02, 86.80it/s] 37%|███▋      | 146/391 [00:02<00:03, 80.27it/s] 40%|███▉      | 155/391 [00:02<00:03, 75.91it/s] 42%|████▏     | 165/391 [00:02<00:02, 80.80it/s] 45%|████▍     | 175/391 [00:02<00:02, 84.57it/s] 47%|████▋     | 185/391 [00:02<00:02, 87.01it/s] 50%|████▉     | 195/391 [00:02<00:02, 89.36it/s] 52%|█████▏    | 205/391 [00:02<00:02, 90.77it/s] 55%|█████▍    | 215/391 [00:02<00:01, 92.27it/s] 58%|█████▊    | 225/391 [00:03<00:01, 92.74it/s] 60%|██████    | 235/391 [00:03<00:01, 93.14it/s] 63%|██████▎   | 245/391 [00:03<00:01, 93.59it/s] 65%|██████▌   | 255/391 [00:03<00:01, 94.04it/s] 68%|██████▊   | 265/391 [00:03<00:01, 94.05it/s] 70%|███████   | 275/391 [00:03<00:01, 94.33it/s] 73%|███████▎  | 285/391 [00:03<00:01, 94.33it/s] 75%|███████▌  | 295/391 [00:03<00:01, 94.37it/s] 78%|███████▊  | 305/391 [00:03<00:00, 94.27it/s] 81%|████████  | 315/391 [00:03<00:00, 94.22it/s] 83%|████████▎ | 325/391 [00:04<00:00, 94.61it/s] 86%|████████▌ | 335/391 [00:04<00:00, 94.93it/s] 88%|████████▊ | 345/391 [00:04<00:00, 94.75it/s] 91%|█████████ | 355/391 [00:04<00:00, 93.83it/s] 93%|█████████▎| 365/391 [00:04<00:00, 89.82it/s] 96%|█████████▌| 375/391 [00:04<00:00, 83.31it/s] 98%|█████████▊| 384/391 [00:04<00:00, 78.95it/s]100%|██████████| 391/391 [00:04<00:00, 79.21it/s]
50000 images processed, 5.032854795455933 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:45,  1.73it/s] 14%|█▍        | 11/79 [00:00<00:03, 20.74it/s] 27%|██▋       | 21/79 [00:00<00:01, 37.02it/s] 39%|███▉      | 31/79 [00:00<00:00, 50.40it/s] 51%|█████     | 40/79 [00:01<00:00, 59.34it/s] 63%|██████▎   | 50/79 [00:01<00:00, 68.20it/s] 76%|███████▌  | 60/79 [00:01<00:00, 75.15it/s] 87%|████████▋ | 69/79 [00:01<00:00, 77.72it/s] 99%|█████████▊| 78/79 [00:01<00:00, 73.95it/s]100%|██████████| 79/79 [00:01<00:00, 45.14it/s]
10000 images processed, 1.7970094680786133 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:34,  2.15it/s]  4%|▍         | 9/204 [00:00<00:09, 20.03it/s]  9%|▉         | 18/204 [00:00<00:05, 36.88it/s] 14%|█▎        | 28/204 [00:00<00:03, 51.95it/s] 18%|█▊        | 36/204 [00:00<00:02, 58.45it/s] 22%|██▏       | 44/204 [00:01<00:02, 56.32it/s] 25%|██▌       | 51/204 [00:01<00:02, 53.83it/s] 28%|██▊       | 58/204 [00:01<00:02, 57.53it/s] 32%|███▏      | 65/204 [00:01<00:02, 57.17it/s] 35%|███▌      | 72/204 [00:01<00:02, 58.12it/s] 39%|███▊      | 79/204 [00:01<00:02, 60.30it/s] 43%|████▎     | 87/204 [00:01<00:01, 65.45it/s] 48%|████▊     | 97/204 [00:01<00:01, 73.28it/s] 52%|█████▏    | 107/204 [00:01<00:01, 79.18it/s] 57%|█████▋    | 117/204 [00:02<00:01, 83.09it/s] 62%|██████▏   | 127/204 [00:02<00:00, 86.69it/s] 67%|██████▋   | 137/204 [00:02<00:00, 88.93it/s] 72%|███████▏  | 146/204 [00:02<00:00, 89.00it/s] 76%|███████▋  | 156/204 [00:02<00:00, 89.71it/s] 81%|████████▏ | 166/204 [00:02<00:00, 91.25it/s] 86%|████████▋ | 176/204 [00:02<00:00, 90.78it/s] 91%|█████████ | 186/204 [00:02<00:00, 92.46it/s] 96%|█████████▌| 196/204 [00:02<00:00, 93.42it/s]100%|██████████| 204/204 [00:02<00:00, 68.00it/s]
26032 images processed, 3.0395925045013428 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:43,  1.78it/s] 10%|█         | 8/79 [00:00<00:04, 15.44it/s] 22%|██▏       | 17/79 [00:00<00:01, 31.47it/s] 34%|███▍      | 27/79 [00:00<00:01, 46.81it/s] 47%|████▋     | 37/79 [00:00<00:00, 58.96it/s] 58%|█████▊    | 46/79 [00:01<00:00, 65.90it/s] 70%|██████▉   | 55/79 [00:01<00:00, 71.32it/s] 82%|████████▏ | 65/79 [00:01<00:00, 77.98it/s] 95%|█████████▍| 75/79 [00:01<00:00, 83.03it/s]100%|██████████| 79/79 [00:01<00:00, 54.30it/s]
10000 images processed, 1.485147476196289 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:38,  2.05it/s] 13%|█▎        | 10/79 [00:00<00:03, 21.87it/s] 25%|██▌       | 20/79 [00:00<00:01, 39.94it/s] 38%|███▊      | 30/79 [00:00<00:00, 53.77it/s] 51%|█████     | 40/79 [00:00<00:00, 64.80it/s] 63%|██████▎   | 50/79 [00:01<00:00, 72.83it/s] 76%|███████▌  | 60/79 [00:01<00:00, 78.66it/s] 89%|████████▊ | 70/79 [00:01<00:00, 83.45it/s]100%|██████████| 79/79 [00:01<00:00, 59.43it/s]
10000 images processed, 1.3600554466247559 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:36,  1.90it/s] 13%|█▎        | 9/70 [00:00<00:03, 18.59it/s] 26%|██▌       | 18/70 [00:00<00:01, 34.88it/s] 39%|███▊      | 27/70 [00:00<00:00, 48.36it/s] 53%|█████▎    | 37/70 [00:00<00:00, 60.33it/s] 67%|██████▋   | 47/70 [00:01<00:00, 69.97it/s] 81%|████████▏ | 57/70 [00:01<00:00, 76.97it/s] 96%|█████████▌| 67/70 [00:01<00:00, 82.50it/s]100%|██████████| 70/70 [00:01<00:00, 53.51it/s]
8925 images processed, 1.3420097827911377 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:55,  1.26s/it] 22%|██▏       | 10/45 [00:01<00:03,  9.86it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.90it/s] 60%|██████    | 27/45 [00:01<00:00, 22.89it/s] 73%|███████▎  | 33/45 [00:02<00:00, 17.93it/s] 96%|█████████▌| 43/45 [00:02<00:00, 27.03it/s]100%|██████████| 45/45 [00:02<00:00, 18.06it/s]
5640 images processed, 2.523803472518921 seconds used

18.184785842895508
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.58  99.36  98.01
places365     68.07  81.18  78.69
LSUN          17.81  96.08  96.08
iSUN          72.37  81.78  84.18
dtd           38.49  91.37  94.31
AVG           39.86  89.95  90.25
Retain-Acc: 0.7481
Forget-as-OOD (retain known vs forget novel):
  FPR: 54.70 AUROC: 89.29 AUIN: 98.64
7.674313545227051
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns_rf.png
[Run] lambda=0.2 lr=0.001 epochs=10
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:16<02:28, 16.51s/it] 20%|██        | 2/10 [00:30<01:59, 14.93s/it] 30%|███       | 3/10 [00:44<01:40, 14.39s/it] 40%|████      | 4/10 [00:58<01:26, 14.43s/it] 50%|█████     | 5/10 [01:12<01:11, 14.36s/it] 60%|██████    | 6/10 [01:28<00:59, 14.90s/it][loss] ep 0 it 0 total=9.0403 mle=1.2805 pcon=5.2951 forget=2.4647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1039 mle=1.3920 pcon=5.2887 forget=2.4232 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.2412 mle=1.5079 pcon=5.2827 forget=2.4506 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2781 mle=1.5644 pcon=5.2770 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2053 mle=1.5330 pcon=5.2708 forget=2.4014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1117 mle=1.3805 pcon=5.2650 forget=2.4662 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.0054 mle=1.2922 pcon=5.2596 forget=2.4536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1731 mle=1.4380 pcon=5.2543 forget=2.4809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 1 it 10 total=9.0164 mle=1.3066 pcon=5.2487 forget=2.4611 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0373 mle=1.3162 pcon=5.2433 forget=2.4779 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.9651 mle=1.2902 pcon=5.2382 forget=2.4367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.9541 mle=1.2380 pcon=5.2331 forget=2.4829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0631 mle=1.3724 pcon=5.2283 forget=2.4623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9879 mle=1.3467 pcon=5.2235 forget=2.4177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9850 mle=1.3082 pcon=5.2185 forget=2.4582 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0554 mle=1.3767 pcon=5.2136 forget=2.4651 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 2 it 20 total=8.9288 mle=1.2744 pcon=5.2089 forget=2.4455 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0968 mle=1.3988 pcon=5.2044 forget=2.4937 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.9870 mle=1.3588 pcon=5.1998 forget=2.4285 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9913 mle=1.3581 pcon=5.1954 forget=2.4378 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.9822 mle=1.3299 pcon=5.1908 forget=2.4614 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0215 mle=1.3660 pcon=5.1866 forget=2.4690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.8722 mle=1.2606 pcon=5.1824 forget=2.4292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.0076 mle=1.3836 pcon=5.1787 forget=2.4453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 3 it 30 total=8.9980 mle=1.3639 pcon=5.1749 forget=2.4593 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9245 mle=1.2957 pcon=5.1710 forget=2.4578 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.8988 mle=1.2904 pcon=5.1672 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.0414 mle=1.4600 pcon=5.1635 forget=2.4180 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9799 mle=1.3523 pcon=5.1601 forget=2.4674 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0520 mle=1.4266 pcon=5.1568 forget=2.4686 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0461 mle=1.4482 pcon=5.1531 forget=2.4448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9153 mle=1.3140 pcon=5.1498 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 4 it 40 total=8.9573 mle=1.3771 pcon=5.1465 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9559 mle=1.3336 pcon=5.1435 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.1339 mle=1.5853 pcon=5.1406 forget=2.4079 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=8.9933 mle=1.3939 pcon=5.1375 forget=2.4619 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0240 mle=1.4142 pcon=5.1344 forget=2.4754 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=8.7698 mle=1.1863 pcon=5.1316 forget=2.4518 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.0391 mle=1.4611 pcon=5.1291 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 5 it 0 total=8.8316 mle=1.2046 pcon=5.1261 forget=2.5009 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=8.9576 mle=1.3649 pcon=5.1234 forget=2.4693 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.0086 mle=1.4006 pcon=5.1207 forget=2.4873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.0681 mle=1.4808 pcon=5.1179 forget=2.4694 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.0086 mle=1.4524 pcon=5.1152 forget=2.4410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=8.9116 mle=1.3486 pcon=5.1125 forget=2.4504 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=8.9730 mle=1.4294 pcon=5.1099 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=8.9550 mle=1.4045 pcon=5.1073 forget=2.4432 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 6 it 10 total=8.8857 mle=1.2784 pcon=5.1048 forget=2.5024 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=8.9183 mle=1.3787 pcon=5.1025 forget=2.4371 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=8.8930 mle=1.3183 pcon=5.1003 forget=2.4744 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.8890 mle=1.3320 pcon=5.0980 forget=2.4590 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9234 mle=1.3655 pcon=5.0956 forget=2.4624 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=8.9545 mle=1.3833 pcon=5.0934 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=8.8669 mle=1.3536 pcon=5.0915 forget=2.4219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 70%|███████   | 7/10 [01:44<00:45, 15.08s/it] 80%|████████  | 8/10 [01:58<00:29, 14.87s/it] 90%|█████████ | 9/10 [02:12<00:14, 14.52s/it]100%|██████████| 10/10 [02:26<00:00, 14.53s/it]100%|██████████| 10/10 [02:26<00:00, 14.69s/it]
[loss] ep 6 it 360 total=8.9528 mle=1.4569 pcon=5.0893 forget=2.4067 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 7 it 20 total=8.7926 mle=1.2500 pcon=5.0872 forget=2.4554 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=8.7893 mle=1.2464 pcon=5.0849 forget=2.4580 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.9122 mle=1.3875 pcon=5.0829 forget=2.4417 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.9198 mle=1.4330 pcon=5.0810 forget=2.4058 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=8.8354 mle=1.3209 pcon=5.0794 forget=2.4351 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=8.9747 mle=1.4542 pcon=5.0777 forget=2.4428 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=8.8528 mle=1.3167 pcon=5.0756 forget=2.4606 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=8.9363 mle=1.3707 pcon=5.0736 forget=2.4920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 8 it 30 total=8.9610 mle=1.4805 pcon=5.0719 forget=2.4087 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=8.8051 mle=1.2745 pcon=5.0702 forget=2.4604 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=8.9212 mle=1.3983 pcon=5.0685 forget=2.4545 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=8.8647 mle=1.3446 pcon=5.0668 forget=2.4532 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=8.8877 mle=1.3661 pcon=5.0652 forget=2.4564 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.7837 mle=1.2569 pcon=5.0636 forget=2.4633 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.0220 mle=1.5798 pcon=5.0618 forget=2.3805 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.8763 mle=1.3644 pcon=5.0604 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 9 it 40 total=8.9254 mle=1.4107 pcon=5.0589 forget=2.4558 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=8.9710 mle=1.4930 pcon=5.0574 forget=2.4205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=8.8783 mle=1.3538 pcon=5.0563 forget=2.4682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.7494 mle=1.2283 pcon=5.0550 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=8.9737 mle=1.5025 pcon=5.0540 forget=2.4171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=8.8660 mle=1.3564 pcon=5.0526 forget=2.4571 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.8158 mle=1.3067 pcon=5.0514 forget=2.4577 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:12,  2.03it/s]  3%|▎         | 10/391 [00:00<00:17, 21.57it/s]  5%|▍         | 19/391 [00:00<00:09, 37.31it/s]  7%|▋         | 28/391 [00:00<00:07, 50.09it/s]  9%|▉         | 37/391 [00:00<00:05, 60.31it/s] 12%|█▏        | 47/391 [00:01<00:04, 69.79it/s] 15%|█▍        | 57/391 [00:01<00:04, 76.79it/s] 17%|█▋        | 67/391 [00:01<00:03, 82.14it/s] 20%|█▉        | 77/391 [00:01<00:03, 85.29it/s] 22%|██▏       | 87/391 [00:01<00:03, 87.32it/s] 25%|██▍       | 97/391 [00:01<00:03, 89.25it/s] 27%|██▋       | 107/391 [00:01<00:03, 91.07it/s] 30%|██▉       | 117/391 [00:01<00:03, 90.65it/s] 32%|███▏      | 127/391 [00:01<00:02, 91.27it/s] 35%|███▌      | 137/391 [00:01<00:02, 90.97it/s] 38%|███▊      | 147/391 [00:02<00:02, 91.77it/s] 40%|████      | 157/391 [00:02<00:02, 92.72it/s] 43%|████▎     | 167/391 [00:02<00:02, 93.17it/s] 45%|████▌     | 177/391 [00:02<00:02, 93.17it/s] 48%|████▊     | 187/391 [00:02<00:02, 93.41it/s] 50%|█████     | 197/391 [00:02<00:02, 93.29it/s] 53%|█████▎    | 207/391 [00:02<00:01, 94.02it/s] 55%|█████▌    | 217/391 [00:02<00:01, 93.90it/s] 58%|█████▊    | 227/391 [00:02<00:01, 93.92it/s] 61%|██████    | 237/391 [00:03<00:01, 93.84it/s] 63%|██████▎   | 247/391 [00:03<00:01, 93.30it/s] 66%|██████▌   | 257/391 [00:03<00:01, 93.81it/s] 68%|██████▊   | 267/391 [00:03<00:01, 93.34it/s] 71%|███████   | 277/391 [00:03<00:01, 90.38it/s] 73%|███████▎  | 287/391 [00:03<00:01, 91.86it/s] 76%|███████▌  | 297/391 [00:03<00:01, 92.98it/s] 79%|███████▊  | 307/391 [00:03<00:00, 93.86it/s] 81%|████████  | 317/391 [00:03<00:00, 94.13it/s] 84%|████████▎ | 327/391 [00:04<00:00, 94.54it/s] 86%|████████▌ | 337/391 [00:04<00:00, 94.85it/s] 89%|████████▊ | 347/391 [00:04<00:00, 93.77it/s] 91%|█████████▏| 357/391 [00:04<00:00, 93.27it/s] 94%|█████████▍| 367/391 [00:04<00:00, 93.19it/s] 96%|█████████▋| 377/391 [00:04<00:00, 93.12it/s] 99%|█████████▉| 387/391 [00:04<00:00, 91.06it/s]100%|██████████| 391/391 [00:04<00:00, 82.77it/s]
50000 images processed, 4.807439565658569 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:33,  2.34it/s] 11%|█▏        | 9/79 [00:00<00:03, 21.32it/s] 23%|██▎       | 18/79 [00:00<00:01, 38.49it/s] 34%|███▍      | 27/79 [00:00<00:01, 51.78it/s] 46%|████▌     | 36/79 [00:00<00:00, 62.25it/s] 58%|█████▊    | 46/79 [00:00<00:00, 70.79it/s] 71%|███████   | 56/79 [00:01<00:00, 77.08it/s] 84%|████████▎ | 66/79 [00:01<00:00, 81.85it/s] 96%|█████████▌| 76/79 [00:01<00:00, 85.21it/s]100%|██████████| 79/79 [00:02<00:00, 32.16it/s]
10000 images processed, 2.521925926208496 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:37,  2.09it/s]  5%|▍         | 10/204 [00:00<00:08, 21.80it/s]  9%|▉         | 19/204 [00:00<00:04, 38.08it/s] 14%|█▍        | 29/204 [00:00<00:03, 53.02it/s] 19%|█▉        | 39/204 [00:00<00:02, 63.81it/s] 24%|██▍       | 49/204 [00:01<00:02, 71.40it/s] 28%|██▊       | 58/204 [00:01<00:01, 74.22it/s] 33%|███▎      | 67/204 [00:01<00:01, 75.72it/s] 37%|███▋      | 76/204 [00:01<00:01, 73.77it/s] 41%|████      | 84/204 [00:01<00:01, 73.64it/s] 45%|████▌     | 92/204 [00:01<00:01, 74.77it/s] 50%|█████     | 102/204 [00:01<00:01, 79.91it/s] 55%|█████▍    | 112/204 [00:01<00:01, 84.17it/s] 60%|█████▉    | 122/204 [00:01<00:00, 86.62it/s] 65%|██████▍   | 132/204 [00:02<00:00, 88.23it/s] 70%|██████▉   | 142/204 [00:02<00:00, 89.63it/s] 75%|███████▍  | 152/204 [00:02<00:00, 90.59it/s] 79%|███████▉  | 162/204 [00:02<00:00, 91.51it/s] 84%|████████▍ | 172/204 [00:02<00:00, 92.11it/s] 89%|████████▉ | 182/204 [00:02<00:00, 93.09it/s] 94%|█████████▍| 192/204 [00:02<00:00, 93.95it/s] 99%|█████████▉| 202/204 [00:02<00:00, 94.51it/s]100%|██████████| 204/204 [00:02<00:00, 73.16it/s]
26032 images processed, 2.827554702758789 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:48,  1.61it/s] 14%|█▍        | 11/79 [00:00<00:03, 19.67it/s] 27%|██▋       | 21/79 [00:00<00:01, 35.80it/s] 39%|███▉      | 31/79 [00:00<00:00, 49.31it/s] 52%|█████▏    | 41/79 [00:01<00:00, 60.38it/s] 65%|██████▍   | 51/79 [00:01<00:00, 68.83it/s] 77%|███████▋  | 61/79 [00:01<00:00, 75.32it/s] 90%|████████▉ | 71/79 [00:01<00:00, 80.85it/s]100%|██████████| 79/79 [00:01<00:00, 54.18it/s]
10000 images processed, 1.488227128982544 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:37,  2.09it/s] 11%|█▏        | 9/79 [00:00<00:03, 19.53it/s] 23%|██▎       | 18/79 [00:00<00:01, 36.12it/s] 35%|███▌      | 28/79 [00:00<00:00, 51.50it/s] 48%|████▊     | 38/79 [00:00<00:00, 63.06it/s] 61%|██████    | 48/79 [00:01<00:00, 71.16it/s] 73%|███████▎  | 58/79 [00:01<00:00, 77.98it/s] 86%|████████▌ | 68/79 [00:01<00:00, 83.05it/s] 99%|█████████▊| 78/79 [00:01<00:00, 86.70it/s]100%|██████████| 79/79 [00:01<00:00, 59.11it/s]
10000 images processed, 1.356574535369873 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:33,  2.06it/s] 16%|█▌        | 11/70 [00:00<00:02, 23.81it/s] 30%|███       | 21/70 [00:00<00:01, 41.19it/s] 41%|████▏     | 29/70 [00:00<00:00, 45.47it/s] 54%|█████▍    | 38/70 [00:00<00:00, 56.18it/s] 67%|██████▋   | 47/70 [00:01<00:00, 63.91it/s] 81%|████████▏ | 57/70 [00:01<00:00, 72.68it/s] 96%|█████████▌| 67/70 [00:01<00:00, 79.31it/s]100%|██████████| 70/70 [00:01<00:00, 53.52it/s]
8925 images processed, 1.339775800704956 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:51,  1.17s/it]  4%|▍         | 2/45 [00:01<00:24,  1.76it/s] 22%|██▏       | 10/45 [00:01<00:02, 11.79it/s] 38%|███▊      | 17/45 [00:01<00:01, 17.59it/s] 47%|████▋     | 21/45 [00:01<00:01, 20.38it/s] 56%|█████▌    | 25/45 [00:01<00:00, 20.78it/s] 73%|███████▎  | 33/45 [00:02<00:00, 23.58it/s] 80%|████████  | 36/45 [00:02<00:00, 24.37it/s]100%|██████████| 45/45 [00:02<00:00, 35.93it/s]100%|██████████| 45/45 [00:02<00:00, 18.37it/s]
5640 images processed, 2.472608804702759 seconds used

18.441701889038086
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.59  99.36  98.00
places365     68.14  81.17  78.70
LSUN          17.68  96.09  96.10
iSUN          72.46  81.74  84.15
dtd           38.56  91.35  94.29
AVG           39.89  89.94  90.25
Retain-Acc: 0.7487
Forget-as-OOD (retain known vs forget novel):
  FPR: 54.50 AUROC: 89.28 AUIN: 98.64
24.150643587112427
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns_rf.png
[Run] lambda=0.2 lr=0.001 epochs=15
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=15, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/15 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  7%|▋         | 1/15 [00:20<04:41, 20.10s/it] 13%|█▎        | 2/15 [00:34<03:38, 16.83s/it] 20%|██        | 3/15 [00:48<03:08, 15.69s/it] 27%|██▋       | 4/15 [01:04<02:53, 15.74s/it] 33%|███▎      | 5/15 [01:19<02:33, 15.39s/it] 40%|████      | 6/15 [01:33<02:15, 15.05s/it][loss] ep 0 it 0 total=9.0403 mle=1.2805 pcon=5.2951 forget=2.4647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1040 mle=1.3921 pcon=5.2887 forget=2.4232 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.2412 mle=1.5079 pcon=5.2827 forget=2.4506 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2780 mle=1.5644 pcon=5.2770 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2054 mle=1.5331 pcon=5.2708 forget=2.4014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1116 mle=1.3804 pcon=5.2650 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.0053 mle=1.2921 pcon=5.2596 forget=2.4536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1731 mle=1.4380 pcon=5.2543 forget=2.4809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 1 it 10 total=9.0164 mle=1.3066 pcon=5.2487 forget=2.4611 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0372 mle=1.3161 pcon=5.2433 forget=2.4779 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.9652 mle=1.2903 pcon=5.2382 forget=2.4367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.9539 mle=1.2379 pcon=5.2331 forget=2.4829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0630 mle=1.3724 pcon=5.2283 forget=2.4623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9881 mle=1.3469 pcon=5.2235 forget=2.4177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9849 mle=1.3081 pcon=5.2185 forget=2.4583 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0553 mle=1.3766 pcon=5.2136 forget=2.4651 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 2 it 20 total=8.9290 mle=1.2746 pcon=5.2089 forget=2.4455 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0969 mle=1.3989 pcon=5.2044 forget=2.4937 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.9870 mle=1.3588 pcon=5.1998 forget=2.4284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9913 mle=1.3580 pcon=5.1954 forget=2.4379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.9823 mle=1.3300 pcon=5.1908 forget=2.4615 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0214 mle=1.3659 pcon=5.1866 forget=2.4690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.8722 mle=1.2606 pcon=5.1824 forget=2.4292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.0075 mle=1.3835 pcon=5.1787 forget=2.4453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 3 it 30 total=8.9980 mle=1.3639 pcon=5.1749 forget=2.4592 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9247 mle=1.2958 pcon=5.1710 forget=2.4579 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.8987 mle=1.2903 pcon=5.1672 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.0415 mle=1.4600 pcon=5.1635 forget=2.4180 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9798 mle=1.3522 pcon=5.1601 forget=2.4674 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0520 mle=1.4266 pcon=5.1568 forget=2.4686 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0460 mle=1.4481 pcon=5.1531 forget=2.4448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9153 mle=1.3140 pcon=5.1498 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 4 it 40 total=8.9573 mle=1.3771 pcon=5.1465 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9560 mle=1.3336 pcon=5.1435 forget=2.4790 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.1338 mle=1.5852 pcon=5.1406 forget=2.4080 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=8.9933 mle=1.3939 pcon=5.1375 forget=2.4620 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0239 mle=1.4141 pcon=5.1344 forget=2.4754 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=8.7697 mle=1.1862 pcon=5.1316 forget=2.4518 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.0390 mle=1.4611 pcon=5.1291 forget=2.4489 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 5 it 0 total=8.8317 mle=1.2046 pcon=5.1261 forget=2.5010 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=8.9577 mle=1.3649 pcon=5.1235 forget=2.4693 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.0087 mle=1.4007 pcon=5.1207 forget=2.4873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.0681 mle=1.4808 pcon=5.1179 forget=2.4694 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.0087 mle=1.4525 pcon=5.1152 forget=2.4410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=8.9115 mle=1.3486 pcon=5.1125 forget=2.4504 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=8.9730 mle=1.4294 pcon=5.1099 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=8.9549 mle=1.4045 pcon=5.1073 forget=2.4432 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 6 it 10 total=8.8858 mle=1.2785 pcon=5.1049 forget=2.5024 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=8.9182 mle=1.3786 pcon=5.1026 forget=2.4370 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=8.8930 mle=1.3183 pcon=5.1003 forget=2.4744 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.8890 mle=1.3320 pcon=5.0981 forget=2.4590 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9235 mle=1.3654 pcon=5.0956 forget=2.4625 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=8.9545 mle=1.3832 pcon=5.0935 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=8.8670 mle=1.3536 pcon=5.0915 forget=2.4219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 47%|████▋     | 7/15 [01:49<02:00, 15.12s/it] 53%|█████▎    | 8/15 [02:04<01:45, 15.12s/it] 60%|██████    | 9/15 [02:18<01:29, 14.92s/it] 67%|██████▋   | 10/15 [02:34<01:15, 15.09s/it] 73%|███████▎  | 11/15 [02:49<01:00, 15.00s/it] 80%|████████  | 12/15 [03:03<00:44, 14.77s/it] 87%|████████▋ | 13/15 [03:17<00:29, 14.64s/it][loss] ep 6 it 360 total=8.9530 mle=1.4570 pcon=5.0893 forget=2.4067 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 7 it 20 total=8.7926 mle=1.2500 pcon=5.0872 forget=2.4554 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=8.7892 mle=1.2462 pcon=5.0849 forget=2.4580 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.9121 mle=1.3875 pcon=5.0829 forget=2.4417 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.9197 mle=1.4330 pcon=5.0810 forget=2.4057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=8.8357 mle=1.3211 pcon=5.0794 forget=2.4352 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=8.9746 mle=1.4541 pcon=5.0777 forget=2.4427 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=8.8528 mle=1.3167 pcon=5.0756 forget=2.4606 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=8.9364 mle=1.3708 pcon=5.0736 forget=2.4920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 8 it 30 total=8.9611 mle=1.4805 pcon=5.0719 forget=2.4088 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=8.8051 mle=1.2745 pcon=5.0702 forget=2.4604 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=8.9212 mle=1.3983 pcon=5.0685 forget=2.4544 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=8.8647 mle=1.3446 pcon=5.0668 forget=2.4533 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=8.8876 mle=1.3661 pcon=5.0652 forget=2.4564 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.7838 mle=1.2569 pcon=5.0636 forget=2.4633 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.0221 mle=1.5797 pcon=5.0618 forget=2.3806 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.8762 mle=1.3644 pcon=5.0604 forget=2.4514 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 9 it 40 total=8.9254 mle=1.4107 pcon=5.0589 forget=2.4558 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=8.9708 mle=1.4928 pcon=5.0575 forget=2.4205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=8.8783 mle=1.3538 pcon=5.0563 forget=2.4682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.7494 mle=1.2283 pcon=5.0551 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=8.9735 mle=1.5023 pcon=5.0540 forget=2.4171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=8.8660 mle=1.3563 pcon=5.0526 forget=2.4571 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.8158 mle=1.3067 pcon=5.0514 forget=2.4577 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 10 it 0 total=8.7874 mle=1.2681 pcon=5.0497 forget=2.4696 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.7534 mle=1.2563 pcon=5.0489 forget=2.4483 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=8.8060 mle=1.3266 pcon=5.0475 forget=2.4319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=8.8731 mle=1.3438 pcon=5.0463 forget=2.4831 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=8.9087 mle=1.4083 pcon=5.0453 forget=2.4551 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=8.9167 mle=1.4252 pcon=5.0443 forget=2.4472 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=8.9386 mle=1.5124 pcon=5.0431 forget=2.3831 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.7568 mle=1.2608 pcon=5.0423 forget=2.4537 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 11 it 10 total=8.9133 mle=1.4404 pcon=5.0411 forget=2.4319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.8453 mle=1.3726 pcon=5.0402 forget=2.4325 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.8333 mle=1.3448 pcon=5.0394 forget=2.4491 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.7809 mle=1.2732 pcon=5.0383 forget=2.4694 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.7530 mle=1.2650 pcon=5.0374 forget=2.4505 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.7372 mle=1.2664 pcon=5.0365 forget=2.4343 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.8674 mle=1.3905 pcon=5.0357 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.8313 mle=1.3185 pcon=5.0350 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 12 it 20 total=8.8033 mle=1.3133 pcon=5.0338 forget=2.4562 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.8427 mle=1.3550 pcon=5.0328 forget=2.4548 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.7970 mle=1.3615 pcon=5.0318 forget=2.4036 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.8376 mle=1.3385 pcon=5.0308 forget=2.4683 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.7332 mle=1.2241 pcon=5.0302 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.7535 mle=1.2957 pcon=5.0297 forget=2.4281 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.8912 mle=1.4129 pcon=5.0291 forget=2.4492 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.8536 mle=1.3793 pcon=5.0284 forget=2.4459 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 13 it 30 total=8.8180 mle=1.3138 pcon=5.0280 forget=2.4762 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.9415 mle=1.5019 pcon=5.0273 forget=2.4124 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.8191 mle=1.3533 pcon=5.0265 forget=2.4393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=8.8648 mle=1.3563 pcon=5.0254 forget=2.4831 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 93%|█████████▎| 14/15 [03:32<00:14, 14.63s/it]100%|██████████| 15/15 [03:47<00:00, 14.68s/it]100%|██████████| 15/15 [03:47<00:00, 15.14s/it]
[loss] ep 13 it 230 total=8.7542 mle=1.3016 pcon=5.0247 forget=2.4279 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.7473 mle=1.2232 pcon=5.0240 forget=2.5001 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.8543 mle=1.3967 pcon=5.0235 forget=2.4341 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.7803 mle=1.3291 pcon=5.0229 forget=2.4282 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 14 it 40 total=8.7364 mle=1.2763 pcon=5.0222 forget=2.4379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.8080 mle=1.3277 pcon=5.0218 forget=2.4586 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.7975 mle=1.3045 pcon=5.0212 forget=2.4718 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.7479 mle=1.2654 pcon=5.0207 forget=2.4618 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.8137 mle=1.3328 pcon=5.0198 forget=2.4611 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.7305 mle=1.2976 pcon=5.0193 forget=2.4136 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.7027 mle=1.2496 pcon=5.0189 forget=2.4342 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:04,  2.12it/s]  2%|▏         | 9/391 [00:00<00:18, 20.16it/s]  5%|▍         | 18/391 [00:00<00:09, 37.49it/s]  7%|▋         | 27/391 [00:00<00:07, 51.04it/s]  9%|▉         | 36/391 [00:00<00:05, 60.77it/s] 12%|█▏        | 45/391 [00:00<00:05, 68.14it/s] 14%|█▍        | 54/391 [00:01<00:04, 71.33it/s] 16%|█▌        | 63/391 [00:01<00:04, 74.15it/s] 19%|█▊        | 73/391 [00:01<00:03, 79.73it/s] 21%|██        | 83/391 [00:01<00:03, 82.88it/s] 24%|██▍       | 93/391 [00:01<00:03, 85.85it/s] 26%|██▌       | 102/391 [00:01<00:03, 85.52it/s] 28%|██▊       | 111/391 [00:01<00:03, 82.46it/s] 31%|███       | 121/391 [00:01<00:03, 85.52it/s] 34%|███▎      | 131/391 [00:01<00:02, 87.20it/s] 36%|███▌      | 141/391 [00:02<00:02, 88.85it/s] 39%|███▊      | 151/391 [00:02<00:02, 89.51it/s] 41%|████      | 161/391 [00:02<00:02, 90.56it/s] 44%|████▎     | 171/391 [00:02<00:02, 91.02it/s] 46%|████▋     | 181/391 [00:02<00:02, 90.32it/s] 49%|████▉     | 191/391 [00:02<00:02, 91.98it/s] 51%|█████▏    | 201/391 [00:02<00:02, 91.73it/s] 54%|█████▍    | 211/391 [00:02<00:01, 91.36it/s] 57%|█████▋    | 221/391 [00:02<00:01, 91.89it/s] 59%|█████▉    | 231/391 [00:03<00:01, 92.93it/s] 62%|██████▏   | 241/391 [00:03<00:01, 92.66it/s] 64%|██████▍   | 251/391 [00:03<00:01, 90.73it/s] 67%|██████▋   | 261/391 [00:03<00:01, 90.38it/s] 69%|██████▉   | 271/391 [00:03<00:01, 90.80it/s] 72%|███████▏  | 281/391 [00:03<00:01, 91.24it/s] 74%|███████▍  | 291/391 [00:03<00:01, 90.84it/s] 77%|███████▋  | 301/391 [00:03<00:00, 91.75it/s] 80%|███████▉  | 311/391 [00:03<00:00, 91.36it/s] 82%|████████▏ | 321/391 [00:04<00:00, 91.13it/s] 85%|████████▍ | 331/391 [00:04<00:00, 92.41it/s] 87%|████████▋ | 341/391 [00:04<00:00, 92.48it/s] 90%|████████▉ | 351/391 [00:04<00:00, 92.53it/s] 92%|█████████▏| 361/391 [00:04<00:00, 90.90it/s] 95%|█████████▍| 371/391 [00:04<00:00, 91.36it/s] 97%|█████████▋| 381/391 [00:04<00:00, 93.24it/s]100%|██████████| 391/391 [00:04<00:00, 90.88it/s]100%|██████████| 391/391 [00:04<00:00, 81.20it/s]
50000 images processed, 4.912892818450928 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:31,  2.45it/s] 13%|█▎        | 10/79 [00:00<00:02, 25.01it/s] 24%|██▍       | 19/79 [00:00<00:01, 42.06it/s] 35%|███▌      | 28/79 [00:00<00:00, 54.66it/s] 47%|████▋     | 37/79 [00:00<00:00, 63.61it/s] 57%|█████▋    | 45/79 [00:00<00:00, 67.93it/s] 70%|██████▉   | 55/79 [00:01<00:00, 75.43it/s] 82%|████████▏ | 65/79 [00:01<00:00, 80.85it/s] 95%|█████████▍| 75/79 [00:01<00:00, 84.08it/s]100%|██████████| 79/79 [00:01<00:00, 42.45it/s]
10000 images processed, 1.9194591045379639 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:32,  2.21it/s]  4%|▍         | 9/204 [00:00<00:09, 20.76it/s]  9%|▉         | 19/204 [00:00<00:04, 40.24it/s] 14%|█▍        | 29/204 [00:00<00:03, 54.81it/s] 19%|█▉        | 39/204 [00:00<00:02, 65.33it/s] 24%|██▎       | 48/204 [00:00<00:02, 71.50it/s] 28%|██▊       | 57/204 [00:01<00:01, 74.35it/s] 32%|███▏      | 66/204 [00:01<00:01, 77.50it/s] 37%|███▋      | 76/204 [00:01<00:01, 82.47it/s] 42%|████▏     | 86/204 [00:01<00:01, 85.52it/s] 47%|████▋     | 95/204 [00:01<00:01, 86.74it/s] 51%|█████▏    | 105/204 [00:01<00:01, 88.76it/s] 56%|█████▋    | 115/204 [00:01<00:00, 90.23it/s] 61%|██████▏   | 125/204 [00:01<00:00, 90.81it/s] 66%|██████▌   | 135/204 [00:01<00:00, 91.75it/s] 71%|███████   | 145/204 [00:02<00:00, 92.30it/s] 76%|███████▌  | 155/204 [00:02<00:00, 92.50it/s] 81%|████████  | 165/204 [00:02<00:00, 92.47it/s] 86%|████████▌ | 175/204 [00:02<00:00, 92.47it/s] 91%|█████████ | 185/204 [00:02<00:00, 93.61it/s] 96%|█████████▌| 195/204 [00:02<00:00, 94.34it/s]100%|██████████| 204/204 [00:02<00:00, 76.07it/s]
26032 images processed, 2.723025321960449 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:47,  1.63it/s]  4%|▍         | 3/79 [00:00<00:16,  4.69it/s] 13%|█▎        | 10/79 [00:00<00:03, 17.50it/s] 23%|██▎       | 18/79 [00:00<00:01, 30.95it/s] 30%|███       | 24/79 [00:01<00:01, 34.17it/s] 42%|████▏     | 33/79 [00:01<00:00, 47.17it/s] 54%|█████▍    | 43/79 [00:01<00:00, 59.14it/s] 67%|██████▋   | 53/79 [00:01<00:00, 68.35it/s] 80%|███████▉  | 63/79 [00:01<00:00, 75.73it/s] 92%|█████████▏| 73/79 [00:01<00:00, 81.21it/s]100%|██████████| 79/79 [00:01<00:00, 46.33it/s]
10000 images processed, 1.7327549457550049 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:39,  1.99it/s] 14%|█▍        | 11/79 [00:00<00:02, 23.32it/s] 25%|██▌       | 20/79 [00:00<00:01, 39.02it/s] 38%|███▊      | 30/79 [00:00<00:00, 53.61it/s] 49%|████▉     | 39/79 [00:00<00:00, 62.63it/s] 62%|██████▏   | 49/79 [00:01<00:00, 71.51it/s] 75%|███████▍  | 59/79 [00:01<00:00, 78.33it/s] 87%|████████▋ | 69/79 [00:01<00:00, 83.29it/s]100%|██████████| 79/79 [00:01<00:00, 59.21it/s]
10000 images processed, 1.3557076454162598 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:51,  1.34it/s] 16%|█▌        | 11/70 [00:00<00:03, 16.89it/s] 29%|██▊       | 20/70 [00:00<00:01, 29.95it/s] 43%|████▎     | 30/70 [00:01<00:00, 43.55it/s] 56%|█████▌    | 39/70 [00:01<00:00, 52.48it/s] 69%|██████▊   | 48/70 [00:01<00:00, 60.13it/s] 83%|████████▎ | 58/70 [00:01<00:00, 69.29it/s] 97%|█████████▋| 68/70 [00:01<00:00, 76.38it/s]100%|██████████| 70/70 [00:01<00:00, 45.74it/s]
8925 images processed, 1.5606427192687988 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:44,  1.01s/it]  4%|▍         | 2/45 [00:01<00:20,  2.10it/s] 18%|█▊        | 8/45 [00:01<00:04,  9.12it/s] 38%|███▊      | 17/45 [00:01<00:01, 19.95it/s] 51%|█████     | 23/45 [00:01<00:01, 21.96it/s] 60%|██████    | 27/45 [00:01<00:00, 22.24it/s] 73%|███████▎  | 33/45 [00:02<00:00, 24.27it/s] 87%|████████▋ | 39/45 [00:02<00:00, 30.08it/s] 96%|█████████▌| 43/45 [00:02<00:00, 26.79it/s]100%|██████████| 45/45 [00:02<00:00, 18.43it/s]
5640 images processed, 2.4648444652557373 seconds used

18.23999786376953
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.60  99.35  97.98
places365     68.38  81.14  78.72
LSUN          17.76  96.08  96.09
iSUN          72.64  81.75  84.17
dtd           38.72  91.34  94.28
AVG           40.02  89.93  90.25
Retain-Acc: 0.7487
Forget-as-OOD (retain known vs forget novel):
  FPR: 55.20 AUROC: 89.20 AUIN: 98.63
7.864380121231079
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns_rf.png
[Run] lambda=0.2 lr=0.001 epochs=20
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:20<06:33, 20.73s/it] 10%|█         | 2/20 [00:34<05:02, 16.79s/it] 15%|█▌        | 3/20 [00:49<04:28, 15.79s/it] 20%|██        | 4/20 [01:03<04:02, 15.19s/it] 25%|██▌       | 5/20 [01:18<03:44, 14.99s/it] 30%|███       | 6/20 [01:34<03:33, 15.25s/it][loss] ep 0 it 0 total=9.0401 mle=1.2803 pcon=5.2951 forget=2.4647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1044 mle=1.3924 pcon=5.2887 forget=2.4232 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.2412 mle=1.5078 pcon=5.2827 forget=2.4506 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2781 mle=1.5645 pcon=5.2770 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2050 mle=1.5328 pcon=5.2708 forget=2.4014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1117 mle=1.3806 pcon=5.2650 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.0054 mle=1.2922 pcon=5.2596 forget=2.4536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1731 mle=1.4380 pcon=5.2543 forget=2.4809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 1 it 10 total=9.0163 mle=1.3066 pcon=5.2487 forget=2.4610 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0369 mle=1.3158 pcon=5.2433 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.9651 mle=1.2903 pcon=5.2382 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.9537 mle=1.2378 pcon=5.2331 forget=2.4829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0630 mle=1.3723 pcon=5.2283 forget=2.4623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9878 mle=1.3466 pcon=5.2235 forget=2.4177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9849 mle=1.3082 pcon=5.2185 forget=2.4582 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0553 mle=1.3765 pcon=5.2136 forget=2.4652 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 2 it 20 total=8.9291 mle=1.2747 pcon=5.2089 forget=2.4455 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0967 mle=1.3987 pcon=5.2044 forget=2.4936 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.9870 mle=1.3588 pcon=5.1998 forget=2.4284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9913 mle=1.3580 pcon=5.1954 forget=2.4378 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.9823 mle=1.3300 pcon=5.1908 forget=2.4615 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0217 mle=1.3661 pcon=5.1865 forget=2.4690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.8723 mle=1.2607 pcon=5.1824 forget=2.4292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.0075 mle=1.3835 pcon=5.1787 forget=2.4453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 3 it 30 total=8.9981 mle=1.3640 pcon=5.1749 forget=2.4593 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9245 mle=1.2957 pcon=5.1710 forget=2.4578 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.8989 mle=1.2905 pcon=5.1672 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.0412 mle=1.4598 pcon=5.1635 forget=2.4179 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9799 mle=1.3524 pcon=5.1601 forget=2.4674 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0521 mle=1.4267 pcon=5.1568 forget=2.4686 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0459 mle=1.4480 pcon=5.1531 forget=2.4448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9152 mle=1.3139 pcon=5.1497 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 4 it 40 total=8.9571 mle=1.3768 pcon=5.1465 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9561 mle=1.3338 pcon=5.1435 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.1336 mle=1.5850 pcon=5.1406 forget=2.4080 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=8.9931 mle=1.3936 pcon=5.1375 forget=2.4619 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0242 mle=1.4143 pcon=5.1344 forget=2.4754 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=8.7696 mle=1.1862 pcon=5.1316 forget=2.4518 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.0390 mle=1.4610 pcon=5.1291 forget=2.4489 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 5 it 0 total=8.8317 mle=1.2046 pcon=5.1261 forget=2.5009 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=8.9574 mle=1.3648 pcon=5.1235 forget=2.4692 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.0085 mle=1.4005 pcon=5.1207 forget=2.4873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.0682 mle=1.4809 pcon=5.1179 forget=2.4695 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.0087 mle=1.4524 pcon=5.1152 forget=2.4411 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=8.9114 mle=1.3485 pcon=5.1125 forget=2.4504 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=8.9731 mle=1.4295 pcon=5.1099 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=8.9549 mle=1.4044 pcon=5.1073 forget=2.4432 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 6 it 10 total=8.8857 mle=1.2783 pcon=5.1049 forget=2.5025 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=8.9184 mle=1.3789 pcon=5.1025 forget=2.4370 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=8.8928 mle=1.3181 pcon=5.1003 forget=2.4744 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.8888 mle=1.3318 pcon=5.0980 forget=2.4590 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9235 mle=1.3655 pcon=5.0956 forget=2.4625 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=8.9542 mle=1.3830 pcon=5.0934 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=8.8669 mle=1.3536 pcon=5.0915 forget=2.4219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 35%|███▌      | 7/20 [01:49<03:20, 15.40s/it] 40%|████      | 8/20 [02:05<03:07, 15.66s/it] 45%|████▌     | 9/20 [02:21<02:51, 15.55s/it] 50%|█████     | 10/20 [02:37<02:36, 15.64s/it] 55%|█████▌    | 11/20 [02:52<02:21, 15.67s/it] 60%|██████    | 12/20 [03:08<02:04, 15.58s/it] 65%|██████▌   | 13/20 [03:23<01:48, 15.56s/it][loss] ep 6 it 360 total=8.9528 mle=1.4569 pcon=5.0893 forget=2.4066 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 7 it 20 total=8.7926 mle=1.2499 pcon=5.0872 forget=2.4555 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=8.7894 mle=1.2464 pcon=5.0849 forget=2.4581 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.9123 mle=1.3876 pcon=5.0829 forget=2.4418 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.9198 mle=1.4331 pcon=5.0810 forget=2.4057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=8.8356 mle=1.3211 pcon=5.0794 forget=2.4352 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=8.9747 mle=1.4543 pcon=5.0777 forget=2.4427 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=8.8528 mle=1.3167 pcon=5.0756 forget=2.4606 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=8.9363 mle=1.3707 pcon=5.0736 forget=2.4920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 8 it 30 total=8.9610 mle=1.4805 pcon=5.0719 forget=2.4087 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=8.8049 mle=1.2743 pcon=5.0702 forget=2.4604 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=8.9212 mle=1.3983 pcon=5.0685 forget=2.4545 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=8.8646 mle=1.3446 pcon=5.0668 forget=2.4532 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=8.8878 mle=1.3662 pcon=5.0652 forget=2.4564 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.7836 mle=1.2568 pcon=5.0636 forget=2.4633 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.0220 mle=1.5798 pcon=5.0618 forget=2.3805 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.8764 mle=1.3645 pcon=5.0604 forget=2.4514 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 9 it 40 total=8.9252 mle=1.4106 pcon=5.0589 forget=2.4557 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=8.9710 mle=1.4930 pcon=5.0575 forget=2.4206 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=8.8784 mle=1.3538 pcon=5.0563 forget=2.4682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.7492 mle=1.2281 pcon=5.0550 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=8.9736 mle=1.5024 pcon=5.0540 forget=2.4172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=8.8660 mle=1.3564 pcon=5.0526 forget=2.4571 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.8157 mle=1.3066 pcon=5.0514 forget=2.4577 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 10 it 0 total=8.7872 mle=1.2679 pcon=5.0497 forget=2.4695 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.7535 mle=1.2564 pcon=5.0488 forget=2.4483 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=8.8057 mle=1.3264 pcon=5.0475 forget=2.4319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=8.8731 mle=1.3437 pcon=5.0463 forget=2.4831 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=8.9085 mle=1.4081 pcon=5.0453 forget=2.4550 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=8.9169 mle=1.4254 pcon=5.0442 forget=2.4472 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=8.9385 mle=1.5125 pcon=5.0431 forget=2.3829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.7569 mle=1.2610 pcon=5.0423 forget=2.4537 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 11 it 10 total=8.9131 mle=1.4403 pcon=5.0411 forget=2.4317 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.8453 mle=1.3727 pcon=5.0402 forget=2.4324 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.8326 mle=1.3444 pcon=5.0394 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.7805 mle=1.2733 pcon=5.0383 forget=2.4689 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.7524 mle=1.2651 pcon=5.0374 forget=2.4499 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.7360 mle=1.2659 pcon=5.0365 forget=2.4336 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.8656 mle=1.3898 pcon=5.0357 forget=2.4402 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.8291 mle=1.3176 pcon=5.0350 forget=2.4765 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 12 it 20 total=8.8009 mle=1.3124 pcon=5.0338 forget=2.4547 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.8400 mle=1.3538 pcon=5.0328 forget=2.4534 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.7916 mle=1.3597 pcon=5.0318 forget=2.4001 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.8304 mle=1.3345 pcon=5.0308 forget=2.4651 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.7284 mle=1.2221 pcon=5.0302 forget=2.4761 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.7446 mle=1.2926 pcon=5.0297 forget=2.4223 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.8817 mle=1.4090 pcon=5.0291 forget=2.4436 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.8429 mle=1.3755 pcon=5.0284 forget=2.4391 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 13 it 30 total=8.8069 mle=1.3128 pcon=5.0279 forget=2.4662 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.9149 mle=1.4887 pcon=5.0272 forget=2.3990 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.7969 mle=1.3440 pcon=5.0263 forget=2.4266 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=8.8464 mle=1.3577 pcon=5.0251 forget=2.4637 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 70%|███████   | 14/20 [03:38<01:31, 15.25s/it] 75%|███████▌  | 15/20 [03:53<01:15, 15.18s/it] 80%|████████  | 16/20 [04:08<01:00, 15.21s/it] 85%|████████▌ | 17/20 [04:24<00:45, 15.30s/it] 90%|█████████ | 18/20 [04:39<00:30, 15.26s/it] 95%|█████████▌| 19/20 [04:53<00:15, 15.06s/it]100%|██████████| 20/20 [05:09<00:00, 15.20s/it]100%|██████████| 20/20 [05:09<00:00, 15.47s/it]
[loss] ep 13 it 230 total=8.7173 mle=1.2971 pcon=5.0244 forget=2.3958 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.7141 mle=1.2156 pcon=5.0235 forget=2.4750 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.8142 mle=1.3853 pcon=5.0228 forget=2.4061 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.7208 mle=1.3168 pcon=5.0220 forget=2.3820 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 14 it 40 total=8.6913 mle=1.2702 pcon=5.0209 forget=2.4002 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.7196 mle=1.3075 pcon=5.0200 forget=2.3921 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.7029 mle=1.2966 pcon=5.0190 forget=2.3873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.6482 mle=1.2814 pcon=5.0179 forget=2.3489 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.7235 mle=1.3234 pcon=5.0163 forget=2.3838 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.5868 mle=1.3086 pcon=5.0150 forget=2.2631 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.5515 mle=1.2677 pcon=5.0137 forget=2.2701 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 15 it 0 total=8.6362 mle=1.3723 pcon=5.0120 forget=2.2519 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.5447 mle=1.2779 pcon=5.0106 forget=2.2561 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.5973 mle=1.4133 pcon=5.0094 forget=2.1745 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.5655 mle=1.3943 pcon=5.0081 forget=2.1631 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.5136 mle=1.3242 pcon=5.0069 forget=2.1825 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.5639 mle=1.3683 pcon=5.0058 forget=2.1897 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.5978 mle=1.4529 pcon=5.0047 forget=2.1401 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.5803 mle=1.4353 pcon=5.0041 forget=2.1409 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 16 it 10 total=8.5427 mle=1.3683 pcon=5.0035 forget=2.1709 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.5066 mle=1.3502 pcon=5.0034 forget=2.1530 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.4993 mle=1.3243 pcon=5.0033 forget=2.1718 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.5590 mle=1.3544 pcon=5.0037 forget=2.2009 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.5920 mle=1.3713 pcon=5.0042 forget=2.2164 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.7863 mle=1.5471 pcon=5.0047 forget=2.2345 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.8067 mle=1.5590 pcon=5.0053 forget=2.2424 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.7029 mle=1.4433 pcon=5.0063 forget=2.2532 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.7558 mle=1.5066 pcon=5.0078 forget=2.2415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.8235 mle=1.5342 pcon=5.0095 forget=2.2799 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.6577 mle=1.3608 pcon=5.0108 forget=2.2861 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.8283 mle=1.4673 pcon=5.0125 forget=2.3485 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.8650 mle=1.5330 pcon=5.0142 forget=2.3177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.8559 mle=1.5089 pcon=5.0162 forget=2.3308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.8476 mle=1.4915 pcon=5.0182 forget=2.3379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.8463 mle=1.4670 pcon=5.0202 forget=2.3591 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.8147 mle=1.3955 pcon=5.0224 forget=2.3968 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.9901 mle=1.5646 pcon=5.0247 forget=2.4009 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=9.0085 mle=1.5649 pcon=5.0270 forget=2.4165 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.9248 mle=1.4673 pcon=5.0294 forget=2.4280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.9835 mle=1.4876 pcon=5.0316 forget=2.4644 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=9.0249 mle=1.5127 pcon=5.0338 forget=2.4784 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.9993 mle=1.4784 pcon=5.0362 forget=2.4847 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.9276 mle=1.4087 pcon=5.0388 forget=2.4801 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.9476 mle=1.4190 pcon=5.0411 forget=2.4875 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=9.0882 mle=1.5616 pcon=5.0434 forget=2.4832 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=9.0022 mle=1.4196 pcon=5.0458 forget=2.5367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.9948 mle=1.4322 pcon=5.0480 forget=2.5147 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=9.0923 mle=1.4972 pcon=5.0504 forget=2.5448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=9.1253 mle=1.5296 pcon=5.0532 forget=2.5425 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=9.0925 mle=1.4807 pcon=5.0560 forget=2.5558 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:13,  2.02it/s]  2%|▏         | 9/391 [00:00<00:19, 19.30it/s]  5%|▍         | 19/391 [00:00<00:09, 37.97it/s]  7%|▋         | 29/391 [00:00<00:06, 52.84it/s] 10%|▉         | 39/391 [00:00<00:05, 64.20it/s] 13%|█▎        | 49/391 [00:01<00:04, 72.80it/s] 15%|█▌        | 59/391 [00:01<00:04, 79.05it/s] 18%|█▊        | 69/391 [00:01<00:03, 83.43it/s] 20%|██        | 79/391 [00:01<00:03, 86.81it/s] 23%|██▎       | 89/391 [00:01<00:03, 89.37it/s] 25%|██▌       | 99/391 [00:01<00:03, 91.02it/s] 28%|██▊       | 109/391 [00:01<00:03, 92.20it/s] 30%|███       | 119/391 [00:01<00:02, 93.07it/s] 33%|███▎      | 129/391 [00:01<00:02, 93.62it/s] 36%|███▌      | 139/391 [00:01<00:02, 94.12it/s] 38%|███▊      | 149/391 [00:02<00:02, 94.57it/s] 41%|████      | 159/391 [00:02<00:02, 94.85it/s] 43%|████▎     | 169/391 [00:02<00:02, 95.06it/s] 46%|████▌     | 179/391 [00:02<00:02, 95.25it/s] 48%|████▊     | 189/391 [00:02<00:02, 95.44it/s] 51%|█████     | 199/391 [00:02<00:02, 94.47it/s] 53%|█████▎    | 209/391 [00:02<00:01, 94.66it/s] 56%|█████▌    | 219/391 [00:02<00:01, 94.91it/s] 59%|█████▊    | 229/391 [00:02<00:01, 93.25it/s] 61%|██████    | 239/391 [00:03<00:01, 93.12it/s] 64%|██████▎   | 249/391 [00:03<00:01, 92.26it/s] 66%|██████▌   | 259/391 [00:03<00:01, 93.15it/s] 69%|██████▉   | 269/391 [00:03<00:01, 92.76it/s] 71%|███████▏  | 279/391 [00:03<00:01, 92.64it/s] 74%|███████▍  | 289/391 [00:03<00:01, 92.50it/s] 76%|███████▋  | 299/391 [00:03<00:01, 91.91it/s] 79%|███████▉  | 309/391 [00:03<00:00, 92.86it/s] 82%|████████▏ | 319/391 [00:03<00:00, 93.06it/s] 84%|████████▍ | 329/391 [00:04<00:00, 92.66it/s] 87%|████████▋ | 339/391 [00:04<00:00, 93.63it/s] 89%|████████▉ | 349/391 [00:04<00:00, 93.06it/s] 92%|█████████▏| 359/391 [00:04<00:00, 93.30it/s] 94%|█████████▍| 369/391 [00:04<00:00, 93.68it/s] 97%|█████████▋| 379/391 [00:04<00:00, 94.55it/s] 99%|█████████▉| 389/391 [00:04<00:00, 95.30it/s]100%|██████████| 391/391 [00:04<00:00, 83.72it/s]
50000 images processed, 4.772221803665161 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:33,  2.35it/s] 10%|█         | 8/79 [00:00<00:03, 18.91it/s] 22%|██▏       | 17/79 [00:00<00:01, 36.30it/s] 32%|███▏      | 25/79 [00:00<00:01, 47.18it/s] 43%|████▎     | 34/79 [00:00<00:00, 57.13it/s] 53%|█████▎    | 42/79 [00:00<00:00, 63.32it/s] 66%|██████▌   | 52/79 [00:01<00:00, 71.82it/s] 78%|███████▊  | 62/79 [00:01<00:00, 78.03it/s] 91%|█████████ | 72/79 [00:01<00:00, 81.85it/s]100%|██████████| 79/79 [00:01<00:00, 45.60it/s]
10000 images processed, 1.7920477390289307 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:36,  2.10it/s]  3%|▎         | 7/204 [00:00<00:13, 14.79it/s]  8%|▊         | 16/204 [00:00<00:05, 32.07it/s] 12%|█▏        | 25/204 [00:00<00:03, 46.11it/s] 17%|█▋        | 34/204 [00:00<00:03, 56.41it/s] 21%|██        | 43/204 [00:01<00:02, 64.31it/s] 25%|██▌       | 51/204 [00:01<00:02, 67.53it/s] 29%|██▉       | 60/204 [00:01<00:01, 73.63it/s] 34%|███▍      | 69/204 [00:01<00:01, 77.52it/s] 38%|███▊      | 78/204 [00:01<00:01, 78.42it/s] 43%|████▎     | 87/204 [00:01<00:01, 80.04it/s] 47%|████▋     | 96/204 [00:01<00:01, 74.18it/s] 51%|█████     | 104/204 [00:01<00:01, 73.33it/s] 55%|█████▍    | 112/204 [00:01<00:01, 74.24it/s] 59%|█████▉    | 121/204 [00:02<00:01, 76.23it/s] 63%|██████▎   | 129/204 [00:02<00:00, 76.52it/s] 67%|██████▋   | 137/204 [00:02<00:00, 75.73it/s] 72%|███████▏  | 147/204 [00:02<00:00, 80.96it/s] 76%|███████▋  | 156/204 [00:02<00:00, 82.85it/s] 81%|████████  | 165/204 [00:02<00:00, 79.95it/s] 85%|████████▌ | 174/204 [00:02<00:00, 76.86it/s] 89%|████████▉ | 182/204 [00:02<00:00, 76.13it/s] 94%|█████████▎| 191/204 [00:02<00:00, 79.28it/s] 99%|█████████▊| 201/204 [00:03<00:00, 84.11it/s]100%|██████████| 204/204 [00:03<00:00, 66.93it/s]
26032 images processed, 3.0966367721557617 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:49,  1.58it/s] 13%|█▎        | 10/79 [00:00<00:03, 17.36it/s] 25%|██▌       | 20/79 [00:00<00:01, 33.47it/s] 37%|███▋      | 29/79 [00:00<00:01, 45.28it/s] 49%|████▉     | 39/79 [00:01<00:00, 57.17it/s] 61%|██████    | 48/79 [00:01<00:00, 64.51it/s] 72%|███████▏  | 57/79 [00:01<00:00, 69.19it/s] 85%|████████▍ | 67/79 [00:01<00:00, 76.54it/s] 97%|█████████▋| 77/79 [00:01<00:00, 82.14it/s]100%|██████████| 79/79 [00:01<00:00, 52.01it/s]
10000 images processed, 1.5499591827392578 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.92it/s] 14%|█▍        | 11/79 [00:00<00:03, 22.59it/s] 27%|██▋       | 21/79 [00:00<00:01, 39.91it/s] 39%|███▉      | 31/79 [00:00<00:00, 53.84it/s] 52%|█████▏    | 41/79 [00:00<00:00, 64.58it/s] 65%|██████▍   | 51/79 [00:01<00:00, 72.78it/s] 77%|███████▋  | 61/79 [00:01<00:00, 78.96it/s] 90%|████████▉ | 71/79 [00:01<00:00, 83.67it/s]100%|██████████| 79/79 [00:01<00:00, 58.82it/s]
10000 images processed, 1.3647987842559814 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:29,  2.32it/s] 11%|█▏        | 8/70 [00:00<00:03, 18.58it/s] 24%|██▍       | 17/70 [00:00<00:01, 35.93it/s] 39%|███▊      | 27/70 [00:00<00:00, 51.35it/s] 51%|█████▏    | 36/70 [00:00<00:00, 59.75it/s] 64%|██████▍   | 45/70 [00:00<00:00, 67.59it/s] 79%|███████▊  | 55/70 [00:01<00:00, 75.63it/s] 93%|█████████▎| 65/70 [00:01<00:00, 81.39it/s]100%|██████████| 70/70 [00:01<00:00, 56.12it/s]
8925 images processed, 1.2822890281677246 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:59,  1.34s/it]  4%|▍         | 2/45 [00:01<00:26,  1.61it/s] 22%|██▏       | 10/45 [00:01<00:03, 10.90it/s] 38%|███▊      | 17/45 [00:01<00:01, 16.00it/s] 47%|████▋     | 21/45 [00:01<00:01, 19.08it/s] 56%|█████▌    | 25/45 [00:02<00:00, 20.21it/s] 71%|███████   | 32/45 [00:02<00:00, 28.70it/s] 82%|████████▏ | 37/45 [00:02<00:00, 23.02it/s]100%|██████████| 45/45 [00:02<00:00, 17.20it/s]
5640 images processed, 2.6469709873199463 seconds used

18.19426417350769
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.60  99.34  97.95
places365     68.32  81.10  78.78
LSUN          17.65  96.07  96.08
iSUN          72.50  81.86  84.30
dtd           38.51  91.32  94.27
AVG           39.92  89.94  90.28
Retain-Acc: 0.7479
Forget-as-OOD (retain known vs forget novel):
  FPR: 56.00 AUROC: 88.93 AUIN: 98.60
8.33966612815857
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns_rf.png
[Run] lambda=0.2 lr=0.001 epochs=25
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=25, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/25 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  4%|▍         | 1/25 [00:20<08:05, 20.25s/it]  8%|▊         | 2/25 [00:35<06:33, 17.09s/it] 12%|█▏        | 3/25 [00:49<05:52, 16.00s/it] 16%|█▌        | 4/25 [01:04<05:21, 15.32s/it] 20%|██        | 5/25 [01:18<04:57, 14.88s/it] 24%|██▍       | 6/25 [01:32<04:38, 14.65s/it][loss] ep 0 it 0 total=9.0403 mle=1.2805 pcon=5.2951 forget=2.4647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1044 mle=1.3924 pcon=5.2887 forget=2.4232 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.2415 mle=1.5081 pcon=5.2827 forget=2.4507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2781 mle=1.5645 pcon=5.2770 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2050 mle=1.5328 pcon=5.2708 forget=2.4014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1117 mle=1.3806 pcon=5.2650 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.0052 mle=1.2921 pcon=5.2596 forget=2.4536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1731 mle=1.4379 pcon=5.2543 forget=2.4809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 1 it 10 total=9.0162 mle=1.3065 pcon=5.2487 forget=2.4610 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0371 mle=1.3159 pcon=5.2433 forget=2.4779 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.9650 mle=1.2901 pcon=5.2382 forget=2.4367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.9541 mle=1.2381 pcon=5.2331 forget=2.4829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0632 mle=1.3725 pcon=5.2283 forget=2.4623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9879 mle=1.3468 pcon=5.2235 forget=2.4177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9851 mle=1.3083 pcon=5.2185 forget=2.4583 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0553 mle=1.3765 pcon=5.2136 forget=2.4652 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 2 it 20 total=8.9289 mle=1.2745 pcon=5.2089 forget=2.4455 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0968 mle=1.3988 pcon=5.2044 forget=2.4937 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.9872 mle=1.3590 pcon=5.1998 forget=2.4284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9913 mle=1.3579 pcon=5.1955 forget=2.4379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.9824 mle=1.3300 pcon=5.1909 forget=2.4615 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0215 mle=1.3659 pcon=5.1865 forget=2.4690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.8721 mle=1.2605 pcon=5.1824 forget=2.4292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.0074 mle=1.3835 pcon=5.1787 forget=2.4452 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 3 it 30 total=8.9980 mle=1.3639 pcon=5.1749 forget=2.4592 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9247 mle=1.2959 pcon=5.1710 forget=2.4578 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.8987 mle=1.2903 pcon=5.1672 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.0414 mle=1.4600 pcon=5.1635 forget=2.4179 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9798 mle=1.3523 pcon=5.1601 forget=2.4675 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0521 mle=1.4266 pcon=5.1568 forget=2.4686 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0461 mle=1.4481 pcon=5.1531 forget=2.4448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9151 mle=1.3139 pcon=5.1498 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 4 it 40 total=8.9572 mle=1.3770 pcon=5.1465 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9560 mle=1.3336 pcon=5.1435 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.1336 mle=1.5850 pcon=5.1406 forget=2.4079 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=8.9932 mle=1.3938 pcon=5.1375 forget=2.4620 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0241 mle=1.4143 pcon=5.1344 forget=2.4754 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=8.7697 mle=1.1862 pcon=5.1316 forget=2.4519 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.0389 mle=1.4610 pcon=5.1291 forget=2.4489 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 5 it 0 total=8.8317 mle=1.2046 pcon=5.1261 forget=2.5009 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=8.9578 mle=1.3650 pcon=5.1235 forget=2.4693 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.0084 mle=1.4004 pcon=5.1207 forget=2.4873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.0680 mle=1.4808 pcon=5.1179 forget=2.4694 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.0085 mle=1.4522 pcon=5.1153 forget=2.4410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=8.9114 mle=1.3483 pcon=5.1125 forget=2.4505 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=8.9731 mle=1.4295 pcon=5.1099 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=8.9548 mle=1.4043 pcon=5.1073 forget=2.4432 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 6 it 10 total=8.8857 mle=1.2783 pcon=5.1049 forget=2.5025 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=8.9183 mle=1.3787 pcon=5.1026 forget=2.4370 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=8.8929 mle=1.3182 pcon=5.1003 forget=2.4744 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.8890 mle=1.3320 pcon=5.0980 forget=2.4590 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9234 mle=1.3653 pcon=5.0956 forget=2.4625 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=8.9544 mle=1.3831 pcon=5.0934 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 310 total=8.8671 mle=1.3538 pcon=5.0915 forget=2.4219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 28%|██▊       | 7/25 [01:47<04:24, 14.71s/it] 32%|███▏      | 8/25 [02:01<04:08, 14.59s/it] 36%|███▌      | 9/25 [02:16<03:57, 14.84s/it] 40%|████      | 10/25 [02:30<03:38, 14.59s/it] 44%|████▍     | 11/25 [02:45<03:23, 14.57s/it] 48%|████▊     | 12/25 [03:01<03:13, 14.87s/it] 52%|█████▏    | 13/25 [03:16<02:59, 14.96s/it][loss] ep 6 it 360 total=8.9529 mle=1.4569 pcon=5.0893 forget=2.4067 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 7 it 20 total=8.7926 mle=1.2500 pcon=5.0872 forget=2.4554 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=8.7894 mle=1.2464 pcon=5.0849 forget=2.4580 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.9122 mle=1.3876 pcon=5.0829 forget=2.4418 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.9197 mle=1.4330 pcon=5.0810 forget=2.4057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=8.8356 mle=1.3211 pcon=5.0794 forget=2.4351 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=8.9746 mle=1.4542 pcon=5.0777 forget=2.4427 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=8.8528 mle=1.3167 pcon=5.0755 forget=2.4606 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=8.9364 mle=1.3708 pcon=5.0736 forget=2.4920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 8 it 30 total=8.9610 mle=1.4805 pcon=5.0719 forget=2.4087 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=8.8050 mle=1.2744 pcon=5.0702 forget=2.4605 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=8.9214 mle=1.3984 pcon=5.0685 forget=2.4544 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=8.8648 mle=1.3447 pcon=5.0668 forget=2.4533 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=8.8877 mle=1.3662 pcon=5.0652 forget=2.4563 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.7838 mle=1.2570 pcon=5.0636 forget=2.4633 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.0220 mle=1.5797 pcon=5.0618 forget=2.3805 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.8763 mle=1.3645 pcon=5.0604 forget=2.4514 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 9 it 40 total=8.9253 mle=1.4106 pcon=5.0589 forget=2.4558 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=8.9710 mle=1.4931 pcon=5.0575 forget=2.4205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=8.8782 mle=1.3537 pcon=5.0563 forget=2.4682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.7494 mle=1.2282 pcon=5.0551 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=8.9737 mle=1.5025 pcon=5.0540 forget=2.4172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=8.8660 mle=1.3564 pcon=5.0526 forget=2.4570 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.8157 mle=1.3066 pcon=5.0514 forget=2.4577 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 10 it 0 total=8.7870 mle=1.2678 pcon=5.0497 forget=2.4695 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.7534 mle=1.2563 pcon=5.0489 forget=2.4483 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=8.8059 mle=1.3266 pcon=5.0475 forget=2.4318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=8.8732 mle=1.3438 pcon=5.0463 forget=2.4831 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=8.9087 mle=1.4083 pcon=5.0453 forget=2.4550 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=8.9165 mle=1.4251 pcon=5.0443 forget=2.4472 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=8.9386 mle=1.5125 pcon=5.0431 forget=2.3830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.7569 mle=1.2609 pcon=5.0423 forget=2.4537 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 11 it 10 total=8.9130 mle=1.4401 pcon=5.0411 forget=2.4318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.8450 mle=1.3726 pcon=5.0402 forget=2.4322 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.8328 mle=1.3446 pcon=5.0394 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.7802 mle=1.2732 pcon=5.0383 forget=2.4688 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.7523 mle=1.2649 pcon=5.0374 forget=2.4499 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.7357 mle=1.2658 pcon=5.0365 forget=2.4334 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.8654 mle=1.3899 pcon=5.0357 forget=2.4399 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.8284 mle=1.3172 pcon=5.0350 forget=2.4762 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 12 it 20 total=8.8006 mle=1.3123 pcon=5.0338 forget=2.4545 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.8394 mle=1.3535 pcon=5.0328 forget=2.4530 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.7901 mle=1.3590 pcon=5.0318 forget=2.3993 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.8289 mle=1.3339 pcon=5.0308 forget=2.4642 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.7272 mle=1.2218 pcon=5.0302 forget=2.4752 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.7425 mle=1.2923 pcon=5.0297 forget=2.4205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.8787 mle=1.4078 pcon=5.0291 forget=2.4418 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.8395 mle=1.3745 pcon=5.0284 forget=2.4367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 13 it 30 total=8.8007 mle=1.3108 pcon=5.0279 forget=2.4620 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.9067 mle=1.4868 pcon=5.0271 forget=2.3928 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.7872 mle=1.3418 pcon=5.0262 forget=2.4191 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 180 total=8.8335 mle=1.3572 pcon=5.0249 forget=2.4514 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 56%|█████▌    | 14/25 [03:31<02:44, 14.92s/it] 60%|██████    | 15/25 [03:46<02:32, 15.22s/it] 64%|██████▍   | 16/25 [04:02<02:17, 15.25s/it] 68%|██████▊   | 17/25 [04:17<02:02, 15.28s/it] 72%|███████▏  | 18/25 [04:32<01:45, 15.05s/it] 76%|███████▌  | 19/25 [04:47<01:31, 15.23s/it] 80%|████████  | 20/25 [05:02<01:15, 15.06s/it][loss] ep 13 it 230 total=8.6956 mle=1.2984 pcon=5.0241 forget=2.3730 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.6866 mle=1.2111 pcon=5.0230 forget=2.4525 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.7793 mle=1.3863 pcon=5.0221 forget=2.3708 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.6607 mle=1.3157 pcon=5.0210 forget=2.3239 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 14 it 40 total=8.6496 mle=1.2906 pcon=5.0196 forget=2.3394 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.6280 mle=1.3269 pcon=5.0183 forget=2.2828 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.5903 mle=1.3309 pcon=5.0168 forget=2.2425 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.5327 mle=1.3344 pcon=5.0153 forget=2.1830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.5804 mle=1.3528 pcon=5.0134 forget=2.2142 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.4712 mle=1.3906 pcon=5.0121 forget=2.0685 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.4313 mle=1.3555 pcon=5.0111 forget=2.0647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 15 it 0 total=8.5156 mle=1.4595 pcon=5.0101 forget=2.0460 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.4082 mle=1.3598 pcon=5.0098 forget=2.0386 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.4353 mle=1.4595 pcon=5.0099 forget=1.9658 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.4468 mle=1.4618 pcon=5.0103 forget=1.9747 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.4063 mle=1.3912 pcon=5.0110 forget=2.0041 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.4539 mle=1.4270 pcon=5.0120 forget=2.0149 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.4765 mle=1.4869 pcon=5.0131 forget=1.9765 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.4935 mle=1.4945 pcon=5.0147 forget=1.9844 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[loss] ep 16 it 10 total=8.4615 mle=1.4248 pcon=5.0162 forget=2.0205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.4110 mle=1.4003 pcon=5.0181 forget=1.9926 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.4147 mle=1.3642 pcon=5.0200 forget=2.0305 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.4794 mle=1.3946 pcon=5.0222 forget=2.0625 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.5147 mle=1.4059 pcon=5.0245 forget=2.0844 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.7290 mle=1.5703 pcon=5.0265 forget=2.1322 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.7160 mle=1.5710 pcon=5.0285 forget=2.1165 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.6569 mle=1.5049 pcon=5.0308 forget=2.1212 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.6473 mle=1.5381 pcon=5.0333 forget=2.0759 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.7469 mle=1.5539 pcon=5.0360 forget=2.1570 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.5789 mle=1.4049 pcon=5.0380 forget=2.1360 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.7211 mle=1.4727 pcon=5.0402 forget=2.2081 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.7558 mle=1.5603 pcon=5.0423 forget=2.1532 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.7249 mle=1.5254 pcon=5.0445 forget=2.1551 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.7273 mle=1.5304 pcon=5.0466 forget=2.1503 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.7385 mle=1.4858 pcon=5.0484 forget=2.2043 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.6877 mle=1.4355 pcon=5.0503 forget=2.2020 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.8731 mle=1.6201 pcon=5.0521 forget=2.2009 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.8435 mle=1.5541 pcon=5.0539 forget=2.2354 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.7289 mle=1.4864 pcon=5.0556 forget=2.1869 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.8136 mle=1.5153 pcon=5.0569 forget=2.2414 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.8098 mle=1.5147 pcon=5.0581 forget=2.2370 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.8067 mle=1.5101 pcon=5.0593 forget=2.2373 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.7247 mle=1.4365 pcon=5.0605 forget=2.2277 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.7436 mle=1.4503 pcon=5.0615 forget=2.2318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.8405 mle=1.5723 pcon=5.0622 forget=2.2060 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.7315 mle=1.4091 pcon=5.0629 forget=2.2594 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.7250 mle=1.4423 pcon=5.0633 forget=2.2194 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.7920 mle=1.4713 pcon=5.0638 forget=2.2569 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.8350 mle=1.5380 pcon=5.0645 forget=2.2324 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.7894 mle=1.4656 pcon=5.0653 forget=2.2585 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 0 total=8.7503 mle=1.4323 pcon=5.0656 forget=2.2524 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.7973 mle=1.4482 pcon=5.0657 forget=2.2834 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.7742 mle=1.4559 pcon=5.0656 forget=2.2527 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.8176 mle=1.4768 pcon=5.0656 forget=2.2753 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.8755 mle=1.5446 pcon=5.0655 forget=2.2653 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=8.7988 mle=1.4531 pcon=5.0654 forget=2.2803 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=8.8836 mle=1.5467 pcon=5.0652 forget=2.2717 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 84%|████████▍ | 21/25 [05:17<00:59, 14.99s/it] 88%|████████▊ | 22/25 [05:31<00:44, 14.83s/it] 92%|█████████▏| 23/25 [05:46<00:29, 14.87s/it] 96%|█████████▌| 24/25 [06:00<00:14, 14.49s/it]100%|██████████| 25/25 [06:13<00:00, 14.17s/it]100%|██████████| 25/25 [06:13<00:00, 14.95s/it]
[loss] ep 20 it 350 total=8.8787 mle=1.5299 pcon=5.0648 forget=2.2840 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 10 total=8.6914 mle=1.3407 pcon=5.0642 forget=2.2866 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=8.8429 mle=1.4662 pcon=5.0636 forget=2.3131 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.8215 mle=1.4586 pcon=5.0629 forget=2.3000 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.8684 mle=1.5124 pcon=5.0621 forget=2.2938 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=8.7671 mle=1.3876 pcon=5.0614 forget=2.3181 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.9054 mle=1.5313 pcon=5.0609 forget=2.3132 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.8901 mle=1.4823 pcon=5.0599 forget=2.3479 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.8623 mle=1.4391 pcon=5.0591 forget=2.3641 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 20 total=8.8700 mle=1.4577 pcon=5.0584 forget=2.3538 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.8572 mle=1.4348 pcon=5.0574 forget=2.3650 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.9161 mle=1.4778 pcon=5.0566 forget=2.3818 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.8661 mle=1.4147 pcon=5.0556 forget=2.3958 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.9197 mle=1.4785 pcon=5.0549 forget=2.3863 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.8951 mle=1.4126 pcon=5.0542 forget=2.4282 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.8484 mle=1.3761 pcon=5.0534 forget=2.4189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=9.0444 mle=1.5678 pcon=5.0525 forget=2.4241 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 30 total=8.8982 mle=1.4071 pcon=5.0518 forget=2.4393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.9387 mle=1.4142 pcon=5.0510 forget=2.4735 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=9.0120 mle=1.5177 pcon=5.0501 forget=2.4443 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.9466 mle=1.4416 pcon=5.0491 forget=2.4558 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.9767 mle=1.4622 pcon=5.0485 forget=2.4660 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=9.0653 mle=1.5303 pcon=5.0478 forget=2.4872 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=9.0305 mle=1.5055 pcon=5.0469 forget=2.4781 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.9128 mle=1.3536 pcon=5.0462 forget=2.5129 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 40 total=8.9961 mle=1.4295 pcon=5.0458 forget=2.5208 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=9.0774 mle=1.5313 pcon=5.0450 forget=2.5011 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.9114 mle=1.3507 pcon=5.0443 forget=2.5164 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.9376 mle=1.3752 pcon=5.0439 forget=2.5185 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=9.0209 mle=1.4468 pcon=5.0435 forget=2.5307 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.9877 mle=1.4083 pcon=5.0428 forget=2.5367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.9842 mle=1.3971 pcon=5.0420 forget=2.5451 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:52,  1.68it/s]  2%|▏         | 7/391 [00:00<00:29, 12.94it/s]  4%|▍         | 16/391 [00:00<00:12, 29.42it/s]  6%|▋         | 25/391 [00:00<00:08, 43.11it/s]  9%|▊         | 34/391 [00:01<00:06, 53.82it/s] 11%|█▏        | 44/391 [00:01<00:05, 64.16it/s] 14%|█▍        | 54/391 [00:01<00:04, 72.67it/s] 16%|█▋        | 64/391 [00:01<00:04, 78.79it/s] 19%|█▉        | 74/391 [00:01<00:03, 82.89it/s] 21%|██▏       | 84/391 [00:01<00:03, 85.44it/s] 24%|██▍       | 94/391 [00:01<00:03, 88.08it/s] 27%|██▋       | 104/391 [00:01<00:03, 89.16it/s] 29%|██▉       | 114/391 [00:01<00:03, 90.97it/s] 32%|███▏      | 124/391 [00:01<00:02, 92.12it/s] 34%|███▍      | 134/391 [00:02<00:02, 92.73it/s] 37%|███▋      | 144/391 [00:02<00:02, 93.49it/s] 39%|███▉      | 154/391 [00:02<00:02, 93.89it/s] 42%|████▏     | 164/391 [00:02<00:02, 77.79it/s] 45%|████▍     | 174/391 [00:02<00:02, 82.17it/s] 47%|████▋     | 184/391 [00:02<00:02, 85.80it/s] 49%|████▉     | 193/391 [00:02<00:02, 77.56it/s] 52%|█████▏    | 202/391 [00:02<00:02, 74.96it/s] 54%|█████▍    | 212/391 [00:03<00:02, 79.99it/s] 57%|█████▋    | 222/391 [00:03<00:02, 83.23it/s] 59%|█████▉    | 232/391 [00:03<00:01, 86.55it/s] 62%|██████▏   | 242/391 [00:03<00:01, 88.85it/s] 64%|██████▍   | 252/391 [00:03<00:01, 87.18it/s] 67%|██████▋   | 261/391 [00:03<00:01, 86.73it/s] 69%|██████▉   | 270/391 [00:03<00:01, 87.57it/s] 72%|███████▏  | 280/391 [00:03<00:01, 89.21it/s] 74%|███████▍  | 290/391 [00:03<00:01, 90.23it/s] 77%|███████▋  | 300/391 [00:04<00:00, 91.73it/s] 79%|███████▉  | 310/391 [00:04<00:00, 91.27it/s] 82%|████████▏ | 320/391 [00:04<00:00, 89.32it/s] 84%|████████▍ | 329/391 [00:04<00:00, 86.57it/s] 86%|████████▋ | 338/391 [00:04<00:00, 82.19it/s] 89%|████████▉ | 348/391 [00:04<00:00, 85.44it/s] 92%|█████████▏| 358/391 [00:04<00:00, 87.07it/s] 94%|█████████▍| 367/391 [00:04<00:00, 87.59it/s] 96%|█████████▋| 377/391 [00:04<00:00, 90.17it/s] 99%|█████████▉| 387/391 [00:05<00:00, 92.00it/s]100%|██████████| 391/391 [00:05<00:00, 77.15it/s]
50000 images processed, 5.1531102657318115 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:36,  2.13it/s] 11%|█▏        | 9/79 [00:00<00:03, 20.27it/s] 23%|██▎       | 18/79 [00:00<00:01, 36.65it/s] 33%|███▎      | 26/79 [00:00<00:01, 47.50it/s] 44%|████▍     | 35/79 [00:00<00:00, 57.68it/s] 57%|█████▋    | 45/79 [00:01<00:00, 67.24it/s] 70%|██████▉   | 55/79 [00:01<00:00, 74.59it/s] 82%|████████▏ | 65/79 [00:01<00:00, 79.88it/s] 95%|█████████▍| 75/79 [00:01<00:00, 83.74it/s]100%|██████████| 79/79 [00:01<00:00, 57.27it/s]
10000 images processed, 1.4115171432495117 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_reruns/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:37,  2.09it/s]  4%|▍         | 8/204 [00:00<00:11, 17.38it/s]  8%|▊         | 17/204 [00:00<00:05, 34.51it/s] 13%|█▎        | 26/204 [00:00<00:03, 48.51it/s] 18%|█▊        | 36/204 [00:00<00:02, 59.86it/s] 23%|██▎       | 46/204 [00:01<00:02, 69.23it/s] 27%|██▋       | 55/204 [00:01<00:02, 73.30it/s] 32%|███▏      | 65/204 [00:01<00:01, 79.05it/s] 36%|███▋      | 74/204 [00:01<00:01, 82.07it/s] 41%|████      | 84/204 [00:01<00:01, 84.84it/s] 46%|████▌     | 93/204 [00:01<00:01, 86.27it/s] 50%|█████     | 102/204 [00:01<00:01, 86.91it/s] 54%|█████▍    | 111/204 [00:01<00:01, 78.39it/s] 59%|█████▉    | 120/204 [00:01<00:01, 71.45it/s] 64%|██████▎   | 130/204 [00:02<00:00, 77.39it/s] 69%|██████▊   | 140/204 [00:02<00:00, 80.83it/s] 73%|███████▎  | 149/204 [00:02<00:00, 80.40it/s] 78%|███████▊  | 159/204 [00:02<00:00, 84.12it/s] 83%|████████▎ | 169/204 [00:02<00:00, 86.44it/s] 88%|████████▊ | 179/204 [00:02<00:00, 88.91it/s] 93%|█████████▎| 189/204 [00:02<00:00, 90.87it/s] 98%|█████████▊| 199/204 [00:02<00:00, 92.14it/s]100%|██████████| 204/204 [00:02<00:00, 71.38it/s]
26032 images processed, 2.8942878246307373 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:50,  1.54it/s]  9%|▉         | 7/79 [00:00<00:06, 11.99it/s] 22%|██▏       | 17/79 [00:00<00:02, 29.45it/s] 33%|███▎      | 26/79 [00:00<00:01, 42.90it/s] 46%|████▌     | 36/79 [00:01<00:00, 55.35it/s] 57%|█████▋    | 45/79 [00:01<00:00, 63.83it/s] 68%|██████▊   | 54/79 [00:01<00:00, 61.41it/s] 81%|████████  | 64/79 [00:01<00:00, 69.52it/s] 91%|█████████ | 72/79 [00:01<00:00, 55.95it/s]100%|██████████| 79/79 [00:01<00:00, 56.08it/s]100%|██████████| 79/79 [00:01<00:00, 44.49it/s]
10000 images processed, 1.8708808422088623 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:39,  1.97it/s]  6%|▋         | 5/79 [00:00<00:07, 10.31it/s] 14%|█▍        | 11/79 [00:00<00:03, 22.16it/s] 22%|██▏       | 17/79 [00:00<00:01, 31.12it/s] 29%|██▉       | 23/79 [00:00<00:01, 37.13it/s] 38%|███▊      | 30/79 [00:01<00:01, 43.51it/s] 46%|████▌     | 36/79 [00:01<00:00, 47.38it/s] 56%|█████▌    | 44/79 [00:01<00:00, 54.53it/s] 63%|██████▎   | 50/79 [00:01<00:00, 54.44it/s] 76%|███████▌  | 60/79 [00:01<00:00, 66.13it/s] 89%|████████▊ | 70/79 [00:01<00:00, 74.34it/s]100%|██████████| 79/79 [00:01<00:00, 47.11it/s]
10000 images processed, 1.7044949531555176 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:37,  1.83it/s] 11%|█▏        | 8/70 [00:00<00:03, 15.70it/s] 23%|██▎       | 16/70 [00:00<00:01, 30.16it/s] 34%|███▍      | 24/70 [00:00<00:01, 42.23it/s] 47%|████▋     | 33/70 [00:00<00:00, 53.91it/s] 60%|██████    | 42/70 [00:01<00:00, 62.65it/s] 74%|███████▍  | 52/70 [00:01<00:00, 71.91it/s] 89%|████████▊ | 62/70 [00:01<00:00, 78.59it/s]100%|██████████| 70/70 [00:01<00:00, 51.20it/s]
8925 images processed, 1.3979899883270264 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:41,  1.06it/s]  4%|▍         | 2/45 [00:01<00:19,  2.17it/s] 24%|██▍       | 11/45 [00:01<00:02, 15.70it/s] 38%|███▊      | 17/45 [00:01<00:01, 18.99it/s] 47%|████▋     | 21/45 [00:01<00:01, 18.40it/s] 67%|██████▋   | 30/45 [00:01<00:00, 29.96it/s] 78%|███████▊  | 35/45 [00:02<00:00, 22.33it/s] 87%|████████▋ | 39/45 [00:02<00:00, 21.80it/s]100%|██████████| 45/45 [00:02<00:00, 18.85it/s]
5640 images processed, 2.4198954105377197 seconds used

18.771378755569458
