nohup: ignoring input
==== Stage 1: forget_inc={0,8,11,40,51}; forget_seen={}; all={0,8,11,40,51,66,67,88,94,57} ====
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1', adapter_load_path=None, adapter_load_paths=None, lora_new_adapter_name=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc='0,8,11,40,51', forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [01:06<54:31, 66.77s/it]  4%|▍         | 2/50 [01:18<27:43, 34.66s/it]  6%|▌         | 3/50 [01:27<17:54, 22.87s/it]  8%|▊         | 4/50 [01:41<14:37, 19.08s/it] 10%|█         | 5/50 [01:50<11:35, 15.46s/it] 12%|█▏        | 6/50 [01:59<09:44, 13.27s/it] 14%|█▍        | 7/50 [02:08<08:35, 11.98s/it][loss] ep 0 it 0 total=8.4097 mle=1.6267 pcon=5.2951 forget=1.4879 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 0 it 50 total=8.7564 mle=1.9942 pcon=5.2910 forget=1.4712 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 100 total=8.8806 mle=2.1359 pcon=5.2868 forget=1.4579 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 150 total=9.2350 mle=2.4876 pcon=5.2829 forget=1.4644 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 0 it 200 total=9.2717 mle=2.5434 pcon=5.2787 forget=1.4496 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 250 total=8.6727 mle=1.9317 pcon=5.2747 forget=1.4662 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 0 it 300 total=8.4974 mle=1.7805 pcon=5.2707 forget=1.4462 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 350 total=9.0520 mle=2.3361 pcon=5.2668 forget=1.4491 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 1 it 10 total=8.3921 mle=1.6600 pcon=5.2628 forget=1.4693 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 1 it 60 total=8.3575 mle=1.6497 pcon=5.2590 forget=1.4488 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 1 it 110 total=8.4059 mle=1.6989 pcon=5.2552 forget=1.4517 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 1 it 160 total=8.5318 mle=1.7840 pcon=5.2515 forget=1.4964 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 1 it 210 total=8.2691 mle=1.5550 pcon=5.2477 forget=1.4664 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 1 it 260 total=8.7203 mle=2.0039 pcon=5.2439 forget=1.4725 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 1 it 310 total=8.9387 mle=2.2284 pcon=5.2404 forget=1.4699 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 1 it 360 total=8.5474 mle=1.8685 pcon=5.2366 forget=1.4422 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 2 it 20 total=8.0425 mle=1.3553 pcon=5.2331 forget=1.4541 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 2 it 70 total=8.4408 mle=1.8002 pcon=5.2298 forget=1.4108 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 2 it 120 total=8.3368 mle=1.6571 pcon=5.2262 forget=1.4534 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 2 it 170 total=8.7014 mle=2.0434 pcon=5.2228 forget=1.4353 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 2 it 220 total=9.0547 mle=2.4082 pcon=5.2195 forget=1.4270 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 2 it 270 total=8.4800 mle=1.8173 pcon=5.2162 forget=1.4466 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 2 it 320 total=8.2500 mle=1.5927 pcon=5.2130 forget=1.4443 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 2 it 370 total=8.6815 mle=2.0349 pcon=5.2098 forget=1.4367 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 3 it 30 total=8.2825 mle=1.6333 pcon=5.2065 forget=1.4428 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 3 it 80 total=8.7730 mle=2.0478 pcon=5.2032 forget=1.5221 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 3 it 130 total=8.5098 mle=1.9320 pcon=5.2003 forget=1.3775 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 3 it 180 total=8.7391 mle=2.1012 pcon=5.1969 forget=1.4410 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 3 it 230 total=8.4278 mle=1.8242 pcon=5.1937 forget=1.4099 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 3 it 280 total=8.4482 mle=1.8524 pcon=5.1909 forget=1.4049 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 3 it 330 total=8.7577 mle=2.0950 pcon=5.1880 forget=1.4747 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 3 it 380 total=8.3134 mle=1.6390 pcon=5.1849 forget=1.4895 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 4 it 40 total=8.5240 mle=1.8600 pcon=5.1821 forget=1.4819 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 4 it 90 total=8.2766 mle=1.7145 pcon=5.1790 forget=1.3830 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 4 it 140 total=9.0109 mle=2.3583 pcon=5.1762 forget=1.4764 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 4 it 190 total=8.4974 mle=1.8555 pcon=5.1731 forget=1.4688 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 4 it 240 total=8.4979 mle=1.8216 pcon=5.1701 forget=1.5062 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 4 it 290 total=8.2816 mle=1.7155 pcon=5.1672 forget=1.3989 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 4 it 340 total=8.6378 mle=2.0381 pcon=5.1643 forget=1.4354 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 5 it 0 total=8.5793 mle=2.0080 pcon=5.1619 forget=1.4095 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 5 it 50 total=8.8866 mle=2.3012 pcon=5.1590 forget=1.4264 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 5 it 100 total=8.8500 mle=2.2186 pcon=5.1564 forget=1.4750 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 5 it 150 total=8.4290 mle=1.8599 pcon=5.1539 forget=1.4152 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 5 it 200 total=9.1522 mle=2.5830 pcon=5.1511 forget=1.4182 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 5 it 250 total=8.3932 mle=1.7599 pcon=5.1485 forget=1.4848 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 5 it 300 total=8.7812 mle=2.2383 pcon=5.1460 forget=1.3969 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 5 it 350 total=8.8127 mle=2.2539 pcon=5.1434 forget=1.4154 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 6 it 10 total=8.1657 mle=1.6209 pcon=5.1409 forget=1.4039 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 6 it 60 total=8.4755 mle=1.9204 pcon=5.1384 forget=1.4168 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 6 it 110 total=8.3710 mle=1.8520 pcon=5.1357 forget=1.3833 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 6 it 160 total=8.5824 mle=2.0546 pcon=5.1330 forget=1.3949 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 6 it 210 total=8.4011 mle=1.8516 pcon=5.1299 forget=1.4196 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 6 it 260 total=8.4748 mle=1.9266 pcon=5.1272 forget=1.4209 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 6 it 310 total=8.5549 mle=1.9828 pcon=5.1246 forget=1.4475 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 6 it 360 total=8.5020 mle=1.9070 pcon=5.1225 forget=1.4725 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 7 it 20 total=8.2132 mle=1.6841 pcon=5.1198 forget=1.4093 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 7 it 70 total=8.1565 mle=1.6642 pcon=5.1172 forget=1.3751 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 7 it 120 total=8.6477 mle=2.1499 pcon=5.1149 forget=1.3829 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 7 it 170 total=8.2571 mle=1.6815 pcon=5.1126 forget=1.4630 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
 16%|█▌        | 8/50 [02:17<07:40, 10.96s/it] 18%|█▊        | 9/50 [02:26<07:05, 10.38s/it] 20%|██        | 10/50 [02:35<06:40, 10.02s/it] 22%|██▏       | 11/50 [02:44<06:23,  9.83s/it] 24%|██▍       | 12/50 [02:57<06:48, 10.75s/it] 26%|██▌       | 13/50 [03:07<06:28, 10.49s/it] 28%|██▊       | 14/50 [03:17<06:08, 10.25s/it] 30%|███       | 15/50 [03:31<06:37, 11.35s/it][loss] ep 7 it 220 total=8.0991 mle=1.5847 pcon=5.1101 forget=1.4043 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 7 it 270 total=9.2855 mle=2.8269 pcon=5.1075 forget=1.3510 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 7 it 320 total=8.4350 mle=1.9333 pcon=5.1049 forget=1.3967 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 7 it 370 total=8.3677 mle=1.8716 pcon=5.1024 forget=1.3938 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 8 it 30 total=8.5727 mle=2.0812 pcon=5.1002 forget=1.3913 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 8 it 80 total=8.1368 mle=1.6264 pcon=5.0975 forget=1.4130 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 8 it 130 total=8.5033 mle=1.9968 pcon=5.0950 forget=1.4114 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 8 it 180 total=8.4668 mle=1.9922 pcon=5.0927 forget=1.3819 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 8 it 230 total=8.1780 mle=1.6771 pcon=5.0902 forget=1.4106 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 8 it 280 total=7.8588 mle=1.4211 pcon=5.0876 forget=1.3501 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 8 it 330 total=8.8792 mle=2.3863 pcon=5.0848 forget=1.4081 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 8 it 380 total=8.2306 mle=1.7396 pcon=5.0823 forget=1.4087 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 9 it 40 total=8.3515 mle=1.9135 pcon=5.0797 forget=1.3583 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 9 it 90 total=8.3207 mle=1.8322 pcon=5.0773 forget=1.4113 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 9 it 140 total=8.1279 mle=1.6918 pcon=5.0751 forget=1.3610 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 9 it 190 total=8.0828 mle=1.6186 pcon=5.0730 forget=1.3913 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 9 it 240 total=8.5544 mle=2.1159 pcon=5.0707 forget=1.3678 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 9 it 290 total=8.1487 mle=1.6794 pcon=5.0685 forget=1.4007 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 9 it 340 total=8.0062 mle=1.5737 pcon=5.0663 forget=1.3661 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 10 it 0 total=8.0605 mle=1.5901 pcon=5.0639 forget=1.4065 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 10 it 50 total=8.1403 mle=1.7013 pcon=5.0618 forget=1.3772 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 10 it 100 total=7.9371 mle=1.4902 pcon=5.0593 forget=1.3876 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 10 it 150 total=8.2565 mle=1.8150 pcon=5.0570 forget=1.3844 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 10 it 200 total=8.4020 mle=1.9571 pcon=5.0551 forget=1.3898 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 10 it 250 total=8.9021 mle=2.4602 pcon=5.0529 forget=1.3890 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 10 it 300 total=8.2753 mle=1.8250 pcon=5.0507 forget=1.3996 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 10 it 350 total=8.2064 mle=1.7590 pcon=5.0488 forget=1.3987 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 11 it 10 total=8.1486 mle=1.7126 pcon=5.0468 forget=1.3892 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 11 it 60 total=8.0435 mle=1.6033 pcon=5.0448 forget=1.3954 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 11 it 110 total=7.8337 mle=1.3933 pcon=5.0428 forget=1.3975 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 11 it 160 total=8.2049 mle=1.7618 pcon=5.0409 forget=1.4023 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 11 it 210 total=8.1239 mle=1.6799 pcon=5.0388 forget=1.4052 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 11 it 260 total=7.9285 mle=1.4741 pcon=5.0365 forget=1.4178 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 11 it 310 total=8.4018 mle=1.9597 pcon=5.0343 forget=1.4078 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 11 it 360 total=8.2263 mle=1.7765 pcon=5.0321 forget=1.4177 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 12 it 20 total=7.9590 mle=1.5068 pcon=5.0299 forget=1.4224 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 70 total=8.2094 mle=1.7425 pcon=5.0276 forget=1.4393 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 12 it 120 total=8.1212 mle=1.6431 pcon=5.0254 forget=1.4528 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 12 it 170 total=8.3886 mle=1.9213 pcon=5.0230 forget=1.4443 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 220 total=8.1086 mle=1.6329 pcon=5.0206 forget=1.4552 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 12 it 270 total=8.0877 mle=1.6124 pcon=5.0181 forget=1.4572 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 320 total=8.2820 mle=1.8134 pcon=5.0158 forget=1.4529 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 12 it 370 total=8.4808 mle=2.0046 pcon=5.0134 forget=1.4628 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 13 it 30 total=8.4346 mle=1.9813 pcon=5.0110 forget=1.4422 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 13 it 80 total=8.2355 mle=1.7451 pcon=5.0088 forget=1.4816 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 13 it 130 total=8.2973 mle=1.8093 pcon=5.0065 forget=1.4815 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 13 it 180 total=8.0985 mle=1.6244 pcon=5.0039 forget=1.4701 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 13 it 230 total=8.0365 mle=1.5616 pcon=5.0015 forget=1.4734 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 13 it 280 total=8.0169 mle=1.5695 pcon=4.9992 forget=1.4482 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 13 it 330 total=8.1584 mle=1.6761 pcon=4.9968 forget=1.4855 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 13 it 380 total=8.1332 mle=1.6762 pcon=4.9946 forget=1.4625 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 14 it 40 total=7.9689 mle=1.5053 pcon=4.9923 forget=1.4712 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 90 total=8.2729 mle=1.7830 pcon=4.9902 forget=1.4997 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 140 total=8.0912 mle=1.6238 pcon=4.9880 forget=1.4794 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 14 it 190 total=7.9724 mle=1.5291 pcon=4.9858 forget=1.4575 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 240 total=8.0072 mle=1.5841 pcon=4.9835 forget=1.4396 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 14 it 290 total=7.9034 mle=1.4808 pcon=4.9812 forget=1.4414 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 14 it 340 total=7.9942 mle=1.5970 pcon=4.9793 forget=1.4179 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 15 it 0 total=7.9166 mle=1.5308 pcon=4.9769 forget=1.4088 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
 32%|███▏      | 16/50 [03:50<07:51, 13.86s/it] 34%|███▍      | 17/50 [04:08<08:18, 15.11s/it] 36%|███▌      | 18/50 [04:27<08:34, 16.08s/it] 38%|███▊      | 19/50 [04:45<08:38, 16.74s/it] 40%|████      | 20/50 [05:03<08:35, 17.18s/it] 42%|████▏     | 21/50 [05:22<08:31, 17.63s/it] 44%|████▍     | 22/50 [05:42<08:34, 18.38s/it][loss] ep 15 it 50 total=7.9809 mle=1.5789 pcon=4.9748 forget=1.4272 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 15 it 100 total=8.0130 mle=1.6047 pcon=4.9729 forget=1.4354 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 150 total=8.2903 mle=1.9115 pcon=4.9707 forget=1.4081 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 200 total=7.9313 mle=1.6027 pcon=4.9687 forget=1.3599 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 15 it 250 total=8.0306 mle=1.7053 pcon=4.9669 forget=1.3584 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 15 it 300 total=7.9782 mle=1.7220 pcon=4.9647 forget=1.2915 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 350 total=7.6937 mle=1.5095 pcon=4.9627 forget=1.2216 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 16 it 10 total=7.7668 mle=1.4586 pcon=4.9606 forget=1.3476 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 16 it 60 total=7.7679 mle=1.5524 pcon=4.9586 forget=1.2569 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 16 it 110 total=7.8893 mle=1.6982 pcon=4.9567 forget=1.2345 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 16 it 160 total=7.7833 mle=1.5686 pcon=4.9548 forget=1.2599 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 16 it 210 total=7.9963 mle=1.8023 pcon=4.9527 forget=1.2413 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 16 it 260 total=7.8324 mle=1.6985 pcon=4.9506 forget=1.1833 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 16 it 310 total=7.8269 mle=1.6618 pcon=4.9486 forget=1.2165 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 16 it 360 total=7.8345 mle=1.7057 pcon=4.9466 forget=1.1822 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 17 it 20 total=8.0538 mle=1.9516 pcon=4.9447 forget=1.1575 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 70 total=7.8168 mle=1.6451 pcon=4.9429 forget=1.2288 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 120 total=7.8665 mle=1.8122 pcon=4.9409 forget=1.1134 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 170 total=7.8539 mle=1.8055 pcon=4.9389 forget=1.1095 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 17 it 220 total=7.8704 mle=1.8362 pcon=4.9367 forget=1.0976 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 17 it 270 total=7.6134 mle=1.6105 pcon=4.9344 forget=1.0685 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 17 it 320 total=7.5929 mle=1.6577 pcon=4.9319 forget=1.0033 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 17 it 370 total=7.5905 mle=1.6700 pcon=4.9292 forget=0.9913 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 18 it 30 total=7.4781 mle=1.5509 pcon=4.9266 forget=1.0006 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 18 it 80 total=7.6349 mle=1.7102 pcon=4.9240 forget=1.0007 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 18 it 130 total=7.4351 mle=1.5265 pcon=4.9214 forget=0.9872 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 18 it 180 total=7.5898 mle=1.7035 pcon=4.9186 forget=0.9677 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 18 it 230 total=7.5425 mle=1.6864 pcon=4.9160 forget=0.9401 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 18 it 280 total=7.5394 mle=1.6612 pcon=4.9133 forget=0.9650 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 18 it 330 total=7.8068 mle=1.9090 pcon=4.9104 forget=0.9874 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 18 it 380 total=7.3196 mle=1.4341 pcon=4.9079 forget=0.9776 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 19 it 40 total=7.5949 mle=1.7155 pcon=4.9054 forget=0.9741 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 19 it 90 total=7.4477 mle=1.5589 pcon=4.9027 forget=0.9862 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 19 it 140 total=7.5273 mle=1.6587 pcon=4.9000 forget=0.9686 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 19 it 190 total=7.4413 mle=1.5470 pcon=4.8973 forget=0.9969 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 19 it 240 total=7.5046 mle=1.6205 pcon=4.8946 forget=0.9894 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 19 it 290 total=7.5094 mle=1.6644 pcon=4.8923 forget=0.9527 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 19 it 340 total=7.3044 mle=1.4290 pcon=4.8897 forget=0.9857 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 20 it 0 total=7.3087 mle=1.4360 pcon=4.8873 forget=0.9853 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 20 it 50 total=7.5040 mle=1.6387 pcon=4.8848 forget=0.9804 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 20 it 100 total=7.4670 mle=1.5841 pcon=4.8822 forget=1.0007 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 20 it 150 total=7.7331 mle=1.8656 pcon=4.8797 forget=0.9878 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 20 it 200 total=7.5599 mle=1.6813 pcon=4.8774 forget=1.0013 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 20 it 250 total=7.6082 mle=1.7475 pcon=4.8749 forget=0.9857 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 20 it 300 total=7.4160 mle=1.5459 pcon=4.8726 forget=0.9976 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 20 it 350 total=7.4688 mle=1.5931 pcon=4.8703 forget=1.0053 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 21 it 10 total=7.3548 mle=1.4997 pcon=4.8680 forget=0.9871 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 21 it 60 total=7.4454 mle=1.5708 pcon=4.8655 forget=1.0091 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 21 it 110 total=7.3405 mle=1.4717 pcon=4.8632 forget=1.0056 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 21 it 160 total=7.4052 mle=1.5420 pcon=4.8610 forget=1.0022 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 21 it 210 total=7.3489 mle=1.4666 pcon=4.8586 forget=1.0236 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 21 it 260 total=7.6593 mle=1.7737 pcon=4.8565 forget=1.0291 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 21 it 310 total=7.4913 mle=1.6161 pcon=4.8544 forget=1.0208 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 21 it 360 total=7.5602 mle=1.6721 pcon=4.8524 forget=1.0356 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 22 it 20 total=7.5059 mle=1.6185 pcon=4.8504 forget=1.0370 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 22 it 70 total=7.7840 mle=1.9008 pcon=4.8482 forget=1.0351 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 22 it 120 total=7.6891 mle=1.7848 pcon=4.8460 forget=1.0582 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 22 it 170 total=7.4467 mle=1.5637 pcon=4.8441 forget=1.0389 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 22 it 220 total=7.5918 mle=1.6994 pcon=4.8420 forget=1.0503 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 22 it 270 total=7.5025 mle=1.5951 pcon=4.8402 forget=1.0673 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
 46%|████▌     | 23/50 [06:00<08:16, 18.37s/it] 48%|████▊     | 24/50 [06:19<08:00, 18.47s/it] 50%|█████     | 25/50 [06:37<07:39, 18.37s/it] 52%|█████▏    | 26/50 [06:59<07:45, 19.40s/it] 54%|█████▍    | 27/50 [07:17<07:17, 19.01s/it] 56%|█████▌    | 28/50 [07:36<06:54, 18.85s/it] 58%|█████▊    | 29/50 [07:54<06:33, 18.76s/it] 60%|██████    | 30/50 [08:13<06:15, 18.76s/it] 62%|██████▏   | 31/50 [08:31<05:54, 18.67s/it][loss] ep 22 it 320 total=7.5140 mle=1.6128 pcon=4.8383 forget=1.0629 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 22 it 370 total=7.6672 mle=1.7557 pcon=4.8364 forget=1.0751 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 23 it 30 total=7.6249 mle=1.7342 pcon=4.8345 forget=1.0562 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 23 it 80 total=7.5377 mle=1.6336 pcon=4.8325 forget=1.0716 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 23 it 130 total=7.5924 mle=1.6897 pcon=4.8305 forget=1.0723 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 23 it 180 total=7.5340 mle=1.6272 pcon=4.8286 forget=1.0782 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 23 it 230 total=7.3625 mle=1.4526 pcon=4.8266 forget=1.0833 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 23 it 280 total=7.6258 mle=1.7060 pcon=4.8250 forget=1.0949 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 23 it 330 total=7.6239 mle=1.7094 pcon=4.8232 forget=1.0913 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 23 it 380 total=7.6049 mle=1.6860 pcon=4.8213 forget=1.0977 favg=0.0000 nr=21 nf=21 protos=570 fproto_sim=NA
[loss] ep 24 it 40 total=7.4477 mle=1.5323 pcon=4.8195 forget=1.0959 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 24 it 90 total=7.5819 mle=1.6112 pcon=4.8178 forget=1.1530 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 24 it 140 total=7.3739 mle=1.4401 pcon=4.8161 forget=1.1177 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 24 it 190 total=7.5764 mle=1.6442 pcon=4.8144 forget=1.1178 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 24 it 240 total=7.5135 mle=1.5727 pcon=4.8128 forget=1.1279 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 24 it 290 total=7.8372 mle=1.8983 pcon=4.8109 forget=1.1279 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 24 it 340 total=7.5921 mle=1.6384 pcon=4.8094 forget=1.1444 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 0 total=7.4831 mle=1.5199 pcon=4.8079 forget=1.1553 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 50 total=7.5430 mle=1.5883 pcon=4.8065 forget=1.1483 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 25 it 100 total=7.6997 mle=1.7572 pcon=4.8049 forget=1.1376 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 25 it 150 total=7.5820 mle=1.6230 pcon=4.8034 forget=1.1556 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 25 it 200 total=7.7148 mle=1.7393 pcon=4.8018 forget=1.1736 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 250 total=7.4745 mle=1.5185 pcon=4.8004 forget=1.1557 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 25 it 300 total=7.6716 mle=1.6902 pcon=4.7989 forget=1.1826 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 350 total=7.6588 mle=1.6925 pcon=4.7975 forget=1.1688 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 10 total=7.7064 mle=1.7400 pcon=4.7960 forget=1.1704 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 26 it 60 total=7.6837 mle=1.7124 pcon=4.7947 forget=1.1766 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 110 total=7.5900 mle=1.6096 pcon=4.7934 forget=1.1870 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 26 it 160 total=7.4999 mle=1.5281 pcon=4.7920 forget=1.1797 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 26 it 210 total=7.6869 mle=1.7036 pcon=4.7907 forget=1.1925 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 26 it 260 total=7.5883 mle=1.6158 pcon=4.7894 forget=1.1831 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 26 it 310 total=7.6118 mle=1.6162 pcon=4.7881 forget=1.2075 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 360 total=7.6354 mle=1.6374 pcon=4.7867 forget=1.2113 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 27 it 20 total=7.5725 mle=1.5843 pcon=4.7853 forget=1.2029 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 27 it 70 total=7.4567 mle=1.4573 pcon=4.7839 forget=1.2154 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 27 it 120 total=7.8395 mle=1.8532 pcon=4.7826 forget=1.2038 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 27 it 170 total=7.6231 mle=1.6254 pcon=4.7813 forget=1.2164 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 27 it 220 total=7.4778 mle=1.4772 pcon=4.7801 forget=1.2206 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 27 it 270 total=7.4863 mle=1.4767 pcon=4.7787 forget=1.2309 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 27 it 320 total=7.5817 mle=1.5769 pcon=4.7775 forget=1.2273 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 27 it 370 total=7.5064 mle=1.4863 pcon=4.7762 forget=1.2439 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 28 it 30 total=7.8767 mle=1.8770 pcon=4.7750 forget=1.2248 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 28 it 80 total=7.7336 mle=1.7251 pcon=4.7738 forget=1.2347 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 28 it 130 total=7.5576 mle=1.5404 pcon=4.7727 forget=1.2446 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 28 it 180 total=7.5261 mle=1.5145 pcon=4.7715 forget=1.2401 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 28 it 230 total=7.6835 mle=1.6660 pcon=4.7705 forget=1.2469 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 28 it 280 total=7.5871 mle=1.5761 pcon=4.7694 forget=1.2416 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 28 it 330 total=7.5626 mle=1.5359 pcon=4.7683 forget=1.2583 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 28 it 380 total=7.6338 mle=1.6134 pcon=4.7671 forget=1.2533 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 29 it 40 total=7.6633 mle=1.6231 pcon=4.7659 forget=1.2743 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 29 it 90 total=7.7296 mle=1.7029 pcon=4.7650 forget=1.2617 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 29 it 140 total=7.6591 mle=1.6304 pcon=4.7639 forget=1.2648 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 29 it 190 total=7.7344 mle=1.6861 pcon=4.7628 forget=1.2855 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 29 it 240 total=7.8097 mle=1.7653 pcon=4.7619 forget=1.2824 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 29 it 290 total=7.6541 mle=1.6118 pcon=4.7608 forget=1.2815 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 29 it 340 total=7.7059 mle=1.6635 pcon=4.7599 forget=1.2824 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 30 it 0 total=7.7425 mle=1.6961 pcon=4.7589 forget=1.2875 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 30 it 50 total=7.7205 mle=1.6808 pcon=4.7579 forget=1.2818 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 30 it 100 total=7.6380 mle=1.5930 pcon=4.7571 forget=1.2878 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 30 it 150 total=7.7781 mle=1.7297 pcon=4.7562 forget=1.2922 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 30 it 200 total=7.7443 mle=1.6919 pcon=4.7554 forget=1.2970 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 30 it 250 total=7.5321 mle=1.4713 pcon=4.7545 forget=1.3062 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 30 it 300 total=7.7318 mle=1.6673 pcon=4.7536 forget=1.3109 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 30 it 350 total=7.6490 mle=1.5998 pcon=4.7527 forget=1.2965 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 31 it 10 total=7.6097 mle=1.5588 pcon=4.7519 forget=1.2990 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 31 it 60 total=7.7549 mle=1.6871 pcon=4.7510 forget=1.3168 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 31 it 110 total=7.4755 mle=1.4092 pcon=4.7502 forget=1.3161 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 31 it 160 total=7.5633 mle=1.4828 pcon=4.7495 forget=1.3310 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 31 it 210 total=7.7769 mle=1.7224 pcon=4.7486 forget=1.3058 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
 64%|██████▍   | 32/50 [08:51<05:38, 18.81s/it] 66%|██████▌   | 33/50 [09:09<05:19, 18.79s/it] 68%|██████▊   | 34/50 [09:28<05:02, 18.90s/it] 70%|███████   | 35/50 [09:47<04:42, 18.83s/it] 72%|███████▏  | 36/50 [10:06<04:24, 18.86s/it] 74%|███████▍  | 37/50 [10:25<04:06, 18.96s/it] 76%|███████▌  | 38/50 [10:44<03:47, 18.93s/it] 78%|███████▊  | 39/50 [11:03<03:29, 19.02s/it] 80%|████████  | 40/50 [11:22<03:09, 18.90s/it][loss] ep 31 it 260 total=7.6738 mle=1.6020 pcon=4.7479 forget=1.3240 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 31 it 310 total=7.5897 mle=1.5231 pcon=4.7470 forget=1.3196 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 31 it 360 total=7.6438 mle=1.5710 pcon=4.7464 forget=1.3264 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 32 it 20 total=7.5597 mle=1.4866 pcon=4.7457 forget=1.3274 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 32 it 70 total=7.5820 mle=1.5068 pcon=4.7448 forget=1.3304 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 32 it 120 total=7.6281 mle=1.5365 pcon=4.7441 forget=1.3475 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 32 it 170 total=7.7520 mle=1.6625 pcon=4.7433 forget=1.3462 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 32 it 220 total=7.7665 mle=1.6733 pcon=4.7426 forget=1.3506 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 32 it 270 total=7.6911 mle=1.6053 pcon=4.7417 forget=1.3441 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 32 it 320 total=7.7658 mle=1.6825 pcon=4.7408 forget=1.3426 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 32 it 370 total=7.5646 mle=1.4750 pcon=4.7402 forget=1.3494 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 33 it 30 total=7.9213 mle=1.8320 pcon=4.7395 forget=1.3498 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 33 it 80 total=7.6027 mle=1.5158 pcon=4.7388 forget=1.3481 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 33 it 130 total=7.6372 mle=1.5372 pcon=4.7381 forget=1.3619 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 33 it 180 total=7.7338 mle=1.6399 pcon=4.7374 forget=1.3565 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 33 it 230 total=7.8380 mle=1.7391 pcon=4.7368 forget=1.3621 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 33 it 280 total=8.1925 mle=2.0765 pcon=4.7361 forget=1.3799 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 33 it 330 total=7.5894 mle=1.4917 pcon=4.7353 forget=1.3624 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 33 it 380 total=7.6520 mle=1.5427 pcon=4.7348 forget=1.3745 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 34 it 40 total=7.7610 mle=1.6516 pcon=4.7342 forget=1.3752 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 34 it 90 total=7.7547 mle=1.6415 pcon=4.7336 forget=1.3796 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 34 it 140 total=7.8579 mle=1.7448 pcon=4.7330 forget=1.3800 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 34 it 190 total=7.7054 mle=1.5810 pcon=4.7324 forget=1.3919 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 34 it 240 total=7.7103 mle=1.5961 pcon=4.7318 forget=1.3824 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 34 it 290 total=7.6004 mle=1.4768 pcon=4.7313 forget=1.3923 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 34 it 340 total=7.6389 mle=1.5149 pcon=4.7307 forget=1.3934 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 35 it 0 total=7.6818 mle=1.5543 pcon=4.7301 forget=1.3974 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 35 it 50 total=7.9825 mle=1.8516 pcon=4.7295 forget=1.4015 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 35 it 100 total=7.7215 mle=1.5890 pcon=4.7291 forget=1.4034 favg=0.0000 nr=43 nf=43 protos=570 fproto_sim=NA
[loss] ep 35 it 150 total=7.7150 mle=1.5843 pcon=4.7286 forget=1.4021 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 35 it 200 total=7.6173 mle=1.4791 pcon=4.7281 forget=1.4101 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 35 it 250 total=7.8432 mle=1.7032 pcon=4.7276 forget=1.4124 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 35 it 300 total=7.7599 mle=1.6167 pcon=4.7271 forget=1.4161 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 35 it 350 total=7.6696 mle=1.5266 pcon=4.7266 forget=1.4165 favg=0.0000 nr=21 nf=21 protos=570 fproto_sim=NA
[loss] ep 36 it 10 total=7.7218 mle=1.5806 pcon=4.7260 forget=1.4152 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 36 it 60 total=7.6365 mle=1.4900 pcon=4.7256 forget=1.4210 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 36 it 110 total=7.8418 mle=1.6811 pcon=4.7251 forget=1.4356 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 36 it 160 total=7.5494 mle=1.3867 pcon=4.7246 forget=1.4380 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 36 it 210 total=7.8745 mle=1.6887 pcon=4.7242 forget=1.4615 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 36 it 260 total=7.8411 mle=1.6657 pcon=4.7239 forget=1.4516 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 36 it 310 total=7.7048 mle=1.5398 pcon=4.7235 forget=1.4416 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 36 it 360 total=7.7127 mle=1.5392 pcon=4.7232 forget=1.4503 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 37 it 20 total=7.6591 mle=1.4816 pcon=4.7229 forget=1.4546 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 37 it 70 total=8.0228 mle=1.8495 pcon=4.7225 forget=1.4509 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 37 it 120 total=7.6630 mle=1.4778 pcon=4.7220 forget=1.4632 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 37 it 170 total=7.6200 mle=1.4307 pcon=4.7216 forget=1.4677 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 37 it 220 total=7.9382 mle=1.7493 pcon=4.7212 forget=1.4677 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 37 it 270 total=7.7972 mle=1.6147 pcon=4.7208 forget=1.4617 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 37 it 320 total=7.6951 mle=1.5080 pcon=4.7204 forget=1.4666 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 37 it 370 total=7.7903 mle=1.5903 pcon=4.7201 forget=1.4799 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 38 it 30 total=7.9306 mle=1.7276 pcon=4.7198 forget=1.4832 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 38 it 80 total=7.7952 mle=1.5888 pcon=4.7194 forget=1.4869 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 38 it 130 total=7.8056 mle=1.5922 pcon=4.7192 forget=1.4943 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 38 it 180 total=7.6497 mle=1.4290 pcon=4.7188 forget=1.5019 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 38 it 230 total=7.6681 mle=1.4391 pcon=4.7186 forget=1.5104 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 38 it 280 total=8.2339 mle=2.0120 pcon=4.7182 forget=1.5037 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 38 it 330 total=7.7990 mle=1.5700 pcon=4.7179 forget=1.5111 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 38 it 380 total=7.8304 mle=1.5987 pcon=4.7176 forget=1.5141 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 39 it 40 total=7.8399 mle=1.5993 pcon=4.7173 forget=1.5233 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 39 it 90 total=7.8184 mle=1.5587 pcon=4.7169 forget=1.5428 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 39 it 140 total=8.0725 mle=1.8392 pcon=4.7167 forget=1.5166 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 39 it 190 total=7.9631 mle=1.6965 pcon=4.7165 forget=1.5501 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 39 it 240 total=7.9921 mle=1.7488 pcon=4.7163 forget=1.5270 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 39 it 290 total=8.0896 mle=1.8206 pcon=4.7161 forget=1.5529 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 39 it 340 total=7.8937 mle=1.6411 pcon=4.7159 forget=1.5367 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 40 it 0 total=7.6899 mle=1.4203 pcon=4.7155 forget=1.5541 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 40 it 50 total=7.7305 mle=1.4676 pcon=4.7152 forget=1.5477 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 40 it 100 total=8.0990 mle=1.8279 pcon=4.7150 forget=1.5561 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 40 it 150 total=7.8395 mle=1.5618 pcon=4.7147 forget=1.5630 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
 82%|████████▏ | 41/50 [11:41<02:51, 19.03s/it] 84%|████████▍ | 42/50 [12:01<02:33, 19.15s/it] 86%|████████▌ | 43/50 [12:20<02:14, 19.18s/it] 88%|████████▊ | 44/50 [12:39<01:55, 19.24s/it] 90%|█████████ | 45/50 [12:58<01:35, 19.14s/it] 92%|█████████▏| 46/50 [13:17<01:16, 19.08s/it] 94%|█████████▍| 47/50 [13:36<00:57, 19.04s/it] 96%|█████████▌| 48/50 [13:55<00:38, 19.06s/it] 98%|█████████▊| 49/50 [14:14<00:18, 18.93s/it][loss] ep 40 it 200 total=7.9567 mle=1.6691 pcon=4.7144 forget=1.5732 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 40 it 250 total=7.9667 mle=1.6914 pcon=4.7142 forget=1.5611 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 40 it 300 total=7.9755 mle=1.6693 pcon=4.7139 forget=1.5923 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 40 it 350 total=7.7575 mle=1.4727 pcon=4.7137 forget=1.5711 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 41 it 10 total=7.8645 mle=1.5648 pcon=4.7136 forget=1.5861 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 41 it 60 total=7.9507 mle=1.6594 pcon=4.7134 forget=1.5780 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 41 it 110 total=8.0291 mle=1.7120 pcon=4.7132 forget=1.6039 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 41 it 160 total=7.7134 mle=1.4015 pcon=4.7130 forget=1.5988 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 41 it 210 total=8.1637 mle=1.8498 pcon=4.7127 forget=1.6012 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 41 it 260 total=8.0465 mle=1.7326 pcon=4.7125 forget=1.6014 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 41 it 310 total=7.8126 mle=1.4962 pcon=4.7124 forget=1.6040 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 41 it 360 total=7.9555 mle=1.6361 pcon=4.7122 forget=1.6072 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 42 it 20 total=7.9008 mle=1.5728 pcon=4.7121 forget=1.6158 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 70 total=7.9846 mle=1.6467 pcon=4.7120 forget=1.6259 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 120 total=8.1846 mle=1.8514 pcon=4.7118 forget=1.6214 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 42 it 170 total=8.1401 mle=1.7963 pcon=4.7117 forget=1.6321 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 220 total=7.8808 mle=1.5391 pcon=4.7115 forget=1.6303 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 42 it 270 total=8.0207 mle=1.6677 pcon=4.7113 forget=1.6417 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 42 it 320 total=8.1254 mle=1.7777 pcon=4.7112 forget=1.6366 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 42 it 370 total=7.8862 mle=1.5401 pcon=4.7110 forget=1.6351 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 43 it 30 total=8.3213 mle=1.9794 pcon=4.7109 forget=1.6310 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 43 it 80 total=8.2338 mle=1.8693 pcon=4.7107 forget=1.6539 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 43 it 130 total=8.3584 mle=1.9850 pcon=4.7105 forget=1.6629 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 43 it 180 total=7.9412 mle=1.5640 pcon=4.7104 forget=1.6667 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 43 it 230 total=7.8553 mle=1.4781 pcon=4.7104 forget=1.6668 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 43 it 280 total=7.8019 mle=1.4323 pcon=4.7104 forget=1.6593 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 43 it 330 total=8.0306 mle=1.6478 pcon=4.7103 forget=1.6725 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 43 it 380 total=8.1693 mle=1.7905 pcon=4.7103 forget=1.6685 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 44 it 40 total=8.5224 mle=2.1315 pcon=4.7102 forget=1.6807 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 44 it 90 total=7.8518 mle=1.4442 pcon=4.7100 forget=1.6976 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 44 it 140 total=8.0538 mle=1.6634 pcon=4.7099 forget=1.6805 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 44 it 190 total=8.0833 mle=1.6795 pcon=4.7098 forget=1.6940 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 44 it 240 total=8.1393 mle=1.7244 pcon=4.7098 forget=1.7052 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 44 it 290 total=8.3295 mle=1.9323 pcon=4.7098 forget=1.6874 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 44 it 340 total=7.9451 mle=1.5473 pcon=4.7097 forget=1.6881 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 45 it 0 total=7.7936 mle=1.3909 pcon=4.7096 forget=1.6930 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 50 total=8.0060 mle=1.5903 pcon=4.7095 forget=1.7062 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 100 total=8.1065 mle=1.6806 pcon=4.7094 forget=1.7164 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 45 it 150 total=8.0502 mle=1.6103 pcon=4.7094 forget=1.7304 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 45 it 200 total=8.0272 mle=1.5983 pcon=4.7093 forget=1.7197 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 45 it 250 total=8.1019 mle=1.6666 pcon=4.7091 forget=1.7262 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 300 total=7.8468 mle=1.4225 pcon=4.7091 forget=1.7152 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 45 it 350 total=8.3134 mle=1.8668 pcon=4.7091 forget=1.7375 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 46 it 10 total=8.0380 mle=1.5910 pcon=4.7090 forget=1.7380 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 46 it 60 total=7.9822 mle=1.5286 pcon=4.7090 forget=1.7446 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 46 it 110 total=7.9293 mle=1.4863 pcon=4.7089 forget=1.7341 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 46 it 160 total=8.2825 mle=1.8382 pcon=4.7090 forget=1.7354 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 46 it 210 total=8.2227 mle=1.7765 pcon=4.7088 forget=1.7374 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 46 it 260 total=8.2621 mle=1.7848 pcon=4.7087 forget=1.7687 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 46 it 310 total=8.1557 mle=1.6773 pcon=4.7087 forget=1.7698 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 46 it 360 total=8.2009 mle=1.7537 pcon=4.7087 forget=1.7385 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 47 it 20 total=8.4648 mle=1.9746 pcon=4.7087 forget=1.7815 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 47 it 70 total=7.9372 mle=1.4578 pcon=4.7086 forget=1.7708 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 47 it 120 total=8.0565 mle=1.5818 pcon=4.7086 forget=1.7661 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 47 it 170 total=8.1209 mle=1.6514 pcon=4.7086 forget=1.7610 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 47 it 220 total=8.0762 mle=1.6169 pcon=4.7085 forget=1.7508 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 47 it 270 total=8.1681 mle=1.6840 pcon=4.7085 forget=1.7756 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 47 it 320 total=7.9951 mle=1.5188 pcon=4.7085 forget=1.7677 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 47 it 370 total=8.0441 mle=1.5478 pcon=4.7084 forget=1.7878 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 30 total=8.2216 mle=1.7422 pcon=4.7084 forget=1.7710 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 48 it 80 total=8.1852 mle=1.6790 pcon=4.7084 forget=1.7979 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 48 it 130 total=8.0785 mle=1.5781 pcon=4.7084 forget=1.7920 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 180 total=8.2699 mle=1.7717 pcon=4.7084 forget=1.7898 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 48 it 230 total=8.3347 mle=1.8276 pcon=4.7084 forget=1.7987 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 48 it 280 total=8.1766 mle=1.6696 pcon=4.7085 forget=1.7985 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 48 it 330 total=7.9772 mle=1.4533 pcon=4.7086 forget=1.8154 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 380 total=8.1601 mle=1.6422 pcon=4.7086 forget=1.8093 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 49 it 40 total=7.9677 mle=1.4594 pcon=4.7086 forget=1.7998 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 49 it 90 total=8.1778 mle=1.6576 pcon=4.7086 forget=1.8117 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
100%|██████████| 50/50 [14:35<00:00, 19.67s/it]100%|██████████| 50/50 [14:35<00:00, 17.52s/it]
[loss] ep 49 it 140 total=7.9465 mle=1.4313 pcon=4.7086 forget=1.8066 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 49 it 190 total=8.1242 mle=1.5900 pcon=4.7086 forget=1.8256 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 49 it 240 total=8.3049 mle=1.7621 pcon=4.7086 forget=1.8342 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 49 it 290 total=8.0650 mle=1.5550 pcon=4.7087 forget=1.8014 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 49 it 340 total=8.1618 mle=1.6320 pcon=4.7087 forget=1.8211 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] loaded adapter 'stage1' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] set_adapter(list) failed: unhashable type: 'list'; try adapter fusion
[peft] adapter fusion failed: adapter fusion API not available or single adapter provided; fallback to last adapter
[peft] active adapter set to: stage1
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1: Number of model parameters: 22082496
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:27,  1.88it/s]  2%|▏         | 9/391 [00:00<00:20, 18.28it/s]  4%|▍         | 17/391 [00:00<00:11, 31.71it/s]  6%|▋         | 25/391 [00:00<00:08, 42.92it/s]  8%|▊         | 32/391 [00:00<00:07, 49.61it/s] 10%|█         | 40/391 [00:01<00:06, 57.24it/s] 12%|█▏        | 48/391 [00:01<00:05, 61.72it/s] 14%|█▍        | 56/391 [00:01<00:05, 66.39it/s] 16%|█▋        | 64/391 [00:01<00:04, 68.00it/s] 18%|█▊        | 72/391 [00:01<00:04, 69.23it/s] 20%|██        | 80/391 [00:01<00:04, 71.59it/s] 23%|██▎       | 88/391 [00:01<00:04, 71.78it/s] 25%|██▍       | 96/391 [00:01<00:04, 73.59it/s] 27%|██▋       | 104/391 [00:01<00:03, 73.14it/s] 29%|██▊       | 112/391 [00:02<00:03, 74.52it/s] 31%|███       | 120/391 [00:02<00:03, 73.75it/s] 33%|███▎      | 128/391 [00:02<00:03, 74.60it/s] 35%|███▍      | 136/391 [00:02<00:03, 74.27it/s] 37%|███▋      | 144/391 [00:02<00:03, 73.59it/s] 39%|███▉      | 152/391 [00:02<00:03, 73.96it/s] 41%|████      | 160/391 [00:02<00:03, 73.65it/s] 43%|████▎     | 168/391 [00:02<00:03, 72.63it/s] 45%|████▌     | 176/391 [00:02<00:02, 71.99it/s] 47%|████▋     | 184/391 [00:03<00:02, 70.61it/s] 49%|████▉     | 192/391 [00:03<00:02, 70.61it/s] 51%|█████     | 200/391 [00:03<00:02, 70.66it/s] 53%|█████▎    | 208/391 [00:03<00:02, 70.87it/s] 55%|█████▌    | 216/391 [00:03<00:02, 73.02it/s] 58%|█████▊    | 226/391 [00:03<00:02, 79.73it/s] 60%|██████    | 235/391 [00:03<00:01, 80.67it/s] 62%|██████▏   | 244/391 [00:03<00:01, 78.36it/s] 64%|██████▍   | 252/391 [00:03<00:01, 75.34it/s] 66%|██████▋   | 260/391 [00:04<00:01, 75.80it/s] 69%|██████▊   | 268/391 [00:04<00:01, 74.79it/s] 71%|███████   | 276/391 [00:04<00:01, 75.85it/s] 73%|███████▎  | 284/391 [00:04<00:01, 74.15it/s] 75%|███████▍  | 292/391 [00:04<00:01, 73.25it/s] 77%|███████▋  | 300/391 [00:04<00:01, 74.34it/s] 79%|███████▉  | 308/391 [00:04<00:01, 73.74it/s] 81%|████████  | 316/391 [00:04<00:01, 72.92it/s] 83%|████████▎ | 324/391 [00:04<00:00, 74.18it/s] 85%|████████▍ | 332/391 [00:04<00:00, 73.51it/s] 87%|████████▋ | 340/391 [00:05<00:00, 74.71it/s] 89%|████████▉ | 348/391 [00:05<00:00, 72.92it/s] 91%|█████████ | 356/391 [00:05<00:00, 72.76it/s] 93%|█████████▎| 364/391 [00:05<00:00, 72.25it/s] 95%|█████████▌| 372/391 [00:05<00:00, 73.57it/s] 97%|█████████▋| 380/391 [00:05<00:00, 73.16it/s] 99%|█████████▉| 388/391 [00:05<00:00, 72.46it/s]100%|██████████| 391/391 [00:05<00:00, 67.33it/s]
50000 images processed, 6.208981037139893 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.92it/s] 10%|█         | 8/79 [00:00<00:04, 16.36it/s] 20%|██        | 16/79 [00:00<00:02, 30.82it/s] 29%|██▉       | 23/79 [00:00<00:01, 40.24it/s] 38%|███▊      | 30/79 [00:00<00:01, 47.73it/s] 48%|████▊     | 38/79 [00:01<00:00, 55.40it/s] 57%|█████▋    | 45/79 [00:01<00:00, 59.09it/s] 67%|██████▋   | 53/79 [00:01<00:00, 63.19it/s] 77%|███████▋  | 61/79 [00:01<00:00, 67.24it/s] 87%|████████▋ | 69/79 [00:01<00:00, 68.84it/s] 97%|█████████▋| 77/79 [00:01<00:00, 69.42it/s]100%|██████████| 79/79 [00:01<00:00, 49.03it/s]
10000 images processed, 1.6795909404754639 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:40,  2.02it/s]  4%|▍         | 9/204 [00:00<00:10, 19.25it/s]  8%|▊         | 16/204 [00:00<00:05, 31.42it/s] 12%|█▏        | 24/204 [00:00<00:04, 43.04it/s] 15%|█▌        | 31/204 [00:00<00:03, 49.94it/s] 19%|█▉        | 39/204 [00:01<00:02, 56.58it/s] 23%|██▎       | 47/204 [00:01<00:02, 62.13it/s] 27%|██▋       | 55/204 [00:01<00:02, 65.13it/s] 31%|███       | 63/204 [00:01<00:02, 66.53it/s] 35%|███▍      | 71/204 [00:01<00:01, 67.09it/s] 39%|███▊      | 79/204 [00:01<00:01, 69.51it/s] 43%|████▎     | 87/204 [00:01<00:01, 69.58it/s] 47%|████▋     | 95/204 [00:01<00:01, 69.54it/s] 50%|█████     | 103/204 [00:01<00:01, 69.07it/s] 54%|█████▍    | 111/204 [00:02<00:01, 69.64it/s] 58%|█████▊    | 119/204 [00:02<00:01, 71.77it/s] 62%|██████▏   | 127/204 [00:02<00:01, 71.49it/s] 66%|██████▌   | 135/204 [00:02<00:00, 72.96it/s] 70%|███████   | 143/204 [00:02<00:00, 71.98it/s] 74%|███████▍  | 151/204 [00:02<00:00, 71.00it/s] 78%|███████▊  | 159/204 [00:02<00:00, 70.51it/s] 82%|████████▏ | 167/204 [00:02<00:00, 70.50it/s] 86%|████████▌ | 175/204 [00:02<00:00, 71.97it/s] 90%|████████▉ | 183/204 [00:03<00:00, 72.09it/s] 94%|█████████▎| 191/204 [00:03<00:00, 71.76it/s] 98%|█████████▊| 199/204 [00:03<00:00, 73.32it/s]100%|██████████| 204/204 [00:03<00:00, 61.45it/s]
26032 images processed, 3.50669002532959 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:48,  1.61it/s] 11%|█▏        | 9/79 [00:00<00:04, 16.12it/s] 22%|██▏       | 17/79 [00:00<00:02, 28.80it/s] 32%|███▏      | 25/79 [00:00<00:01, 39.90it/s] 42%|████▏     | 33/79 [00:01<00:00, 48.31it/s] 52%|█████▏    | 41/79 [00:01<00:00, 54.37it/s] 62%|██████▏   | 49/79 [00:01<00:00, 59.44it/s] 72%|███████▏  | 57/79 [00:01<00:00, 63.22it/s] 82%|████████▏ | 65/79 [00:01<00:00, 67.36it/s] 92%|█████████▏| 73/79 [00:01<00:00, 68.78it/s]100%|██████████| 79/79 [00:01<00:00, 47.20it/s]
10000 images processed, 1.764819622039795 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:39,  1.99it/s] 10%|█         | 8/79 [00:00<00:04, 16.87it/s] 20%|██        | 16/79 [00:00<00:01, 31.65it/s] 30%|███       | 24/79 [00:00<00:01, 42.63it/s] 39%|███▉      | 31/79 [00:00<00:00, 49.54it/s] 49%|████▉     | 39/79 [00:01<00:00, 56.82it/s] 58%|█████▊    | 46/79 [00:01<00:00, 60.19it/s] 68%|██████▊   | 54/79 [00:01<00:00, 64.41it/s] 78%|███████▊  | 62/79 [00:01<00:00, 67.11it/s] 89%|████████▊ | 70/79 [00:01<00:00, 68.99it/s] 99%|█████████▊| 78/79 [00:01<00:00, 71.68it/s]100%|██████████| 79/79 [00:01<00:00, 50.52it/s]
10000 images processed, 1.6444807052612305 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:40,  1.69it/s] 10%|█         | 7/70 [00:00<00:04, 12.92it/s] 21%|██▏       | 15/70 [00:00<00:02, 26.99it/s] 33%|███▎      | 23/70 [00:00<00:01, 38.33it/s] 44%|████▍     | 31/70 [00:01<00:00, 46.91it/s] 56%|█████▌    | 39/70 [00:01<00:00, 54.68it/s] 67%|██████▋   | 47/70 [00:01<00:00, 59.61it/s] 77%|███████▋  | 54/70 [00:01<00:00, 62.02it/s] 89%|████████▊ | 62/70 [00:01<00:00, 66.39it/s]100%|██████████| 70/70 [00:01<00:00, 67.55it/s]100%|██████████| 70/70 [00:01<00:00, 44.78it/s]
8925 images processed, 1.6459472179412842 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:43,  1.02it/s]  4%|▍         | 2/45 [00:01<00:21,  2.01it/s] 16%|█▌        | 7/45 [00:01<00:04,  8.92it/s] 33%|███▎      | 15/45 [00:01<00:01, 21.00it/s] 44%|████▍     | 20/45 [00:01<00:01, 18.35it/s] 53%|█████▎    | 24/45 [00:01<00:01, 18.07it/s] 71%|███████   | 32/45 [00:02<00:00, 27.73it/s] 82%|████████▏ | 37/45 [00:02<00:00, 21.33it/s]100%|██████████| 45/45 [00:02<00:00, 30.13it/s]100%|██████████| 45/45 [00:02<00:00, 18.16it/s]
5640 images processed, 2.5310909748077393 seconds used

20.67068910598755
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           5.52  98.78
places365     77.71  77.60
LSUN          43.10  88.75
iSUN          74.43  80.89
dtd           46.79  89.22
AVG           49.51  87.05
Retain-Acc: 0.7374
Traceback (most recent call last):
  File "/home/shaokun/PALM/eval_cifar.py", line 19, in <module>
    eval_maha(args)
  File "/home/shaokun/PALM/util/evaluations/mahalanobis.py", line 173, in eval_maha
    fr_results = metrics.cal_metric(retain_scores, forget_scores)
  File "/home/shaokun/PALM/util/evaluations/metrics.py", line 16, in cal_metric
    tp, fp, fpr_at_tpr95 = get_curve(known, novel, method)
  File "/home/shaokun/PALM/util/evaluations/metrics.py", line 85, in get_curve
    end = np.max([np.max(known), np.max(novel)])
  File "/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/numpy/core/fromnumeric.py", line 2810, in max
    return _wrapreduction(a, np.maximum, 'max', axis, None, out,
  File "/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/numpy/core/fromnumeric.py", line 88, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
ValueError: zero-size array to reduction operation maximum which has no identity
