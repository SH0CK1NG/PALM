nohup: ignoring input
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=25, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-baseline-randlabel-b128-e25-lr0.001-wd1e-4-fl1-CIFAR-100forget15.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=0.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=False, lora_impl='native', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='head', adapter_save_path=None, adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=1.0, forget_margin=100.0, forget_strategy='randlabel', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21605312
[debug] trainable_count = 112
[debug] trainable: encoder.conv1.weight
[debug] trainable: encoder.bn1.weight
[debug] trainable: encoder.bn1.bias
[debug] trainable: encoder.layer1.0.conv1.weight
[debug] trainable: encoder.layer1.0.bn1.weight
[debug] trainable: encoder.layer1.0.bn1.bias
[debug] trainable: encoder.layer1.0.conv2.weight
[debug] trainable: encoder.layer1.0.bn2.weight
[debug] trainable: encoder.layer1.0.bn2.bias
[debug] trainable: encoder.layer1.1.conv1.weight
[debug] trainable: encoder.layer1.1.bn1.weight
[debug] trainable: encoder.layer1.1.bn1.bias
[debug] trainable: encoder.layer1.1.conv2.weight
[debug] trainable: encoder.layer1.1.bn2.weight
[debug] trainable: encoder.layer1.1.bn2.bias
[debug] trainable: encoder.layer1.2.conv1.weight
[debug] trainable: encoder.layer1.2.bn1.weight
[debug] trainable: encoder.layer1.2.bn1.bias
[debug] trainable: encoder.layer1.2.conv2.weight
[debug] trainable: encoder.layer1.2.bn2.weight
[debug] trainable: encoder.layer1.2.bn2.bias
[debug] trainable: encoder.layer2.0.conv1.weight
[debug] trainable: encoder.layer2.0.bn1.weight
[debug] trainable: encoder.layer2.0.bn1.bias
[debug] trainable: encoder.layer2.0.conv2.weight
[debug] trainable: encoder.layer2.0.bn2.weight
[debug] trainable: encoder.layer2.0.bn2.bias
[debug] trainable: encoder.layer2.0.shortcut.0.weight
[debug] trainable: encoder.layer2.0.shortcut.1.weight
[debug] trainable: encoder.layer2.0.shortcut.1.bias
[debug] trainable: encoder.layer2.1.conv1.weight
[debug] trainable: encoder.layer2.1.bn1.weight
[debug] trainable: encoder.layer2.1.bn1.bias
[debug] trainable: encoder.layer2.1.conv2.weight
[debug] trainable: encoder.layer2.1.bn2.weight
[debug] trainable: encoder.layer2.1.bn2.bias
[debug] trainable: encoder.layer2.2.conv1.weight
[debug] trainable: encoder.layer2.2.bn1.weight
[debug] trainable: encoder.layer2.2.bn1.bias
[debug] trainable: encoder.layer2.2.conv2.weight
[debug] trainable: encoder.layer2.2.bn2.weight
[debug] trainable: encoder.layer2.2.bn2.bias
[debug] trainable: encoder.layer2.3.conv1.weight
[debug] trainable: encoder.layer2.3.bn1.weight
[debug] trainable: encoder.layer2.3.bn1.bias
[debug] trainable: encoder.layer2.3.conv2.weight
[debug] trainable: encoder.layer2.3.bn2.weight
[debug] trainable: encoder.layer2.3.bn2.bias
[debug] trainable: encoder.layer3.0.conv1.weight
[debug] trainable: encoder.layer3.0.bn1.weight
[debug][warn] non-LoRA trainables detected: ['encoder.conv1.weight', 'encoder.bn1.weight', 'encoder.bn1.bias', 'encoder.layer1.0.conv1.weight', 'encoder.layer1.0.bn1.weight', 'encoder.layer1.0.bn1.bias', 'encoder.layer1.0.conv2.weight', 'encoder.layer1.0.bn2.weight', 'encoder.layer1.0.bn2.bias', 'encoder.layer1.1.conv1.weight']
[trainable] param_count=21605312 tensors=112
  0%|          | 0/25 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:550: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=bool(scaler is not None)):
/home/shaokun/PALM/trainer.py:638: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.
Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)
  print(f"[loss-{args.forget_strategy}] ep {epoch} it {it} total={loss.item():.4f} ce_r={float(loss_retain):.4f} ce_f={float(loss_forget):.4f}")
  4%|▍         | 1/25 [00:28<11:27, 28.65s/it]  8%|▊         | 2/25 [00:43<07:58, 20.80s/it] 12%|█▏        | 3/25 [00:59<06:40, 18.21s/it] 16%|█▌        | 4/25 [01:14<05:55, 16.91s/it] 20%|██        | 5/25 [01:29<05:25, 16.29s/it] 24%|██▍       | 6/25 [01:44<05:03, 15.98s/it] 28%|██▊       | 7/25 [02:00<04:47, 15.97s/it] 32%|███▏      | 8/25 [02:15<04:27, 15.72s/it] 36%|███▌      | 9/25 [02:31<04:10, 15.63s/it] 40%|████      | 10/25 [02:46<03:51, 15.41s/it] 44%|████▍     | 11/25 [03:01<03:35, 15.37s/it] 48%|████▊     | 12/25 [03:16<03:20, 15.39s/it] 52%|█████▏    | 13/25 [03:31<03:02, 15.23s/it] 56%|█████▌    | 14/25 [03:47<02:48, 15.28s/it] 60%|██████    | 15/25 [04:02<02:34, 15.41s/it][loss-randlabel] ep 0 it 0 total=10.4157 ce_r=0.0170 ce_f=10.3987
[loss-randlabel] ep 0 it 50 total=10.1016 ce_r=0.0319 ce_f=10.0698
[loss-randlabel] ep 0 it 100 total=9.6119 ce_r=0.0326 ce_f=9.5793
[loss-randlabel] ep 0 it 150 total=7.8304 ce_r=0.6561 ce_f=7.1743
[loss-randlabel] ep 0 it 200 total=6.9106 ce_r=0.8792 ce_f=6.0314
[loss-randlabel] ep 0 it 250 total=6.5798 ce_r=0.9258 ce_f=5.6540
[loss-randlabel] ep 0 it 300 total=6.6062 ce_r=0.8652 ce_f=5.7410
[loss-randlabel] ep 0 it 350 total=6.4251 ce_r=0.8586 ce_f=5.5665
[loss-randlabel] ep 1 it 10 total=6.1078 ce_r=0.8453 ce_f=5.2625
[loss-randlabel] ep 1 it 60 total=6.3507 ce_r=0.8853 ce_f=5.4655
[loss-randlabel] ep 1 it 110 total=6.0638 ce_r=0.6534 ce_f=5.4104
[loss-randlabel] ep 1 it 160 total=6.1147 ce_r=0.8749 ce_f=5.2398
[loss-randlabel] ep 1 it 210 total=6.1919 ce_r=0.6054 ce_f=5.5864
[loss-randlabel] ep 1 it 260 total=6.0890 ce_r=0.6600 ce_f=5.4290
[loss-randlabel] ep 1 it 310 total=6.1169 ce_r=0.9331 ce_f=5.1838
[loss-randlabel] ep 1 it 360 total=6.1183 ce_r=0.7705 ce_f=5.3478
[loss-randlabel] ep 2 it 20 total=6.0521 ce_r=0.9240 ce_f=5.1281
[loss-randlabel] ep 2 it 70 total=5.9497 ce_r=0.6108 ce_f=5.3388
[loss-randlabel] ep 2 it 120 total=5.8561 ce_r=0.7220 ce_f=5.1340
[loss-randlabel] ep 2 it 170 total=6.0557 ce_r=0.7547 ce_f=5.3010
[loss-randlabel] ep 2 it 220 total=5.8073 ce_r=0.7184 ce_f=5.0889
[loss-randlabel] ep 2 it 270 total=5.9565 ce_r=0.7012 ce_f=5.2553
[loss-randlabel] ep 2 it 320 total=6.0661 ce_r=0.7183 ce_f=5.3478
[loss-randlabel] ep 2 it 370 total=5.9459 ce_r=0.7574 ce_f=5.1885
[loss-randlabel] ep 3 it 30 total=6.0386 ce_r=0.7194 ce_f=5.3192
[loss-randlabel] ep 3 it 80 total=5.8826 ce_r=0.7997 ce_f=5.0828
[loss-randlabel] ep 3 it 130 total=5.9067 ce_r=0.6233 ce_f=5.2834
[loss-randlabel] ep 3 it 180 total=5.9424 ce_r=0.8279 ce_f=5.1146
[loss-randlabel] ep 3 it 230 total=5.7787 ce_r=0.8027 ce_f=4.9760
[loss-randlabel] ep 3 it 280 total=5.9880 ce_r=0.8061 ce_f=5.1819
[loss-randlabel] ep 3 it 330 total=5.8285 ce_r=0.6737 ce_f=5.1549
[loss-randlabel] ep 3 it 380 total=5.9049 ce_r=0.8799 ce_f=5.0250
[loss-randlabel] ep 4 it 40 total=5.8920 ce_r=0.7422 ce_f=5.1498
[loss-randlabel] ep 4 it 90 total=5.7647 ce_r=0.6881 ce_f=5.0766
[loss-randlabel] ep 4 it 140 total=5.9238 ce_r=0.7160 ce_f=5.2078
[loss-randlabel] ep 4 it 190 total=5.9136 ce_r=0.7809 ce_f=5.1327
[loss-randlabel] ep 4 it 240 total=5.7546 ce_r=0.7030 ce_f=5.0517
[loss-randlabel] ep 4 it 290 total=5.8203 ce_r=0.8148 ce_f=5.0055
[loss-randlabel] ep 4 it 340 total=5.8072 ce_r=0.7920 ce_f=5.0152
[loss-randlabel] ep 5 it 0 total=5.7759 ce_r=0.6579 ce_f=5.1181
[loss-randlabel] ep 5 it 50 total=5.7291 ce_r=0.7531 ce_f=4.9760
[loss-randlabel] ep 5 it 100 total=5.7702 ce_r=0.5429 ce_f=5.2272
[loss-randlabel] ep 5 it 150 total=5.7740 ce_r=0.6944 ce_f=5.0797
[loss-randlabel] ep 5 it 200 total=5.8778 ce_r=0.7036 ce_f=5.1741
[loss-randlabel] ep 5 it 250 total=5.8240 ce_r=0.7844 ce_f=5.0396
[loss-randlabel] ep 5 it 300 total=5.8257 ce_r=0.7205 ce_f=5.1052
[loss-randlabel] ep 5 it 350 total=5.6204 ce_r=0.6350 ce_f=4.9854
[loss-randlabel] ep 6 it 10 total=5.8089 ce_r=0.8237 ce_f=4.9852
[loss-randlabel] ep 6 it 60 total=5.8551 ce_r=0.7329 ce_f=5.1223
[loss-randlabel] ep 6 it 110 total=5.8083 ce_r=0.7316 ce_f=5.0768
[loss-randlabel] ep 6 it 160 total=5.7657 ce_r=0.6856 ce_f=5.0801
[loss-randlabel] ep 6 it 210 total=5.6967 ce_r=0.6534 ce_f=5.0433
[loss-randlabel] ep 6 it 260 total=5.7227 ce_r=0.7620 ce_f=4.9607
[loss-randlabel] ep 6 it 310 total=5.7012 ce_r=0.6238 ce_f=5.0774
[loss-randlabel] ep 6 it 360 total=5.8317 ce_r=0.6662 ce_f=5.1655
[loss-randlabel] ep 7 it 20 total=5.8046 ce_r=0.7558 ce_f=5.0488
[loss-randlabel] ep 7 it 70 total=5.7052 ce_r=0.6079 ce_f=5.0972
[loss-randlabel] ep 7 it 120 total=5.8468 ce_r=0.6939 ce_f=5.1529
[loss-randlabel] ep 7 it 170 total=5.8267 ce_r=0.7548 ce_f=5.0719
[loss-randlabel] ep 7 it 220 total=5.6220 ce_r=0.6253 ce_f=4.9967
[loss-randlabel] ep 7 it 270 total=5.5371 ce_r=0.7451 ce_f=4.7920
[loss-randlabel] ep 7 it 320 total=5.7344 ce_r=0.6028 ce_f=5.1315
[loss-randlabel] ep 7 it 370 total=5.6857 ce_r=0.6813 ce_f=5.0044
[loss-randlabel] ep 8 it 30 total=5.6962 ce_r=0.5938 ce_f=5.1024
[loss-randlabel] ep 8 it 80 total=5.7836 ce_r=0.7315 ce_f=5.0520
[loss-randlabel] ep 8 it 130 total=5.7871 ce_r=0.7114 ce_f=5.0757
[loss-randlabel] ep 8 it 180 total=5.6786 ce_r=0.6154 ce_f=5.0632
[loss-randlabel] ep 8 it 230 total=5.7404 ce_r=0.7640 ce_f=4.9763
[loss-randlabel] ep 8 it 280 total=5.6506 ce_r=0.6296 ce_f=5.0210
[loss-randlabel] ep 8 it 330 total=5.6701 ce_r=0.5731 ce_f=5.0970
[loss-randlabel] ep 8 it 380 total=5.7446 ce_r=0.7021 ce_f=5.0426
[loss-randlabel] ep 9 it 40 total=5.6731 ce_r=0.6928 ce_f=4.9803
[loss-randlabel] ep 9 it 90 total=5.6443 ce_r=0.6857 ce_f=4.9586
[loss-randlabel] ep 9 it 140 total=5.6810 ce_r=0.7385 ce_f=4.9426
[loss-randlabel] ep 9 it 190 total=5.6167 ce_r=0.7594 ce_f=4.8573
[loss-randlabel] ep 9 it 240 total=5.7505 ce_r=0.6690 ce_f=5.0815
[loss-randlabel] ep 9 it 290 total=5.8036 ce_r=0.7297 ce_f=5.0739
[loss-randlabel] ep 9 it 340 total=5.6232 ce_r=0.5978 ce_f=5.0254
[loss-randlabel] ep 10 it 0 total=5.7200 ce_r=0.6598 ce_f=5.0601
[loss-randlabel] ep 10 it 50 total=5.6758 ce_r=0.7132 ce_f=4.9626
[loss-randlabel] ep 10 it 100 total=5.7080 ce_r=0.6579 ce_f=5.0501
[loss-randlabel] ep 10 it 150 total=5.7172 ce_r=0.6209 ce_f=5.0963
[loss-randlabel] ep 10 it 200 total=5.6682 ce_r=0.6272 ce_f=5.0410
[loss-randlabel] ep 10 it 250 total=5.7004 ce_r=0.6802 ce_f=5.0202
[loss-randlabel] ep 10 it 300 total=5.7556 ce_r=0.7244 ce_f=5.0312
[loss-randlabel] ep 10 it 350 total=5.6429 ce_r=0.6536 ce_f=4.9893
[loss-randlabel] ep 11 it 10 total=5.6067 ce_r=0.5963 ce_f=5.0104
[loss-randlabel] ep 11 it 60 total=5.7449 ce_r=0.7159 ce_f=5.0290
[loss-randlabel] ep 11 it 110 total=5.4696 ce_r=0.6582 ce_f=4.8114
[loss-randlabel] ep 11 it 160 total=5.5438 ce_r=0.6105 ce_f=4.9333
[loss-randlabel] ep 11 it 210 total=5.7459 ce_r=0.6350 ce_f=5.1110
[loss-randlabel] ep 11 it 260 total=5.6679 ce_r=0.7643 ce_f=4.9036
[loss-randlabel] ep 11 it 310 total=5.5653 ce_r=0.5963 ce_f=4.9690
[loss-randlabel] ep 11 it 360 total=5.7553 ce_r=0.6015 ce_f=5.1538
[loss-randlabel] ep 12 it 20 total=5.5928 ce_r=0.6330 ce_f=4.9599
[loss-randlabel] ep 12 it 70 total=5.6284 ce_r=0.6241 ce_f=5.0043
[loss-randlabel] ep 12 it 120 total=5.7034 ce_r=0.6651 ce_f=5.0382
[loss-randlabel] ep 12 it 170 total=5.5588 ce_r=0.6162 ce_f=4.9426
[loss-randlabel] ep 12 it 220 total=5.7000 ce_r=0.5970 ce_f=5.1030
[loss-randlabel] ep 12 it 270 total=5.6480 ce_r=0.5730 ce_f=5.0750
[loss-randlabel] ep 12 it 320 total=5.6104 ce_r=0.6828 ce_f=4.9276
[loss-randlabel] ep 12 it 370 total=5.6496 ce_r=0.5864 ce_f=5.0631
[loss-randlabel] ep 13 it 30 total=5.6798 ce_r=0.7555 ce_f=4.9242
[loss-randlabel] ep 13 it 80 total=5.7249 ce_r=0.6708 ce_f=5.0541
[loss-randlabel] ep 13 it 130 total=5.6511 ce_r=0.5877 ce_f=5.0635
[loss-randlabel] ep 13 it 180 total=5.6931 ce_r=0.6971 ce_f=4.9960
[loss-randlabel] ep 13 it 230 total=5.6461 ce_r=0.6562 ce_f=4.9899
[loss-randlabel] ep 13 it 280 total=5.7489 ce_r=0.6862 ce_f=5.0627
[loss-randlabel] ep 13 it 330 total=5.6233 ce_r=0.6555 ce_f=4.9678
[loss-randlabel] ep 13 it 380 total=5.6002 ce_r=0.6882 ce_f=4.9120
[loss-randlabel] ep 14 it 40 total=5.6012 ce_r=0.6967 ce_f=4.9044
[loss-randlabel] ep 14 it 90 total=5.6335 ce_r=0.6086 ce_f=5.0249
[loss-randlabel] ep 14 it 140 total=5.5663 ce_r=0.7017 ce_f=4.8646
[loss-randlabel] ep 14 it 190 total=5.7260 ce_r=0.6577 ce_f=5.0683
[loss-randlabel] ep 14 it 240 total=5.7524 ce_r=0.5307 ce_f=5.2217
[loss-randlabel] ep 14 it 290 total=5.6354 ce_r=0.5733 ce_f=5.0621
[loss-randlabel] ep 14 it 340 total=5.7393 ce_r=0.6331 ce_f=5.1062
[loss-randlabel] ep 15 it 0 total=5.5427 ce_r=0.7347 ce_f=4.8081
[loss-randlabel] ep 15 it 50 total=5.6816 ce_r=0.5670 ce_f=5.1146
[loss-randlabel] ep 15 it 100 total=5.6481 ce_r=0.6687 ce_f=4.9793
[loss-randlabel] ep 15 it 150 total=5.7339 ce_r=0.6438 ce_f=5.0901
[loss-randlabel] ep 15 it 200 total=5.6863 ce_r=0.6548 ce_f=5.0315
[loss-randlabel] ep 15 it 250 total=5.6065 ce_r=0.7070 ce_f=4.8995
 64%|██████▍   | 16/25 [04:17<02:18, 15.36s/it] 68%|██████▊   | 17/25 [04:33<02:02, 15.27s/it] 72%|███████▏  | 18/25 [04:48<01:46, 15.25s/it] 76%|███████▌  | 19/25 [05:03<01:31, 15.24s/it] 80%|████████  | 20/25 [05:18<01:15, 15.05s/it] 84%|████████▍ | 21/25 [05:33<01:00, 15.17s/it] 88%|████████▊ | 22/25 [05:48<00:45, 15.15s/it] 92%|█████████▏| 23/25 [06:03<00:30, 15.10s/it] 96%|█████████▌| 24/25 [06:18<00:15, 15.03s/it]100%|██████████| 25/25 [06:33<00:00, 14.97s/it]100%|██████████| 25/25 [06:33<00:00, 15.73s/it]
[loss-randlabel] ep 15 it 300 total=5.6686 ce_r=0.6324 ce_f=5.0361
[loss-randlabel] ep 15 it 350 total=5.6305 ce_r=0.6466 ce_f=4.9839
[loss-randlabel] ep 16 it 10 total=5.6016 ce_r=0.5521 ce_f=5.0495
[loss-randlabel] ep 16 it 60 total=5.6774 ce_r=0.6417 ce_f=5.0357
[loss-randlabel] ep 16 it 110 total=5.6645 ce_r=0.6837 ce_f=4.9808
[loss-randlabel] ep 16 it 160 total=5.5751 ce_r=0.5901 ce_f=4.9850
[loss-randlabel] ep 16 it 210 total=5.7781 ce_r=0.5563 ce_f=5.2218
[loss-randlabel] ep 16 it 260 total=5.7279 ce_r=0.6981 ce_f=5.0298
[loss-randlabel] ep 16 it 310 total=5.7012 ce_r=0.5956 ce_f=5.1056
[loss-randlabel] ep 16 it 360 total=5.6757 ce_r=0.6152 ce_f=5.0606
[loss-randlabel] ep 17 it 20 total=5.6445 ce_r=0.5842 ce_f=5.0604
[loss-randlabel] ep 17 it 70 total=5.6599 ce_r=0.7222 ce_f=4.9377
[loss-randlabel] ep 17 it 120 total=5.6645 ce_r=0.6424 ce_f=5.0220
[loss-randlabel] ep 17 it 170 total=5.7133 ce_r=0.7338 ce_f=4.9795
[loss-randlabel] ep 17 it 220 total=5.7235 ce_r=0.6185 ce_f=5.1050
[loss-randlabel] ep 17 it 270 total=5.6407 ce_r=0.6048 ce_f=5.0359
[loss-randlabel] ep 17 it 320 total=5.7095 ce_r=0.6613 ce_f=5.0483
[loss-randlabel] ep 17 it 370 total=5.6323 ce_r=0.6240 ce_f=5.0083
[loss-randlabel] ep 18 it 30 total=5.6548 ce_r=0.5393 ce_f=5.1156
[loss-randlabel] ep 18 it 80 total=5.7303 ce_r=0.6276 ce_f=5.1027
[loss-randlabel] ep 18 it 130 total=5.6876 ce_r=0.6701 ce_f=5.0175
[loss-randlabel] ep 18 it 180 total=5.6346 ce_r=0.5662 ce_f=5.0685
[loss-randlabel] ep 18 it 230 total=5.6845 ce_r=0.6172 ce_f=5.0673
[loss-randlabel] ep 18 it 280 total=5.6347 ce_r=0.6668 ce_f=4.9679
[loss-randlabel] ep 18 it 330 total=5.6410 ce_r=0.5970 ce_f=5.0440
[loss-randlabel] ep 18 it 380 total=5.6603 ce_r=0.5810 ce_f=5.0793
[loss-randlabel] ep 19 it 40 total=5.6265 ce_r=0.5837 ce_f=5.0428
[loss-randlabel] ep 19 it 90 total=5.7110 ce_r=0.6834 ce_f=5.0277
[loss-randlabel] ep 19 it 140 total=5.6352 ce_r=0.5836 ce_f=5.0516
[loss-randlabel] ep 19 it 190 total=5.5827 ce_r=0.6540 ce_f=4.9286
[loss-randlabel] ep 19 it 240 total=5.6864 ce_r=0.6421 ce_f=5.0444
[loss-randlabel] ep 19 it 290 total=5.4897 ce_r=0.6070 ce_f=4.8827
[loss-randlabel] ep 19 it 340 total=5.6938 ce_r=0.6495 ce_f=5.0443
[loss-randlabel] ep 20 it 0 total=5.6896 ce_r=0.6447 ce_f=5.0449
[loss-randlabel] ep 20 it 50 total=5.6715 ce_r=0.5694 ce_f=5.1020
[loss-randlabel] ep 20 it 100 total=5.6788 ce_r=0.6298 ce_f=5.0490
[loss-randlabel] ep 20 it 150 total=5.4722 ce_r=0.5979 ce_f=4.8743
[loss-randlabel] ep 20 it 200 total=5.6200 ce_r=0.6094 ce_f=5.0105
[loss-randlabel] ep 20 it 250 total=5.5563 ce_r=0.5815 ce_f=4.9748
[loss-randlabel] ep 20 it 300 total=5.7313 ce_r=0.6529 ce_f=5.0784
[loss-randlabel] ep 20 it 350 total=5.7003 ce_r=0.6828 ce_f=5.0175
[loss-randlabel] ep 21 it 10 total=5.6943 ce_r=0.6640 ce_f=5.0303
[loss-randlabel] ep 21 it 60 total=5.6942 ce_r=0.5855 ce_f=5.1086
[loss-randlabel] ep 21 it 110 total=5.6556 ce_r=0.5939 ce_f=5.0617
[loss-randlabel] ep 21 it 160 total=5.6359 ce_r=0.6002 ce_f=5.0357
[loss-randlabel] ep 21 it 210 total=5.6920 ce_r=0.6118 ce_f=5.0802
[loss-randlabel] ep 21 it 260 total=5.5299 ce_r=0.6256 ce_f=4.9043
[loss-randlabel] ep 21 it 310 total=5.6192 ce_r=0.5754 ce_f=5.0437
[loss-randlabel] ep 21 it 360 total=5.7243 ce_r=0.7170 ce_f=5.0073
[loss-randlabel] ep 22 it 20 total=5.6567 ce_r=0.6428 ce_f=5.0138
[loss-randlabel] ep 22 it 70 total=5.5738 ce_r=0.6061 ce_f=4.9677
[loss-randlabel] ep 22 it 120 total=5.6501 ce_r=0.5993 ce_f=5.0508
[loss-randlabel] ep 22 it 170 total=5.7942 ce_r=0.7323 ce_f=5.0619
[loss-randlabel] ep 22 it 220 total=5.7109 ce_r=0.5999 ce_f=5.1110
[loss-randlabel] ep 22 it 270 total=5.6349 ce_r=0.5824 ce_f=5.0525
[loss-randlabel] ep 22 it 320 total=5.5598 ce_r=0.6647 ce_f=4.8951
[loss-randlabel] ep 22 it 370 total=5.6355 ce_r=0.6072 ce_f=5.0283
[loss-randlabel] ep 23 it 30 total=5.6145 ce_r=0.6147 ce_f=4.9999
[loss-randlabel] ep 23 it 80 total=5.5500 ce_r=0.6591 ce_f=4.8909
[loss-randlabel] ep 23 it 130 total=5.5526 ce_r=0.5494 ce_f=5.0032
[loss-randlabel] ep 23 it 180 total=5.6830 ce_r=0.6835 ce_f=4.9995
[loss-randlabel] ep 23 it 230 total=5.6284 ce_r=0.7066 ce_f=4.9218
[loss-randlabel] ep 23 it 280 total=5.5945 ce_r=0.7235 ce_f=4.8710
[loss-randlabel] ep 23 it 330 total=5.5515 ce_r=0.6023 ce_f=4.9492
[loss-randlabel] ep 23 it 380 total=5.6825 ce_r=0.6032 ce_f=5.0793
[loss-randlabel] ep 24 it 40 total=5.6029 ce_r=0.6622 ce_f=4.9407
[loss-randlabel] ep 24 it 90 total=5.6222 ce_r=0.6754 ce_f=4.9468
[loss-randlabel] ep 24 it 140 total=5.4864 ce_r=0.5912 ce_f=4.8952
[loss-randlabel] ep 24 it 190 total=5.5554 ce_r=0.5735 ce_f=4.9819
[loss-randlabel] ep 24 it 240 total=5.5196 ce_r=0.5713 ce_f=4.9483
[loss-randlabel] ep 24 it 290 total=5.6974 ce_r=0.6560 ce_f=5.0415
[loss-randlabel] ep 24 it 340 total=5.6366 ce_r=0.6414 ce_f=4.9952
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-baseline-randlabel-b128-e25-lr0.001-wd1e-4-fl1-CIFAR-100forget15.pt
resnet34-top5-palm-cache6-ema0.999-baseline-randlabel-b128-e25-lr0.001-wd1e-4-fl1-CIFAR-100forget15: Number of model parameters: 21605312
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:30,  2.58it/s]  3%|▎         | 11/391 [00:00<00:13, 28.60it/s]  6%|▌         | 22/391 [00:00<00:07, 49.90it/s]  8%|▊         | 33/391 [00:00<00:05, 65.07it/s] 11%|█         | 43/391 [00:00<00:04, 74.64it/s] 14%|█▍        | 54/391 [00:00<00:04, 82.59it/s] 17%|█▋        | 65/391 [00:01<00:03, 88.20it/s] 19%|█▉        | 76/391 [00:01<00:03, 92.09it/s] 22%|██▏       | 86/391 [00:01<00:03, 93.78it/s] 25%|██▍       | 97/391 [00:01<00:03, 96.17it/s] 27%|██▋       | 107/391 [00:01<00:02, 97.09it/s] 30%|██▉       | 117/391 [00:01<00:02, 97.35it/s] 33%|███▎      | 128/391 [00:01<00:02, 98.47it/s] 36%|███▌      | 139/391 [00:01<00:02, 99.27it/s] 38%|███▊      | 150/391 [00:01<00:02, 99.84it/s] 41%|████      | 161/391 [00:01<00:02, 99.04it/s] 44%|████▍     | 172/391 [00:02<00:02, 99.49it/s] 47%|████▋     | 182/391 [00:02<00:02, 98.86it/s] 49%|████▉     | 193/391 [00:02<00:01, 99.68it/s] 52%|█████▏    | 204/391 [00:02<00:01, 100.05it/s] 55%|█████▍    | 215/391 [00:02<00:01, 100.51it/s] 58%|█████▊    | 226/391 [00:02<00:01, 100.65it/s] 61%|██████    | 237/391 [00:02<00:01, 100.59it/s] 63%|██████▎   | 248/391 [00:02<00:01, 100.79it/s] 66%|██████▌   | 259/391 [00:02<00:01, 100.53it/s] 69%|██████▉   | 270/391 [00:03<00:01, 100.39it/s] 72%|███████▏  | 281/391 [00:03<00:01, 98.41it/s]  75%|███████▍  | 292/391 [00:03<00:00, 99.15it/s] 77%|███████▋  | 303/391 [00:03<00:00, 99.72it/s] 80%|████████  | 314/391 [00:03<00:00, 100.01it/s] 83%|████████▎ | 325/391 [00:03<00:00, 99.82it/s]  86%|████████▌ | 336/391 [00:03<00:00, 100.16it/s] 89%|████████▊ | 347/391 [00:03<00:00, 100.31it/s] 92%|█████████▏| 358/391 [00:03<00:00, 100.39it/s] 94%|█████████▍| 369/391 [00:04<00:00, 100.66it/s] 97%|█████████▋| 380/391 [00:04<00:00, 101.16it/s]100%|██████████| 391/391 [00:04<00:00, 99.78it/s] 100%|██████████| 391/391 [00:04<00:00, 91.25it/s]
50000 images processed, 4.38004994392395 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:33,  2.35it/s] 14%|█▍        | 11/79 [00:00<00:02, 26.69it/s] 27%|██▋       | 21/79 [00:00<00:01, 45.62it/s] 39%|███▉      | 31/79 [00:00<00:00, 59.72it/s] 52%|█████▏    | 41/79 [00:00<00:00, 69.91it/s] 65%|██████▍   | 51/79 [00:00<00:00, 77.56it/s] 77%|███████▋  | 61/79 [00:01<00:00, 83.59it/s] 90%|████████▉ | 71/79 [00:01<00:00, 87.81it/s]100%|██████████| 79/79 [00:01<00:00, 63.92it/s]
10000 images processed, 1.2674126625061035 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-baseline-randlabel-b128-e25-lr0.001-wd1e-4-fl1-CIFAR-100forget15/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:36,  2.09it/s]  5%|▌         | 11/204 [00:00<00:07, 24.34it/s]  8%|▊         | 17/204 [00:01<00:11, 16.55it/s] 13%|█▎        | 26/204 [00:01<00:06, 27.38it/s] 17%|█▋        | 34/204 [00:01<00:04, 36.59it/s] 22%|██▏       | 44/204 [00:01<00:03, 48.45it/s] 26%|██▋       | 54/204 [00:01<00:02, 59.26it/s] 31%|███▏      | 64/204 [00:01<00:02, 67.30it/s] 36%|███▋      | 74/204 [00:01<00:01, 74.86it/s] 41%|████      | 84/204 [00:01<00:01, 81.36it/s] 47%|████▋     | 95/204 [00:01<00:01, 86.89it/s] 51%|█████▏    | 105/204 [00:02<00:01, 90.35it/s] 56%|█████▋    | 115/204 [00:02<00:00, 92.85it/s] 61%|██████▏   | 125/204 [00:02<00:00, 94.08it/s] 66%|██████▌   | 135/204 [00:02<00:00, 95.51it/s] 71%|███████   | 145/204 [00:02<00:00, 95.77it/s] 76%|███████▌  | 155/204 [00:02<00:00, 95.51it/s] 81%|████████  | 165/204 [00:02<00:00, 96.55it/s] 86%|████████▌ | 175/204 [00:02<00:00, 97.37it/s] 91%|█████████ | 186/204 [00:02<00:00, 98.89it/s] 97%|█████████▋| 197/204 [00:02<00:00, 99.82it/s]100%|██████████| 204/204 [00:03<00:00, 67.31it/s]
26032 images processed, 3.0721051692962646 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:46,  1.68it/s]  5%|▌         | 4/79 [00:00<00:11,  6.46it/s] 19%|█▉        | 15/79 [00:00<00:02, 26.61it/s] 27%|██▋       | 21/79 [00:01<00:01, 29.50it/s] 39%|███▉      | 31/79 [00:01<00:01, 44.46it/s] 48%|████▊     | 38/79 [00:01<00:00, 48.70it/s] 61%|██████    | 48/79 [00:01<00:00, 61.17it/s] 71%|███████   | 56/79 [00:01<00:00, 56.18it/s] 84%|████████▎ | 66/79 [00:01<00:00, 66.49it/s] 97%|█████████▋| 77/79 [00:01<00:00, 75.91it/s]100%|██████████| 79/79 [00:01<00:00, 45.52it/s]
10000 images processed, 1.7624998092651367 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:37,  2.10it/s] 13%|█▎        | 10/79 [00:00<00:03, 22.33it/s] 24%|██▍       | 19/79 [00:00<00:01, 39.00it/s] 37%|███▋      | 29/79 [00:00<00:00, 54.33it/s] 49%|████▉     | 39/79 [00:00<00:00, 66.16it/s] 62%|██████▏   | 49/79 [00:00<00:00, 74.79it/s] 76%|███████▌  | 60/79 [00:01<00:00, 82.68it/s] 90%|████████▉ | 71/79 [00:01<00:00, 88.30it/s]100%|██████████| 79/79 [00:01<00:00, 61.84it/s]
10000 images processed, 1.2987568378448486 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:32,  2.11it/s] 16%|█▌        | 11/70 [00:00<00:02, 24.62it/s] 30%|███       | 21/70 [00:00<00:01, 43.26it/s] 46%|████▌     | 32/70 [00:00<00:00, 59.19it/s] 60%|██████    | 42/70 [00:00<00:00, 69.98it/s] 76%|███████▌  | 53/70 [00:00<00:00, 79.19it/s] 91%|█████████▏| 64/70 [00:01<00:00, 85.75it/s]100%|██████████| 70/70 [00:01<00:00, 60.03it/s]
8925 images processed, 1.196352481842041 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:40,  1.09it/s] 11%|█         | 5/45 [00:01<00:06,  6.24it/s] 33%|███▎      | 15/45 [00:01<00:01, 21.09it/s] 47%|████▋     | 21/45 [00:01<00:01, 14.34it/s] 69%|██████▉   | 31/45 [00:01<00:00, 24.20it/s] 82%|████████▏ | 37/45 [00:02<00:00, 21.05it/s]100%|██████████| 45/45 [00:02<00:00, 19.33it/s]
5640 images processed, 2.3485801219940186 seconds used

16.889485359191895
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.35  98.93  96.86
places365     72.36  78.79  74.35
LSUN          32.65  93.29  93.11
iSUN          69.74  83.68  85.38
dtd           45.27  89.56  92.80
AVG           44.87  88.85  88.50
Retain-Acc: 0.7544
Forget-as-OOD (retain known vs forget novel):
  FPR: 49.27 AUROC: 90.21 AUIN: 98.03
8.52863883972168
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-baseline-randlabel-b128-e25-lr0.001-wd1e-4-fl1-CIFAR-100forget15_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-baseline-randlabel-b128-e25-lr0.001-wd1e-4-fl1-CIFAR-100forget15_rf.png
