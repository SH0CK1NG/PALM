nohup: ignoring input
==== Stage 1: inc={0,8,11,40,51}; seen={}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='0,8,11,40,51', forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:10<08:24, 10.30s/it]  4%|▍         | 2/50 [00:11<04:01,  5.04s/it]  6%|▌         | 3/50 [00:12<02:20,  3.00s/it]  8%|▊         | 4/50 [00:12<01:34,  2.06s/it] 10%|█         | 5/50 [00:13<01:09,  1.55s/it] 12%|█▏        | 6/50 [00:14<00:54,  1.23s/it] 14%|█▍        | 7/50 [00:14<00:43,  1.01s/it] 16%|█▌        | 8/50 [00:15<00:36,  1.14it/s] 18%|█▊        | 9/50 [00:15<00:34,  1.20it/s] 20%|██        | 10/50 [00:16<00:32,  1.24it/s] 22%|██▏       | 11/50 [00:17<00:29,  1.32it/s] 24%|██▍       | 12/50 [00:17<00:26,  1.43it/s] 26%|██▌       | 13/50 [00:18<00:24,  1.50it/s] 28%|██▊       | 14/50 [00:19<00:24,  1.49it/s] 30%|███       | 15/50 [00:19<00:22,  1.54it/s] 32%|███▏      | 16/50 [00:20<00:21,  1.59it/s] 34%|███▍      | 17/50 [00:21<00:21,  1.56it/s] 36%|███▌      | 18/50 [00:21<00:21,  1.47it/s] 38%|███▊      | 19/50 [00:22<00:21,  1.47it/s] 40%|████      | 20/50 [00:23<00:19,  1.52it/s] 42%|████▏     | 21/50 [00:23<00:19,  1.51it/s] 44%|████▍     | 22/50 [00:24<00:18,  1.52it/s] 46%|████▌     | 23/50 [00:25<00:17,  1.51it/s] 48%|████▊     | 24/50 [00:25<00:18,  1.42it/s] 50%|█████     | 25/50 [00:26<00:16,  1.48it/s] 52%|█████▏    | 26/50 [00:27<00:16,  1.42it/s] 54%|█████▍    | 27/50 [00:28<00:16,  1.38it/s] 56%|█████▌    | 28/50 [00:28<00:16,  1.36it/s] 58%|█████▊    | 29/50 [00:29<00:15,  1.40it/s] 60%|██████    | 30/50 [00:30<00:14,  1.41it/s][loss] ep 0 it 0 total=17.3683 mle=11.7426 pcon=5.6257 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 2 it 10 total=17.0159 mle=11.3822 pcon=5.6338 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 5 it 0 total=13.1338 mle=7.4960 pcon=5.6378 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 7 it 10 total=10.8175 mle=5.1958 pcon=5.6217 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 10 it 0 total=10.3560 mle=4.7560 pcon=5.6000 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 12 it 10 total=10.1757 mle=4.5980 pcon=5.5777 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 15 it 0 total=9.8509 mle=4.2952 pcon=5.5557 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 17 it 10 total=9.6579 mle=4.1240 pcon=5.5339 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 20 it 0 total=9.5005 mle=3.9882 pcon=5.5123 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 22 it 10 total=9.2993 mle=3.8085 pcon=5.4908 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 25 it 0 total=9.1661 mle=3.6970 pcon=5.4692 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 27 it 10 total=9.0701 mle=3.6226 pcon=5.4475 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
 62%|██████▏   | 31/50 [00:30<00:13,  1.39it/s] 64%|██████▍   | 32/50 [00:31<00:13,  1.36it/s] 66%|██████▌   | 33/50 [00:32<00:12,  1.37it/s] 68%|██████▊   | 34/50 [00:33<00:11,  1.39it/s] 70%|███████   | 35/50 [00:33<00:10,  1.41it/s] 72%|███████▏  | 36/50 [00:34<00:09,  1.42it/s] 74%|███████▍  | 37/50 [00:35<00:09,  1.32it/s] 76%|███████▌  | 38/50 [00:36<00:08,  1.35it/s] 78%|███████▊  | 39/50 [00:36<00:07,  1.40it/s] 80%|████████  | 40/50 [00:37<00:07,  1.34it/s] 82%|████████▏ | 41/50 [00:38<00:06,  1.35it/s] 84%|████████▍ | 42/50 [00:39<00:05,  1.37it/s] 86%|████████▌ | 43/50 [00:39<00:05,  1.40it/s] 88%|████████▊ | 44/50 [00:40<00:04,  1.37it/s] 90%|█████████ | 45/50 [00:41<00:03,  1.29it/s] 92%|█████████▏| 46/50 [00:41<00:02,  1.36it/s] 94%|█████████▍| 47/50 [00:42<00:02,  1.39it/s] 96%|█████████▌| 48/50 [00:43<00:01,  1.41it/s] 98%|█████████▊| 49/50 [00:43<00:00,  1.45it/s]100%|██████████| 50/50 [00:44<00:00,  1.45it/s]100%|██████████| 50/50 [00:44<00:00,  1.12it/s]
[loss] ep 30 it 0 total=8.9878 mle=3.5618 pcon=5.4260 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 32 it 10 total=8.9617 mle=3.5572 pcon=5.4045 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 35 it 0 total=8.8745 mle=3.4914 pcon=5.3832 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 37 it 10 total=8.8194 mle=3.4572 pcon=5.3622 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 40 it 0 total=8.7944 mle=3.4528 pcon=5.3416 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 42 it 10 total=8.7464 mle=3.4247 pcon=5.3216 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 45 it 0 total=8.7255 mle=3.4232 pcon=5.3023 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[loss] ep 47 it 10 total=8.6747 mle=3.3912 pcon=5.2835 forget=0.0000 orth=0.0000 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stage1-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<03:58,  1.80it/s]  3%|▎         | 11/430 [00:00<00:19, 21.63it/s]  5%|▍         | 20/430 [00:00<00:11, 36.85it/s]  7%|▋         | 30/430 [00:00<00:07, 51.55it/s]  9%|▉         | 40/430 [00:00<00:06, 63.02it/s] 12%|█▏        | 50/430 [00:01<00:05, 71.76it/s] 14%|█▍        | 60/430 [00:01<00:04, 78.58it/s] 16%|█▋        | 70/430 [00:01<00:04, 83.63it/s] 19%|█▊        | 80/430 [00:01<00:04, 86.77it/s] 21%|██        | 90/430 [00:01<00:03, 89.27it/s] 23%|██▎       | 100/430 [00:01<00:03, 90.89it/s] 26%|██▌       | 110/430 [00:01<00:03, 92.47it/s] 28%|██▊       | 120/430 [00:01<00:03, 91.44it/s] 30%|███       | 130/430 [00:01<00:03, 92.23it/s] 33%|███▎      | 140/430 [00:02<00:03, 91.68it/s] 35%|███▍      | 150/430 [00:02<00:03, 92.44it/s] 37%|███▋      | 160/430 [00:02<00:02, 93.64it/s] 40%|███▉      | 170/430 [00:02<00:02, 92.90it/s] 42%|████▏     | 180/430 [00:02<00:02, 92.56it/s] 44%|████▍     | 190/430 [00:02<00:02, 93.21it/s] 47%|████▋     | 200/430 [00:02<00:02, 93.99it/s] 49%|████▉     | 210/430 [00:02<00:02, 92.56it/s] 51%|█████     | 220/430 [00:02<00:02, 92.10it/s] 53%|█████▎    | 230/430 [00:03<00:02, 92.80it/s] 56%|█████▌    | 240/430 [00:03<00:02, 92.54it/s] 58%|█████▊    | 250/430 [00:03<00:01, 93.25it/s] 60%|██████    | 260/430 [00:03<00:01, 92.58it/s] 63%|██████▎   | 270/430 [00:03<00:01, 93.21it/s] 65%|██████▌   | 280/430 [00:03<00:01, 89.90it/s] 67%|██████▋   | 290/430 [00:03<00:01, 82.00it/s] 70%|██████▉   | 299/430 [00:03<00:01, 83.29it/s] 72%|███████▏  | 309/430 [00:03<00:01, 86.05it/s] 74%|███████▍  | 318/430 [00:04<00:01, 83.59it/s] 76%|███████▌  | 327/430 [00:04<00:01, 77.47it/s] 78%|███████▊  | 335/430 [00:04<00:01, 75.26it/s] 80%|███████▉  | 343/430 [00:04<00:01, 72.21it/s] 82%|████████▏ | 351/430 [00:04<00:01, 67.81it/s] 83%|████████▎ | 358/430 [00:04<00:01, 65.57it/s] 85%|████████▍ | 365/430 [00:04<00:00, 65.94it/s] 87%|████████▋ | 372/430 [00:04<00:00, 66.46it/s] 88%|████████▊ | 379/430 [00:04<00:00, 65.53it/s] 90%|█████████ | 388/430 [00:05<00:00, 70.36it/s] 92%|█████████▏| 396/430 [00:05<00:00, 65.86it/s] 94%|█████████▎| 403/430 [00:05<00:00, 65.87it/s] 95%|█████████▌| 410/430 [00:05<00:00, 63.24it/s] 97%|█████████▋| 417/430 [00:05<00:00, 62.62it/s] 99%|█████████▊| 424/430 [00:05<00:00, 61.15it/s]100%|██████████| 430/430 [00:05<00:00, 74.39it/s]
55000 images processed, 5.861139535903931 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<00:56,  1.51it/s] 10%|█         | 9/86 [00:00<00:05, 15.16it/s] 19%|█▊        | 16/86 [00:00<00:02, 25.79it/s] 27%|██▋       | 23/86 [00:00<00:01, 34.99it/s] 38%|███▊      | 33/86 [00:01<00:01, 49.16it/s] 48%|████▊     | 41/86 [00:01<00:00, 55.01it/s] 57%|█████▋    | 49/86 [00:01<00:00, 58.89it/s] 65%|██████▌   | 56/86 [00:01<00:00, 61.06it/s] 73%|███████▎  | 63/86 [00:01<00:00, 62.84it/s] 83%|████████▎ | 71/86 [00:01<00:00, 66.53it/s] 92%|█████████▏| 79/86 [00:01<00:00, 68.38it/s]100%|██████████| 86/86 [00:01<00:00, 46.63it/s]
11000 images processed, 1.858818769454956 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:21,  1.43it/s]  3%|▎         | 7/204 [00:00<00:17, 11.31it/s]  7%|▋         | 15/204 [00:00<00:07, 24.44it/s] 11%|█▏        | 23/204 [00:01<00:05, 36.12it/s] 15%|█▍        | 30/204 [00:01<00:04, 43.20it/s] 18%|█▊        | 37/204 [00:01<00:03, 46.54it/s] 22%|██▏       | 44/204 [00:01<00:03, 50.89it/s] 26%|██▋       | 54/204 [00:01<00:02, 62.13it/s] 30%|███       | 62/204 [00:01<00:02, 64.59it/s] 34%|███▍      | 70/204 [00:01<00:02, 62.43it/s] 38%|███▊      | 77/204 [00:01<00:02, 59.74it/s] 41%|████      | 84/204 [00:01<00:01, 60.15it/s] 45%|████▍     | 91/204 [00:02<00:01, 61.46it/s] 48%|████▊     | 98/204 [00:02<00:01, 60.49it/s] 51%|█████▏    | 105/204 [00:02<00:01, 61.34it/s] 55%|█████▌    | 113/204 [00:02<00:01, 64.58it/s] 60%|█████▉    | 122/204 [00:02<00:01, 71.27it/s] 64%|██████▍   | 131/204 [00:02<00:00, 74.93it/s] 68%|██████▊   | 139/204 [00:02<00:00, 74.14it/s] 72%|███████▏  | 147/204 [00:02<00:00, 74.04it/s] 76%|███████▌  | 155/204 [00:02<00:00, 70.25it/s] 80%|███████▉  | 163/204 [00:03<00:00, 71.72it/s] 84%|████████▍ | 171/204 [00:03<00:00, 67.25it/s] 87%|████████▋ | 178/204 [00:03<00:00, 65.10it/s] 91%|█████████ | 185/204 [00:03<00:00, 64.21it/s] 95%|█████████▍| 193/204 [00:03<00:00, 67.51it/s] 98%|█████████▊| 200/204 [00:03<00:00, 64.84it/s]100%|██████████| 204/204 [00:03<00:00, 54.61it/s]
26032 images processed, 3.804117202758789 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:06,  1.18it/s] 10%|█         | 8/79 [00:00<00:06, 11.04it/s] 19%|█▉        | 15/79 [00:01<00:03, 20.77it/s] 28%|██▊       | 22/79 [00:01<00:01, 29.92it/s] 37%|███▋      | 29/79 [00:01<00:01, 37.30it/s] 46%|████▌     | 36/79 [00:01<00:00, 44.26it/s] 54%|█████▍    | 43/79 [00:01<00:00, 50.05it/s] 63%|██████▎   | 50/79 [00:01<00:00, 54.98it/s] 76%|███████▌  | 60/79 [00:01<00:00, 65.62it/s] 87%|████████▋ | 69/79 [00:01<00:00, 71.67it/s] 97%|█████████▋| 77/79 [00:01<00:00, 70.87it/s]100%|██████████| 79/79 [00:02<00:00, 30.58it/s]
10000 images processed, 2.608039140701294 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:54,  1.43it/s] 10%|█         | 8/79 [00:00<00:05, 12.89it/s] 19%|█▉        | 15/79 [00:00<00:02, 22.88it/s] 29%|██▉       | 23/79 [00:01<00:01, 34.77it/s] 38%|███▊      | 30/79 [00:01<00:01, 42.45it/s] 47%|████▋     | 37/79 [00:01<00:00, 47.37it/s] 56%|█████▌    | 44/79 [00:01<00:00, 51.36it/s] 65%|██████▍   | 51/79 [00:01<00:00, 50.00it/s] 73%|███████▎  | 58/79 [00:01<00:00, 54.03it/s] 82%|████████▏ | 65/79 [00:01<00:00, 56.23it/s] 91%|█████████ | 72/79 [00:01<00:00, 59.05it/s]100%|██████████| 79/79 [00:01<00:00, 40.86it/s]
10000 images processed, 1.9547536373138428 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:46,  1.47it/s] 11%|█▏        | 8/70 [00:00<00:04, 13.12it/s] 21%|██▏       | 15/70 [00:00<00:02, 23.96it/s] 31%|███▏      | 22/70 [00:01<00:01, 32.85it/s] 41%|████▏     | 29/70 [00:01<00:01, 40.42it/s] 51%|█████▏    | 36/70 [00:01<00:00, 46.88it/s] 61%|██████▏   | 43/70 [00:01<00:00, 48.07it/s] 71%|███████▏  | 50/70 [00:01<00:00, 52.97it/s] 83%|████████▎ | 58/70 [00:01<00:00, 58.32it/s] 97%|█████████▋| 68/70 [00:01<00:00, 68.31it/s]100%|██████████| 70/70 [00:01<00:00, 40.75it/s]
8925 images processed, 1.7471442222595215 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<01:02,  1.42s/it] 18%|█▊        | 8/45 [00:01<00:05,  6.93it/s] 33%|███▎      | 15/45 [00:01<00:02, 13.80it/s] 44%|████▍     | 20/45 [00:02<00:01, 12.95it/s] 60%|██████    | 27/45 [00:02<00:00, 19.71it/s] 73%|███████▎  | 33/45 [00:02<00:00, 14.87it/s] 91%|█████████ | 41/45 [00:02<00:00, 21.76it/s]100%|██████████| 45/45 [00:02<00:00, 15.28it/s]
5640 images processed, 2.965902090072632 seconds used

22.666632175445557
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.54  99.04  73.65
places365     49.43  89.46  51.37
LSUN          25.65  95.16  76.03
iSUN          25.79  95.00  73.66
dtd           24.50  94.41  71.71
AVG           25.78  94.61  69.28
[incremental] Overall: 0.8500 New: 0.8500 Old: nan
[incremental] Final(Top-1): 0.5087  Average: 0.6627
4.851658344268799
==== Stage 2: inc={66,67,88,94,57}; seen={0,8,11,40,51}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1', lora_stack=False, lora_orth_enable=True, lora_orth_lambda=1.0, lora_orth_ref_paths='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1', forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='66,67,88,94,57', forget_classes_seen='0,8,11,40,51', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/shaokun/PALM/trainer.py:244: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
  2%|▏         | 1/50 [00:10<08:38, 10.58s/it]  4%|▍         | 2/50 [00:11<04:07,  5.16s/it]  6%|▌         | 3/50 [00:12<02:29,  3.18s/it]  8%|▊         | 4/50 [00:13<01:39,  2.17s/it] 10%|█         | 5/50 [00:14<01:12,  1.61s/it] 12%|█▏        | 6/50 [00:14<00:56,  1.28s/it] 14%|█▍        | 7/50 [00:15<00:46,  1.08s/it] 16%|█▌        | 8/50 [00:15<00:39,  1.07it/s] 18%|█▊        | 9/50 [00:16<00:35,  1.16it/s] 20%|██        | 10/50 [00:17<00:32,  1.24it/s] 22%|██▏       | 11/50 [00:18<00:30,  1.28it/s] 24%|██▍       | 12/50 [00:18<00:27,  1.36it/s] 26%|██▌       | 13/50 [00:19<00:25,  1.45it/s] 28%|██▊       | 14/50 [00:20<00:25,  1.41it/s] 30%|███       | 15/50 [00:20<00:25,  1.37it/s] 32%|███▏      | 16/50 [00:21<00:24,  1.36it/s] 34%|███▍      | 17/50 [00:22<00:24,  1.37it/s] 36%|███▌      | 18/50 [00:22<00:22,  1.42it/s] 38%|███▊      | 19/50 [00:23<00:20,  1.49it/s] 40%|████      | 20/50 [00:24<00:19,  1.50it/s] 42%|████▏     | 21/50 [00:24<00:19,  1.47it/s] 44%|████▍     | 22/50 [00:25<00:20,  1.39it/s] 46%|████▌     | 23/50 [00:26<00:18,  1.43it/s] 48%|████▊     | 24/50 [00:26<00:17,  1.46it/s] 50%|█████     | 25/50 [00:27<00:17,  1.42it/s] 52%|█████▏    | 26/50 [00:28<00:17,  1.36it/s] 54%|█████▍    | 27/50 [00:29<00:16,  1.38it/s] 56%|█████▌    | 28/50 [00:29<00:15,  1.39it/s] 58%|█████▊    | 29/50 [00:30<00:14,  1.41it/s] 60%|██████    | 30/50 [00:31<00:14,  1.42it/s][loss] ep 0 it 0 total=15.7028 mle=5.5830 pcon=8.9774 forget=0.0000 orth=1.1424 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 2 it 10 total=15.2759 mle=5.1693 pcon=8.9647 forget=0.0000 orth=1.1419 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 5 it 0 total=14.9035 mle=4.8137 pcon=8.9495 forget=0.0000 orth=1.1403 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 7 it 10 total=14.6232 mle=4.5536 pcon=8.9320 forget=0.0000 orth=1.1376 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 10 it 0 total=14.2909 mle=4.2442 pcon=8.9131 forget=0.0000 orth=1.1337 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 12 it 10 total=14.0620 mle=4.0398 pcon=8.8929 forget=0.0000 orth=1.1292 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 15 it 0 total=13.8663 mle=3.8697 pcon=8.8720 forget=0.0000 orth=1.1247 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 17 it 10 total=13.7550 mle=3.7842 pcon=8.8505 forget=0.0000 orth=1.1203 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 20 it 0 total=13.6025 mle=3.6578 pcon=8.8285 forget=0.0000 orth=1.1162 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 22 it 10 total=13.4919 mle=3.5734 pcon=8.8063 forget=0.0000 orth=1.1122 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 25 it 0 total=13.4430 mle=3.5505 pcon=8.7839 forget=0.0000 orth=1.1085 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 27 it 10 total=13.3805 mle=3.5136 pcon=8.7615 forget=0.0000 orth=1.1053 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
 62%|██████▏   | 31/50 [00:31<00:13,  1.44it/s] 64%|██████▍   | 32/50 [00:32<00:12,  1.48it/s] 66%|██████▌   | 33/50 [00:33<00:11,  1.49it/s] 68%|██████▊   | 34/50 [00:33<00:10,  1.49it/s] 70%|███████   | 35/50 [00:34<00:10,  1.47it/s] 72%|███████▏  | 36/50 [00:35<00:09,  1.51it/s] 74%|███████▍  | 37/50 [00:35<00:08,  1.53it/s] 76%|███████▌  | 38/50 [00:36<00:07,  1.56it/s] 78%|███████▊  | 39/50 [00:37<00:06,  1.58it/s] 80%|████████  | 40/50 [00:37<00:06,  1.49it/s] 82%|████████▏ | 41/50 [00:38<00:06,  1.48it/s] 84%|████████▍ | 42/50 [00:39<00:05,  1.47it/s] 86%|████████▌ | 43/50 [00:39<00:04,  1.48it/s] 88%|████████▊ | 44/50 [00:40<00:03,  1.51it/s] 90%|█████████ | 45/50 [00:41<00:03,  1.42it/s] 92%|█████████▏| 46/50 [00:42<00:02,  1.40it/s] 94%|█████████▍| 47/50 [00:42<00:02,  1.34it/s] 96%|█████████▌| 48/50 [00:43<00:01,  1.35it/s] 98%|█████████▊| 49/50 [00:44<00:00,  1.32it/s]100%|██████████| 50/50 [00:45<00:00,  1.31it/s]100%|██████████| 50/50 [00:45<00:00,  1.11it/s]
[loss] ep 30 it 0 total=13.3165 mle=3.4747 pcon=8.7393 forget=0.0000 orth=1.1025 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 32 it 10 total=13.2666 mle=3.4492 pcon=8.7172 forget=0.0000 orth=1.1002 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 35 it 0 total=13.2199 mle=3.4259 pcon=8.6956 forget=0.0000 orth=1.0984 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 37 it 10 total=13.1967 mle=3.4255 pcon=8.6743 forget=0.0000 orth=1.0970 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 40 it 0 total=13.1696 mle=3.4200 pcon=8.6536 forget=0.0000 orth=1.0960 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 42 it 10 total=13.1368 mle=3.4079 pcon=8.6336 forget=0.0000 orth=1.0954 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 45 it 0 total=13.1048 mle=3.3956 pcon=8.6142 forget=0.0000 orth=1.0950 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[loss] ep 47 it 10 total=13.0842 mle=3.3937 pcon=8.5956 forget=0.0000 orth=1.0949 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stage2-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<04:39,  1.54it/s]  2%|▏         | 9/430 [00:00<00:27, 15.40it/s]  4%|▍         | 17/430 [00:00<00:15, 27.38it/s]  6%|▌         | 24/430 [00:00<00:11, 36.56it/s]  7%|▋         | 32/430 [00:01<00:08, 45.83it/s]  9%|▉         | 40/430 [00:01<00:07, 53.36it/s] 11%|█         | 48/430 [00:01<00:06, 59.10it/s] 13%|█▎        | 56/430 [00:01<00:05, 63.00it/s] 15%|█▍        | 64/430 [00:01<00:05, 66.62it/s] 17%|█▋        | 72/430 [00:01<00:05, 65.98it/s] 19%|█▊        | 80/430 [00:01<00:05, 68.52it/s] 20%|██        | 88/430 [00:01<00:05, 68.29it/s] 22%|██▏       | 96/430 [00:01<00:04, 67.83it/s] 24%|██▍       | 104/430 [00:02<00:04, 69.52it/s] 26%|██▌       | 112/430 [00:02<00:04, 71.35it/s] 28%|██▊       | 120/430 [00:02<00:04, 72.29it/s] 30%|██▉       | 128/430 [00:02<00:04, 70.61it/s] 32%|███▏      | 136/430 [00:02<00:04, 70.49it/s] 34%|███▎      | 145/430 [00:02<00:03, 73.55it/s] 36%|███▌      | 153/430 [00:02<00:03, 75.16it/s] 37%|███▋      | 161/430 [00:02<00:03, 73.88it/s] 39%|███▉      | 169/430 [00:02<00:03, 72.93it/s] 41%|████      | 177/430 [00:03<00:03, 73.59it/s] 43%|████▎     | 185/430 [00:03<00:03, 71.55it/s] 45%|████▍     | 193/430 [00:03<00:03, 70.11it/s] 47%|████▋     | 201/430 [00:03<00:03, 67.49it/s] 49%|████▊     | 209/430 [00:03<00:03, 69.86it/s] 51%|█████     | 218/430 [00:03<00:02, 73.90it/s] 53%|█████▎    | 227/430 [00:03<00:02, 75.39it/s] 55%|█████▍    | 235/430 [00:03<00:02, 66.59it/s] 56%|█████▋    | 242/430 [00:04<00:02, 66.79it/s] 58%|█████▊    | 249/430 [00:04<00:02, 63.81it/s] 60%|█████▉    | 256/430 [00:04<00:02, 64.64it/s] 61%|██████    | 263/430 [00:04<00:02, 63.68it/s] 63%|██████▎   | 270/430 [00:04<00:02, 62.58it/s] 64%|██████▍   | 277/430 [00:04<00:02, 60.03it/s] 66%|██████▌   | 284/430 [00:04<00:02, 61.56it/s] 68%|██████▊   | 291/430 [00:04<00:02, 61.92it/s] 69%|██████▉   | 298/430 [00:04<00:02, 57.98it/s] 71%|███████   | 304/430 [00:05<00:02, 58.48it/s] 72%|███████▏  | 310/430 [00:05<00:02, 58.85it/s] 73%|███████▎  | 316/430 [00:05<00:01, 58.08it/s] 75%|███████▍  | 322/430 [00:05<00:01, 57.57it/s] 76%|███████▋  | 328/430 [00:05<00:01, 57.62it/s] 78%|███████▊  | 335/430 [00:05<00:01, 60.14it/s] 80%|███████▉  | 343/430 [00:05<00:01, 64.64it/s] 81%|████████▏ | 350/430 [00:05<00:01, 64.29it/s] 83%|████████▎ | 357/430 [00:05<00:01, 65.11it/s] 85%|████████▍ | 364/430 [00:06<00:01, 65.78it/s] 86%|████████▋ | 371/430 [00:06<00:00, 64.21it/s] 88%|████████▊ | 378/430 [00:06<00:00, 61.55it/s] 90%|████████▉ | 385/430 [00:06<00:00, 60.24it/s] 91%|█████████ | 392/430 [00:06<00:00, 58.68it/s] 93%|█████████▎| 398/430 [00:06<00:00, 57.62it/s] 94%|█████████▍| 406/430 [00:06<00:00, 62.52it/s] 96%|█████████▌| 413/430 [00:06<00:00, 63.44it/s] 98%|█████████▊| 420/430 [00:06<00:00, 62.44it/s] 99%|█████████▉| 427/430 [00:07<00:00, 63.40it/s]100%|██████████| 430/430 [00:07<00:00, 60.45it/s]
55000 images processed, 7.180910348892212 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<01:01,  1.39it/s]  9%|▉         | 8/86 [00:00<00:06, 12.49it/s] 15%|█▌        | 13/86 [00:00<00:03, 19.41it/s] 22%|██▏       | 19/86 [00:01<00:02, 27.85it/s] 29%|██▉       | 25/86 [00:01<00:01, 33.94it/s] 37%|███▋      | 32/86 [00:01<00:01, 41.30it/s] 45%|████▌     | 39/86 [00:01<00:00, 47.18it/s] 53%|█████▎    | 46/86 [00:01<00:00, 50.42it/s] 62%|██████▏   | 53/86 [00:01<00:00, 53.32it/s] 70%|██████▉   | 60/86 [00:01<00:00, 54.68it/s] 78%|███████▊  | 67/86 [00:01<00:00, 58.00it/s] 86%|████████▌ | 74/86 [00:01<00:00, 58.54it/s] 94%|█████████▍| 81/86 [00:02<00:00, 59.59it/s]100%|██████████| 86/86 [00:02<00:00, 39.94it/s]
11000 images processed, 2.170234203338623 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:20,  1.45it/s]  3%|▎         | 6/204 [00:00<00:20,  9.64it/s]  7%|▋         | 14/204 [00:00<00:08, 22.00it/s] 10%|█         | 21/204 [00:01<00:05, 31.51it/s] 13%|█▎        | 27/204 [00:01<00:04, 37.75it/s] 16%|█▌        | 33/204 [00:01<00:04, 39.84it/s] 20%|█▉        | 40/204 [00:01<00:03, 45.34it/s] 23%|██▎       | 47/204 [00:01<00:03, 50.67it/s] 26%|██▌       | 53/204 [00:01<00:02, 51.97it/s] 29%|██▉       | 59/204 [00:01<00:02, 53.68it/s] 32%|███▏      | 65/204 [00:01<00:02, 54.35it/s] 35%|███▍      | 71/204 [00:01<00:02, 55.77it/s] 38%|███▊      | 77/204 [00:02<00:02, 55.61it/s] 41%|████      | 83/204 [00:02<00:02, 55.68it/s] 44%|████▍     | 90/204 [00:02<00:01, 58.56it/s] 49%|████▊     | 99/204 [00:02<00:01, 65.22it/s] 52%|█████▏    | 107/204 [00:02<00:01, 65.90it/s] 56%|█████▌    | 114/204 [00:02<00:01, 62.88it/s] 59%|█████▉    | 121/204 [00:02<00:01, 60.63it/s] 63%|██████▎   | 128/204 [00:02<00:01, 61.01it/s] 66%|██████▌   | 135/204 [00:02<00:01, 61.06it/s] 70%|██████▉   | 142/204 [00:03<00:01, 60.16it/s] 73%|███████▎  | 149/204 [00:03<00:00, 61.13it/s] 76%|███████▋  | 156/204 [00:03<00:00, 61.34it/s] 80%|███████▉  | 163/204 [00:03<00:00, 60.55it/s] 83%|████████▎ | 170/204 [00:03<00:00, 58.36it/s] 87%|████████▋ | 177/204 [00:03<00:00, 61.15it/s] 91%|█████████ | 186/204 [00:03<00:00, 67.37it/s] 95%|█████████▍| 193/204 [00:03<00:00, 64.79it/s] 98%|█████████▊| 200/204 [00:03<00:00, 62.20it/s]100%|██████████| 204/204 [00:04<00:00, 50.31it/s]
26032 images processed, 4.109417676925659 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:55,  1.39it/s]  6%|▋         | 5/79 [00:00<00:09,  7.75it/s] 14%|█▍        | 11/79 [00:00<00:03, 17.65it/s] 22%|██▏       | 17/79 [00:01<00:02, 26.09it/s] 29%|██▉       | 23/79 [00:01<00:01, 33.38it/s] 37%|███▋      | 29/79 [00:01<00:01, 39.46it/s] 44%|████▍     | 35/79 [00:01<00:00, 44.56it/s] 52%|█████▏    | 41/79 [00:01<00:00, 48.44it/s] 61%|██████    | 48/79 [00:01<00:00, 54.12it/s] 70%|██████▉   | 55/79 [00:01<00:00, 58.01it/s] 78%|███████▊  | 62/79 [00:01<00:00, 60.49it/s] 87%|████████▋ | 69/79 [00:01<00:00, 61.79it/s] 96%|█████████▌| 76/79 [00:01<00:00, 58.71it/s]100%|██████████| 79/79 [00:02<00:00, 38.00it/s]
10000 images processed, 2.1049201488494873 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:49,  1.59it/s] 10%|█         | 8/79 [00:00<00:05, 14.07it/s] 18%|█▊        | 14/79 [00:00<00:02, 23.21it/s] 27%|██▋       | 21/79 [00:00<00:01, 33.21it/s] 35%|███▌      | 28/79 [00:01<00:01, 41.14it/s] 46%|████▌     | 36/79 [00:01<00:00, 49.48it/s] 58%|█████▊    | 46/79 [00:01<00:00, 61.35it/s] 70%|██████▉   | 55/79 [00:01<00:00, 67.86it/s] 80%|███████▉  | 63/79 [00:01<00:00, 68.41it/s] 90%|████████▉ | 71/79 [00:01<00:00, 68.56it/s]100%|██████████| 79/79 [00:01<00:00, 70.31it/s]100%|██████████| 79/79 [00:01<00:00, 45.82it/s]
10000 images processed, 1.7522954940795898 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:44,  1.55it/s] 11%|█▏        | 8/70 [00:00<00:04, 13.61it/s] 23%|██▎       | 16/70 [00:00<00:02, 26.12it/s] 33%|███▎      | 23/70 [00:00<00:01, 35.23it/s] 43%|████▎     | 30/70 [00:01<00:00, 42.93it/s] 53%|█████▎    | 37/70 [00:01<00:00, 48.80it/s] 63%|██████▎   | 44/70 [00:01<00:00, 54.11it/s] 77%|███████▋  | 54/70 [00:01<00:00, 65.11it/s] 91%|█████████▏| 64/70 [00:01<00:00, 71.53it/s]100%|██████████| 70/70 [00:01<00:00, 43.55it/s]
8925 images processed, 1.6415319442749023 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:52,  1.20s/it]  4%|▍         | 2/45 [00:01<00:24,  1.77it/s] 27%|██▋       | 12/45 [00:01<00:02, 14.39it/s] 38%|███▊      | 17/45 [00:01<00:02, 13.90it/s] 47%|████▋     | 21/45 [00:01<00:01, 16.99it/s] 67%|██████▋   | 30/45 [00:02<00:00, 22.44it/s] 76%|███████▌  | 34/45 [00:02<00:00, 16.32it/s] 91%|█████████ | 41/45 [00:02<00:00, 22.78it/s]100%|██████████| 45/45 [00:02<00:00, 15.94it/s]
5640 images processed, 2.8463656902313232 seconds used

23.79421067237854
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.78  98.86  78.81
places365     52.99  85.09  47.13
LSUN          44.45  89.40  62.30
iSUN          35.89  91.31  65.44
dtd           33.07  92.09  73.38
AVG           34.04  91.35  65.41
[incremental] Overall: 0.7390 New: 0.7180 Old: 0.7600
[incremental] Final(Top-1): 0.5087  Average: 0.6623
8.916260004043579
==== Stage 3: inc={59,58,44,93,10}; seen={0,8,11,40,51,66,67,88,94,57}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2', lora_stack=False, lora_orth_enable=True, lora_orth_lambda=1.0, lora_orth_ref_paths='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2', forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='59,58,44,93,10', forget_classes_seen='0,8,11,40,51,66,67,88,94,57', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/shaokun/PALM/trainer.py:244: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
  2%|▏         | 1/50 [00:08<06:44,  8.25s/it]  4%|▍         | 2/50 [00:09<03:05,  3.87s/it]  6%|▌         | 3/50 [00:09<01:56,  2.47s/it]  8%|▊         | 4/50 [00:10<01:22,  1.80s/it] 10%|█         | 5/50 [00:11<01:04,  1.43s/it] 12%|█▏        | 6/50 [00:12<00:53,  1.22s/it] 14%|█▍        | 7/50 [00:12<00:45,  1.06s/it] 16%|█▌        | 8/50 [00:13<00:39,  1.05it/s] 18%|█▊        | 9/50 [00:14<00:35,  1.15it/s] 20%|██        | 10/50 [00:15<00:33,  1.20it/s] 22%|██▏       | 11/50 [00:15<00:31,  1.22it/s] 24%|██▍       | 12/50 [00:16<00:31,  1.21it/s] 26%|██▌       | 13/50 [00:17<00:30,  1.22it/s] 28%|██▊       | 14/50 [00:18<00:29,  1.23it/s] 30%|███       | 15/50 [00:19<00:28,  1.24it/s] 32%|███▏      | 16/50 [00:20<00:28,  1.18it/s] 34%|███▍      | 17/50 [00:20<00:27,  1.18it/s] 36%|███▌      | 18/50 [00:21<00:26,  1.20it/s] 38%|███▊      | 19/50 [00:22<00:25,  1.22it/s] 40%|████      | 20/50 [00:23<00:24,  1.24it/s] 42%|████▏     | 21/50 [00:24<00:23,  1.25it/s] 44%|████▍     | 22/50 [00:24<00:23,  1.21it/s] 46%|████▌     | 23/50 [00:25<00:22,  1.18it/s] 48%|████▊     | 24/50 [00:26<00:21,  1.20it/s] 50%|█████     | 25/50 [00:27<00:19,  1.26it/s] 52%|█████▏    | 26/50 [00:27<00:18,  1.33it/s] 54%|█████▍    | 27/50 [00:28<00:16,  1.38it/s] 56%|█████▌    | 28/50 [00:29<00:15,  1.42it/s] 58%|█████▊    | 29/50 [00:30<00:15,  1.40it/s] 60%|██████    | 30/50 [00:30<00:14,  1.37it/s][loss] ep 0 it 0 total=16.2110 mle=5.4493 pcon=9.6888 forget=0.0000 orth=1.0729 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 2 it 10 total=15.8065 mle=5.0570 pcon=9.6767 forget=0.0000 orth=1.0728 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 5 it 0 total=15.3970 mle=4.6626 pcon=9.6618 forget=0.0000 orth=1.0726 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 7 it 10 total=15.0679 mle=4.3516 pcon=9.6443 forget=0.0000 orth=1.0720 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 10 it 0 total=14.7746 mle=4.0789 pcon=9.6246 forget=0.0000 orth=1.0710 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 12 it 10 total=14.5768 mle=3.9037 pcon=9.6036 forget=0.0000 orth=1.0696 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 15 it 0 total=14.4078 mle=3.7583 pcon=9.5814 forget=0.0000 orth=1.0681 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 17 it 10 total=14.3019 mle=3.6768 pcon=9.5585 forget=0.0000 orth=1.0665 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 20 it 0 total=14.1943 mle=3.5942 pcon=9.5351 forget=0.0000 orth=1.0650 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 22 it 10 total=14.0826 mle=3.5076 pcon=9.5115 forget=0.0000 orth=1.0636 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 25 it 0 total=14.0441 mle=3.4944 pcon=9.4875 forget=0.0000 orth=1.0622 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 27 it 10 total=13.9343 mle=3.4096 pcon=9.4638 forget=0.0000 orth=1.0609 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
 62%|██████▏   | 31/50 [00:31<00:13,  1.39it/s] 64%|██████▍   | 32/50 [00:32<00:12,  1.43it/s] 66%|██████▌   | 33/50 [00:32<00:12,  1.37it/s] 68%|██████▊   | 34/50 [00:33<00:11,  1.39it/s] 70%|███████   | 35/50 [00:34<00:11,  1.28it/s] 72%|███████▏  | 36/50 [00:35<00:10,  1.31it/s] 74%|███████▍  | 37/50 [00:36<00:10,  1.28it/s] 76%|███████▌  | 38/50 [00:37<00:09,  1.23it/s] 78%|███████▊  | 39/50 [00:37<00:08,  1.27it/s] 80%|████████  | 40/50 [00:38<00:07,  1.28it/s] 82%|████████▏ | 41/50 [00:39<00:06,  1.29it/s] 84%|████████▍ | 42/50 [00:40<00:06,  1.28it/s] 86%|████████▌ | 43/50 [00:40<00:05,  1.30it/s] 88%|████████▊ | 44/50 [00:41<00:04,  1.25it/s] 90%|█████████ | 45/50 [00:42<00:03,  1.28it/s] 92%|█████████▏| 46/50 [00:43<00:03,  1.30it/s] 94%|█████████▍| 47/50 [00:43<00:02,  1.31it/s] 96%|█████████▌| 48/50 [00:44<00:01,  1.25it/s] 98%|█████████▊| 49/50 [00:45<00:00,  1.22it/s]100%|██████████| 50/50 [00:46<00:00,  1.24it/s]100%|██████████| 50/50 [00:46<00:00,  1.08it/s]
[loss] ep 30 it 0 total=13.9296 mle=3.4297 pcon=9.4402 forget=0.0000 orth=1.0598 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 32 it 10 total=13.8887 mle=3.4129 pcon=9.4170 forget=0.0000 orth=1.0588 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 35 it 0 total=13.8444 mle=3.3921 pcon=9.3942 forget=0.0000 orth=1.0581 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 37 it 10 total=13.8065 mle=3.3770 pcon=9.3720 forget=0.0000 orth=1.0575 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 40 it 0 total=13.7827 mle=3.3750 pcon=9.3506 forget=0.0000 orth=1.0571 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 42 it 10 total=13.7754 mle=3.3887 pcon=9.3299 forget=0.0000 orth=1.0568 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 45 it 0 total=13.7303 mle=3.3638 pcon=9.3098 forget=0.0000 orth=1.0567 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[loss] ep 47 it 10 total=13.7121 mle=3.3649 pcon=9.2906 forget=0.0000 orth=1.0566 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stage3-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<04:32,  1.57it/s]  3%|▎         | 11/430 [00:00<00:21, 19.41it/s]  5%|▍         | 21/430 [00:00<00:11, 35.65it/s]  7%|▋         | 31/430 [00:00<00:08, 49.30it/s] 10%|▉         | 41/430 [00:01<00:06, 60.68it/s] 12%|█▏        | 51/430 [00:01<00:05, 69.54it/s] 14%|█▍        | 60/430 [00:01<00:04, 74.67it/s] 16%|█▋        | 70/430 [00:01<00:04, 79.53it/s] 18%|█▊        | 79/430 [00:01<00:04, 79.09it/s] 20%|██        | 88/430 [00:01<00:04, 74.83it/s] 23%|██▎       | 97/430 [00:01<00:04, 76.14it/s] 24%|██▍       | 105/430 [00:01<00:04, 73.15it/s] 26%|██▋       | 113/430 [00:02<00:04, 65.18it/s] 28%|██▊       | 121/430 [00:02<00:04, 68.51it/s] 30%|███       | 129/430 [00:02<00:04, 70.85it/s] 32%|███▏      | 137/430 [00:02<00:04, 72.01it/s] 34%|███▎      | 145/430 [00:02<00:03, 72.14it/s] 36%|███▌      | 153/430 [00:02<00:04, 67.69it/s] 37%|███▋      | 160/430 [00:02<00:04, 64.81it/s] 39%|███▉      | 168/430 [00:02<00:03, 67.16it/s] 41%|████      | 175/430 [00:02<00:03, 64.48it/s] 42%|████▏     | 182/430 [00:03<00:03, 63.63it/s] 44%|████▍     | 190/430 [00:03<00:03, 67.36it/s] 46%|████▋     | 199/430 [00:03<00:03, 71.02it/s] 48%|████▊     | 207/430 [00:03<00:03, 73.40it/s] 50%|█████     | 215/430 [00:03<00:02, 74.12it/s] 52%|█████▏    | 225/430 [00:03<00:02, 80.57it/s] 54%|█████▍    | 234/430 [00:03<00:02, 82.19it/s] 57%|█████▋    | 243/430 [00:03<00:02, 81.75it/s] 59%|█████▊    | 252/430 [00:03<00:02, 78.49it/s] 60%|██████    | 260/430 [00:04<00:02, 67.02it/s] 62%|██████▏   | 267/430 [00:04<00:02, 63.66it/s] 64%|██████▍   | 276/430 [00:04<00:02, 68.65it/s] 66%|██████▌   | 284/430 [00:04<00:02, 64.80it/s] 68%|██████▊   | 292/430 [00:04<00:02, 66.84it/s] 70%|██████▉   | 300/430 [00:04<00:01, 69.51it/s] 72%|███████▏  | 309/430 [00:04<00:01, 73.51it/s] 74%|███████▍  | 318/430 [00:04<00:01, 77.47it/s] 76%|███████▌  | 327/430 [00:04<00:01, 78.61it/s] 78%|███████▊  | 335/430 [00:05<00:01, 76.86it/s] 80%|████████  | 344/430 [00:05<00:01, 78.19it/s] 82%|████████▏ | 353/430 [00:05<00:00, 78.84it/s] 84%|████████▍ | 361/430 [00:05<00:00, 77.50it/s] 86%|████████▌ | 369/430 [00:05<00:00, 76.27it/s] 88%|████████▊ | 377/430 [00:05<00:00, 71.13it/s] 90%|████████▉ | 385/430 [00:05<00:00, 70.09it/s] 91%|█████████▏| 393/430 [00:05<00:00, 70.77it/s] 93%|█████████▎| 401/430 [00:06<00:00, 71.14it/s] 95%|█████████▌| 409/430 [00:06<00:00, 65.76it/s] 97%|█████████▋| 417/430 [00:06<00:00, 67.00it/s] 99%|█████████▉| 425/430 [00:06<00:00, 69.28it/s]100%|██████████| 430/430 [00:06<00:00, 66.19it/s]
55000 images processed, 6.543814420700073 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<01:06,  1.28it/s] 10%|█         | 9/86 [00:00<00:05, 13.36it/s] 20%|█▉        | 17/86 [00:00<00:02, 24.91it/s] 28%|██▊       | 24/86 [00:01<00:01, 33.66it/s] 37%|███▋      | 32/86 [00:01<00:01, 42.72it/s] 45%|████▌     | 39/86 [00:01<00:01, 46.82it/s] 53%|█████▎    | 46/86 [00:01<00:00, 52.07it/s] 62%|██████▏   | 53/86 [00:01<00:00, 55.97it/s] 70%|██████▉   | 60/86 [00:01<00:00, 58.56it/s] 78%|███████▊  | 67/86 [00:01<00:00, 60.65it/s] 87%|████████▋ | 75/86 [00:01<00:00, 64.97it/s] 97%|█████████▋| 83/86 [00:01<00:00, 66.98it/s]100%|██████████| 86/86 [00:02<00:00, 42.71it/s]
11000 images processed, 2.0284063816070557 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:18,  1.46it/s]  2%|▏         | 5/204 [00:00<00:24,  8.04it/s]  6%|▌         | 12/204 [00:00<00:09, 19.81it/s]  9%|▉         | 19/204 [00:01<00:06, 30.56it/s] 14%|█▎        | 28/204 [00:01<00:04, 43.28it/s] 17%|█▋        | 35/204 [00:01<00:03, 48.50it/s] 21%|██        | 42/204 [00:01<00:03, 51.36it/s] 25%|██▌       | 51/204 [00:01<00:02, 59.25it/s] 29%|██▉       | 59/204 [00:01<00:02, 63.29it/s] 32%|███▏      | 66/204 [00:01<00:02, 64.82it/s] 36%|███▋      | 74/204 [00:01<00:01, 66.97it/s] 41%|████      | 83/204 [00:01<00:01, 70.96it/s] 45%|████▍     | 91/204 [00:01<00:01, 72.22it/s] 49%|████▊     | 99/204 [00:02<00:01, 70.74it/s] 52%|█████▏    | 107/204 [00:02<00:01, 72.69it/s] 56%|█████▋    | 115/204 [00:02<00:01, 74.00it/s] 60%|██████    | 123/204 [00:02<00:01, 74.89it/s] 65%|██████▍   | 132/204 [00:02<00:00, 77.19it/s] 69%|██████▊   | 140/204 [00:02<00:00, 76.55it/s] 73%|███████▎  | 148/204 [00:02<00:00, 72.23it/s] 76%|███████▋  | 156/204 [00:02<00:00, 73.91it/s] 80%|████████  | 164/204 [00:02<00:00, 75.26it/s] 84%|████████▍ | 172/204 [00:03<00:00, 76.07it/s] 88%|████████▊ | 180/204 [00:03<00:00, 73.58it/s] 93%|█████████▎| 189/204 [00:03<00:00, 74.30it/s] 97%|█████████▋| 197/204 [00:03<00:00, 74.52it/s]100%|██████████| 204/204 [00:03<00:00, 58.16it/s]
26032 images processed, 3.5480895042419434 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:04,  1.21it/s] 11%|█▏        | 9/79 [00:00<00:05, 12.48it/s] 20%|██        | 16/79 [00:01<00:02, 22.17it/s] 29%|██▉       | 23/79 [00:01<00:01, 31.43it/s] 39%|███▉      | 31/79 [00:01<00:01, 41.30it/s] 48%|████▊     | 38/79 [00:01<00:00, 42.62it/s] 56%|█████▌    | 44/79 [00:01<00:00, 46.20it/s] 65%|██████▍   | 51/79 [00:01<00:00, 51.15it/s] 73%|███████▎  | 58/79 [00:01<00:00, 51.06it/s] 81%|████████  | 64/79 [00:01<00:00, 52.72it/s] 89%|████████▊ | 70/79 [00:01<00:00, 50.48it/s] 96%|█████████▌| 76/79 [00:02<00:00, 47.85it/s]100%|██████████| 79/79 [00:02<00:00, 36.04it/s]
10000 images processed, 2.2124593257904053 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:45,  1.73it/s] 10%|█         | 8/79 [00:00<00:04, 15.15it/s] 20%|██        | 16/79 [00:00<00:02, 29.22it/s] 30%|███       | 24/79 [00:00<00:01, 40.40it/s] 39%|███▉      | 31/79 [00:00<00:01, 47.59it/s] 49%|████▉     | 39/79 [00:01<00:00, 54.51it/s] 58%|█████▊    | 46/79 [00:01<00:00, 55.75it/s] 68%|██████▊   | 54/79 [00:01<00:00, 60.57it/s] 78%|███████▊  | 62/79 [00:01<00:00, 65.11it/s] 89%|████████▊ | 70/79 [00:01<00:00, 68.89it/s] 99%|█████████▊| 78/79 [00:01<00:00, 71.84it/s]100%|██████████| 79/79 [00:01<00:00, 47.92it/s]
10000 images processed, 1.6785955429077148 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:01<01:19,  1.15s/it]  3%|▎         | 2/70 [00:01<00:36,  1.85it/s] 13%|█▎        | 9/70 [00:01<00:05, 10.94it/s] 23%|██▎       | 16/70 [00:01<00:02, 20.38it/s] 34%|███▍      | 24/70 [00:01<00:01, 30.98it/s] 44%|████▍     | 31/70 [00:01<00:01, 38.51it/s] 54%|█████▍    | 38/70 [00:01<00:00, 44.34it/s] 64%|██████▍   | 45/70 [00:01<00:00, 47.57it/s] 73%|███████▎  | 51/70 [00:02<00:00, 49.11it/s] 81%|████████▏ | 57/70 [00:02<00:00, 49.96it/s] 91%|█████████▏| 64/70 [00:02<00:00, 53.13it/s]100%|██████████| 70/70 [00:02<00:00, 29.62it/s]
8925 images processed, 2.390259265899658 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:56,  1.29s/it]  4%|▍         | 2/45 [00:01<00:30,  1.40it/s] 20%|██        | 9/45 [00:01<00:04,  8.43it/s] 33%|███▎      | 15/45 [00:01<00:02, 14.70it/s] 42%|████▏     | 19/45 [00:02<00:02, 12.14it/s] 51%|█████     | 23/45 [00:02<00:01, 14.41it/s] 64%|██████▍   | 29/45 [00:02<00:00, 20.18it/s] 73%|███████▎  | 33/45 [00:02<00:00, 21.93it/s] 82%|████████▏ | 37/45 [00:02<00:00, 19.60it/s] 96%|█████████▌| 43/45 [00:03<00:00, 26.20it/s]100%|██████████| 45/45 [00:03<00:00, 14.50it/s]
5640 images processed, 3.1298482418060303 seconds used

23.244205713272095
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.85  98.76  81.76
places365     61.18  83.08  47.98
LSUN          45.00  89.41  68.68
iSUN          40.40  90.61  71.39
dtd           35.37  90.88  75.78
AVG           37.16  90.55  69.12
[incremental] Overall: 0.6973 New: 0.7000 Old: 0.6960
[incremental] Final(Top-1): 0.5087  Average: 0.6622
5.9995341300964355
==== Stage 4: inc={64,22,42,9,90}; seen={0,8,11,40,51,66,67,88,94,57,59,58,44,93,10}; all(union)={0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109} ====
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-110', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, pcon_inc='split', incremental=True, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4', adapter_load_path='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3', lora_stack=False, lora_orth_enable=True, lora_orth_lambda=1.0, lora_orth_ref_paths='checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage1,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage2,checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3', forget_classes='0,8,9,10,11,22,40,42,44,51,57,58,59,64,66,67,88,90,93,94,100,101,102,103,104,105,106,107,108,109', forget_list_path=None, forget_classes_inc='64,22,42,9,90', forget_classes_seen='0,8,11,40,51,66,67,88,94,57,59,58,44,93,10', retain_exclude_csv=None, forget_csv=None, forget_lambda=0.0, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='forget_only', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage3
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
[incremental] resumed from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
/home/shaokun/PALM/trainer.py:244: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
  2%|▏         | 1/50 [00:07<06:14,  7.65s/it]  4%|▍         | 2/50 [00:08<02:49,  3.52s/it]  6%|▌         | 3/50 [00:08<01:44,  2.22s/it]  8%|▊         | 4/50 [00:09<01:13,  1.59s/it] 10%|█         | 5/50 [00:10<00:56,  1.25s/it] 12%|█▏        | 6/50 [00:11<00:48,  1.10s/it] 14%|█▍        | 7/50 [00:11<00:41,  1.05it/s] 16%|█▌        | 8/50 [00:12<00:35,  1.18it/s] 18%|█▊        | 9/50 [00:12<00:31,  1.29it/s] 20%|██        | 10/50 [00:13<00:29,  1.33it/s] 22%|██▏       | 11/50 [00:14<00:27,  1.42it/s] 24%|██▍       | 12/50 [00:14<00:26,  1.44it/s] 26%|██▌       | 13/50 [00:15<00:24,  1.49it/s] 28%|██▊       | 14/50 [00:16<00:24,  1.49it/s] 30%|███       | 15/50 [00:16<00:22,  1.52it/s] 32%|███▏      | 16/50 [00:17<00:22,  1.53it/s] 34%|███▍      | 17/50 [00:18<00:21,  1.53it/s] 36%|███▌      | 18/50 [00:18<00:22,  1.44it/s] 38%|███▊      | 19/50 [00:19<00:22,  1.38it/s] 40%|████      | 20/50 [00:20<00:22,  1.34it/s] 42%|████▏     | 21/50 [00:21<00:22,  1.29it/s] 44%|████▍     | 22/50 [00:22<00:21,  1.28it/s] 46%|████▌     | 23/50 [00:22<00:21,  1.25it/s] 48%|████▊     | 24/50 [00:23<00:20,  1.29it/s] 50%|█████     | 25/50 [00:24<00:19,  1.32it/s] 52%|█████▏    | 26/50 [00:25<00:18,  1.32it/s] 54%|█████▍    | 27/50 [00:25<00:17,  1.32it/s] 56%|█████▌    | 28/50 [00:26<00:17,  1.27it/s] 58%|█████▊    | 29/50 [00:27<00:16,  1.29it/s] 60%|██████    | 30/50 [00:28<00:15,  1.29it/s][loss] ep 0 it 0 total=16.8143 mle=5.6769 pcon=10.0895 forget=0.0000 orth=1.0480 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 2 it 10 total=16.3479 mle=5.2199 pcon=10.0801 forget=0.0000 orth=1.0479 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 5 it 0 total=15.9256 mle=4.8103 pcon=10.0675 forget=0.0000 orth=1.0478 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 7 it 10 total=15.5495 mle=4.4500 pcon=10.0519 forget=0.0000 orth=1.0476 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 10 it 0 total=15.3082 mle=4.2270 pcon=10.0341 forget=0.0000 orth=1.0471 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 12 it 10 total=15.0420 mle=3.9811 pcon=10.0143 forget=0.0000 orth=1.0465 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 15 it 0 total=14.8516 mle=3.8124 pcon=9.9933 forget=0.0000 orth=1.0459 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 17 it 10 total=14.7075 mle=3.6911 pcon=9.9713 forget=0.0000 orth=1.0451 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 20 it 0 total=14.5769 mle=3.5840 pcon=9.9486 forget=0.0000 orth=1.0443 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 22 it 10 total=14.5171 mle=3.5481 pcon=9.9254 forget=0.0000 orth=1.0436 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 25 it 0 total=14.4491 mle=3.5043 pcon=9.9020 forget=0.0000 orth=1.0428 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 27 it 10 total=14.3733 mle=3.4528 pcon=9.8784 forget=0.0000 orth=1.0421 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
 62%|██████▏   | 31/50 [00:29<00:15,  1.25it/s] 64%|██████▍   | 32/50 [00:29<00:14,  1.24it/s] 66%|██████▌   | 33/50 [00:30<00:13,  1.27it/s] 68%|██████▊   | 34/50 [00:31<00:12,  1.29it/s] 70%|███████   | 35/50 [00:32<00:11,  1.27it/s] 72%|███████▏  | 36/50 [00:33<00:11,  1.25it/s] 74%|███████▍  | 37/50 [00:33<00:10,  1.28it/s] 76%|███████▌  | 38/50 [00:34<00:09,  1.28it/s] 78%|███████▊  | 39/50 [00:35<00:08,  1.28it/s] 80%|████████  | 40/50 [00:36<00:07,  1.25it/s] 82%|████████▏ | 41/50 [00:36<00:06,  1.30it/s] 84%|████████▍ | 42/50 [00:37<00:06,  1.32it/s] 86%|████████▌ | 43/50 [00:38<00:05,  1.29it/s] 88%|████████▊ | 44/50 [00:39<00:04,  1.30it/s] 90%|█████████ | 45/50 [00:40<00:03,  1.29it/s] 92%|█████████▏| 46/50 [00:40<00:03,  1.28it/s] 94%|█████████▍| 47/50 [00:41<00:02,  1.24it/s] 96%|█████████▌| 48/50 [00:42<00:01,  1.28it/s] 98%|█████████▊| 49/50 [00:43<00:00,  1.26it/s]100%|██████████| 50/50 [00:44<00:00,  1.23it/s]100%|██████████| 50/50 [00:44<00:00,  1.13it/s]
[loss] ep 30 it 0 total=14.3436 mle=3.4472 pcon=9.8550 forget=0.0000 orth=1.0415 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 32 it 10 total=14.3032 mle=3.4305 pcon=9.8318 forget=0.0000 orth=1.0409 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 35 it 0 total=14.2681 mle=3.4187 pcon=9.8089 forget=0.0000 orth=1.0405 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 37 it 10 total=14.2132 mle=3.3864 pcon=9.7866 forget=0.0000 orth=1.0402 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 40 it 0 total=14.2038 mle=3.3991 pcon=9.7649 forget=0.0000 orth=1.0399 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 42 it 10 total=14.1785 mle=3.3949 pcon=9.7438 forget=0.0000 orth=1.0398 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 45 it 0 total=14.1497 mle=3.3865 pcon=9.7236 forget=0.0000 orth=1.0397 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[loss] ep 47 it 10 total=14.1290 mle=3.3853 pcon=9.7041 forget=0.0000 orth=1.0396 favg=0.0000 nr=0 nf=128 protos=None fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] adapter saved to checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-upper-retain-c80.pt
[peft] adapter loaded from checkpoints/CIFAR-110-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stack/stage4
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmforget_only-fl0.2-lora_r8a32d0.05-temp0.08-ol1-continual-from-c80-to-CIFAR-110-pcon_split-stage4-seen-inc: Number of model parameters: 21843904
Processing in-distribution CIFAR-110 images
  0%|          | 0/430 [00:00<?, ?it/s]  0%|          | 1/430 [00:00<04:28,  1.60it/s]  2%|▏         | 10/430 [00:00<00:23, 18.01it/s]  5%|▍         | 20/430 [00:00<00:11, 34.54it/s]  7%|▋         | 30/430 [00:00<00:08, 48.93it/s]  9%|▉         | 40/430 [00:01<00:06, 60.43it/s] 12%|█▏        | 50/430 [00:01<00:05, 69.59it/s] 14%|█▍        | 60/430 [00:01<00:04, 76.66it/s] 16%|█▋        | 70/430 [00:01<00:04, 80.62it/s] 19%|█▊        | 80/430 [00:01<00:04, 82.93it/s] 21%|██        | 90/430 [00:01<00:03, 85.71it/s] 23%|██▎       | 100/430 [00:01<00:03, 88.20it/s] 26%|██▌       | 110/430 [00:01<00:03, 90.03it/s] 28%|██▊       | 120/430 [00:01<00:03, 90.88it/s] 30%|███       | 130/430 [00:02<00:03, 91.94it/s] 33%|███▎      | 140/430 [00:02<00:03, 92.26it/s] 35%|███▍      | 150/430 [00:02<00:03, 92.83it/s] 37%|███▋      | 160/430 [00:02<00:02, 91.87it/s] 40%|███▉      | 170/430 [00:02<00:02, 92.00it/s] 42%|████▏     | 180/430 [00:02<00:02, 92.92it/s] 44%|████▍     | 190/430 [00:02<00:02, 93.57it/s] 47%|████▋     | 200/430 [00:02<00:02, 92.63it/s] 49%|████▉     | 210/430 [00:02<00:02, 92.55it/s] 51%|█████     | 220/430 [00:02<00:02, 93.42it/s] 53%|█████▎    | 230/430 [00:03<00:02, 94.21it/s] 56%|█████▌    | 240/430 [00:03<00:02, 93.67it/s] 58%|█████▊    | 250/430 [00:03<00:01, 93.93it/s] 60%|██████    | 260/430 [00:03<00:01, 93.09it/s] 63%|██████▎   | 270/430 [00:03<00:01, 94.04it/s] 65%|██████▌   | 280/430 [00:03<00:01, 93.93it/s] 67%|██████▋   | 290/430 [00:03<00:01, 93.33it/s] 70%|██████▉   | 300/430 [00:03<00:01, 89.65it/s] 72%|███████▏  | 310/430 [00:03<00:01, 91.46it/s] 74%|███████▍  | 320/430 [00:04<00:01, 91.54it/s] 77%|███████▋  | 330/430 [00:04<00:01, 92.73it/s] 79%|███████▉  | 340/430 [00:04<00:00, 93.59it/s] 81%|████████▏ | 350/430 [00:04<00:00, 94.09it/s] 84%|████████▎ | 360/430 [00:04<00:00, 94.54it/s] 86%|████████▌ | 370/430 [00:04<00:00, 94.82it/s] 88%|████████▊ | 380/430 [00:04<00:00, 95.04it/s] 91%|█████████ | 390/430 [00:04<00:00, 95.02it/s] 93%|█████████▎| 400/430 [00:04<00:00, 94.94it/s] 95%|█████████▌| 410/430 [00:05<00:00, 95.72it/s] 98%|█████████▊| 420/430 [00:05<00:00, 95.44it/s]100%|██████████| 430/430 [00:05<00:00, 93.78it/s]100%|██████████| 430/430 [00:05<00:00, 82.36it/s]
55000 images processed, 5.285918474197388 seconds used

Processing in-distribution CIFAR-110 images
  0%|          | 0/86 [00:00<?, ?it/s]  1%|          | 1/86 [00:00<01:00,  1.40it/s] 13%|█▎        | 11/86 [00:00<00:04, 17.56it/s] 24%|██▍       | 21/86 [00:00<00:01, 32.77it/s] 35%|███▍      | 30/86 [00:01<00:01, 44.27it/s] 47%|████▋     | 40/86 [00:01<00:00, 56.37it/s] 58%|█████▊    | 50/86 [00:01<00:00, 66.19it/s] 70%|██████▉   | 60/86 [00:01<00:00, 73.86it/s] 81%|████████▏ | 70/86 [00:01<00:00, 80.06it/s] 93%|█████████▎| 80/86 [00:01<00:00, 84.66it/s]100%|██████████| 86/86 [00:01<00:00, 52.79it/s]
11000 images processed, 1.6465415954589844 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<02:29,  1.36it/s]  4%|▍         | 9/204 [00:00<00:13, 14.01it/s]  9%|▉         | 19/204 [00:00<00:06, 29.58it/s] 14%|█▎        | 28/204 [00:01<00:04, 42.10it/s] 18%|█▊        | 37/204 [00:01<00:03, 52.54it/s] 23%|██▎       | 47/204 [00:01<00:02, 62.94it/s] 28%|██▊       | 57/204 [00:01<00:02, 70.85it/s] 33%|███▎      | 67/204 [00:01<00:01, 76.89it/s] 38%|███▊      | 77/204 [00:01<00:01, 80.87it/s] 42%|████▏     | 86/204 [00:01<00:01, 83.09it/s] 47%|████▋     | 96/204 [00:01<00:01, 85.26it/s] 52%|█████▏    | 106/204 [00:01<00:01, 87.13it/s] 57%|█████▋    | 116/204 [00:02<00:00, 88.71it/s] 62%|██████▏   | 126/204 [00:02<00:00, 89.75it/s] 67%|██████▋   | 136/204 [00:02<00:00, 90.55it/s] 72%|███████▏  | 146/204 [00:02<00:00, 91.09it/s] 76%|███████▋  | 156/204 [00:02<00:00, 91.65it/s] 81%|████████▏ | 166/204 [00:02<00:00, 90.79it/s] 86%|████████▋ | 176/204 [00:02<00:00, 91.31it/s] 91%|█████████ | 186/204 [00:02<00:00, 92.76it/s] 96%|█████████▌| 196/204 [00:02<00:00, 93.19it/s]100%|██████████| 204/204 [00:02<00:00, 68.64it/s]
26032 images processed, 3.0414867401123047 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<01:14,  1.05it/s] 13%|█▎        | 10/79 [00:01<00:05, 12.54it/s] 24%|██▍       | 19/79 [00:01<00:02, 24.33it/s] 37%|███▋      | 29/79 [00:01<00:01, 37.58it/s] 49%|████▉     | 39/79 [00:01<00:00, 49.54it/s] 62%|██████▏   | 49/79 [00:01<00:00, 59.48it/s] 75%|███████▍  | 59/79 [00:01<00:00, 68.14it/s] 87%|████████▋ | 69/79 [00:01<00:00, 75.16it/s]100%|██████████| 79/79 [00:01<00:00, 77.94it/s]100%|██████████| 79/79 [00:01<00:00, 43.45it/s]
10000 images processed, 1.8620634078979492 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:55,  1.39it/s] 13%|█▎        | 10/79 [00:00<00:04, 16.07it/s] 25%|██▌       | 20/79 [00:00<00:01, 31.68it/s] 38%|███▊      | 30/79 [00:01<00:01, 45.50it/s] 51%|█████     | 40/79 [00:01<00:00, 57.34it/s] 63%|██████▎   | 50/79 [00:01<00:00, 66.06it/s] 76%|███████▌  | 60/79 [00:01<00:00, 73.99it/s] 89%|████████▊ | 70/79 [00:01<00:00, 79.83it/s]100%|██████████| 79/79 [00:01<00:00, 51.11it/s]
10000 images processed, 1.5739245414733887 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:47,  1.47it/s] 13%|█▎        | 9/70 [00:00<00:04, 14.82it/s] 26%|██▌       | 18/70 [00:00<00:01, 29.48it/s] 40%|████      | 28/70 [00:01<00:00, 43.86it/s] 54%|█████▍    | 38/70 [00:01<00:00, 56.21it/s] 69%|██████▊   | 48/70 [00:01<00:00, 66.40it/s] 83%|████████▎ | 58/70 [00:01<00:00, 74.40it/s] 97%|█████████▋| 68/70 [00:01<00:00, 80.45it/s]100%|██████████| 70/70 [00:01<00:00, 48.35it/s]
8925 images processed, 1.4794096946716309 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:51,  1.17s/it]  9%|▉         | 4/45 [00:01<00:10,  3.99it/s] 31%|███       | 14/45 [00:01<00:01, 16.60it/s] 44%|████▍     | 20/45 [00:01<00:01, 16.28it/s] 53%|█████▎    | 24/45 [00:01<00:01, 17.64it/s] 73%|███████▎  | 33/45 [00:02<00:00, 19.06it/s] 93%|█████████▎| 42/45 [00:02<00:00, 27.85it/s]100%|██████████| 45/45 [00:02<00:00, 17.70it/s]
5640 images processed, 2.5667216777801514 seconds used

19.308868885040283
eval.sh: line 47: syntax error near unexpected token `||'
