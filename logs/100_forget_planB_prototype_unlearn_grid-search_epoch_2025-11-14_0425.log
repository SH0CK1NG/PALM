nohup: ignoring input
[Full Grid] Searching over: lambdas(0.2) × lrs(0.001) × epochs(5 10 15 20 25 30 35 40 45 50) × lora_r(8)
[Run] lambda=0.2 lr=0.001 epochs=5 lora_r=8
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:23<01:32, 23.17s/it] 40%|████      | 2/5 [00:37<00:54, 18.14s/it] 60%|██████    | 3/5 [00:53<00:33, 16.95s/it] 80%|████████  | 4/5 [01:07<00:16, 16.05s/it]100%|██████████| 5/5 [01:23<00:00, 15.69s/it]100%|██████████| 5/5 [01:23<00:00, 16.61s/it]
[loss] ep 0 it 0 total=9.0402 mle=1.2805 pcon=5.2951 forget=2.4647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1041 mle=1.3921 pcon=5.2887 forget=2.4233 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.2412 mle=1.5078 pcon=5.2827 forget=2.4506 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2781 mle=1.5644 pcon=5.2770 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2053 mle=1.5331 pcon=5.2708 forget=2.4014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1118 mle=1.3806 pcon=5.2650 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.0054 mle=1.2922 pcon=5.2596 forget=2.4536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1732 mle=1.4380 pcon=5.2543 forget=2.4809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 1 it 10 total=9.0164 mle=1.3066 pcon=5.2487 forget=2.4611 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0373 mle=1.3162 pcon=5.2433 forget=2.4779 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.9651 mle=1.2902 pcon=5.2382 forget=2.4367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.9539 mle=1.2379 pcon=5.2331 forget=2.4829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0630 mle=1.3724 pcon=5.2283 forget=2.4623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9880 mle=1.3468 pcon=5.2235 forget=2.4177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9849 mle=1.3082 pcon=5.2185 forget=2.4583 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0553 mle=1.3766 pcon=5.2136 forget=2.4651 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 2 it 20 total=8.9289 mle=1.2745 pcon=5.2089 forget=2.4455 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0970 mle=1.3989 pcon=5.2044 forget=2.4937 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.9871 mle=1.3588 pcon=5.1998 forget=2.4284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9913 mle=1.3581 pcon=5.1954 forget=2.4378 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.9824 mle=1.3300 pcon=5.1909 forget=2.4615 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0215 mle=1.3659 pcon=5.1866 forget=2.4690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.8723 mle=1.2607 pcon=5.1824 forget=2.4292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.0075 mle=1.3836 pcon=5.1787 forget=2.4453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 3 it 30 total=8.9979 mle=1.3638 pcon=5.1749 forget=2.4592 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9245 mle=1.2957 pcon=5.1710 forget=2.4578 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.8987 mle=1.2903 pcon=5.1673 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.0415 mle=1.4600 pcon=5.1635 forget=2.4180 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9800 mle=1.3524 pcon=5.1601 forget=2.4674 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0521 mle=1.4267 pcon=5.1568 forget=2.4686 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0461 mle=1.4481 pcon=5.1531 forget=2.4448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9153 mle=1.3140 pcon=5.1498 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 4 it 40 total=8.9572 mle=1.3770 pcon=5.1465 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9559 mle=1.3335 pcon=5.1435 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.1339 mle=1.5853 pcon=5.1407 forget=2.4079 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=8.9932 mle=1.3938 pcon=5.1375 forget=2.4619 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0241 mle=1.4142 pcon=5.1344 forget=2.4754 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=8.7696 mle=1.1862 pcon=5.1316 forget=2.4518 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.0391 mle=1.4612 pcon=5.1291 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[Run] lambda=0.2 lr=0.001 epochs=10 lora_r=8
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:18<02:43, 18.18s/it] 20%|██        | 2/10 [00:30<01:59, 14.93s/it] 30%|███       | 3/10 [00:42<01:32, 13.25s/it] 40%|████      | 4/10 [00:52<01:12, 12.05s/it] 50%|█████     | 5/10 [01:02<00:57, 11.43s/it] 60%|██████    | 6/10 [01:13<00:44, 11.15s/it][loss] ep 0 it 0 total=9.0402 mle=1.2805 pcon=5.2951 forget=2.4647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1040 mle=1.3921 pcon=5.2887 forget=2.4232 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.2412 mle=1.5078 pcon=5.2827 forget=2.4507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2781 mle=1.5644 pcon=5.2770 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2054 mle=1.5331 pcon=5.2708 forget=2.4014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1116 mle=1.3805 pcon=5.2650 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.0054 mle=1.2922 pcon=5.2596 forget=2.4536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1732 mle=1.4380 pcon=5.2543 forget=2.4809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 1 it 10 total=9.0164 mle=1.3067 pcon=5.2487 forget=2.4610 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0373 mle=1.3162 pcon=5.2433 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.9651 mle=1.2903 pcon=5.2382 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.9540 mle=1.2380 pcon=5.2331 forget=2.4829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0631 mle=1.3724 pcon=5.2283 forget=2.4623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9880 mle=1.3468 pcon=5.2235 forget=2.4177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9850 mle=1.3082 pcon=5.2185 forget=2.4583 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0553 mle=1.3766 pcon=5.2136 forget=2.4651 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 2 it 20 total=8.9289 mle=1.2745 pcon=5.2089 forget=2.4455 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0969 mle=1.3988 pcon=5.2044 forget=2.4937 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.9870 mle=1.3589 pcon=5.1998 forget=2.4284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9914 mle=1.3581 pcon=5.1954 forget=2.4379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.9822 mle=1.3299 pcon=5.1909 forget=2.4615 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0214 mle=1.3659 pcon=5.1865 forget=2.4690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.8723 mle=1.2607 pcon=5.1824 forget=2.4291 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.0074 mle=1.3834 pcon=5.1787 forget=2.4453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 3 it 30 total=8.9979 mle=1.3638 pcon=5.1749 forget=2.4592 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9245 mle=1.2957 pcon=5.1710 forget=2.4579 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.8989 mle=1.2905 pcon=5.1673 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.0416 mle=1.4601 pcon=5.1635 forget=2.4180 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9798 mle=1.3522 pcon=5.1601 forget=2.4674 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0521 mle=1.4267 pcon=5.1568 forget=2.4686 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0461 mle=1.4482 pcon=5.1531 forget=2.4448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9152 mle=1.3140 pcon=5.1498 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 4 it 40 total=8.9573 mle=1.3770 pcon=5.1465 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9559 mle=1.3335 pcon=5.1435 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.1340 mle=1.5855 pcon=5.1407 forget=2.4079 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=8.9932 mle=1.3937 pcon=5.1375 forget=2.4620 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0241 mle=1.4143 pcon=5.1344 forget=2.4754 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=8.7698 mle=1.1863 pcon=5.1317 forget=2.4519 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.0391 mle=1.4611 pcon=5.1291 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 5 it 0 total=8.8317 mle=1.2047 pcon=5.1261 forget=2.5010 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=8.9576 mle=1.3650 pcon=5.1235 forget=2.4692 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.0086 mle=1.4006 pcon=5.1207 forget=2.4873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.0680 mle=1.4807 pcon=5.1179 forget=2.4694 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.0086 mle=1.4523 pcon=5.1153 forget=2.4410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=8.9115 mle=1.3486 pcon=5.1125 forget=2.4504 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=8.9731 mle=1.4294 pcon=5.1099 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=8.9551 mle=1.4046 pcon=5.1073 forget=2.4432 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 6 it 10 total=8.8857 mle=1.2784 pcon=5.1049 forget=2.5024 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=8.9183 mle=1.3787 pcon=5.1025 forget=2.4371 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=8.8930 mle=1.3183 pcon=5.1003 forget=2.4744 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.8890 mle=1.3320 pcon=5.0980 forget=2.4590 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9234 mle=1.3654 pcon=5.0956 forget=2.4624 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=8.9543 mle=1.3830 pcon=5.0934 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 70%|███████   | 7/10 [01:23<00:32, 10.88s/it] 80%|████████  | 8/10 [01:33<00:21, 10.71s/it] 90%|█████████ | 9/10 [01:46<00:11, 11.17s/it]100%|██████████| 10/10 [01:58<00:00, 11.57s/it]100%|██████████| 10/10 [01:58<00:00, 11.86s/it]
[loss] ep 6 it 310 total=8.8670 mle=1.3537 pcon=5.0915 forget=2.4219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 360 total=8.9529 mle=1.4569 pcon=5.0893 forget=2.4067 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 7 it 20 total=8.7926 mle=1.2500 pcon=5.0872 forget=2.4554 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=8.7894 mle=1.2464 pcon=5.0849 forget=2.4580 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.9120 mle=1.3875 pcon=5.0829 forget=2.4417 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.9197 mle=1.4329 pcon=5.0810 forget=2.4058 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=8.8356 mle=1.3211 pcon=5.0794 forget=2.4351 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=8.9746 mle=1.4542 pcon=5.0777 forget=2.4427 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=8.8528 mle=1.3167 pcon=5.0755 forget=2.4605 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=8.9364 mle=1.3709 pcon=5.0736 forget=2.4920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 8 it 30 total=8.9611 mle=1.4805 pcon=5.0719 forget=2.4087 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=8.8051 mle=1.2745 pcon=5.0702 forget=2.4605 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=8.9214 mle=1.3984 pcon=5.0685 forget=2.4545 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=8.8647 mle=1.3447 pcon=5.0668 forget=2.4533 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=8.8877 mle=1.3662 pcon=5.0652 forget=2.4563 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.7838 mle=1.2569 pcon=5.0636 forget=2.4633 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.0219 mle=1.5796 pcon=5.0618 forget=2.3805 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.8763 mle=1.3644 pcon=5.0604 forget=2.4514 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 9 it 40 total=8.9253 mle=1.4107 pcon=5.0589 forget=2.4558 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=8.9708 mle=1.4928 pcon=5.0575 forget=2.4205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=8.8783 mle=1.3538 pcon=5.0563 forget=2.4682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.7493 mle=1.2282 pcon=5.0550 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=8.9737 mle=1.5026 pcon=5.0540 forget=2.4172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=8.8661 mle=1.3565 pcon=5.0526 forget=2.4571 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.8157 mle=1.3066 pcon=5.0514 forget=2.4577 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[Run] lambda=0.2 lr=0.001 epochs=15 lora_r=8
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=15, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/15 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  7%|▋         | 1/15 [00:18<04:19, 18.56s/it] 13%|█▎        | 2/15 [00:31<03:14, 14.96s/it] 20%|██        | 3/15 [00:42<02:42, 13.52s/it] 27%|██▋       | 4/15 [00:54<02:21, 12.83s/it] 33%|███▎      | 5/15 [01:05<02:01, 12.20s/it] 40%|████      | 6/15 [01:17<01:49, 12.14s/it][loss] ep 0 it 0 total=9.0402 mle=1.2805 pcon=5.2951 forget=2.4647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1040 mle=1.3921 pcon=5.2887 forget=2.4232 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.2412 mle=1.5078 pcon=5.2827 forget=2.4507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2781 mle=1.5644 pcon=5.2770 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2053 mle=1.5331 pcon=5.2708 forget=2.4014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1116 mle=1.3804 pcon=5.2650 forget=2.4662 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.0054 mle=1.2922 pcon=5.2596 forget=2.4536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1731 mle=1.4380 pcon=5.2543 forget=2.4809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 1 it 10 total=9.0164 mle=1.3067 pcon=5.2487 forget=2.4611 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0373 mle=1.3162 pcon=5.2433 forget=2.4779 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.9651 mle=1.2902 pcon=5.2382 forget=2.4367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.9540 mle=1.2380 pcon=5.2331 forget=2.4830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0630 mle=1.3723 pcon=5.2283 forget=2.4623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9881 mle=1.3469 pcon=5.2235 forget=2.4177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9850 mle=1.3083 pcon=5.2185 forget=2.4582 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0555 mle=1.3768 pcon=5.2136 forget=2.4651 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 2 it 20 total=8.9289 mle=1.2745 pcon=5.2089 forget=2.4455 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0969 mle=1.3989 pcon=5.2044 forget=2.4936 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.9870 mle=1.3588 pcon=5.1998 forget=2.4284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9914 mle=1.3581 pcon=5.1954 forget=2.4379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.9822 mle=1.3298 pcon=5.1909 forget=2.4615 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0215 mle=1.3659 pcon=5.1866 forget=2.4690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.8722 mle=1.2606 pcon=5.1824 forget=2.4292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.0075 mle=1.3835 pcon=5.1787 forget=2.4453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 3 it 30 total=8.9978 mle=1.3638 pcon=5.1749 forget=2.4592 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9246 mle=1.2957 pcon=5.1710 forget=2.4579 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.8988 mle=1.2903 pcon=5.1673 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.0415 mle=1.4600 pcon=5.1635 forget=2.4180 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9798 mle=1.3523 pcon=5.1601 forget=2.4674 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0521 mle=1.4267 pcon=5.1568 forget=2.4686 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0461 mle=1.4482 pcon=5.1531 forget=2.4448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9153 mle=1.3140 pcon=5.1498 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 4 it 40 total=8.9573 mle=1.3771 pcon=5.1465 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9560 mle=1.3336 pcon=5.1435 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.1339 mle=1.5853 pcon=5.1406 forget=2.4080 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=8.9933 mle=1.3938 pcon=5.1375 forget=2.4620 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0240 mle=1.4142 pcon=5.1344 forget=2.4754 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=8.7697 mle=1.1863 pcon=5.1317 forget=2.4518 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.0391 mle=1.4612 pcon=5.1291 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 5 it 0 total=8.8318 mle=1.2046 pcon=5.1261 forget=2.5010 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=8.9579 mle=1.3651 pcon=5.1235 forget=2.4693 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.0086 mle=1.4006 pcon=5.1207 forget=2.4873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.0682 mle=1.4809 pcon=5.1179 forget=2.4694 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.0087 mle=1.4524 pcon=5.1152 forget=2.4410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=8.9115 mle=1.3486 pcon=5.1125 forget=2.4504 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=8.9731 mle=1.4295 pcon=5.1099 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=8.9550 mle=1.4045 pcon=5.1073 forget=2.4432 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 6 it 10 total=8.8858 mle=1.2785 pcon=5.1049 forget=2.5024 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=8.9182 mle=1.3786 pcon=5.1025 forget=2.4371 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=8.8930 mle=1.3183 pcon=5.1003 forget=2.4744 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.8891 mle=1.3320 pcon=5.0981 forget=2.4590 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9234 mle=1.3654 pcon=5.0956 forget=2.4624 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=8.9544 mle=1.3832 pcon=5.0935 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 47%|████▋     | 7/15 [01:28<01:33, 11.74s/it] 53%|█████▎    | 8/15 [01:40<01:21, 11.65s/it] 60%|██████    | 9/15 [01:51<01:08, 11.49s/it] 67%|██████▋   | 10/15 [02:01<00:55, 11.12s/it] 73%|███████▎  | 11/15 [02:12<00:44, 11.21s/it] 80%|████████  | 12/15 [02:24<00:33, 11.18s/it] 87%|████████▋ | 13/15 [02:35<00:22, 11.32s/it][loss] ep 6 it 310 total=8.8669 mle=1.3536 pcon=5.0915 forget=2.4219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 360 total=8.9529 mle=1.4570 pcon=5.0893 forget=2.4066 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 7 it 20 total=8.7927 mle=1.2500 pcon=5.0872 forget=2.4554 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=8.7893 mle=1.2464 pcon=5.0849 forget=2.4580 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.9121 mle=1.3874 pcon=5.0829 forget=2.4417 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.9197 mle=1.4331 pcon=5.0810 forget=2.4057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=8.8355 mle=1.3210 pcon=5.0794 forget=2.4351 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=8.9746 mle=1.4541 pcon=5.0777 forget=2.4427 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=8.8529 mle=1.3168 pcon=5.0756 forget=2.4605 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=8.9364 mle=1.3708 pcon=5.0736 forget=2.4920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 8 it 30 total=8.9610 mle=1.4804 pcon=5.0719 forget=2.4087 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=8.8052 mle=1.2745 pcon=5.0702 forget=2.4604 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=8.9212 mle=1.3981 pcon=5.0685 forget=2.4545 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=8.8647 mle=1.3447 pcon=5.0668 forget=2.4532 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=8.8877 mle=1.3662 pcon=5.0652 forget=2.4564 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.7838 mle=1.2569 pcon=5.0636 forget=2.4633 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.0221 mle=1.5798 pcon=5.0618 forget=2.3806 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.8763 mle=1.3644 pcon=5.0604 forget=2.4514 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 9 it 40 total=8.9253 mle=1.4106 pcon=5.0589 forget=2.4558 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=8.9710 mle=1.4930 pcon=5.0575 forget=2.4206 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=8.8783 mle=1.3537 pcon=5.0563 forget=2.4682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.7493 mle=1.2282 pcon=5.0551 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=8.9736 mle=1.5024 pcon=5.0540 forget=2.4171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=8.8660 mle=1.3563 pcon=5.0526 forget=2.4571 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.8156 mle=1.3065 pcon=5.0514 forget=2.4577 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 10 it 0 total=8.7872 mle=1.2679 pcon=5.0497 forget=2.4695 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.7535 mle=1.2563 pcon=5.0488 forget=2.4483 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=8.8059 mle=1.3265 pcon=5.0475 forget=2.4318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=8.8732 mle=1.3438 pcon=5.0463 forget=2.4831 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=8.9086 mle=1.4082 pcon=5.0453 forget=2.4551 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=8.9166 mle=1.4251 pcon=5.0443 forget=2.4472 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=8.9386 mle=1.5124 pcon=5.0431 forget=2.3831 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.7568 mle=1.2608 pcon=5.0423 forget=2.4538 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 11 it 10 total=8.9133 mle=1.4403 pcon=5.0411 forget=2.4319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.8455 mle=1.3728 pcon=5.0402 forget=2.4325 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.8334 mle=1.3449 pcon=5.0394 forget=2.4492 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.7808 mle=1.2732 pcon=5.0383 forget=2.4693 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.7531 mle=1.2651 pcon=5.0374 forget=2.4505 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.7375 mle=1.2666 pcon=5.0365 forget=2.4344 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.8674 mle=1.3905 pcon=5.0357 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.8313 mle=1.3185 pcon=5.0350 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 12 it 20 total=8.8033 mle=1.3132 pcon=5.0338 forget=2.4563 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.8429 mle=1.3552 pcon=5.0328 forget=2.4549 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.7970 mle=1.3616 pcon=5.0318 forget=2.4036 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.8376 mle=1.3385 pcon=5.0308 forget=2.4683 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.7332 mle=1.2241 pcon=5.0302 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.7535 mle=1.2957 pcon=5.0297 forget=2.4281 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.8912 mle=1.4130 pcon=5.0291 forget=2.4492 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.8538 mle=1.3795 pcon=5.0284 forget=2.4459 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 13 it 30 total=8.8179 mle=1.3138 pcon=5.0279 forget=2.4762 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.9416 mle=1.5019 pcon=5.0273 forget=2.4124 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.8190 mle=1.3533 pcon=5.0264 forget=2.4393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 93%|█████████▎| 14/15 [02:47<00:11, 11.61s/it]100%|██████████| 15/15 [02:59<00:00, 11.55s/it]100%|██████████| 15/15 [02:59<00:00, 11.96s/it]
[loss] ep 13 it 180 total=8.8646 mle=1.3562 pcon=5.0253 forget=2.4831 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 230 total=8.7541 mle=1.3015 pcon=5.0247 forget=2.4278 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.7473 mle=1.2232 pcon=5.0240 forget=2.5001 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.8543 mle=1.3968 pcon=5.0235 forget=2.4341 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.7803 mle=1.3292 pcon=5.0229 forget=2.4282 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 14 it 40 total=8.7365 mle=1.2764 pcon=5.0222 forget=2.4379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.8081 mle=1.3278 pcon=5.0217 forget=2.4586 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.7974 mle=1.3044 pcon=5.0212 forget=2.4718 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.7479 mle=1.2654 pcon=5.0207 forget=2.4618 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.8136 mle=1.3327 pcon=5.0198 forget=2.4611 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.7304 mle=1.2975 pcon=5.0193 forget=2.4136 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.7027 mle=1.2496 pcon=5.0189 forget=2.4342 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e15-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[Run] lambda=0.2 lr=0.001 epochs=20 lora_r=8
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:17<05:23, 17.04s/it] 10%|█         | 2/20 [00:25<03:37, 12.10s/it] 15%|█▌        | 3/20 [00:35<03:05, 10.90s/it] 20%|██        | 4/20 [00:45<02:52, 10.78s/it] 25%|██▌       | 5/20 [00:56<02:43, 10.91s/it] 30%|███       | 6/20 [01:07<02:31, 10.80s/it][loss] ep 0 it 0 total=9.0402 mle=1.2805 pcon=5.2951 forget=2.4647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1040 mle=1.3921 pcon=5.2887 forget=2.4232 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.2412 mle=1.5078 pcon=5.2827 forget=2.4507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2781 mle=1.5644 pcon=5.2770 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2053 mle=1.5331 pcon=5.2708 forget=2.4014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1116 mle=1.3804 pcon=5.2650 forget=2.4662 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.0054 mle=1.2922 pcon=5.2596 forget=2.4536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1731 mle=1.4380 pcon=5.2543 forget=2.4809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 1 it 10 total=9.0164 mle=1.3067 pcon=5.2487 forget=2.4611 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0373 mle=1.3162 pcon=5.2433 forget=2.4779 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.9651 mle=1.2902 pcon=5.2382 forget=2.4367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.9540 mle=1.2380 pcon=5.2331 forget=2.4830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0630 mle=1.3723 pcon=5.2283 forget=2.4623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9881 mle=1.3469 pcon=5.2235 forget=2.4177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9850 mle=1.3083 pcon=5.2185 forget=2.4582 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0555 mle=1.3768 pcon=5.2136 forget=2.4651 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 2 it 20 total=8.9289 mle=1.2745 pcon=5.2089 forget=2.4455 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0969 mle=1.3989 pcon=5.2044 forget=2.4936 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.9870 mle=1.3588 pcon=5.1998 forget=2.4284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9914 mle=1.3581 pcon=5.1954 forget=2.4379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.9822 mle=1.3298 pcon=5.1909 forget=2.4615 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0215 mle=1.3659 pcon=5.1866 forget=2.4690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.8722 mle=1.2606 pcon=5.1824 forget=2.4292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.0075 mle=1.3835 pcon=5.1787 forget=2.4453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 3 it 30 total=8.9978 mle=1.3638 pcon=5.1749 forget=2.4592 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9246 mle=1.2957 pcon=5.1710 forget=2.4579 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.8988 mle=1.2903 pcon=5.1673 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.0414 mle=1.4600 pcon=5.1635 forget=2.4179 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9798 mle=1.3523 pcon=5.1601 forget=2.4674 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0520 mle=1.4267 pcon=5.1568 forget=2.4686 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0461 mle=1.4482 pcon=5.1531 forget=2.4448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9152 mle=1.3140 pcon=5.1498 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 4 it 40 total=8.9571 mle=1.3769 pcon=5.1465 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9560 mle=1.3336 pcon=5.1435 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.1340 mle=1.5854 pcon=5.1407 forget=2.4079 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=8.9933 mle=1.3939 pcon=5.1375 forget=2.4619 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0241 mle=1.4143 pcon=5.1344 forget=2.4754 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=8.7697 mle=1.1862 pcon=5.1317 forget=2.4519 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.0391 mle=1.4611 pcon=5.1291 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 5 it 0 total=8.8317 mle=1.2046 pcon=5.1261 forget=2.5010 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=8.9576 mle=1.3648 pcon=5.1235 forget=2.4693 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.0087 mle=1.4007 pcon=5.1207 forget=2.4873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.0682 mle=1.4809 pcon=5.1179 forget=2.4694 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.0087 mle=1.4525 pcon=5.1153 forget=2.4410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=8.9115 mle=1.3485 pcon=5.1125 forget=2.4504 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=8.9731 mle=1.4295 pcon=5.1099 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=8.9550 mle=1.4045 pcon=5.1073 forget=2.4433 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 6 it 10 total=8.8858 mle=1.2785 pcon=5.1049 forget=2.5025 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=8.9183 mle=1.3786 pcon=5.1026 forget=2.4370 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=8.8931 mle=1.3183 pcon=5.1003 forget=2.4744 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.8891 mle=1.3320 pcon=5.0981 forget=2.4590 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9236 mle=1.3656 pcon=5.0956 forget=2.4625 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=8.9544 mle=1.3832 pcon=5.0935 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 35%|███▌      | 7/20 [01:18<02:20, 10.79s/it] 40%|████      | 8/20 [01:29<02:09, 10.83s/it] 45%|████▌     | 9/20 [01:39<01:58, 10.77s/it] 50%|█████     | 10/20 [01:50<01:48, 10.83s/it] 55%|█████▌    | 11/20 [02:01<01:37, 10.81s/it] 60%|██████    | 12/20 [02:12<01:26, 10.85s/it] 65%|██████▌   | 13/20 [02:23<01:16, 10.88s/it][loss] ep 6 it 310 total=8.8670 mle=1.3536 pcon=5.0915 forget=2.4219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 360 total=8.9529 mle=1.4569 pcon=5.0893 forget=2.4067 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 7 it 20 total=8.7926 mle=1.2500 pcon=5.0872 forget=2.4554 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=8.7894 mle=1.2464 pcon=5.0849 forget=2.4580 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.9122 mle=1.3876 pcon=5.0829 forget=2.4417 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.9197 mle=1.4330 pcon=5.0810 forget=2.4057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=8.8355 mle=1.3210 pcon=5.0794 forget=2.4352 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=8.9746 mle=1.4542 pcon=5.0778 forget=2.4427 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=8.8529 mle=1.3167 pcon=5.0756 forget=2.4606 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=8.9364 mle=1.3708 pcon=5.0736 forget=2.4920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 8 it 30 total=8.9611 mle=1.4805 pcon=5.0719 forget=2.4087 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=8.8051 mle=1.2745 pcon=5.0702 forget=2.4605 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=8.9212 mle=1.3982 pcon=5.0685 forget=2.4544 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=8.8647 mle=1.3447 pcon=5.0668 forget=2.4532 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=8.8876 mle=1.3661 pcon=5.0652 forget=2.4563 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.7838 mle=1.2569 pcon=5.0636 forget=2.4633 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.0222 mle=1.5799 pcon=5.0618 forget=2.3805 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.8762 mle=1.3643 pcon=5.0604 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 9 it 40 total=8.9253 mle=1.4106 pcon=5.0589 forget=2.4558 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=8.9708 mle=1.4928 pcon=5.0575 forget=2.4205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=8.8783 mle=1.3537 pcon=5.0563 forget=2.4683 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.7494 mle=1.2283 pcon=5.0551 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=8.9736 mle=1.5025 pcon=5.0540 forget=2.4172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=8.8660 mle=1.3564 pcon=5.0526 forget=2.4571 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.8157 mle=1.3066 pcon=5.0514 forget=2.4577 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 10 it 0 total=8.7872 mle=1.2679 pcon=5.0497 forget=2.4695 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.7534 mle=1.2563 pcon=5.0488 forget=2.4483 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=8.8059 mle=1.3265 pcon=5.0475 forget=2.4319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=8.8731 mle=1.3438 pcon=5.0463 forget=2.4830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=8.9086 mle=1.4082 pcon=5.0453 forget=2.4551 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=8.9166 mle=1.4251 pcon=5.0443 forget=2.4472 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=8.9384 mle=1.5123 pcon=5.0431 forget=2.3830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.7567 mle=1.2607 pcon=5.0423 forget=2.4537 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 11 it 10 total=8.9131 mle=1.4402 pcon=5.0411 forget=2.4318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.8452 mle=1.3726 pcon=5.0402 forget=2.4323 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.8328 mle=1.3446 pcon=5.0394 forget=2.4489 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.7804 mle=1.2733 pcon=5.0383 forget=2.4688 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.7525 mle=1.2651 pcon=5.0374 forget=2.4500 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.7361 mle=1.2660 pcon=5.0365 forget=2.4336 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.8659 mle=1.3901 pcon=5.0357 forget=2.4401 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.8290 mle=1.3175 pcon=5.0350 forget=2.4764 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 12 it 20 total=8.8009 mle=1.3124 pcon=5.0338 forget=2.4547 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.8399 mle=1.3537 pcon=5.0328 forget=2.4534 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.7917 mle=1.3597 pcon=5.0318 forget=2.4002 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.8304 mle=1.3345 pcon=5.0308 forget=2.4651 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.7285 mle=1.2222 pcon=5.0302 forget=2.4761 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.7446 mle=1.2927 pcon=5.0297 forget=2.4222 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.8819 mle=1.4092 pcon=5.0291 forget=2.4436 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.8431 mle=1.3757 pcon=5.0284 forget=2.4390 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 13 it 30 total=8.8068 mle=1.3128 pcon=5.0279 forget=2.4662 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.9160 mle=1.4898 pcon=5.0272 forget=2.3990 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.7971 mle=1.3442 pcon=5.0263 forget=2.4266 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 70%|███████   | 14/20 [02:34<01:04, 10.83s/it] 75%|███████▌  | 15/20 [02:44<00:54, 10.83s/it] 80%|████████  | 16/20 [02:55<00:43, 10.76s/it] 85%|████████▌ | 17/20 [03:08<00:34, 11.45s/it] 90%|█████████ | 18/20 [03:21<00:23, 11.99s/it] 95%|█████████▌| 19/20 [03:32<00:11, 11.58s/it]100%|██████████| 20/20 [03:42<00:00, 11.24s/it]100%|██████████| 20/20 [03:42<00:00, 11.15s/it]
[loss] ep 13 it 180 total=8.8463 mle=1.3575 pcon=5.0251 forget=2.4637 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 230 total=8.7175 mle=1.2973 pcon=5.0244 forget=2.3958 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.7141 mle=1.2157 pcon=5.0235 forget=2.4749 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.8144 mle=1.3856 pcon=5.0228 forget=2.4060 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.7207 mle=1.3166 pcon=5.0220 forget=2.3821 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 14 it 40 total=8.6911 mle=1.2700 pcon=5.0209 forget=2.4002 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.7196 mle=1.3074 pcon=5.0200 forget=2.3921 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.7030 mle=1.2968 pcon=5.0190 forget=2.3873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.6482 mle=1.2814 pcon=5.0178 forget=2.3490 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.7236 mle=1.3234 pcon=5.0163 forget=2.3839 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.5867 mle=1.3086 pcon=5.0150 forget=2.2631 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.5516 mle=1.2678 pcon=5.0136 forget=2.2702 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 15 it 0 total=8.6362 mle=1.3723 pcon=5.0120 forget=2.2518 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.5443 mle=1.2775 pcon=5.0106 forget=2.2563 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.5977 mle=1.4137 pcon=5.0094 forget=2.1746 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.5658 mle=1.3945 pcon=5.0081 forget=2.1632 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.5137 mle=1.3243 pcon=5.0069 forget=2.1826 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.5638 mle=1.3683 pcon=5.0058 forget=2.1897 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.5977 mle=1.4529 pcon=5.0047 forget=2.1401 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.5801 mle=1.4350 pcon=5.0041 forget=2.1410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 16 it 10 total=8.5428 mle=1.3684 pcon=5.0035 forget=2.1709 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.5067 mle=1.3503 pcon=5.0034 forget=2.1530 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.4992 mle=1.3242 pcon=5.0033 forget=2.1717 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.5590 mle=1.3543 pcon=5.0037 forget=2.2009 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.5920 mle=1.3713 pcon=5.0042 forget=2.2165 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.7864 mle=1.5472 pcon=5.0046 forget=2.2346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.8067 mle=1.5591 pcon=5.0053 forget=2.2423 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.7030 mle=1.4435 pcon=5.0063 forget=2.2532 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.7558 mle=1.5064 pcon=5.0078 forget=2.2415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.8236 mle=1.5343 pcon=5.0095 forget=2.2798 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.6576 mle=1.3608 pcon=5.0108 forget=2.2861 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.8284 mle=1.4674 pcon=5.0125 forget=2.3485 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.8648 mle=1.5329 pcon=5.0142 forget=2.3177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.8560 mle=1.5091 pcon=5.0162 forget=2.3308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.8475 mle=1.4914 pcon=5.0182 forget=2.3379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.8462 mle=1.4669 pcon=5.0202 forget=2.3591 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.8149 mle=1.3956 pcon=5.0224 forget=2.3969 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.9900 mle=1.5646 pcon=5.0246 forget=2.4008 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=9.0087 mle=1.5652 pcon=5.0270 forget=2.4165 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.9248 mle=1.4674 pcon=5.0294 forget=2.4280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.9835 mle=1.4875 pcon=5.0316 forget=2.4644 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=9.0248 mle=1.5126 pcon=5.0338 forget=2.4784 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.9993 mle=1.4785 pcon=5.0362 forget=2.4846 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.9277 mle=1.4088 pcon=5.0388 forget=2.4801 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.9475 mle=1.4189 pcon=5.0411 forget=2.4875 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=9.0883 mle=1.5617 pcon=5.0434 forget=2.4832 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=9.0020 mle=1.4195 pcon=5.0458 forget=2.5367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.9949 mle=1.4323 pcon=5.0480 forget=2.5146 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=9.0923 mle=1.4972 pcon=5.0504 forget=2.5447 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=9.1255 mle=1.5298 pcon=5.0532 forget=2.5425 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=9.0927 mle=1.4808 pcon=5.0560 forget=2.5558 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[Run] lambda=0.2 lr=0.001 epochs=25 lora_r=8
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=25, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/25 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  4%|▍         | 1/25 [00:17<07:11, 17.99s/it]  8%|▊         | 2/25 [00:29<05:24, 14.13s/it] 12%|█▏        | 3/25 [00:40<04:39, 12.70s/it] 16%|█▌        | 4/25 [00:51<04:12, 12.04s/it] 20%|██        | 5/25 [01:02<03:51, 11.57s/it] 24%|██▍       | 6/25 [01:13<03:36, 11.39s/it][loss] ep 0 it 0 total=9.0402 mle=1.2805 pcon=5.2951 forget=2.4647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1041 mle=1.3921 pcon=5.2887 forget=2.4233 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.2412 mle=1.5078 pcon=5.2827 forget=2.4506 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2781 mle=1.5644 pcon=5.2770 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2053 mle=1.5331 pcon=5.2708 forget=2.4014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1118 mle=1.3806 pcon=5.2650 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.0054 mle=1.2922 pcon=5.2596 forget=2.4536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1732 mle=1.4380 pcon=5.2543 forget=2.4809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 1 it 10 total=9.0164 mle=1.3066 pcon=5.2487 forget=2.4611 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0373 mle=1.3162 pcon=5.2433 forget=2.4779 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.9651 mle=1.2902 pcon=5.2382 forget=2.4367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.9539 mle=1.2379 pcon=5.2331 forget=2.4829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0630 mle=1.3724 pcon=5.2283 forget=2.4623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9880 mle=1.3468 pcon=5.2235 forget=2.4177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9849 mle=1.3082 pcon=5.2185 forget=2.4583 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0553 mle=1.3766 pcon=5.2136 forget=2.4651 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 2 it 20 total=8.9289 mle=1.2745 pcon=5.2089 forget=2.4455 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0970 mle=1.3989 pcon=5.2044 forget=2.4937 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.9871 mle=1.3588 pcon=5.1998 forget=2.4284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9913 mle=1.3581 pcon=5.1954 forget=2.4378 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.9824 mle=1.3300 pcon=5.1909 forget=2.4615 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0215 mle=1.3659 pcon=5.1866 forget=2.4690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.8723 mle=1.2607 pcon=5.1824 forget=2.4292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.0075 mle=1.3836 pcon=5.1787 forget=2.4453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 3 it 30 total=8.9979 mle=1.3638 pcon=5.1749 forget=2.4592 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9245 mle=1.2957 pcon=5.1710 forget=2.4578 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.8987 mle=1.2903 pcon=5.1673 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.0415 mle=1.4600 pcon=5.1635 forget=2.4180 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9800 mle=1.3524 pcon=5.1601 forget=2.4674 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0521 mle=1.4267 pcon=5.1568 forget=2.4686 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0461 mle=1.4481 pcon=5.1531 forget=2.4448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9153 mle=1.3140 pcon=5.1498 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 4 it 40 total=8.9572 mle=1.3770 pcon=5.1465 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9559 mle=1.3335 pcon=5.1435 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.1339 mle=1.5853 pcon=5.1407 forget=2.4079 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=8.9932 mle=1.3938 pcon=5.1375 forget=2.4619 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0241 mle=1.4142 pcon=5.1344 forget=2.4754 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=8.7696 mle=1.1862 pcon=5.1316 forget=2.4518 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.0391 mle=1.4612 pcon=5.1291 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 5 it 0 total=8.8318 mle=1.2047 pcon=5.1261 forget=2.5010 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=8.9577 mle=1.3650 pcon=5.1235 forget=2.4693 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.0086 mle=1.4006 pcon=5.1207 forget=2.4873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.0681 mle=1.4809 pcon=5.1179 forget=2.4694 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.0086 mle=1.4524 pcon=5.1153 forget=2.4410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=8.9116 mle=1.3486 pcon=5.1125 forget=2.4505 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=8.9731 mle=1.4295 pcon=5.1099 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=8.9550 mle=1.4046 pcon=5.1073 forget=2.4432 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 6 it 10 total=8.8858 mle=1.2785 pcon=5.1049 forget=2.5024 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=8.9181 mle=1.3785 pcon=5.1025 forget=2.4371 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=8.8931 mle=1.3184 pcon=5.1003 forget=2.4744 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.8889 mle=1.3319 pcon=5.0981 forget=2.4590 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9235 mle=1.3655 pcon=5.0956 forget=2.4625 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=8.9543 mle=1.3830 pcon=5.0935 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 28%|██▊       | 7/25 [01:24<03:23, 11.33s/it] 32%|███▏      | 8/25 [01:34<03:06, 10.96s/it] 36%|███▌      | 9/25 [01:45<02:56, 11.00s/it] 40%|████      | 10/25 [01:56<02:43, 10.90s/it] 44%|████▍     | 11/25 [02:06<02:29, 10.65s/it] 48%|████▊     | 12/25 [02:16<02:16, 10.51s/it] 52%|█████▏    | 13/25 [02:26<02:05, 10.46s/it][loss] ep 6 it 310 total=8.8669 mle=1.3535 pcon=5.0915 forget=2.4219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 360 total=8.9530 mle=1.4570 pcon=5.0893 forget=2.4068 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 7 it 20 total=8.7928 mle=1.2501 pcon=5.0872 forget=2.4555 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=8.7893 mle=1.2464 pcon=5.0849 forget=2.4580 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.9121 mle=1.3875 pcon=5.0829 forget=2.4417 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.9198 mle=1.4331 pcon=5.0810 forget=2.4057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=8.8356 mle=1.3211 pcon=5.0794 forget=2.4351 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=8.9745 mle=1.4540 pcon=5.0777 forget=2.4427 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=8.8527 mle=1.3166 pcon=5.0756 forget=2.4606 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=8.9364 mle=1.3708 pcon=5.0736 forget=2.4920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 8 it 30 total=8.9610 mle=1.4804 pcon=5.0719 forget=2.4087 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=8.8051 mle=1.2744 pcon=5.0702 forget=2.4604 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=8.9213 mle=1.3983 pcon=5.0685 forget=2.4545 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=8.8647 mle=1.3447 pcon=5.0668 forget=2.4533 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=8.8877 mle=1.3662 pcon=5.0652 forget=2.4564 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.7836 mle=1.2568 pcon=5.0636 forget=2.4633 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.0221 mle=1.5797 pcon=5.0618 forget=2.3806 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.8764 mle=1.3645 pcon=5.0604 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 9 it 40 total=8.9253 mle=1.4106 pcon=5.0589 forget=2.4558 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=8.9709 mle=1.4929 pcon=5.0575 forget=2.4205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=8.8783 mle=1.3538 pcon=5.0563 forget=2.4682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.7493 mle=1.2282 pcon=5.0550 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=8.9737 mle=1.5026 pcon=5.0540 forget=2.4171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=8.8661 mle=1.3564 pcon=5.0526 forget=2.4571 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.8157 mle=1.3066 pcon=5.0514 forget=2.4577 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 10 it 0 total=8.7872 mle=1.2679 pcon=5.0497 forget=2.4695 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.7535 mle=1.2563 pcon=5.0489 forget=2.4483 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=8.8059 mle=1.3265 pcon=5.0475 forget=2.4319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=8.8731 mle=1.3437 pcon=5.0463 forget=2.4831 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=8.9087 mle=1.4082 pcon=5.0453 forget=2.4551 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=8.9166 mle=1.4251 pcon=5.0443 forget=2.4472 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=8.9385 mle=1.5124 pcon=5.0431 forget=2.3831 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.7568 mle=1.2608 pcon=5.0423 forget=2.4537 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 11 it 10 total=8.9132 mle=1.4404 pcon=5.0411 forget=2.4317 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.8450 mle=1.3725 pcon=5.0402 forget=2.4323 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.8327 mle=1.3446 pcon=5.0394 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.7802 mle=1.2732 pcon=5.0383 forget=2.4687 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.7524 mle=1.2650 pcon=5.0374 forget=2.4499 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.7359 mle=1.2659 pcon=5.0365 forget=2.4335 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.8655 mle=1.3899 pcon=5.0357 forget=2.4399 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.8286 mle=1.3173 pcon=5.0350 forget=2.4762 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 12 it 20 total=8.8004 mle=1.3121 pcon=5.0338 forget=2.4544 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.8392 mle=1.3534 pcon=5.0328 forget=2.4530 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.7902 mle=1.3592 pcon=5.0318 forget=2.3993 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.8289 mle=1.3339 pcon=5.0308 forget=2.4642 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.7272 mle=1.2217 pcon=5.0302 forget=2.4752 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.7425 mle=1.2923 pcon=5.0297 forget=2.4205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.8786 mle=1.4077 pcon=5.0291 forget=2.4418 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.8395 mle=1.3745 pcon=5.0284 forget=2.4367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 13 it 30 total=8.8007 mle=1.3108 pcon=5.0278 forget=2.4620 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.9055 mle=1.4856 pcon=5.0271 forget=2.3928 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.7871 mle=1.3417 pcon=5.0262 forget=2.4191 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 56%|█████▌    | 14/25 [02:37<01:53, 10.34s/it] 60%|██████    | 15/25 [02:46<01:41, 10.16s/it] 64%|██████▍   | 16/25 [02:56<01:30, 10.01s/it] 68%|██████▊   | 17/25 [03:06<01:19,  9.98s/it] 72%|███████▏  | 18/25 [03:16<01:09,  9.91s/it] 76%|███████▌  | 19/25 [03:26<00:59,  9.99s/it] 80%|████████  | 20/25 [03:36<00:49,  9.98s/it][loss] ep 13 it 180 total=8.8335 mle=1.3572 pcon=5.0249 forget=2.4514 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 230 total=8.6957 mle=1.2986 pcon=5.0241 forget=2.3731 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.6905 mle=1.2149 pcon=5.0230 forget=2.4525 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.7794 mle=1.3864 pcon=5.0221 forget=2.3708 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.6606 mle=1.3156 pcon=5.0210 forget=2.3240 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 14 it 40 total=8.6495 mle=1.2905 pcon=5.0196 forget=2.3394 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.6280 mle=1.3270 pcon=5.0183 forget=2.2827 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.5904 mle=1.3311 pcon=5.0168 forget=2.2424 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.5326 mle=1.3343 pcon=5.0153 forget=2.1830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.5805 mle=1.3528 pcon=5.0134 forget=2.2142 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.4712 mle=1.3906 pcon=5.0121 forget=2.0684 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.4313 mle=1.3556 pcon=5.0111 forget=2.0647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 15 it 0 total=8.5157 mle=1.4595 pcon=5.0101 forget=2.0461 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.4083 mle=1.3599 pcon=5.0098 forget=2.0385 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.4353 mle=1.4596 pcon=5.0099 forget=1.9658 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.4470 mle=1.4620 pcon=5.0103 forget=1.9747 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.4062 mle=1.3911 pcon=5.0110 forget=2.0041 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.4537 mle=1.4270 pcon=5.0120 forget=2.0148 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.4764 mle=1.4869 pcon=5.0131 forget=1.9764 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.4933 mle=1.4944 pcon=5.0147 forget=1.9843 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e25-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 16 it 10 total=8.4617 mle=1.4250 pcon=5.0162 forget=2.0205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.4109 mle=1.4001 pcon=5.0181 forget=1.9926 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.4146 mle=1.3642 pcon=5.0200 forget=2.0305 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.4799 mle=1.3951 pcon=5.0222 forget=2.0625 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.5147 mle=1.4058 pcon=5.0245 forget=2.0844 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.7292 mle=1.5705 pcon=5.0265 forget=2.1322 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.7161 mle=1.5711 pcon=5.0286 forget=2.1165 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.6568 mle=1.5048 pcon=5.0308 forget=2.1212 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.6473 mle=1.5381 pcon=5.0333 forget=2.0759 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.7467 mle=1.5538 pcon=5.0360 forget=2.1570 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.5788 mle=1.4048 pcon=5.0380 forget=2.1360 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.7209 mle=1.4725 pcon=5.0402 forget=2.2081 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.7558 mle=1.5602 pcon=5.0423 forget=2.1533 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.7250 mle=1.5254 pcon=5.0445 forget=2.1551 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.7274 mle=1.5305 pcon=5.0466 forget=2.1504 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.7389 mle=1.4862 pcon=5.0484 forget=2.2043 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.6877 mle=1.4355 pcon=5.0503 forget=2.2019 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.8731 mle=1.6201 pcon=5.0522 forget=2.2009 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.8434 mle=1.5541 pcon=5.0539 forget=2.2354 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.7287 mle=1.4862 pcon=5.0556 forget=2.1869 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.8137 mle=1.5155 pcon=5.0569 forget=2.2413 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.8097 mle=1.5146 pcon=5.0581 forget=2.2370 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.8065 mle=1.5100 pcon=5.0593 forget=2.2373 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.7246 mle=1.4364 pcon=5.0605 forget=2.2277 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.7437 mle=1.4505 pcon=5.0615 forget=2.2317 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.8408 mle=1.5727 pcon=5.0622 forget=2.2059 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.7314 mle=1.4091 pcon=5.0629 forget=2.2594 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.7249 mle=1.4423 pcon=5.0633 forget=2.2193 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.7920 mle=1.4714 pcon=5.0638 forget=2.2569 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.8349 mle=1.5380 pcon=5.0645 forget=2.2324 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.7893 mle=1.4656 pcon=5.0653 forget=2.2584 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 0 total=8.7500 mle=1.4321 pcon=5.0656 forget=2.2524 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.7972 mle=1.4482 pcon=5.0657 forget=2.2833 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.7743 mle=1.4560 pcon=5.0656 forget=2.2527 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.8175 mle=1.4767 pcon=5.0656 forget=2.2752 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.8755 mle=1.5447 pcon=5.0655 forget=2.2653 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=8.7986 mle=1.4531 pcon=5.0653 forget=2.2802 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 84%|████████▍ | 21/25 [03:46<00:40, 10.00s/it] 88%|████████▊ | 22/25 [03:55<00:29,  9.88s/it] 92%|█████████▏| 23/25 [04:06<00:20, 10.01s/it] 96%|█████████▌| 24/25 [04:17<00:10, 10.39s/it]100%|██████████| 25/25 [04:28<00:00, 10.66s/it]100%|██████████| 25/25 [04:28<00:00, 10.75s/it]
[loss] ep 20 it 300 total=8.8810 mle=1.5441 pcon=5.0652 forget=2.2717 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=8.8788 mle=1.5299 pcon=5.0648 forget=2.2840 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 10 total=8.6914 mle=1.3407 pcon=5.0641 forget=2.2865 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=8.8429 mle=1.4663 pcon=5.0636 forget=2.3131 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.8214 mle=1.4586 pcon=5.0629 forget=2.3000 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.8686 mle=1.5126 pcon=5.0621 forget=2.2938 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=8.7670 mle=1.3876 pcon=5.0614 forget=2.3181 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.9054 mle=1.5313 pcon=5.0608 forget=2.3132 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.8899 mle=1.4822 pcon=5.0599 forget=2.3478 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.8621 mle=1.4390 pcon=5.0591 forget=2.3640 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 20 total=8.8699 mle=1.4577 pcon=5.0584 forget=2.3538 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.8573 mle=1.4349 pcon=5.0574 forget=2.3650 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.9160 mle=1.4777 pcon=5.0566 forget=2.3817 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.8660 mle=1.4147 pcon=5.0556 forget=2.3957 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.9197 mle=1.4785 pcon=5.0549 forget=2.3863 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.8951 mle=1.4126 pcon=5.0542 forget=2.4282 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.8500 mle=1.3777 pcon=5.0534 forget=2.4189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=9.0443 mle=1.5678 pcon=5.0525 forget=2.4240 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 30 total=8.8981 mle=1.4070 pcon=5.0518 forget=2.4393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.9386 mle=1.4141 pcon=5.0509 forget=2.4736 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=9.0121 mle=1.5177 pcon=5.0501 forget=2.4443 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.9466 mle=1.4417 pcon=5.0491 forget=2.4557 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.9766 mle=1.4622 pcon=5.0484 forget=2.4660 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=9.0654 mle=1.5304 pcon=5.0478 forget=2.4872 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=9.0307 mle=1.5058 pcon=5.0468 forget=2.4780 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.9128 mle=1.3536 pcon=5.0462 forget=2.5130 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 40 total=8.9987 mle=1.4322 pcon=5.0458 forget=2.5208 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=9.0770 mle=1.5310 pcon=5.0450 forget=2.5010 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.9113 mle=1.3507 pcon=5.0443 forget=2.5163 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.9377 mle=1.3753 pcon=5.0439 forget=2.5185 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=9.0207 mle=1.4466 pcon=5.0435 forget=2.5306 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.9876 mle=1.4082 pcon=5.0428 forget=2.5366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.9841 mle=1.3970 pcon=5.0420 forget=2.5450 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[Run] lambda=0.2 lr=0.001 epochs=30 lora_r=8
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=30, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/30 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  3%|▎         | 1/30 [00:17<08:41, 17.99s/it]  7%|▋         | 2/30 [00:29<06:37, 14.21s/it] 10%|█         | 3/30 [00:41<05:51, 13.03s/it] 13%|█▎        | 4/30 [00:52<05:19, 12.29s/it] 17%|█▋        | 5/30 [01:03<04:59, 11.97s/it] 20%|██        | 6/30 [01:15<04:44, 11.84s/it][loss] ep 0 it 0 total=9.0402 mle=1.2804 pcon=5.2951 forget=2.4647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1040 mle=1.3921 pcon=5.2887 forget=2.4232 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.2412 mle=1.5078 pcon=5.2827 forget=2.4507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2780 mle=1.5643 pcon=5.2770 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2055 mle=1.5332 pcon=5.2708 forget=2.4014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1117 mle=1.3805 pcon=5.2650 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.0054 mle=1.2922 pcon=5.2596 forget=2.4536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1731 mle=1.4379 pcon=5.2543 forget=2.4809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 1 it 10 total=9.0165 mle=1.3067 pcon=5.2487 forget=2.4611 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0373 mle=1.3162 pcon=5.2433 forget=2.4779 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.9650 mle=1.2902 pcon=5.2382 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.9540 mle=1.2380 pcon=5.2331 forget=2.4829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0631 mle=1.3724 pcon=5.2283 forget=2.4623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9879 mle=1.3468 pcon=5.2235 forget=2.4177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9850 mle=1.3082 pcon=5.2185 forget=2.4583 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0554 mle=1.3767 pcon=5.2136 forget=2.4651 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 2 it 20 total=8.9290 mle=1.2746 pcon=5.2089 forget=2.4455 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0970 mle=1.3990 pcon=5.2044 forget=2.4936 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.9870 mle=1.3589 pcon=5.1998 forget=2.4284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9914 mle=1.3581 pcon=5.1954 forget=2.4379 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.9823 mle=1.3300 pcon=5.1909 forget=2.4615 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0215 mle=1.3659 pcon=5.1866 forget=2.4690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.8723 mle=1.2607 pcon=5.1824 forget=2.4292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.0075 mle=1.3835 pcon=5.1787 forget=2.4453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 3 it 30 total=8.9980 mle=1.3639 pcon=5.1749 forget=2.4592 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9247 mle=1.2959 pcon=5.1710 forget=2.4579 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.8987 mle=1.2903 pcon=5.1673 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.0415 mle=1.4600 pcon=5.1635 forget=2.4180 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9799 mle=1.3523 pcon=5.1601 forget=2.4674 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0520 mle=1.4266 pcon=5.1568 forget=2.4686 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0461 mle=1.4482 pcon=5.1532 forget=2.4448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9153 mle=1.3141 pcon=5.1498 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 4 it 40 total=8.9573 mle=1.3771 pcon=5.1465 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9561 mle=1.3337 pcon=5.1435 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.1339 mle=1.5854 pcon=5.1406 forget=2.4079 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=8.9932 mle=1.3938 pcon=5.1375 forget=2.4620 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0241 mle=1.4143 pcon=5.1344 forget=2.4754 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=8.7698 mle=1.1862 pcon=5.1316 forget=2.4519 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.0390 mle=1.4611 pcon=5.1291 forget=2.4489 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 5 it 0 total=8.8317 mle=1.2046 pcon=5.1261 forget=2.5010 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=8.9575 mle=1.3647 pcon=5.1235 forget=2.4693 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.0087 mle=1.4007 pcon=5.1207 forget=2.4873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.0682 mle=1.4809 pcon=5.1179 forget=2.4694 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.0086 mle=1.4524 pcon=5.1152 forget=2.4410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=8.9115 mle=1.3485 pcon=5.1125 forget=2.4504 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=8.9732 mle=1.4295 pcon=5.1099 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=8.9550 mle=1.4045 pcon=5.1073 forget=2.4432 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 6 it 10 total=8.8857 mle=1.2784 pcon=5.1049 forget=2.5025 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=8.9182 mle=1.3786 pcon=5.1025 forget=2.4371 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=8.8930 mle=1.3183 pcon=5.1003 forget=2.4743 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.8890 mle=1.3320 pcon=5.0981 forget=2.4590 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9235 mle=1.3655 pcon=5.0956 forget=2.4624 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=8.9543 mle=1.3831 pcon=5.0935 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 23%|██▎       | 7/30 [01:26<04:29, 11.71s/it] 27%|██▋       | 8/30 [01:38<04:18, 11.74s/it] 30%|███       | 9/30 [01:50<04:04, 11.66s/it] 33%|███▎      | 10/30 [02:02<03:57, 11.86s/it] 37%|███▋      | 11/30 [02:14<03:45, 11.89s/it] 40%|████      | 12/30 [02:26<03:34, 11.90s/it] 43%|████▎     | 13/30 [02:37<03:21, 11.83s/it][loss] ep 6 it 310 total=8.8669 mle=1.3536 pcon=5.0915 forget=2.4218 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 360 total=8.9529 mle=1.4569 pcon=5.0893 forget=2.4067 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 7 it 20 total=8.7927 mle=1.2500 pcon=5.0872 forget=2.4554 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=8.7893 mle=1.2464 pcon=5.0849 forget=2.4580 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.9121 mle=1.3874 pcon=5.0829 forget=2.4418 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.9197 mle=1.4330 pcon=5.0810 forget=2.4057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=8.8357 mle=1.3211 pcon=5.0794 forget=2.4352 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=8.9745 mle=1.4541 pcon=5.0777 forget=2.4427 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=8.8528 mle=1.3167 pcon=5.0756 forget=2.4606 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=8.9365 mle=1.3709 pcon=5.0736 forget=2.4920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 8 it 30 total=8.9610 mle=1.4804 pcon=5.0719 forget=2.4087 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=8.8050 mle=1.2744 pcon=5.0702 forget=2.4604 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=8.9212 mle=1.3982 pcon=5.0685 forget=2.4545 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=8.8647 mle=1.3447 pcon=5.0668 forget=2.4532 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=8.8876 mle=1.3661 pcon=5.0652 forget=2.4563 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.7838 mle=1.2569 pcon=5.0636 forget=2.4633 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.0222 mle=1.5798 pcon=5.0618 forget=2.3806 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.8762 mle=1.3644 pcon=5.0604 forget=2.4514 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 9 it 40 total=8.9254 mle=1.4107 pcon=5.0589 forget=2.4558 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=8.9708 mle=1.4928 pcon=5.0575 forget=2.4205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=8.8782 mle=1.3537 pcon=5.0563 forget=2.4682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.7493 mle=1.2281 pcon=5.0551 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=8.9736 mle=1.5024 pcon=5.0540 forget=2.4172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=8.8661 mle=1.3564 pcon=5.0526 forget=2.4571 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.8157 mle=1.3066 pcon=5.0514 forget=2.4577 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 10 it 0 total=8.7873 mle=1.2680 pcon=5.0497 forget=2.4695 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.7534 mle=1.2563 pcon=5.0489 forget=2.4483 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=8.8059 mle=1.3265 pcon=5.0475 forget=2.4319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=8.8732 mle=1.3438 pcon=5.0463 forget=2.4830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=8.9088 mle=1.4084 pcon=5.0453 forget=2.4551 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=8.9166 mle=1.4251 pcon=5.0443 forget=2.4472 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=8.9385 mle=1.5123 pcon=5.0431 forget=2.3831 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.7568 mle=1.2608 pcon=5.0423 forget=2.4537 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 11 it 10 total=8.9131 mle=1.4403 pcon=5.0411 forget=2.4318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.8451 mle=1.3725 pcon=5.0402 forget=2.4323 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.8327 mle=1.3445 pcon=5.0394 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.7802 mle=1.2732 pcon=5.0383 forget=2.4688 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.7523 mle=1.2650 pcon=5.0375 forget=2.4499 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.7358 mle=1.2659 pcon=5.0365 forget=2.4334 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.8655 mle=1.3901 pcon=5.0357 forget=2.4397 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.8282 mle=1.3171 pcon=5.0350 forget=2.4761 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 12 it 20 total=8.8002 mle=1.3121 pcon=5.0338 forget=2.4543 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.8390 mle=1.3533 pcon=5.0328 forget=2.4528 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.7899 mle=1.3590 pcon=5.0318 forget=2.3990 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.8284 mle=1.3337 pcon=5.0308 forget=2.4639 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.7266 mle=1.2216 pcon=5.0303 forget=2.4748 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.7415 mle=1.2921 pcon=5.0297 forget=2.4197 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.8778 mle=1.4077 pcon=5.0291 forget=2.4410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.8383 mle=1.3743 pcon=5.0284 forget=2.4356 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 13 it 30 total=8.7990 mle=1.3109 pcon=5.0278 forget=2.4603 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.9010 mle=1.4841 pcon=5.0271 forget=2.3899 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.7825 mle=1.3409 pcon=5.0261 forget=2.4155 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 47%|████▋     | 14/30 [02:48<03:05, 11.59s/it] 50%|█████     | 15/30 [03:00<02:52, 11.52s/it] 53%|█████▎    | 16/30 [03:11<02:40, 11.48s/it] 57%|█████▋    | 17/30 [03:23<02:29, 11.50s/it] 60%|██████    | 18/30 [03:34<02:16, 11.38s/it] 63%|██████▎   | 19/30 [03:45<02:06, 11.46s/it] 67%|██████▋   | 20/30 [03:57<01:53, 11.39s/it][loss] ep 13 it 180 total=8.8280 mle=1.3584 pcon=5.0248 forget=2.4448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 230 total=8.6843 mle=1.2998 pcon=5.0239 forget=2.3606 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.6733 mle=1.2115 pcon=5.0228 forget=2.4389 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.7585 mle=1.3888 pcon=5.0218 forget=2.3479 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.6280 mle=1.3194 pcon=5.0206 forget=2.2881 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 14 it 40 total=8.6272 mle=1.3081 pcon=5.0190 forget=2.3001 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.5902 mle=1.3453 pcon=5.0176 forget=2.2273 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.5483 mle=1.3548 pcon=5.0160 forget=2.1775 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.4826 mle=1.3528 pcon=5.0145 forget=2.1153 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.5194 mle=1.3658 pcon=5.0129 forget=2.1407 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.4282 mle=1.4168 pcon=5.0119 forget=1.9995 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.3906 mle=1.3790 pcon=5.0114 forget=2.0003 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 15 it 0 total=8.4701 mle=1.4709 pcon=5.0110 forget=1.9882 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.3751 mle=1.3761 pcon=5.0113 forget=1.9876 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.4039 mle=1.4675 pcon=5.0121 forget=1.9243 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.4259 mle=1.4692 pcon=5.0130 forget=1.9437 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.3576 mle=1.3969 pcon=5.0143 forget=1.9464 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.4243 mle=1.4373 pcon=5.0156 forget=1.9713 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.4597 mle=1.4975 pcon=5.0171 forget=1.9451 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.4715 mle=1.5054 pcon=5.0188 forget=1.9474 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 16 it 10 total=8.4516 mle=1.4304 pcon=5.0205 forget=2.0007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.4022 mle=1.4096 pcon=5.0226 forget=1.9700 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.4121 mle=1.3757 pcon=5.0245 forget=2.0120 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.4757 mle=1.4050 pcon=5.0267 forget=2.0440 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.5033 mle=1.4134 pcon=5.0289 forget=2.0610 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.7141 mle=1.5753 pcon=5.0308 forget=2.1080 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.6910 mle=1.5737 pcon=5.0327 forget=2.0846 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.6370 mle=1.5181 pcon=5.0348 forget=2.0842 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.6179 mle=1.5465 pcon=5.0371 forget=2.0343 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.7077 mle=1.5567 pcon=5.0395 forget=2.1115 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.5351 mle=1.4119 pcon=5.0412 forget=2.0820 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.6612 mle=1.4736 pcon=5.0431 forget=2.1444 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.6944 mle=1.5670 pcon=5.0448 forget=2.0826 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.6499 mle=1.5304 pcon=5.0465 forget=2.0730 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.6290 mle=1.5277 pcon=5.0481 forget=2.0532 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.6401 mle=1.4860 pcon=5.0494 forget=2.1047 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.5725 mle=1.4389 pcon=5.0507 forget=2.0829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.7512 mle=1.6260 pcon=5.0519 forget=2.0733 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.7031 mle=1.5477 pcon=5.0530 forget=2.1025 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.5687 mle=1.4956 pcon=5.0539 forget=2.0192 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.6370 mle=1.5115 pcon=5.0544 forget=2.0712 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.6196 mle=1.5198 pcon=5.0548 forget=2.0450 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.6117 mle=1.5149 pcon=5.0551 forget=2.0417 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.5089 mle=1.4411 pcon=5.0555 forget=2.0124 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.5033 mle=1.4542 pcon=5.0555 forget=1.9936 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.5720 mle=1.5502 pcon=5.0552 forget=1.9666 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.4671 mle=1.4083 pcon=5.0548 forget=2.0040 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.4554 mle=1.4555 pcon=5.0542 forget=1.9456 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.4940 mle=1.4633 pcon=5.0536 forget=1.9772 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.5175 mle=1.5386 pcon=5.0533 forget=1.9257 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.4735 mle=1.4682 pcon=5.0528 forget=1.9524 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 0 total=8.4426 mle=1.4525 pcon=5.0520 forget=1.9381 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.4322 mle=1.4361 pcon=5.0510 forget=1.9452 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.4077 mle=1.4507 pcon=5.0498 forget=1.9071 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.4270 mle=1.4745 pcon=5.0487 forget=1.9037 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.4928 mle=1.5482 pcon=5.0475 forget=1.8972 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 250 total=8.3972 mle=1.4647 pcon=5.0462 forget=1.8862 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 70%|███████   | 21/30 [04:08<01:41, 11.33s/it] 73%|███████▎  | 22/30 [04:19<01:29, 11.22s/it] 77%|███████▋  | 23/30 [04:30<01:18, 11.18s/it] 80%|████████  | 24/30 [04:40<01:05, 10.91s/it] 83%|████████▎ | 25/30 [04:51<00:53, 10.80s/it] 87%|████████▋ | 26/30 [05:02<00:43, 10.95s/it] 90%|█████████ | 27/30 [05:13<00:32, 10.84s/it] 93%|█████████▎| 28/30 [05:24<00:21, 10.96s/it][loss] ep 20 it 300 total=8.4950 mle=1.5692 pcon=5.0450 forget=1.8808 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=8.4209 mle=1.5116 pcon=5.0436 forget=1.8656 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 21 it 10 total=8.2488 mle=1.3525 pcon=5.0419 forget=1.8545 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=8.3936 mle=1.4775 pcon=5.0403 forget=1.8758 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.3589 mle=1.4712 pcon=5.0386 forget=1.8491 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.3545 mle=1.4869 pcon=5.0369 forget=1.8307 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=8.2676 mle=1.3959 pcon=5.0351 forget=1.8366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.3913 mle=1.5392 pcon=5.0336 forget=1.8185 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.3932 mle=1.5048 pcon=5.0317 forget=1.8567 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.3103 mle=1.4315 pcon=5.0300 forget=1.8489 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 22 it 20 total=8.3147 mle=1.4599 pcon=5.0284 forget=1.8264 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.3136 mle=1.4502 pcon=5.0265 forget=1.8369 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.3773 mle=1.4889 pcon=5.0247 forget=1.8637 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.2454 mle=1.4040 pcon=5.0228 forget=1.8186 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.3447 mle=1.4864 pcon=5.0212 forget=1.8371 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.3281 mle=1.4435 pcon=5.0198 forget=1.8649 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.2135 mle=1.3847 pcon=5.0182 forget=1.8106 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=8.4459 mle=1.5777 pcon=5.0165 forget=1.8517 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e30-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 23 it 30 total=8.3093 mle=1.4437 pcon=5.0151 forget=1.8505 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.3206 mle=1.3993 pcon=5.0136 forget=1.9077 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=8.4144 mle=1.5294 pcon=5.0121 forget=1.8729 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.3909 mle=1.4983 pcon=5.0105 forget=1.8821 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.4100 mle=1.4830 pcon=5.0093 forget=1.9177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=8.4690 mle=1.5440 pcon=5.0082 forget=1.9168 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=8.4268 mle=1.4858 pcon=5.0068 forget=1.9342 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.3352 mle=1.3754 pcon=5.0058 forget=1.9539 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 40 total=8.4265 mle=1.4273 pcon=5.0051 forget=1.9941 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=8.5087 mle=1.5217 pcon=5.0041 forget=1.9829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.4383 mle=1.4228 pcon=5.0033 forget=2.0123 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.4135 mle=1.3784 pcon=5.0028 forget=2.0323 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=8.4864 mle=1.4382 pcon=5.0024 forget=2.0457 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.5038 mle=1.4533 pcon=5.0020 forget=2.0485 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.5133 mle=1.4368 pcon=5.0014 forget=2.0750 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 0 total=8.5462 mle=1.4845 pcon=5.0012 forget=2.0605 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 50 total=8.5813 mle=1.5138 pcon=5.0010 forget=2.0666 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 100 total=8.6399 mle=1.5036 pcon=5.0008 forget=2.1356 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 150 total=8.6375 mle=1.4670 pcon=5.0009 forget=2.1697 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 200 total=8.6205 mle=1.4596 pcon=5.0011 forget=2.1598 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 250 total=8.6409 mle=1.4370 pcon=5.0016 forget=2.2023 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 300 total=8.6453 mle=1.4560 pcon=5.0020 forget=2.1873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 350 total=8.7725 mle=1.5151 pcon=5.0024 forget=2.2550 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 10 total=8.7266 mle=1.4415 pcon=5.0030 forget=2.2822 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 60 total=8.7250 mle=1.4626 pcon=5.0034 forget=2.2590 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 110 total=8.7313 mle=1.4424 pcon=5.0042 forget=2.2847 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 160 total=8.7005 mle=1.3914 pcon=5.0049 forget=2.3043 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 210 total=8.8005 mle=1.5032 pcon=5.0057 forget=2.2916 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 260 total=8.8022 mle=1.4518 pcon=5.0070 forget=2.3434 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 310 total=9.0144 mle=1.6723 pcon=5.0079 forget=2.3342 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 360 total=8.9607 mle=1.6054 pcon=5.0091 forget=2.3462 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 20 total=8.8647 mle=1.4411 pcon=5.0103 forget=2.4133 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 70 total=8.8931 mle=1.5082 pcon=5.0114 forget=2.3735 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 120 total=8.8917 mle=1.4456 pcon=5.0129 forget=2.4331 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 170 total=8.8953 mle=1.4556 pcon=5.0142 forget=2.4255 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 220 total=8.9137 mle=1.4478 pcon=5.0158 forget=2.4500 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 270 total=8.9252 mle=1.4551 pcon=5.0171 forget=2.4530 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 320 total=9.0135 mle=1.4922 pcon=5.0187 forget=2.5027 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 370 total=8.8961 mle=1.3703 pcon=5.0203 forget=2.5056 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 97%|█████████▋| 29/30 [05:35<00:10, 10.90s/it]100%|██████████| 30/30 [05:45<00:00, 10.88s/it]100%|██████████| 30/30 [05:45<00:00, 11.53s/it]
[loss] ep 28 it 30 total=9.0322 mle=1.4719 pcon=5.0220 forget=2.5382 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 80 total=9.1093 mle=1.6002 pcon=5.0234 forget=2.4857 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 130 total=8.9010 mle=1.3752 pcon=5.0252 forget=2.5005 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 180 total=8.9540 mle=1.3817 pcon=5.0264 forget=2.5459 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 230 total=8.9826 mle=1.4356 pcon=5.0282 forget=2.5188 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 280 total=9.0395 mle=1.4824 pcon=5.0298 forget=2.5272 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 330 total=9.0121 mle=1.4355 pcon=5.0313 forget=2.5452 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 380 total=9.0417 mle=1.4468 pcon=5.0328 forget=2.5621 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 40 total=9.1670 mle=1.5876 pcon=5.0344 forget=2.5451 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 90 total=9.0986 mle=1.4642 pcon=5.0359 forget=2.5984 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 140 total=9.0505 mle=1.4447 pcon=5.0376 forget=2.5682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 190 total=9.2188 mle=1.5980 pcon=5.0390 forget=2.5818 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 240 total=9.1562 mle=1.5344 pcon=5.0408 forget=2.5810 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 290 total=9.0871 mle=1.4623 pcon=5.0420 forget=2.5828 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 340 total=9.1694 mle=1.5025 pcon=5.0439 forget=2.6231 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[Run] lambda=0.2 lr=0.001 epochs=35 lora_r=8
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=35, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/35 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  3%|▎         | 1/35 [00:13<07:46, 13.73s/it]  6%|▌         | 2/35 [00:22<06:05, 11.09s/it]  9%|▊         | 3/35 [00:32<05:25, 10.18s/it] 11%|█▏        | 4/35 [00:41<05:03,  9.78s/it] 14%|█▍        | 5/35 [00:51<05:01, 10.06s/it] 17%|█▋        | 6/35 [01:04<05:15, 10.89s/it][loss] ep 0 it 0 total=9.0402 mle=1.2805 pcon=5.2951 forget=2.4647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1041 mle=1.3921 pcon=5.2887 forget=2.4233 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.2412 mle=1.5078 pcon=5.2827 forget=2.4506 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2781 mle=1.5644 pcon=5.2770 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2053 mle=1.5331 pcon=5.2708 forget=2.4014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1118 mle=1.3806 pcon=5.2650 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.0054 mle=1.2922 pcon=5.2596 forget=2.4536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1732 mle=1.4380 pcon=5.2543 forget=2.4809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 1 it 10 total=9.0164 mle=1.3066 pcon=5.2487 forget=2.4611 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0373 mle=1.3162 pcon=5.2433 forget=2.4779 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.9651 mle=1.2902 pcon=5.2382 forget=2.4367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.9539 mle=1.2379 pcon=5.2331 forget=2.4829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0630 mle=1.3724 pcon=5.2283 forget=2.4623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9880 mle=1.3468 pcon=5.2235 forget=2.4177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9849 mle=1.3082 pcon=5.2185 forget=2.4583 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0553 mle=1.3766 pcon=5.2136 forget=2.4651 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 2 it 20 total=8.9289 mle=1.2745 pcon=5.2089 forget=2.4455 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0970 mle=1.3989 pcon=5.2044 forget=2.4937 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.9871 mle=1.3588 pcon=5.1998 forget=2.4284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9913 mle=1.3581 pcon=5.1954 forget=2.4378 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.9824 mle=1.3300 pcon=5.1909 forget=2.4615 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0215 mle=1.3659 pcon=5.1866 forget=2.4690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.8723 mle=1.2607 pcon=5.1824 forget=2.4292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.0075 mle=1.3836 pcon=5.1787 forget=2.4453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 3 it 30 total=8.9979 mle=1.3638 pcon=5.1749 forget=2.4592 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9245 mle=1.2957 pcon=5.1710 forget=2.4578 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.8987 mle=1.2903 pcon=5.1673 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.0415 mle=1.4600 pcon=5.1635 forget=2.4180 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9800 mle=1.3524 pcon=5.1601 forget=2.4674 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0521 mle=1.4267 pcon=5.1568 forget=2.4686 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0461 mle=1.4481 pcon=5.1531 forget=2.4448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9153 mle=1.3140 pcon=5.1498 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 4 it 40 total=8.9572 mle=1.3770 pcon=5.1465 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9559 mle=1.3335 pcon=5.1435 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.1339 mle=1.5853 pcon=5.1407 forget=2.4079 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=8.9932 mle=1.3938 pcon=5.1375 forget=2.4619 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0241 mle=1.4142 pcon=5.1344 forget=2.4754 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=8.7696 mle=1.1862 pcon=5.1316 forget=2.4518 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.0391 mle=1.4612 pcon=5.1291 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 5 it 0 total=8.8318 mle=1.2047 pcon=5.1261 forget=2.5010 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=8.9577 mle=1.3650 pcon=5.1235 forget=2.4693 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.0086 mle=1.4006 pcon=5.1207 forget=2.4873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.0681 mle=1.4809 pcon=5.1179 forget=2.4694 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.0086 mle=1.4524 pcon=5.1153 forget=2.4410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=8.9116 mle=1.3486 pcon=5.1125 forget=2.4505 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=8.9731 mle=1.4295 pcon=5.1099 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=8.9550 mle=1.4046 pcon=5.1073 forget=2.4432 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 6 it 10 total=8.8858 mle=1.2785 pcon=5.1049 forget=2.5024 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=8.9181 mle=1.3785 pcon=5.1025 forget=2.4371 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=8.8931 mle=1.3184 pcon=5.1003 forget=2.4744 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.8889 mle=1.3319 pcon=5.0981 forget=2.4590 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9235 mle=1.3655 pcon=5.0956 forget=2.4625 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=8.9543 mle=1.3830 pcon=5.0935 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 20%|██        | 7/35 [01:15<05:04, 10.86s/it] 23%|██▎       | 8/35 [01:24<04:43, 10.49s/it] 26%|██▌       | 9/35 [01:34<04:24, 10.18s/it] 29%|██▊       | 10/35 [01:43<04:06,  9.88s/it] 31%|███▏      | 11/35 [01:52<03:52,  9.68s/it] 34%|███▍      | 12/35 [02:01<03:38,  9.50s/it] 37%|███▋      | 13/35 [02:11<03:27,  9.43s/it][loss] ep 6 it 310 total=8.8669 mle=1.3535 pcon=5.0915 forget=2.4219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 360 total=8.9530 mle=1.4570 pcon=5.0893 forget=2.4068 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 7 it 20 total=8.7928 mle=1.2501 pcon=5.0872 forget=2.4555 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=8.7893 mle=1.2464 pcon=5.0849 forget=2.4580 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.9121 mle=1.3875 pcon=5.0829 forget=2.4417 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.9198 mle=1.4331 pcon=5.0810 forget=2.4057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=8.8356 mle=1.3211 pcon=5.0794 forget=2.4351 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=8.9745 mle=1.4540 pcon=5.0777 forget=2.4427 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=8.8527 mle=1.3166 pcon=5.0756 forget=2.4606 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=8.9364 mle=1.3708 pcon=5.0736 forget=2.4920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 8 it 30 total=8.9610 mle=1.4804 pcon=5.0719 forget=2.4087 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=8.8051 mle=1.2744 pcon=5.0702 forget=2.4604 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=8.9213 mle=1.3983 pcon=5.0685 forget=2.4545 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=8.8647 mle=1.3447 pcon=5.0668 forget=2.4533 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=8.8877 mle=1.3662 pcon=5.0652 forget=2.4564 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.7836 mle=1.2568 pcon=5.0636 forget=2.4633 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.0221 mle=1.5797 pcon=5.0618 forget=2.3806 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.8764 mle=1.3645 pcon=5.0604 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 9 it 40 total=8.9253 mle=1.4106 pcon=5.0589 forget=2.4558 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=8.9709 mle=1.4929 pcon=5.0575 forget=2.4205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=8.8783 mle=1.3538 pcon=5.0563 forget=2.4682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.7493 mle=1.2282 pcon=5.0550 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=8.9737 mle=1.5026 pcon=5.0540 forget=2.4171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=8.8661 mle=1.3564 pcon=5.0526 forget=2.4571 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.8157 mle=1.3066 pcon=5.0514 forget=2.4577 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 10 it 0 total=8.7872 mle=1.2679 pcon=5.0497 forget=2.4695 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.7534 mle=1.2562 pcon=5.0489 forget=2.4483 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=8.8059 mle=1.3265 pcon=5.0475 forget=2.4319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=8.8731 mle=1.3438 pcon=5.0463 forget=2.4830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=8.9087 mle=1.4083 pcon=5.0453 forget=2.4551 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=8.9165 mle=1.4251 pcon=5.0443 forget=2.4472 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=8.9384 mle=1.5122 pcon=5.0431 forget=2.3830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.7566 mle=1.2607 pcon=5.0423 forget=2.4537 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 11 it 10 total=8.9129 mle=1.4401 pcon=5.0411 forget=2.4318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.8451 mle=1.3726 pcon=5.0402 forget=2.4323 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.8327 mle=1.3444 pcon=5.0394 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.7803 mle=1.2733 pcon=5.0383 forget=2.4688 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.7522 mle=1.2649 pcon=5.0375 forget=2.4498 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.7358 mle=1.2659 pcon=5.0365 forget=2.4333 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.8654 mle=1.3900 pcon=5.0357 forget=2.4398 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.8282 mle=1.3172 pcon=5.0350 forget=2.4760 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 12 it 20 total=8.8001 mle=1.3121 pcon=5.0338 forget=2.4542 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.8389 mle=1.3532 pcon=5.0329 forget=2.4528 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.7898 mle=1.3591 pcon=5.0318 forget=2.3988 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.8282 mle=1.3337 pcon=5.0308 forget=2.4637 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.7263 mle=1.2214 pcon=5.0303 forget=2.4747 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.7412 mle=1.2920 pcon=5.0297 forget=2.4195 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.8774 mle=1.4075 pcon=5.0291 forget=2.4408 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.8374 mle=1.3740 pcon=5.0284 forget=2.4350 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 13 it 30 total=8.7983 mle=1.3111 pcon=5.0278 forget=2.4594 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.9003 mle=1.4847 pcon=5.0271 forget=2.3885 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.7801 mle=1.3406 pcon=5.0261 forget=2.4134 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 40%|████      | 14/35 [02:21<03:23,  9.68s/it] 43%|████▎     | 15/35 [02:32<03:20, 10.01s/it] 46%|████▌     | 16/35 [02:42<03:13, 10.17s/it] 49%|████▊     | 17/35 [02:53<03:04, 10.23s/it] 51%|█████▏    | 18/35 [03:03<02:54, 10.24s/it] 54%|█████▍    | 19/35 [03:13<02:45, 10.35s/it] 57%|█████▋    | 20/35 [03:24<02:34, 10.30s/it][loss] ep 13 it 180 total=8.8251 mle=1.3591 pcon=5.0248 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 230 total=8.6782 mle=1.3008 pcon=5.0239 forget=2.3536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.6661 mle=1.2123 pcon=5.0227 forget=2.4311 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.7469 mle=1.3906 pcon=5.0216 forget=2.3347 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.6115 mle=1.3231 pcon=5.0204 forget=2.2680 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 14 it 40 total=8.6148 mle=1.3182 pcon=5.0187 forget=2.2779 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.5733 mle=1.3554 pcon=5.0172 forget=2.2007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.5295 mle=1.3665 pcon=5.0157 forget=2.1473 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.4592 mle=1.3600 pcon=5.0143 forget=2.0849 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.4940 mle=1.3729 pcon=5.0128 forget=2.1083 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.4097 mle=1.4266 pcon=5.0121 forget=1.9711 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.3743 mle=1.3867 pcon=5.0118 forget=1.9757 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 15 it 0 total=8.4501 mle=1.4730 pcon=5.0117 forget=1.9654 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.3642 mle=1.3817 pcon=5.0123 forget=1.9703 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.3908 mle=1.4699 pcon=5.0133 forget=1.9077 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.4204 mle=1.4733 pcon=5.0144 forget=1.9327 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.3439 mle=1.4014 pcon=5.0157 forget=1.9268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.4183 mle=1.4414 pcon=5.0172 forget=1.9596 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.4593 mle=1.5022 pcon=5.0187 forget=1.9384 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.4681 mle=1.5085 pcon=5.0205 forget=1.9391 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 16 it 10 total=8.4493 mle=1.4317 pcon=5.0222 forget=1.9953 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.4007 mle=1.4135 pcon=5.0243 forget=1.9630 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.4110 mle=1.3801 pcon=5.0262 forget=2.0048 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.4721 mle=1.4083 pcon=5.0283 forget=2.0354 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.4969 mle=1.4169 pcon=5.0305 forget=2.0496 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.7044 mle=1.5772 pcon=5.0323 forget=2.0949 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.6749 mle=1.5728 pcon=5.0342 forget=2.0679 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.6231 mle=1.5232 pcon=5.0361 forget=2.0637 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.5998 mle=1.5500 pcon=5.0383 forget=2.0115 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.6825 mle=1.5565 pcon=5.0405 forget=2.0854 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.5091 mle=1.4158 pcon=5.0421 forget=2.0512 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.6272 mle=1.4745 pcon=5.0438 forget=2.1089 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.6579 mle=1.5702 pcon=5.0453 forget=2.0425 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.6071 mle=1.5324 pcon=5.0467 forget=2.0279 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.5734 mle=1.5247 pcon=5.0481 forget=2.0006 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.5852 mle=1.4851 pcon=5.0491 forget=2.0511 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.5093 mle=1.4393 pcon=5.0501 forget=2.0198 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.6862 mle=1.6282 pcon=5.0509 forget=2.0071 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.6314 mle=1.5458 pcon=5.0517 forget=2.0340 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.4869 mle=1.4995 pcon=5.0522 forget=1.9352 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.5505 mle=1.5085 pcon=5.0523 forget=1.9898 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.5286 mle=1.5216 pcon=5.0523 forget=1.9548 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.5223 mle=1.5158 pcon=5.0522 forget=1.9544 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.4113 mle=1.4404 pcon=5.0522 forget=1.9188 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.3985 mle=1.4532 pcon=5.0517 forget=1.8936 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.4677 mle=1.5447 pcon=5.0510 forget=1.8719 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.3607 mle=1.4079 pcon=5.0502 forget=1.9026 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.3608 mle=1.4650 pcon=5.0493 forget=1.8466 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.3827 mle=1.4564 pcon=5.0482 forget=1.8781 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.4075 mle=1.5401 pcon=5.0476 forget=1.8198 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.3705 mle=1.4714 pcon=5.0468 forget=1.8523 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 20 it 0 total=8.3355 mle=1.4566 pcon=5.0457 forget=1.8332 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.3108 mle=1.4329 pcon=5.0444 forget=1.8336 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.2860 mle=1.4490 pcon=5.0429 forget=1.7941 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.2996 mle=1.4740 pcon=5.0416 forget=1.7840 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.3721 mle=1.5520 pcon=5.0401 forget=1.7800 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 60%|██████    | 21/35 [03:34<02:25, 10.37s/it] 63%|██████▎   | 22/35 [03:45<02:15, 10.40s/it] 66%|██████▌   | 23/35 [03:55<02:05, 10.43s/it] 69%|██████▊   | 24/35 [04:06<01:54, 10.45s/it] 71%|███████▏  | 25/35 [04:16<01:44, 10.44s/it] 74%|███████▍  | 26/35 [04:27<01:34, 10.51s/it] 77%|███████▋  | 27/35 [04:37<01:24, 10.61s/it][loss] ep 20 it 250 total=8.2728 mle=1.4750 pcon=5.0386 forget=1.7592 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=8.3773 mle=1.5922 pcon=5.0373 forget=1.7477 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=8.2884 mle=1.5210 pcon=5.0357 forget=1.7317 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 21 it 10 total=8.1128 mle=1.3672 pcon=5.0337 forget=1.7119 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=8.2490 mle=1.4905 pcon=5.0320 forget=1.7265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.2359 mle=1.4906 pcon=5.0302 forget=1.7151 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.2185 mle=1.4917 pcon=5.0283 forget=1.6985 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=8.1369 mle=1.4094 pcon=5.0264 forget=1.7011 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.2577 mle=1.5569 pcon=5.0248 forget=1.6761 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.2672 mle=1.5228 pcon=5.0228 forget=1.7216 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.1708 mle=1.4571 pcon=5.0211 forget=1.6926 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 22 it 20 total=8.1894 mle=1.4842 pcon=5.0195 forget=1.6857 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.1811 mle=1.4685 pcon=5.0176 forget=1.6950 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.2572 mle=1.5036 pcon=5.0160 forget=1.7376 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.1320 mle=1.4334 pcon=5.0142 forget=1.6844 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.2465 mle=1.5158 pcon=5.0128 forget=1.7179 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.2222 mle=1.4698 pcon=5.0116 forget=1.7408 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.1152 mle=1.4050 pcon=5.0103 forget=1.6999 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=8.3333 mle=1.5948 pcon=5.0089 forget=1.7296 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e35-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 23 it 30 total=8.2398 mle=1.4877 pcon=5.0079 forget=1.7441 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.2346 mle=1.4422 pcon=5.0069 forget=1.7856 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=8.3329 mle=1.5563 pcon=5.0059 forget=1.7706 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.3284 mle=1.5369 pcon=5.0049 forget=1.7866 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.3331 mle=1.5069 pcon=5.0044 forget=1.8218 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=8.4072 mle=1.5843 pcon=5.0040 forget=1.8189 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=8.3500 mle=1.4998 pcon=5.0034 forget=1.8469 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.2959 mle=1.4313 pcon=5.0031 forget=1.8616 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 40 total=8.3524 mle=1.4403 pcon=5.0032 forget=1.9089 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=8.4780 mle=1.5758 pcon=5.0030 forget=1.8992 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.3953 mle=1.4674 pcon=5.0030 forget=1.9249 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.3798 mle=1.4313 pcon=5.0034 forget=1.9450 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=8.4424 mle=1.4846 pcon=5.0040 forget=1.9539 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.4584 mle=1.4955 pcon=5.0044 forget=1.9586 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.4634 mle=1.4798 pcon=5.0047 forget=1.9790 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 0 total=8.4983 mle=1.5222 pcon=5.0053 forget=1.9708 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 50 total=8.5301 mle=1.5415 pcon=5.0059 forget=1.9826 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 100 total=8.5839 mle=1.5481 pcon=5.0066 forget=2.0292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 150 total=8.5808 mle=1.5018 pcon=5.0076 forget=2.0714 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 200 total=8.5947 mle=1.5220 pcon=5.0086 forget=2.0640 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 250 total=8.5865 mle=1.4806 pcon=5.0099 forget=2.0960 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 300 total=8.5930 mle=1.4995 pcon=5.0110 forget=2.0825 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 350 total=8.6878 mle=1.5371 pcon=5.0121 forget=2.1386 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 10 total=8.6475 mle=1.4824 pcon=5.0134 forget=2.1517 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 60 total=8.6619 mle=1.5144 pcon=5.0145 forget=2.1331 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 110 total=8.6688 mle=1.4899 pcon=5.0159 forget=2.1630 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 160 total=8.6260 mle=1.4434 pcon=5.0170 forget=2.1657 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 210 total=8.7155 mle=1.5345 pcon=5.0184 forget=2.1626 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 260 total=8.6911 mle=1.4779 pcon=5.0202 forget=2.1930 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 310 total=8.8699 mle=1.6520 pcon=5.0215 forget=2.1964 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 360 total=8.8541 mle=1.6306 pcon=5.0230 forget=2.2005 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 20 total=8.7535 mle=1.4755 pcon=5.0246 forget=2.2533 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 70 total=8.7731 mle=1.5300 pcon=5.0260 forget=2.2171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 120 total=8.7549 mle=1.4719 pcon=5.0277 forget=2.2552 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 170 total=8.7597 mle=1.4821 pcon=5.0293 forget=2.2483 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 220 total=8.7879 mle=1.4797 pcon=5.0312 forget=2.2771 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 270 total=8.8166 mle=1.4985 pcon=5.0326 forget=2.2854 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 320 total=8.8754 mle=1.5147 pcon=5.0343 forget=2.3265 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 80%|████████  | 28/35 [04:48<01:14, 10.65s/it] 83%|████████▎ | 29/35 [04:59<01:03, 10.62s/it] 86%|████████▌ | 30/35 [05:10<00:53, 10.66s/it] 89%|████████▊ | 31/35 [05:20<00:42, 10.62s/it] 91%|█████████▏| 32/35 [05:30<00:31, 10.53s/it] 94%|█████████▍| 33/35 [05:41<00:20, 10.48s/it] 97%|█████████▋| 34/35 [05:51<00:10, 10.42s/it]100%|██████████| 35/35 [06:01<00:00, 10.33s/it]100%|██████████| 35/35 [06:01<00:00, 10.33s/it]
[loss] ep 27 it 370 total=8.7562 mle=1.3975 pcon=5.0360 forget=2.3227 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 30 total=8.8695 mle=1.4734 pcon=5.0378 forget=2.3583 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 80 total=8.9493 mle=1.6050 pcon=5.0393 forget=2.3050 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 130 total=8.7731 mle=1.4176 pcon=5.0411 forget=2.3145 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 180 total=8.7927 mle=1.3960 pcon=5.0422 forget=2.3545 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 230 total=8.8490 mle=1.4668 pcon=5.0440 forget=2.3382 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 280 total=8.8679 mle=1.4745 pcon=5.0455 forget=2.3478 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 330 total=8.8182 mle=1.4322 pcon=5.0470 forget=2.3390 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 380 total=8.8442 mle=1.4386 pcon=5.0483 forget=2.3574 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 40 total=8.9983 mle=1.5795 pcon=5.0497 forget=2.3692 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 90 total=8.9451 mle=1.4911 pcon=5.0511 forget=2.4029 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 140 total=8.8610 mle=1.4313 pcon=5.0525 forget=2.3772 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 190 total=9.0490 mle=1.5811 pcon=5.0538 forget=2.4140 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 240 total=9.0077 mle=1.5330 pcon=5.0553 forget=2.4194 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 290 total=8.9133 mle=1.4442 pcon=5.0563 forget=2.4128 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 340 total=8.9715 mle=1.4594 pcon=5.0579 forget=2.4542 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 0 total=8.9538 mle=1.4760 pcon=5.0592 forget=2.4186 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 50 total=8.9898 mle=1.5009 pcon=5.0603 forget=2.4286 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 100 total=8.9941 mle=1.4974 pcon=5.0618 forget=2.4349 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 150 total=8.9993 mle=1.4764 pcon=5.0631 forget=2.4598 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 200 total=9.0061 mle=1.4891 pcon=5.0644 forget=2.4526 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 250 total=9.0603 mle=1.5565 pcon=5.0659 forget=2.4378 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 300 total=9.0498 mle=1.5100 pcon=5.0670 forget=2.4729 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 350 total=9.0964 mle=1.5279 pcon=5.0681 forget=2.5004 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 10 total=9.0134 mle=1.4631 pcon=5.0692 forget=2.4811 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 60 total=8.9707 mle=1.4189 pcon=5.0705 forget=2.4813 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 110 total=9.0536 mle=1.5171 pcon=5.0718 forget=2.4647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 160 total=9.0528 mle=1.4579 pcon=5.0728 forget=2.5221 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 210 total=9.1678 mle=1.5909 pcon=5.0738 forget=2.5031 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 260 total=9.0045 mle=1.4302 pcon=5.0749 forget=2.4993 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 310 total=9.0110 mle=1.4273 pcon=5.0759 forget=2.5077 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 360 total=8.9653 mle=1.3825 pcon=5.0774 forget=2.5054 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 20 total=9.0019 mle=1.3967 pcon=5.0789 forget=2.5262 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 70 total=9.0623 mle=1.4313 pcon=5.0803 forget=2.5507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 120 total=9.0026 mle=1.3984 pcon=5.0811 forget=2.5231 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 170 total=9.0644 mle=1.4134 pcon=5.0823 forget=2.5687 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 220 total=9.1362 mle=1.5052 pcon=5.0831 forget=2.5479 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 270 total=9.0452 mle=1.4105 pcon=5.0841 forget=2.5506 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 320 total=9.1283 mle=1.4697 pcon=5.0848 forget=2.5739 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 370 total=9.0247 mle=1.3666 pcon=5.0858 forget=2.5723 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 30 total=9.1949 mle=1.5197 pcon=5.0868 forget=2.5884 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 80 total=9.1877 mle=1.5091 pcon=5.0876 forget=2.5910 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 130 total=9.1011 mle=1.4594 pcon=5.0888 forget=2.5530 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 180 total=9.1810 mle=1.5316 pcon=5.0897 forget=2.5597 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 230 total=9.1320 mle=1.4761 pcon=5.0910 forget=2.5649 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 280 total=9.1274 mle=1.4728 pcon=5.0917 forget=2.5630 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 330 total=9.2391 mle=1.5644 pcon=5.0924 forget=2.5823 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 380 total=9.1160 mle=1.4230 pcon=5.0932 forget=2.5998 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 40 total=9.1748 mle=1.4951 pcon=5.0940 forget=2.5857 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 90 total=9.1683 mle=1.5006 pcon=5.0949 forget=2.5728 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 140 total=9.1835 mle=1.5032 pcon=5.0954 forget=2.5849 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 190 total=9.1467 mle=1.4567 pcon=5.0963 forget=2.5937 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 240 total=9.1817 mle=1.4628 pcon=5.0971 forget=2.6218 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 290 total=9.1045 mle=1.4374 pcon=5.0981 forget=2.5690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 340 total=9.1033 mle=1.3978 pcon=5.0987 forget=2.6068 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[Run] lambda=0.2 lr=0.001 epochs=40 lora_r=8
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=40, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/40 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▎         | 1/40 [00:17<11:03, 17.03s/it]  5%|▌         | 2/40 [00:27<08:14, 13.02s/it]  8%|▊         | 3/40 [00:37<07:08, 11.58s/it] 10%|█         | 4/40 [00:47<06:42, 11.19s/it] 12%|█▎        | 5/40 [00:57<06:17, 10.77s/it] 15%|█▌        | 6/40 [01:08<06:01, 10.65s/it][loss] ep 0 it 0 total=9.0402 mle=1.2805 pcon=5.2951 forget=2.4647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1041 mle=1.3921 pcon=5.2887 forget=2.4233 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.2412 mle=1.5078 pcon=5.2827 forget=2.4506 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2781 mle=1.5644 pcon=5.2770 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2053 mle=1.5331 pcon=5.2708 forget=2.4014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1118 mle=1.3806 pcon=5.2650 forget=2.4662 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.0054 mle=1.2922 pcon=5.2596 forget=2.4536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1731 mle=1.4379 pcon=5.2543 forget=2.4809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 1 it 10 total=9.0164 mle=1.3066 pcon=5.2487 forget=2.4611 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0373 mle=1.3162 pcon=5.2433 forget=2.4779 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.9651 mle=1.2902 pcon=5.2382 forget=2.4367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.9539 mle=1.2379 pcon=5.2331 forget=2.4829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0631 mle=1.3724 pcon=5.2283 forget=2.4623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9880 mle=1.3469 pcon=5.2235 forget=2.4177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9850 mle=1.3082 pcon=5.2185 forget=2.4583 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0553 mle=1.3766 pcon=5.2136 forget=2.4651 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 2 it 20 total=8.9290 mle=1.2746 pcon=5.2089 forget=2.4455 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0969 mle=1.3989 pcon=5.2044 forget=2.4936 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.9870 mle=1.3588 pcon=5.1998 forget=2.4284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9914 mle=1.3581 pcon=5.1954 forget=2.4378 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.9823 mle=1.3299 pcon=5.1909 forget=2.4615 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0215 mle=1.3659 pcon=5.1865 forget=2.4690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.8723 mle=1.2607 pcon=5.1824 forget=2.4291 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.0074 mle=1.3834 pcon=5.1787 forget=2.4453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 3 it 30 total=8.9979 mle=1.3638 pcon=5.1749 forget=2.4592 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9247 mle=1.2959 pcon=5.1710 forget=2.4579 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.8988 mle=1.2904 pcon=5.1672 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.0414 mle=1.4600 pcon=5.1635 forget=2.4179 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9798 mle=1.3523 pcon=5.1601 forget=2.4674 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0521 mle=1.4267 pcon=5.1568 forget=2.4686 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0461 mle=1.4482 pcon=5.1531 forget=2.4448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9153 mle=1.3140 pcon=5.1498 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 4 it 40 total=8.9573 mle=1.3771 pcon=5.1465 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9560 mle=1.3336 pcon=5.1435 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.1340 mle=1.5855 pcon=5.1406 forget=2.4079 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=8.9932 mle=1.3938 pcon=5.1375 forget=2.4619 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0240 mle=1.4142 pcon=5.1344 forget=2.4754 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=8.7697 mle=1.1862 pcon=5.1316 forget=2.4519 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.0390 mle=1.4611 pcon=5.1291 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 5 it 0 total=8.8316 mle=1.2045 pcon=5.1261 forget=2.5010 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=8.9577 mle=1.3649 pcon=5.1235 forget=2.4693 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.0086 mle=1.4006 pcon=5.1207 forget=2.4873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.0681 mle=1.4808 pcon=5.1179 forget=2.4694 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.0086 mle=1.4524 pcon=5.1152 forget=2.4410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=8.9115 mle=1.3485 pcon=5.1125 forget=2.4504 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=8.9730 mle=1.4294 pcon=5.1099 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=8.9550 mle=1.4045 pcon=5.1073 forget=2.4432 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 6 it 10 total=8.8857 mle=1.2784 pcon=5.1049 forget=2.5024 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=8.9181 mle=1.3785 pcon=5.1025 forget=2.4370 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=8.8930 mle=1.3183 pcon=5.1003 forget=2.4744 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.8890 mle=1.3320 pcon=5.0980 forget=2.4590 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9235 mle=1.3655 pcon=5.0956 forget=2.4624 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=8.9544 mle=1.3831 pcon=5.0934 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 18%|█▊        | 7/40 [01:18<05:45, 10.48s/it] 20%|██        | 8/40 [01:29<05:44, 10.78s/it] 22%|██▎       | 9/40 [01:41<05:42, 11.06s/it] 25%|██▌       | 10/40 [01:52<05:31, 11.06s/it] 28%|██▊       | 11/40 [02:03<05:20, 11.06s/it] 30%|███       | 12/40 [02:14<05:09, 11.06s/it] 32%|███▎      | 13/40 [02:25<04:57, 11.01s/it][loss] ep 6 it 310 total=8.8669 mle=1.3536 pcon=5.0915 forget=2.4219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 360 total=8.9529 mle=1.4569 pcon=5.0893 forget=2.4067 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 7 it 20 total=8.7927 mle=1.2501 pcon=5.0872 forget=2.4555 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=8.7892 mle=1.2463 pcon=5.0849 forget=2.4580 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.9121 mle=1.3875 pcon=5.0829 forget=2.4417 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.9197 mle=1.4330 pcon=5.0810 forget=2.4057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=8.8357 mle=1.3211 pcon=5.0794 forget=2.4352 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=8.9747 mle=1.4542 pcon=5.0777 forget=2.4427 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=8.8527 mle=1.3166 pcon=5.0756 forget=2.4606 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=8.9364 mle=1.3709 pcon=5.0736 forget=2.4920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 8 it 30 total=8.9612 mle=1.4806 pcon=5.0719 forget=2.4087 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=8.8050 mle=1.2745 pcon=5.0702 forget=2.4604 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=8.9213 mle=1.3983 pcon=5.0685 forget=2.4545 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=8.8647 mle=1.3447 pcon=5.0668 forget=2.4532 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=8.8877 mle=1.3662 pcon=5.0652 forget=2.4564 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.7837 mle=1.2569 pcon=5.0636 forget=2.4633 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.0222 mle=1.5799 pcon=5.0618 forget=2.3806 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.8762 mle=1.3644 pcon=5.0604 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 9 it 40 total=8.9253 mle=1.4107 pcon=5.0589 forget=2.4558 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=8.9709 mle=1.4929 pcon=5.0575 forget=2.4205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=8.8783 mle=1.3537 pcon=5.0563 forget=2.4682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.7493 mle=1.2282 pcon=5.0550 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=8.9737 mle=1.5025 pcon=5.0540 forget=2.4172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=8.8661 mle=1.3564 pcon=5.0526 forget=2.4571 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.8156 mle=1.3065 pcon=5.0514 forget=2.4577 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 10 it 0 total=8.7873 mle=1.2680 pcon=5.0497 forget=2.4695 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.7535 mle=1.2564 pcon=5.0489 forget=2.4483 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=8.8059 mle=1.3266 pcon=5.0475 forget=2.4319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=8.8732 mle=1.3438 pcon=5.0463 forget=2.4831 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=8.9086 mle=1.4082 pcon=5.0453 forget=2.4550 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=8.9166 mle=1.4250 pcon=5.0443 forget=2.4472 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=8.9384 mle=1.5122 pcon=5.0431 forget=2.3830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.7567 mle=1.2607 pcon=5.0423 forget=2.4537 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 11 it 10 total=8.9130 mle=1.4402 pcon=5.0411 forget=2.4318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.8450 mle=1.3725 pcon=5.0402 forget=2.4323 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.8328 mle=1.3446 pcon=5.0394 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.7802 mle=1.2731 pcon=5.0383 forget=2.4688 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.7522 mle=1.2649 pcon=5.0375 forget=2.4498 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.7357 mle=1.2659 pcon=5.0365 forget=2.4333 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.8654 mle=1.3900 pcon=5.0357 forget=2.4397 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.8281 mle=1.3171 pcon=5.0350 forget=2.4760 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 12 it 20 total=8.8001 mle=1.3121 pcon=5.0338 forget=2.4542 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.8388 mle=1.3533 pcon=5.0329 forget=2.4527 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.7895 mle=1.3590 pcon=5.0318 forget=2.3988 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.8280 mle=1.3336 pcon=5.0308 forget=2.4636 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.7262 mle=1.2212 pcon=5.0303 forget=2.4747 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.7408 mle=1.2919 pcon=5.0297 forget=2.4193 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.8771 mle=1.4075 pcon=5.0291 forget=2.4405 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.8370 mle=1.3738 pcon=5.0284 forget=2.4348 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 13 it 30 total=8.7977 mle=1.3110 pcon=5.0278 forget=2.4588 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.8978 mle=1.4830 pcon=5.0271 forget=2.3877 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.7788 mle=1.3405 pcon=5.0261 forget=2.4122 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 35%|███▌      | 14/40 [02:36<04:46, 11.03s/it] 38%|███▊      | 15/40 [02:47<04:38, 11.14s/it] 40%|████      | 16/40 [02:59<04:30, 11.28s/it] 42%|████▎     | 17/40 [03:10<04:15, 11.13s/it] 45%|████▌     | 18/40 [03:21<04:03, 11.06s/it] 48%|████▊     | 19/40 [03:32<03:54, 11.18s/it] 50%|█████     | 20/40 [03:44<03:45, 11.26s/it][loss] ep 13 it 180 total=8.8235 mle=1.3596 pcon=5.0248 forget=2.4391 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 230 total=8.6747 mle=1.3015 pcon=5.0238 forget=2.3494 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.6618 mle=1.2128 pcon=5.0226 forget=2.4264 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.7402 mle=1.3918 pcon=5.0215 forget=2.3269 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.6005 mle=1.3238 pcon=5.0202 forget=2.2565 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 14 it 40 total=8.6077 mle=1.3244 pcon=5.0185 forget=2.2648 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.5640 mle=1.3607 pcon=5.0171 forget=2.1863 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.5192 mle=1.3725 pcon=5.0155 forget=2.1311 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.4467 mle=1.3637 pcon=5.0142 forget=2.0688 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.4813 mle=1.3770 pcon=5.0128 forget=2.0915 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.4001 mle=1.4308 pcon=5.0122 forget=1.9570 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.3664 mle=1.3901 pcon=5.0121 forget=1.9642 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 15 it 0 total=8.4392 mle=1.4737 pcon=5.0121 forget=1.9534 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.3590 mle=1.3840 pcon=5.0128 forget=1.9622 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.3847 mle=1.4709 pcon=5.0139 forget=1.8999 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.4179 mle=1.4750 pcon=5.0151 forget=1.9278 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.3393 mle=1.4037 pcon=5.0165 forget=1.9191 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.4163 mle=1.4433 pcon=5.0180 forget=1.9551 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.4594 mle=1.5043 pcon=5.0195 forget=1.9356 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.4671 mle=1.5101 pcon=5.0213 forget=1.9356 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 16 it 10 total=8.4480 mle=1.4323 pcon=5.0231 forget=1.9926 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.4003 mle=1.4157 pcon=5.0251 forget=1.9595 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.4101 mle=1.3825 pcon=5.0270 forget=2.0006 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.4695 mle=1.4100 pcon=5.0292 forget=2.0303 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.4927 mle=1.4187 pcon=5.0312 forget=2.0427 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.6983 mle=1.5782 pcon=5.0331 forget=2.0870 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.6663 mle=1.5733 pcon=5.0348 forget=2.0581 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.6128 mle=1.5245 pcon=5.0367 forget=2.0516 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.5883 mle=1.5515 pcon=5.0388 forget=1.9980 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.6669 mle=1.5559 pcon=5.0410 forget=2.0700 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.4929 mle=1.4174 pcon=5.0424 forget=2.0331 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.6056 mle=1.4734 pcon=5.0441 forget=2.0882 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.6345 mle=1.5698 pcon=5.0454 forget=2.0193 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.5824 mle=1.5333 pcon=5.0468 forget=2.0024 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.5422 mle=1.5228 pcon=5.0480 forget=1.9713 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.5535 mle=1.4835 pcon=5.0488 forget=2.0213 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.4742 mle=1.4390 pcon=5.0496 forget=1.9856 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.6517 mle=1.6293 pcon=5.0503 forget=1.9721 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.5943 mle=1.5450 pcon=5.0508 forget=1.9984 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.4440 mle=1.5000 pcon=5.0511 forget=1.8929 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.5069 mle=1.5061 pcon=5.0510 forget=1.9498 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.4839 mle=1.5211 pcon=5.0508 forget=1.9120 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.4803 mle=1.5159 pcon=5.0505 forget=1.9139 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.3660 mle=1.4390 pcon=5.0503 forget=1.8767 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.3518 mle=1.4521 pcon=5.0497 forget=1.8500 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.4231 mle=1.5422 pcon=5.0488 forget=1.8321 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.3121 mle=1.4064 pcon=5.0478 forget=1.8579 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.3203 mle=1.4692 pcon=5.0467 forget=1.8044 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.3315 mle=1.4521 pcon=5.0455 forget=1.8339 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.3576 mle=1.5414 pcon=5.0447 forget=1.7715 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.3217 mle=1.4722 pcon=5.0438 forget=1.8057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 20 it 0 total=8.2831 mle=1.4571 pcon=5.0426 forget=1.7834 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.2540 mle=1.4336 pcon=5.0412 forget=1.7792 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.2275 mle=1.4486 pcon=5.0396 forget=1.7392 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.2417 mle=1.4760 pcon=5.0381 forget=1.7276 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.3174 mle=1.5552 pcon=5.0366 forget=1.7255 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 52%|█████▎    | 21/40 [03:55<03:33, 11.23s/it] 55%|█████▌    | 22/40 [04:07<03:27, 11.53s/it] 57%|█████▊    | 23/40 [04:18<03:14, 11.46s/it] 60%|██████    | 24/40 [04:30<03:06, 11.68s/it] 62%|██████▎   | 25/40 [04:42<02:52, 11.53s/it] 65%|██████▌   | 26/40 [04:54<02:45, 11.81s/it] 68%|██████▊   | 27/40 [05:07<02:38, 12.23s/it] 70%|███████   | 28/40 [05:20<02:30, 12.51s/it][loss] ep 20 it 250 total=8.2184 mle=1.4798 pcon=5.0350 forget=1.7036 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=8.3258 mle=1.6045 pcon=5.0336 forget=1.6877 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=8.2347 mle=1.5264 pcon=5.0319 forget=1.6763 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 21 it 10 total=8.0565 mle=1.3746 pcon=5.0299 forget=1.6519 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=8.1898 mle=1.4974 pcon=5.0281 forget=1.6643 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.1901 mle=1.4973 pcon=5.0263 forget=1.6665 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.1712 mle=1.4944 pcon=5.0244 forget=1.6524 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=8.0927 mle=1.4134 pcon=5.0225 forget=1.6568 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.2181 mle=1.5655 pcon=5.0209 forget=1.6317 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.2251 mle=1.5234 pcon=5.0190 forget=1.6828 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.1331 mle=1.4689 pcon=5.0174 forget=1.6468 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e40-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 22 it 20 total=8.1543 mle=1.4889 pcon=5.0159 forget=1.6496 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.1430 mle=1.4666 pcon=5.0141 forget=1.6623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.2296 mle=1.5111 pcon=5.0126 forget=1.7059 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.1168 mle=1.4480 pcon=5.0110 forget=1.6578 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.2253 mle=1.5245 pcon=5.0098 forget=1.6910 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.1963 mle=1.4735 pcon=5.0087 forget=1.7141 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.0982 mle=1.4101 pcon=5.0077 forget=1.6805 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=8.3022 mle=1.5912 pcon=5.0065 forget=1.7045 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 30 total=8.2288 mle=1.4983 pcon=5.0058 forget=1.7247 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.2193 mle=1.4552 pcon=5.0050 forget=1.7592 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=8.3201 mle=1.5647 pcon=5.0043 forget=1.7511 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.3149 mle=1.5406 pcon=5.0036 forget=1.7707 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.3181 mle=1.5109 pcon=5.0034 forget=1.8037 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=8.3917 mle=1.5900 pcon=5.0033 forget=1.7985 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=8.3369 mle=1.5080 pcon=5.0029 forget=1.8259 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.2863 mle=1.4429 pcon=5.0030 forget=1.8404 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 40 total=8.3346 mle=1.4441 pcon=5.0033 forget=1.8872 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=8.4722 mle=1.5925 pcon=5.0034 forget=1.8763 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.3722 mle=1.4719 pcon=5.0036 forget=1.8967 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.3637 mle=1.4434 pcon=5.0043 forget=1.9160 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=8.4244 mle=1.4993 pcon=5.0050 forget=1.9201 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.4308 mle=1.4996 pcon=5.0055 forget=1.9257 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.4326 mle=1.4852 pcon=5.0060 forget=1.9415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 0 total=8.4678 mle=1.5258 pcon=5.0067 forget=1.9353 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 50 total=8.5050 mle=1.5524 pcon=5.0074 forget=1.9453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 100 total=8.5445 mle=1.5544 pcon=5.0081 forget=1.9820 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 150 total=8.5289 mle=1.5013 pcon=5.0091 forget=2.0185 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 200 total=8.5554 mle=1.5311 pcon=5.0102 forget=2.0142 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 250 total=8.5340 mle=1.4857 pcon=5.0113 forget=2.0370 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 300 total=8.5451 mle=1.5074 pcon=5.0124 forget=2.0253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 350 total=8.6176 mle=1.5401 pcon=5.0134 forget=2.0642 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 10 total=8.5743 mle=1.4867 pcon=5.0145 forget=2.0731 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 60 total=8.5831 mle=1.5143 pcon=5.0154 forget=2.0534 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 110 total=8.5954 mle=1.4952 pcon=5.0165 forget=2.0837 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 160 total=8.5401 mle=1.4447 pcon=5.0174 forget=2.0781 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 210 total=8.6310 mle=1.5344 pcon=5.0185 forget=2.0782 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 260 total=8.5912 mle=1.4760 pcon=5.0199 forget=2.0952 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 310 total=8.7472 mle=1.6335 pcon=5.0209 forget=2.0928 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 360 total=8.7419 mle=1.6276 pcon=5.0220 forget=2.0922 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 20 total=8.6303 mle=1.4753 pcon=5.0232 forget=2.1317 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 70 total=8.6510 mle=1.5261 pcon=5.0242 forget=2.1007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 120 total=8.6221 mle=1.4686 pcon=5.0254 forget=2.1280 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 170 total=8.6212 mle=1.4755 pcon=5.0266 forget=2.1190 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 220 total=8.6449 mle=1.4793 pcon=5.0280 forget=2.1376 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 270 total=8.6813 mle=1.5055 pcon=5.0290 forget=2.1468 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 320 total=8.7115 mle=1.5087 pcon=5.0301 forget=2.1727 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 370 total=8.5883 mle=1.3894 pcon=5.0312 forget=2.1677 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 30 total=8.6944 mle=1.4694 pcon=5.0325 forget=2.1925 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 72%|███████▎  | 29/40 [05:32<02:14, 12.24s/it] 75%|███████▌  | 30/40 [05:44<02:00, 12.02s/it] 78%|███████▊  | 31/40 [05:55<01:45, 11.73s/it] 80%|████████  | 32/40 [06:07<01:34, 11.82s/it] 82%|████████▎ | 33/40 [06:19<01:22, 11.83s/it] 85%|████████▌ | 34/40 [06:30<01:10, 11.72s/it] 88%|████████▊ | 35/40 [06:42<00:58, 11.67s/it] 90%|█████████ | 36/40 [06:53<00:46, 11.68s/it][loss] ep 28 it 80 total=8.7563 mle=1.5806 pcon=5.0334 forget=2.1423 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 130 total=8.6006 mle=1.4112 pcon=5.0346 forget=2.1548 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 180 total=8.6078 mle=1.3872 pcon=5.0352 forget=2.1854 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 230 total=8.6665 mle=1.4602 pcon=5.0364 forget=2.1699 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 280 total=8.6883 mle=1.4669 pcon=5.0373 forget=2.1842 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 330 total=8.6129 mle=1.4107 pcon=5.0381 forget=2.1641 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 380 total=8.6421 mle=1.4307 pcon=5.0387 forget=2.1726 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 40 total=8.7981 mle=1.5682 pcon=5.0395 forget=2.1904 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 90 total=8.7248 mle=1.4813 pcon=5.0402 forget=2.2033 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 140 total=8.6218 mle=1.4032 pcon=5.0410 forget=2.1776 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 190 total=8.8094 mle=1.5504 pcon=5.0415 forget=2.2175 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 240 total=8.7873 mle=1.5251 pcon=5.0424 forget=2.2199 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 290 total=8.6927 mle=1.4246 pcon=5.0426 forget=2.2254 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 340 total=8.7265 mle=1.4462 pcon=5.0434 forget=2.2369 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 0 total=8.7026 mle=1.4428 pcon=5.0440 forget=2.2158 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 50 total=8.7621 mle=1.4960 pcon=5.0444 forget=2.2218 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 100 total=8.7601 mle=1.4816 pcon=5.0451 forget=2.2334 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 150 total=8.7448 mle=1.4460 pcon=5.0456 forget=2.2532 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 200 total=8.7471 mle=1.4533 pcon=5.0462 forget=2.2476 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 250 total=8.7965 mle=1.5272 pcon=5.0469 forget=2.2224 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 300 total=8.7904 mle=1.4906 pcon=5.0471 forget=2.2527 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 350 total=8.8508 mle=1.5152 pcon=5.0474 forget=2.2882 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 10 total=8.7670 mle=1.4662 pcon=5.0477 forget=2.2531 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 60 total=8.7032 mle=1.3909 pcon=5.0482 forget=2.2641 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 110 total=8.7851 mle=1.4839 pcon=5.0486 forget=2.2526 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 160 total=8.7931 mle=1.4364 pcon=5.0487 forget=2.3080 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 210 total=8.9258 mle=1.5849 pcon=5.0488 forget=2.2922 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 260 total=8.7463 mle=1.4007 pcon=5.0491 forget=2.2966 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 310 total=8.7410 mle=1.4099 pcon=5.0491 forget=2.2820 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 360 total=8.7207 mle=1.3807 pcon=5.0496 forget=2.2904 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 20 total=8.7339 mle=1.3604 pcon=5.0502 forget=2.3233 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 70 total=8.7908 mle=1.4094 pcon=5.0506 forget=2.3308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 120 total=8.7360 mle=1.3736 pcon=5.0504 forget=2.3120 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 170 total=8.7795 mle=1.3740 pcon=5.0506 forget=2.3550 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 220 total=8.8457 mle=1.4645 pcon=5.0504 forget=2.3308 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 270 total=8.7724 mle=1.3848 pcon=5.0504 forget=2.3373 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 320 total=8.8642 mle=1.4312 pcon=5.0500 forget=2.3830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 370 total=8.7371 mle=1.3283 pcon=5.0501 forget=2.3587 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 30 total=8.8948 mle=1.4743 pcon=5.0500 forget=2.3705 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 80 total=8.8965 mle=1.4589 pcon=5.0498 forget=2.3878 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 130 total=8.8317 mle=1.4245 pcon=5.0498 forget=2.3574 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 180 total=8.9092 mle=1.4905 pcon=5.0496 forget=2.3691 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 230 total=8.8494 mle=1.4358 pcon=5.0498 forget=2.3638 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 280 total=8.8520 mle=1.4291 pcon=5.0493 forget=2.3736 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 330 total=9.0031 mle=1.5336 pcon=5.0489 forget=2.4206 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 380 total=8.8131 mle=1.3518 pcon=5.0487 forget=2.4126 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 40 total=8.9096 mle=1.4512 pcon=5.0483 forget=2.4102 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 90 total=8.8860 mle=1.4367 pcon=5.0480 forget=2.4012 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 140 total=8.9466 mle=1.4598 pcon=5.0475 forget=2.4393 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 190 total=8.8618 mle=1.3907 pcon=5.0472 forget=2.4239 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 240 total=8.9379 mle=1.4266 pcon=5.0469 forget=2.4644 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 290 total=8.8511 mle=1.3904 pcon=5.0467 forget=2.4140 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 340 total=8.8569 mle=1.3516 pcon=5.0462 forget=2.4591 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 0 total=8.8478 mle=1.3339 pcon=5.0461 forget=2.4679 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 50 total=8.8911 mle=1.3840 pcon=5.0461 forget=2.4610 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 100 total=8.8975 mle=1.3982 pcon=5.0461 forget=2.4533 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 150 total=8.8490 mle=1.3299 pcon=5.0456 forget=2.4736 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 200 total=8.8871 mle=1.3959 pcon=5.0452 forget=2.4459 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 250 total=8.9697 mle=1.4230 pcon=5.0452 forget=2.5015 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 300 total=8.9813 mle=1.4220 pcon=5.0449 forget=2.5144 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 350 total=9.0296 mle=1.4787 pcon=5.0446 forget=2.5064 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 10 total=8.9766 mle=1.4633 pcon=5.0443 forget=2.4690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 92%|█████████▎| 37/40 [07:05<00:34, 11.66s/it] 95%|█████████▌| 38/40 [07:17<00:23, 11.74s/it] 98%|█████████▊| 39/40 [07:28<00:11, 11.64s/it]100%|██████████| 40/40 [07:40<00:00, 11.80s/it]100%|██████████| 40/40 [07:40<00:00, 11.52s/it]
[loss] ep 36 it 60 total=8.9799 mle=1.4358 pcon=5.0440 forget=2.5001 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 110 total=9.0698 mle=1.5439 pcon=5.0439 forget=2.4820 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 160 total=8.9532 mle=1.4287 pcon=5.0438 forget=2.4807 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 210 total=9.0641 mle=1.5408 pcon=5.0434 forget=2.4799 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 260 total=9.0243 mle=1.4895 pcon=5.0432 forget=2.4917 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 310 total=8.8700 mle=1.3151 pcon=5.0432 forget=2.5117 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 360 total=8.9073 mle=1.3557 pcon=5.0436 forget=2.5079 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 20 total=8.9953 mle=1.4123 pcon=5.0434 forget=2.5395 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 70 total=9.0165 mle=1.4541 pcon=5.0434 forget=2.5190 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 120 total=8.9522 mle=1.3990 pcon=5.0430 forget=2.5102 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 170 total=8.9750 mle=1.3772 pcon=5.0433 forget=2.5545 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 220 total=8.9392 mle=1.3848 pcon=5.0430 forget=2.5114 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 270 total=8.9086 mle=1.3607 pcon=5.0431 forget=2.5049 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 320 total=8.9619 mle=1.3755 pcon=5.0431 forget=2.5433 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 370 total=8.9176 mle=1.3523 pcon=5.0432 forget=2.5222 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 30 total=8.9211 mle=1.3275 pcon=5.0433 forget=2.5504 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 80 total=8.9714 mle=1.3824 pcon=5.0432 forget=2.5459 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 130 total=8.9056 mle=1.3474 pcon=5.0431 forget=2.5151 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 180 total=8.9216 mle=1.3286 pcon=5.0429 forget=2.5501 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 230 total=8.9754 mle=1.4062 pcon=5.0430 forget=2.5261 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 280 total=9.0533 mle=1.4806 pcon=5.0432 forget=2.5295 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 330 total=9.1647 mle=1.5842 pcon=5.0432 forget=2.5373 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 380 total=8.9879 mle=1.4133 pcon=5.0430 forget=2.5316 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 40 total=8.9795 mle=1.3966 pcon=5.0430 forget=2.5399 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 90 total=9.0316 mle=1.4209 pcon=5.0430 forget=2.5677 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 140 total=9.0371 mle=1.4682 pcon=5.0432 forget=2.5257 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 190 total=8.9497 mle=1.3485 pcon=5.0432 forget=2.5581 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 240 total=9.0516 mle=1.4269 pcon=5.0434 forget=2.5814 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 290 total=9.0269 mle=1.4009 pcon=5.0434 forget=2.5826 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 340 total=9.0154 mle=1.4056 pcon=5.0437 forget=2.5662 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[Run] lambda=0.2 lr=0.001 epochs=45 lora_r=8
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=45, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/45 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/45 [00:16<11:58, 16.33s/it]  4%|▍         | 2/45 [00:27<09:21, 13.07s/it]  7%|▋         | 3/45 [00:38<08:34, 12.25s/it]  9%|▉         | 4/45 [00:51<08:36, 12.59s/it] 11%|█         | 5/45 [01:03<08:12, 12.31s/it] 13%|█▎        | 6/45 [01:14<07:41, 11.84s/it][loss] ep 0 it 0 total=9.0402 mle=1.2804 pcon=5.2951 forget=2.4647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1040 mle=1.3921 pcon=5.2887 forget=2.4232 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.2411 mle=1.5077 pcon=5.2827 forget=2.4507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2781 mle=1.5645 pcon=5.2770 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2052 mle=1.5330 pcon=5.2708 forget=2.4014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1116 mle=1.3805 pcon=5.2650 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.0053 mle=1.2921 pcon=5.2596 forget=2.4536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1729 mle=1.4377 pcon=5.2543 forget=2.4809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 1 it 10 total=9.0162 mle=1.3065 pcon=5.2487 forget=2.4610 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0372 mle=1.3161 pcon=5.2433 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.9650 mle=1.2900 pcon=5.2382 forget=2.4367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.9540 mle=1.2380 pcon=5.2331 forget=2.4829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0632 mle=1.3725 pcon=5.2283 forget=2.4624 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9879 mle=1.3468 pcon=5.2235 forget=2.4177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9850 mle=1.3082 pcon=5.2185 forget=2.4583 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0555 mle=1.3767 pcon=5.2136 forget=2.4651 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 2 it 20 total=8.9289 mle=1.2745 pcon=5.2089 forget=2.4454 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0968 mle=1.3988 pcon=5.2044 forget=2.4937 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.9871 mle=1.3589 pcon=5.1998 forget=2.4284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9912 mle=1.3579 pcon=5.1954 forget=2.4378 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.9822 mle=1.3299 pcon=5.1908 forget=2.4615 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0215 mle=1.3659 pcon=5.1865 forget=2.4690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.8723 mle=1.2607 pcon=5.1824 forget=2.4292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.0074 mle=1.3835 pcon=5.1787 forget=2.4453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 3 it 30 total=8.9980 mle=1.3639 pcon=5.1748 forget=2.4592 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9245 mle=1.2957 pcon=5.1710 forget=2.4579 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.8990 mle=1.2905 pcon=5.1672 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.0415 mle=1.4601 pcon=5.1635 forget=2.4179 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9797 mle=1.3522 pcon=5.1601 forget=2.4674 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0520 mle=1.4266 pcon=5.1568 forget=2.4686 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0461 mle=1.4481 pcon=5.1531 forget=2.4448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9152 mle=1.3139 pcon=5.1497 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 4 it 40 total=8.9572 mle=1.3770 pcon=5.1465 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9559 mle=1.3335 pcon=5.1435 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.1337 mle=1.5851 pcon=5.1406 forget=2.4079 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=8.9935 mle=1.3941 pcon=5.1375 forget=2.4620 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0243 mle=1.4145 pcon=5.1344 forget=2.4754 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=8.7696 mle=1.1861 pcon=5.1316 forget=2.4519 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.0393 mle=1.4613 pcon=5.1291 forget=2.4489 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 5 it 0 total=8.8317 mle=1.2046 pcon=5.1261 forget=2.5010 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=8.9578 mle=1.3652 pcon=5.1234 forget=2.4692 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.0086 mle=1.4006 pcon=5.1207 forget=2.4873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.0681 mle=1.4808 pcon=5.1179 forget=2.4694 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.0086 mle=1.4524 pcon=5.1152 forget=2.4410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=8.9115 mle=1.3485 pcon=5.1125 forget=2.4505 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=8.9732 mle=1.4296 pcon=5.1099 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=8.9548 mle=1.4043 pcon=5.1073 forget=2.4432 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 6 it 10 total=8.8857 mle=1.2784 pcon=5.1049 forget=2.5025 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=8.9183 mle=1.3787 pcon=5.1025 forget=2.4371 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=8.8930 mle=1.3183 pcon=5.1003 forget=2.4744 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.8888 mle=1.3319 pcon=5.0980 forget=2.4589 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9233 mle=1.3653 pcon=5.0956 forget=2.4624 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=8.9543 mle=1.3830 pcon=5.0934 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 16%|█▌        | 7/45 [01:25<07:27, 11.77s/it] 18%|█▊        | 8/45 [01:36<07:02, 11.41s/it] 20%|██        | 9/45 [01:47<06:45, 11.27s/it] 22%|██▏       | 10/45 [01:58<06:33, 11.26s/it] 24%|██▍       | 11/45 [02:10<06:25, 11.34s/it] 27%|██▋       | 12/45 [02:21<06:14, 11.34s/it] 29%|██▉       | 13/45 [02:32<06:00, 11.25s/it][loss] ep 6 it 310 total=8.8670 mle=1.3536 pcon=5.0915 forget=2.4219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 360 total=8.9530 mle=1.4570 pcon=5.0893 forget=2.4067 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 7 it 20 total=8.7926 mle=1.2499 pcon=5.0872 forget=2.4554 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=8.7895 mle=1.2465 pcon=5.0849 forget=2.4580 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.9121 mle=1.3874 pcon=5.0829 forget=2.4418 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.9198 mle=1.4331 pcon=5.0810 forget=2.4057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=8.8356 mle=1.3211 pcon=5.0794 forget=2.4352 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=8.9746 mle=1.4542 pcon=5.0777 forget=2.4427 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=8.8529 mle=1.3167 pcon=5.0756 forget=2.4606 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=8.9365 mle=1.3709 pcon=5.0736 forget=2.4920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 8 it 30 total=8.9611 mle=1.4806 pcon=5.0719 forget=2.4087 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=8.8050 mle=1.2744 pcon=5.0702 forget=2.4604 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=8.9213 mle=1.3984 pcon=5.0685 forget=2.4544 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=8.8647 mle=1.3447 pcon=5.0668 forget=2.4532 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=8.8877 mle=1.3661 pcon=5.0652 forget=2.4563 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.7838 mle=1.2570 pcon=5.0636 forget=2.4632 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.0220 mle=1.5797 pcon=5.0618 forget=2.3805 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.8765 mle=1.3647 pcon=5.0604 forget=2.4514 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 9 it 40 total=8.9252 mle=1.4105 pcon=5.0589 forget=2.4557 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=8.9710 mle=1.4930 pcon=5.0575 forget=2.4206 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=8.8783 mle=1.3537 pcon=5.0563 forget=2.4682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.7493 mle=1.2282 pcon=5.0550 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=8.9739 mle=1.5027 pcon=5.0540 forget=2.4172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=8.8660 mle=1.3564 pcon=5.0526 forget=2.4570 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.8156 mle=1.3065 pcon=5.0514 forget=2.4577 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 10 it 0 total=8.7871 mle=1.2678 pcon=5.0497 forget=2.4696 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.7535 mle=1.2564 pcon=5.0489 forget=2.4483 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=8.8060 mle=1.3267 pcon=5.0475 forget=2.4318 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=8.8732 mle=1.3438 pcon=5.0463 forget=2.4830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=8.9086 mle=1.4083 pcon=5.0453 forget=2.4551 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=8.9165 mle=1.4251 pcon=5.0443 forget=2.4472 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=8.9383 mle=1.5122 pcon=5.0431 forget=2.3830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.7568 mle=1.2609 pcon=5.0423 forget=2.4536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 11 it 10 total=8.9130 mle=1.4402 pcon=5.0411 forget=2.4317 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.8451 mle=1.3726 pcon=5.0402 forget=2.4322 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.8326 mle=1.3445 pcon=5.0394 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.7802 mle=1.2732 pcon=5.0383 forget=2.4687 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.7524 mle=1.2651 pcon=5.0374 forget=2.4499 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.7355 mle=1.2657 pcon=5.0365 forget=2.4333 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.8653 mle=1.3898 pcon=5.0357 forget=2.4398 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.8282 mle=1.3173 pcon=5.0350 forget=2.4759 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 12 it 20 total=8.8001 mle=1.3121 pcon=5.0338 forget=2.4542 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.8387 mle=1.3531 pcon=5.0328 forget=2.4528 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.7895 mle=1.3590 pcon=5.0318 forget=2.3987 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.8278 mle=1.3336 pcon=5.0308 forget=2.4635 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.7261 mle=1.2213 pcon=5.0302 forget=2.4746 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.7408 mle=1.2920 pcon=5.0297 forget=2.4191 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.8767 mle=1.4072 pcon=5.0290 forget=2.4405 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.8368 mle=1.3738 pcon=5.0283 forget=2.4346 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 13 it 30 total=8.7975 mle=1.3111 pcon=5.0278 forget=2.4585 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.8993 mle=1.4851 pcon=5.0270 forget=2.3872 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.7779 mle=1.3403 pcon=5.0261 forget=2.4115 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 31%|███       | 14/45 [02:42<05:38, 10.91s/it] 33%|███▎      | 15/45 [02:52<05:20, 10.67s/it] 36%|███▌      | 16/45 [03:03<05:09, 10.68s/it] 38%|███▊      | 17/45 [03:13<04:55, 10.56s/it] 40%|████      | 18/45 [03:23<04:38, 10.31s/it] 42%|████▏     | 19/45 [03:33<04:24, 10.18s/it] 44%|████▍     | 20/45 [03:43<04:10, 10.02s/it][loss] ep 13 it 180 total=8.8224 mle=1.3599 pcon=5.0248 forget=2.4377 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 230 total=8.6727 mle=1.3021 pcon=5.0238 forget=2.3468 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.6591 mle=1.2133 pcon=5.0226 forget=2.4232 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.7363 mle=1.3928 pcon=5.0215 forget=2.3221 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.5959 mle=1.3266 pcon=5.0201 forget=2.2491 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 14 it 40 total=8.6031 mle=1.3282 pcon=5.0184 forget=2.2565 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.5588 mle=1.3642 pcon=5.0169 forget=2.1777 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.5143 mle=1.3776 pcon=5.0154 forget=2.1213 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.4393 mle=1.3660 pcon=5.0141 forget=2.0591 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.4739 mle=1.3793 pcon=5.0128 forget=2.0818 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.3949 mle=1.4335 pcon=5.0123 forget=1.9491 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.3616 mle=1.3919 pcon=5.0123 forget=1.9574 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 15 it 0 total=8.4329 mle=1.4742 pcon=5.0124 forget=1.9463 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.3561 mle=1.3853 pcon=5.0131 forget=1.9577 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.3817 mle=1.4715 pcon=5.0143 forget=1.8959 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.4168 mle=1.4760 pcon=5.0155 forget=1.9253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.3373 mle=1.4052 pcon=5.0169 forget=1.9152 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.4158 mle=1.4445 pcon=5.0184 forget=1.9529 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.4596 mle=1.5054 pcon=5.0200 forget=1.9342 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.4664 mle=1.5109 pcon=5.0218 forget=1.9337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 16 it 10 total=8.4472 mle=1.4326 pcon=5.0235 forget=1.9910 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.4000 mle=1.4172 pcon=5.0256 forget=1.9573 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.4093 mle=1.3838 pcon=5.0274 forget=1.9981 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.4671 mle=1.4104 pcon=5.0296 forget=2.0271 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.4901 mle=1.4199 pcon=5.0317 forget=2.0384 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.6946 mle=1.5790 pcon=5.0335 forget=2.0821 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.6609 mle=1.5738 pcon=5.0352 forget=2.0520 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.6066 mle=1.5256 pcon=5.0370 forget=2.0440 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.5811 mle=1.5525 pcon=5.0391 forget=1.9895 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.6575 mle=1.5559 pcon=5.0412 forget=2.0604 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.4826 mle=1.4183 pcon=5.0426 forget=2.0217 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.5931 mle=1.4735 pcon=5.0441 forget=2.0755 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.6208 mle=1.5704 pcon=5.0454 forget=2.0050 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.5675 mle=1.5339 pcon=5.0467 forget=1.9869 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.5230 mle=1.5215 pcon=5.0478 forget=1.9536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.5353 mle=1.4832 pcon=5.0485 forget=2.0035 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.4521 mle=1.4373 pcon=5.0492 forget=1.9655 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.6315 mle=1.6299 pcon=5.0498 forget=1.9518 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.5731 mle=1.5448 pcon=5.0502 forget=1.9781 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.4208 mle=1.5011 pcon=5.0504 forget=1.8693 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.4815 mle=1.5037 pcon=5.0502 forget=1.9276 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.4598 mle=1.5210 pcon=5.0499 forget=1.8889 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.4573 mle=1.5159 pcon=5.0495 forget=1.8920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.3409 mle=1.4377 pcon=5.0492 forget=1.8541 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.3261 mle=1.4509 pcon=5.0484 forget=1.8268 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.3982 mle=1.5400 pcon=5.0474 forget=1.8108 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.2849 mle=1.4054 pcon=5.0464 forget=1.8331 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.2970 mle=1.4710 pcon=5.0452 forget=1.7809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.3017 mle=1.4498 pcon=5.0439 forget=1.8079 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.3275 mle=1.5413 pcon=5.0430 forget=1.7432 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.2933 mle=1.4729 pcon=5.0421 forget=1.7782 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 20 it 0 total=8.2522 mle=1.4571 pcon=5.0408 forget=1.7543 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.2225 mle=1.4350 pcon=5.0393 forget=1.7482 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.1961 mle=1.4487 pcon=5.0378 forget=1.7096 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.2104 mle=1.4775 pcon=5.0362 forget=1.6967 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.2876 mle=1.5567 pcon=5.0347 forget=1.6962 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 47%|████▋     | 21/45 [03:53<04:00, 10.03s/it] 49%|████▉     | 22/45 [04:03<03:50, 10.02s/it] 51%|█████     | 23/45 [04:13<03:40, 10.02s/it] 53%|█████▎    | 24/45 [04:22<03:29,  9.95s/it] 56%|█████▌    | 25/45 [04:32<03:18,  9.92s/it] 58%|█████▊    | 26/45 [04:42<03:07,  9.89s/it] 60%|██████    | 27/45 [04:52<02:59,  9.98s/it] 62%|██████▏   | 28/45 [05:02<02:47,  9.84s/it][loss] ep 20 it 250 total=8.1900 mle=1.4824 pcon=5.0331 forget=1.6746 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=8.2976 mle=1.6096 pcon=5.0316 forget=1.6564 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=8.2073 mle=1.5287 pcon=5.0299 forget=1.6488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 21 it 10 total=8.0301 mle=1.3787 pcon=5.0279 forget=1.6235 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=8.1607 mle=1.4993 pcon=5.0261 forget=1.6353 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.1683 mle=1.4987 pcon=5.0242 forget=1.6454 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.1499 mle=1.4946 pcon=5.0224 forget=1.6329 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=8.0738 mle=1.4148 pcon=5.0204 forget=1.6385 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.2013 mle=1.5692 pcon=5.0189 forget=1.6132 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.2065 mle=1.5222 pcon=5.0170 forget=1.6672 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.1183 mle=1.4743 pcon=5.0154 forget=1.6286 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e45-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 22 it 20 total=8.1397 mle=1.4902 pcon=5.0140 forget=1.6355 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.1263 mle=1.4641 pcon=5.0123 forget=1.6499 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.2184 mle=1.5151 pcon=5.0109 forget=1.6923 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.1130 mle=1.4553 pcon=5.0094 forget=1.6483 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.2161 mle=1.5280 pcon=5.0083 forget=1.6798 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.1843 mle=1.4739 pcon=5.0073 forget=1.7031 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.0915 mle=1.4119 pcon=5.0064 forget=1.6732 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=8.2874 mle=1.5875 pcon=5.0054 forget=1.6946 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 30 total=8.2233 mle=1.5015 pcon=5.0048 forget=1.7170 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.2123 mle=1.4601 pcon=5.0041 forget=1.7482 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=8.3170 mle=1.5697 pcon=5.0035 forget=1.7438 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.3072 mle=1.5393 pcon=5.0030 forget=1.7649 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.3110 mle=1.5122 pcon=5.0029 forget=1.7959 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=8.3827 mle=1.5898 pcon=5.0029 forget=1.7900 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=8.3298 mle=1.5116 pcon=5.0027 forget=1.8155 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.2792 mle=1.4457 pcon=5.0028 forget=1.8307 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 40 total=8.3239 mle=1.4446 pcon=5.0032 forget=1.8760 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=8.4674 mle=1.5993 pcon=5.0034 forget=1.8647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.3556 mle=1.4706 pcon=5.0038 forget=1.8812 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.3526 mle=1.4477 pcon=5.0044 forget=1.9004 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=8.4116 mle=1.5052 pcon=5.0052 forget=1.9012 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.4111 mle=1.4986 pcon=5.0057 forget=1.9068 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.4119 mle=1.4855 pcon=5.0062 forget=1.9202 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 0 total=8.4467 mle=1.5255 pcon=5.0069 forget=1.9143 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 50 total=8.4866 mle=1.5569 pcon=5.0076 forget=1.9221 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 100 total=8.5176 mle=1.5559 pcon=5.0082 forget=1.9536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 150 total=8.4920 mle=1.4987 pcon=5.0091 forget=1.9842 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 200 total=8.5267 mle=1.5346 pcon=5.0100 forget=1.9821 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 250 total=8.4952 mle=1.4860 pcon=5.0111 forget=1.9981 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 300 total=8.5106 mle=1.5098 pcon=5.0120 forget=1.9887 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 350 total=8.5673 mle=1.5413 pcon=5.0128 forget=2.0132 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 10 total=8.5202 mle=1.4852 pcon=5.0138 forget=2.0212 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 60 total=8.5246 mle=1.5091 pcon=5.0145 forget=2.0011 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 110 total=8.5414 mle=1.4940 pcon=5.0154 forget=2.0320 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 160 total=8.4770 mle=1.4402 pcon=5.0160 forget=2.0208 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 210 total=8.5735 mle=1.5325 pcon=5.0168 forget=2.0241 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 260 total=8.5193 mle=1.4695 pcon=5.0181 forget=2.0317 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 310 total=8.6656 mle=1.6225 pcon=5.0188 forget=2.0243 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 360 total=8.6628 mle=1.6211 pcon=5.0196 forget=2.0220 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 20 total=8.5434 mle=1.4702 pcon=5.0205 forget=2.0527 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 70 total=8.5698 mle=1.5219 pcon=5.0212 forget=2.0267 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 120 total=8.5374 mle=1.4647 pcon=5.0221 forget=2.0506 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 170 total=8.5338 mle=1.4704 pcon=5.0229 forget=2.0405 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 220 total=8.5468 mle=1.4753 pcon=5.0239 forget=2.0476 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 270 total=8.5901 mle=1.5059 pcon=5.0245 forget=2.0597 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 320 total=8.5961 mle=1.5008 pcon=5.0253 forget=2.0701 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 370 total=8.4749 mle=1.3862 pcon=5.0260 forget=2.0626 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 30 total=8.5733 mle=1.4665 pcon=5.0269 forget=2.0799 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 64%|██████▍   | 29/45 [05:12<02:36,  9.80s/it] 67%|██████▋   | 30/45 [05:22<02:29,  9.99s/it] 69%|██████▉   | 31/45 [05:34<02:28, 10.58s/it] 71%|███████   | 32/45 [05:45<02:20, 10.79s/it] 73%|███████▎  | 33/45 [05:56<02:07, 10.64s/it] 76%|███████▌  | 34/45 [06:06<01:56, 10.55s/it] 78%|███████▊  | 35/45 [06:16<01:45, 10.56s/it] 80%|████████  | 36/45 [06:27<01:34, 10.53s/it][loss] ep 28 it 80 total=8.6289 mle=1.5676 pcon=5.0274 forget=2.0338 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 130 total=8.4812 mle=1.4038 pcon=5.0282 forget=2.0492 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 180 total=8.4823 mle=1.3811 pcon=5.0284 forget=2.0728 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 230 total=8.5338 mle=1.4518 pcon=5.0291 forget=2.0529 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 280 total=8.5623 mle=1.4680 pcon=5.0296 forget=2.0648 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 330 total=8.4695 mle=1.3978 pcon=5.0298 forget=2.0418 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 380 total=8.4981 mle=1.4228 pcon=5.0300 forget=2.0453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 40 total=8.6344 mle=1.5579 pcon=5.0302 forget=2.0464 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 90 total=8.5671 mle=1.4698 pcon=5.0303 forget=2.0670 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 140 total=8.4594 mle=1.3887 pcon=5.0306 forget=2.0402 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 190 total=8.6248 mle=1.5300 pcon=5.0305 forget=2.0643 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 240 total=8.6157 mle=1.5269 pcon=5.0307 forget=2.0581 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 290 total=8.5024 mle=1.4113 pcon=5.0302 forget=2.0609 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 340 total=8.5389 mle=1.4440 pcon=5.0304 forget=2.0646 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 0 total=8.5081 mle=1.4307 pcon=5.0302 forget=2.0471 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 50 total=8.5661 mle=1.4969 pcon=5.0298 forget=2.0394 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 100 total=8.5528 mle=1.4709 pcon=5.0298 forget=2.0521 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 150 total=8.5373 mle=1.4365 pcon=5.0295 forget=2.0714 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 200 total=8.5254 mle=1.4457 pcon=5.0293 forget=2.0505 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 250 total=8.5696 mle=1.5135 pcon=5.0290 forget=2.0270 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 300 total=8.5788 mle=1.4986 pcon=5.0284 forget=2.0518 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 350 total=8.6172 mle=1.5066 pcon=5.0277 forget=2.0829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 10 total=8.5489 mle=1.4815 pcon=5.0271 forget=2.0403 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 60 total=8.4757 mle=1.3923 pcon=5.0266 forget=2.0569 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 110 total=8.5380 mle=1.4721 pcon=5.0260 forget=2.0399 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 160 total=8.5423 mle=1.4379 pcon=5.0250 forget=2.0793 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 210 total=8.6896 mle=1.5890 pcon=5.0241 forget=2.0765 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 260 total=8.5121 mle=1.3972 pcon=5.0233 forget=2.0916 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 310 total=8.4962 mle=1.4176 pcon=5.0221 forget=2.0565 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 360 total=8.4695 mle=1.3928 pcon=5.0215 forget=2.0552 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 20 total=8.4889 mle=1.3598 pcon=5.0208 forget=2.1082 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 70 total=8.5268 mle=1.4281 pcon=5.0200 forget=2.0788 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 120 total=8.4588 mle=1.3627 pcon=5.0186 forget=2.0776 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 170 total=8.5094 mle=1.3897 pcon=5.0175 forget=2.1023 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 220 total=8.5470 mle=1.4575 pcon=5.0161 forget=2.0734 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 270 total=8.4967 mle=1.3911 pcon=5.0147 forget=2.0909 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 320 total=8.5797 mle=1.4366 pcon=5.0130 forget=2.1301 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 370 total=8.4440 mle=1.3329 pcon=5.0118 forget=2.0993 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 30 total=8.6013 mle=1.4844 pcon=5.0103 forget=2.1065 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 80 total=8.6015 mle=1.4694 pcon=5.0087 forget=2.1233 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 130 total=8.5528 mle=1.4332 pcon=5.0073 forget=2.1122 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 180 total=8.5809 mle=1.4714 pcon=5.0057 forget=2.1037 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 230 total=8.5777 mle=1.4562 pcon=5.0044 forget=2.1172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 280 total=8.5615 mle=1.4460 pcon=5.0024 forget=2.1130 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 330 total=8.7011 mle=1.5283 pcon=5.0006 forget=2.1723 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 380 total=8.4854 mle=1.3626 pcon=4.9988 forget=2.1239 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 40 total=8.5922 mle=1.4421 pcon=4.9969 forget=2.1533 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 90 total=8.5781 mle=1.4480 pcon=4.9951 forget=2.1349 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 140 total=8.6593 mle=1.4751 pcon=4.9931 forget=2.1912 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 190 total=8.5342 mle=1.3982 pcon=4.9911 forget=2.1449 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 240 total=8.6224 mle=1.4402 pcon=4.9893 forget=2.1930 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 290 total=8.5562 mle=1.4050 pcon=4.9875 forget=2.1638 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 340 total=8.5396 mle=1.3808 pcon=4.9854 forget=2.1734 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 0 total=8.5361 mle=1.3578 pcon=4.9836 forget=2.1947 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 50 total=8.6094 mle=1.4271 pcon=4.9820 forget=2.2003 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 100 total=8.5529 mle=1.4136 pcon=4.9804 forget=2.1590 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 150 total=8.5548 mle=1.3324 pcon=4.9782 forget=2.2442 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 200 total=8.5925 mle=1.4136 pcon=4.9763 forget=2.2026 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 250 total=8.7080 mle=1.4567 pcon=4.9747 forget=2.2767 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 300 total=8.6845 mle=1.4343 pcon=4.9727 forget=2.2774 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 350 total=8.7842 mle=1.5174 pcon=4.9709 forget=2.2959 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 10 total=8.7125 mle=1.4946 pcon=4.9690 forget=2.2489 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 82%|████████▏ | 37/45 [06:37<01:23, 10.50s/it] 84%|████████▍ | 38/45 [06:47<01:12, 10.32s/it] 87%|████████▋ | 39/45 [06:58<01:03, 10.50s/it] 89%|████████▉ | 40/45 [07:09<00:52, 10.54s/it] 91%|█████████ | 41/45 [07:20<00:42, 10.62s/it] 93%|█████████▎| 42/45 [07:31<00:32, 10.84s/it] 96%|█████████▌| 43/45 [07:42<00:22, 11.02s/it] 98%|█████████▊| 44/45 [07:53<00:11, 11.04s/it][loss] ep 36 it 60 total=8.7004 mle=1.4651 pcon=4.9670 forget=2.2684 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 110 total=8.7808 mle=1.5466 pcon=4.9654 forget=2.2689 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 160 total=8.6827 mle=1.4662 pcon=4.9636 forget=2.2530 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 210 total=8.7907 mle=1.5695 pcon=4.9616 forget=2.2596 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 260 total=8.7424 mle=1.4918 pcon=4.9598 forget=2.2909 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 310 total=8.6282 mle=1.3359 pcon=4.9582 forget=2.3341 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 360 total=8.6375 mle=1.3872 pcon=4.9570 forget=2.2932 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 20 total=8.7054 mle=1.4412 pcon=4.9553 forget=2.3088 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 70 total=8.7197 mle=1.4793 pcon=4.9538 forget=2.2866 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 120 total=8.6785 mle=1.4072 pcon=4.9520 forget=2.3193 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 170 total=8.7129 mle=1.3919 pcon=4.9508 forget=2.3701 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 220 total=8.6582 mle=1.3986 pcon=4.9491 forget=2.3105 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 270 total=8.6488 mle=1.3963 pcon=4.9478 forget=2.3047 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 320 total=8.7109 mle=1.4148 pcon=4.9466 forget=2.3495 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 370 total=8.6640 mle=1.3940 pcon=4.9454 forget=2.3247 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 30 total=8.6297 mle=1.3207 pcon=4.9443 forget=2.3647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 80 total=8.7363 mle=1.3960 pcon=4.9430 forget=2.3974 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 130 total=8.6786 mle=1.3769 pcon=4.9417 forget=2.3599 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 180 total=8.7047 mle=1.3773 pcon=4.9405 forget=2.3869 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 230 total=8.6868 mle=1.4257 pcon=4.9396 forget=2.3214 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 280 total=8.8384 mle=1.5324 pcon=4.9388 forget=2.3672 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 330 total=8.8768 mle=1.5740 pcon=4.9379 forget=2.3649 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 380 total=8.7267 mle=1.4213 pcon=4.9369 forget=2.3685 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 40 total=8.7611 mle=1.4549 pcon=4.9360 forget=2.3702 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 90 total=8.7725 mle=1.4093 pcon=4.9354 forget=2.4278 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 140 total=8.7997 mle=1.4730 pcon=4.9349 forget=2.3918 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 190 total=8.6913 mle=1.3799 pcon=4.9343 forget=2.3771 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 240 total=8.7980 mle=1.4219 pcon=4.9339 forget=2.4423 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 290 total=8.7837 mle=1.3992 pcon=4.9334 forget=2.4512 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 340 total=8.8230 mle=1.4369 pcon=4.9332 forget=2.4528 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 0 total=8.7241 mle=1.3364 pcon=4.9328 forget=2.4550 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 50 total=8.7542 mle=1.3754 pcon=4.9325 forget=2.4463 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 100 total=8.7597 mle=1.3561 pcon=4.9324 forget=2.4712 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 150 total=8.8464 mle=1.4483 pcon=4.9322 forget=2.4659 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 200 total=8.7451 mle=1.3754 pcon=4.9321 forget=2.4377 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 250 total=8.8495 mle=1.4865 pcon=4.9318 forget=2.4313 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 300 total=8.8520 mle=1.4452 pcon=4.9318 forget=2.4751 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 350 total=8.7639 mle=1.3784 pcon=4.9317 forget=2.4538 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 10 total=8.8961 mle=1.5153 pcon=4.9317 forget=2.4491 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 60 total=8.8338 mle=1.4413 pcon=4.9322 forget=2.4602 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 110 total=8.8020 mle=1.3706 pcon=4.9323 forget=2.4991 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 160 total=8.7727 mle=1.3483 pcon=4.9321 forget=2.4922 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 210 total=8.7552 mle=1.3658 pcon=4.9322 forget=2.4572 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 260 total=8.7694 mle=1.3486 pcon=4.9324 forget=2.4884 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 310 total=8.7939 mle=1.3548 pcon=4.9329 forget=2.5062 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 360 total=8.7285 mle=1.2876 pcon=4.9334 forget=2.5076 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 20 total=8.8055 mle=1.3714 pcon=4.9338 forget=2.5002 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 70 total=8.7883 mle=1.3398 pcon=4.9341 forget=2.5144 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 120 total=8.8260 mle=1.4146 pcon=4.9343 forget=2.4771 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 170 total=8.8070 mle=1.3596 pcon=4.9348 forget=2.5127 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 220 total=8.7403 mle=1.3045 pcon=4.9351 forget=2.5008 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 270 total=8.8768 mle=1.4195 pcon=4.9352 forget=2.5220 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 320 total=8.9409 mle=1.4900 pcon=4.9355 forget=2.5154 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 370 total=8.8806 mle=1.4635 pcon=4.9358 forget=2.4814 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 30 total=8.8530 mle=1.4744 pcon=4.9363 forget=2.4422 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 80 total=8.9111 mle=1.4547 pcon=4.9366 forget=2.5199 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 130 total=8.9322 mle=1.4795 pcon=4.9370 forget=2.5156 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 180 total=8.8076 mle=1.3186 pcon=4.9374 forget=2.5516 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 230 total=8.7969 mle=1.3106 pcon=4.9379 forget=2.5484 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 280 total=8.8172 mle=1.3459 pcon=4.9384 forget=2.5329 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 330 total=8.8782 mle=1.4461 pcon=4.9388 forget=2.4933 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 380 total=8.8510 mle=1.4165 pcon=4.9393 forget=2.4952 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
100%|██████████| 45/45 [08:05<00:00, 11.07s/it]100%|██████████| 45/45 [08:05<00:00, 10.78s/it]
[loss] ep 44 it 40 total=8.8893 mle=1.4155 pcon=4.9398 forget=2.5340 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 90 total=8.8768 mle=1.3739 pcon=4.9401 forget=2.5628 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 140 total=8.8958 mle=1.4376 pcon=4.9407 forget=2.5175 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 190 total=8.9116 mle=1.4871 pcon=4.9409 forget=2.4835 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 240 total=8.9304 mle=1.4620 pcon=4.9414 forget=2.5271 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 290 total=8.8701 mle=1.4205 pcon=4.9418 forget=2.5078 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 340 total=8.8895 mle=1.4296 pcon=4.9424 forget=2.5175 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[Run] lambda=0.2 lr=0.001 epochs=50 lora_r=8
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:167: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:14<11:38, 14.26s/it]  4%|▍         | 2/50 [00:24<09:32, 11.92s/it]  6%|▌         | 3/50 [00:34<08:37, 11.01s/it]  8%|▊         | 4/50 [00:44<08:16, 10.78s/it] 10%|█         | 5/50 [00:54<07:47, 10.40s/it] 12%|█▏        | 6/50 [01:05<07:37, 10.41s/it][loss] ep 0 it 0 total=9.0402 mle=1.2805 pcon=5.2951 forget=2.4647 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.1041 mle=1.3921 pcon=5.2887 forget=2.4233 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.2412 mle=1.5078 pcon=5.2827 forget=2.4506 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.2781 mle=1.5644 pcon=5.2770 forget=2.4366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.2053 mle=1.5331 pcon=5.2708 forget=2.4014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.1118 mle=1.3806 pcon=5.2650 forget=2.4662 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.0054 mle=1.2922 pcon=5.2596 forget=2.4536 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.1731 mle=1.4379 pcon=5.2543 forget=2.4809 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 1 it 10 total=9.0164 mle=1.3066 pcon=5.2487 forget=2.4611 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0373 mle=1.3162 pcon=5.2433 forget=2.4779 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=8.9651 mle=1.2902 pcon=5.2382 forget=2.4367 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=8.9539 mle=1.2379 pcon=5.2331 forget=2.4829 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.0631 mle=1.3724 pcon=5.2283 forget=2.4623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=8.9880 mle=1.3469 pcon=5.2235 forget=2.4177 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=8.9850 mle=1.3082 pcon=5.2185 forget=2.4583 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0553 mle=1.3766 pcon=5.2136 forget=2.4651 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 2 it 20 total=8.9290 mle=1.2746 pcon=5.2089 forget=2.4455 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0969 mle=1.3989 pcon=5.2044 forget=2.4936 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=8.9870 mle=1.3588 pcon=5.1998 forget=2.4284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9914 mle=1.3581 pcon=5.1954 forget=2.4378 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=8.9823 mle=1.3299 pcon=5.1909 forget=2.4615 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0215 mle=1.3659 pcon=5.1865 forget=2.4690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=8.8723 mle=1.2607 pcon=5.1824 forget=2.4291 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=9.0074 mle=1.3834 pcon=5.1787 forget=2.4453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 3 it 30 total=8.9979 mle=1.3638 pcon=5.1749 forget=2.4592 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=8.9247 mle=1.2959 pcon=5.1710 forget=2.4579 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.8988 mle=1.2904 pcon=5.1672 forget=2.4412 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=9.0414 mle=1.4600 pcon=5.1635 forget=2.4179 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9798 mle=1.3523 pcon=5.1601 forget=2.4674 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0521 mle=1.4267 pcon=5.1568 forget=2.4686 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=9.0461 mle=1.4482 pcon=5.1531 forget=2.4448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9153 mle=1.3140 pcon=5.1498 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 4 it 40 total=8.9573 mle=1.3771 pcon=5.1465 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9560 mle=1.3336 pcon=5.1435 forget=2.4789 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=9.1340 mle=1.5855 pcon=5.1406 forget=2.4079 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=8.9932 mle=1.3938 pcon=5.1375 forget=2.4619 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.0240 mle=1.4142 pcon=5.1344 forget=2.4754 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=8.7697 mle=1.1862 pcon=5.1316 forget=2.4519 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=9.0390 mle=1.4611 pcon=5.1291 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 5 it 0 total=8.8316 mle=1.2045 pcon=5.1261 forget=2.5010 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 50 total=8.9577 mle=1.3649 pcon=5.1235 forget=2.4693 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 100 total=9.0086 mle=1.4006 pcon=5.1207 forget=2.4873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 150 total=9.0681 mle=1.4808 pcon=5.1179 forget=2.4694 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 200 total=9.0086 mle=1.4524 pcon=5.1152 forget=2.4410 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 250 total=8.9115 mle=1.3485 pcon=5.1125 forget=2.4504 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 300 total=8.9730 mle=1.4294 pcon=5.1099 forget=2.4337 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 5 it 350 total=8.9550 mle=1.4045 pcon=5.1073 forget=2.4432 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 6 it 10 total=8.8857 mle=1.2784 pcon=5.1049 forget=2.5024 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 60 total=8.9181 mle=1.3785 pcon=5.1025 forget=2.4370 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 110 total=8.8930 mle=1.3183 pcon=5.1003 forget=2.4744 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 160 total=8.8890 mle=1.3320 pcon=5.0980 forget=2.4590 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 210 total=8.9235 mle=1.3655 pcon=5.0956 forget=2.4624 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 260 total=8.9544 mle=1.3831 pcon=5.0934 forget=2.4778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 14%|█▍        | 7/50 [01:15<07:27, 10.40s/it] 16%|█▌        | 8/50 [01:26<07:30, 10.73s/it] 18%|█▊        | 9/50 [01:39<07:38, 11.18s/it] 20%|██        | 10/50 [01:50<07:33, 11.34s/it] 22%|██▏       | 11/50 [02:02<07:24, 11.41s/it] 24%|██▍       | 12/50 [02:12<07:02, 11.12s/it] 26%|██▌       | 13/50 [02:23<06:48, 11.04s/it][loss] ep 6 it 310 total=8.8669 mle=1.3536 pcon=5.0915 forget=2.4219 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 6 it 360 total=8.9529 mle=1.4569 pcon=5.0893 forget=2.4067 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 7 it 20 total=8.7927 mle=1.2501 pcon=5.0872 forget=2.4555 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 70 total=8.7892 mle=1.2463 pcon=5.0849 forget=2.4580 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 120 total=8.9121 mle=1.3875 pcon=5.0829 forget=2.4417 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 170 total=8.9197 mle=1.4330 pcon=5.0810 forget=2.4057 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 220 total=8.8357 mle=1.3211 pcon=5.0794 forget=2.4352 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 270 total=8.9747 mle=1.4542 pcon=5.0777 forget=2.4427 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 320 total=8.8527 mle=1.3166 pcon=5.0756 forget=2.4606 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 7 it 370 total=8.9364 mle=1.3709 pcon=5.0736 forget=2.4920 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 8 it 30 total=8.9612 mle=1.4806 pcon=5.0719 forget=2.4087 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 80 total=8.8050 mle=1.2745 pcon=5.0702 forget=2.4604 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 130 total=8.9213 mle=1.3983 pcon=5.0685 forget=2.4545 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 180 total=8.8647 mle=1.3447 pcon=5.0668 forget=2.4532 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 230 total=8.8877 mle=1.3662 pcon=5.0652 forget=2.4564 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 280 total=8.7837 mle=1.2569 pcon=5.0636 forget=2.4633 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 330 total=9.0222 mle=1.5799 pcon=5.0618 forget=2.3806 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 8 it 380 total=8.8762 mle=1.3644 pcon=5.0604 forget=2.4515 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 9 it 40 total=8.9253 mle=1.4107 pcon=5.0589 forget=2.4558 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 90 total=8.9709 mle=1.4929 pcon=5.0575 forget=2.4205 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 140 total=8.8783 mle=1.3537 pcon=5.0563 forget=2.4682 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 190 total=8.7493 mle=1.2282 pcon=5.0550 forget=2.4661 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 240 total=8.9737 mle=1.5025 pcon=5.0540 forget=2.4172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 290 total=8.8661 mle=1.3564 pcon=5.0526 forget=2.4571 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 9 it 340 total=8.8156 mle=1.3065 pcon=5.0514 forget=2.4577 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 10 it 0 total=8.7873 mle=1.2680 pcon=5.0497 forget=2.4695 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 50 total=8.7535 mle=1.2564 pcon=5.0489 forget=2.4483 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 100 total=8.8059 mle=1.3266 pcon=5.0475 forget=2.4319 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 150 total=8.8731 mle=1.3437 pcon=5.0463 forget=2.4830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 200 total=8.9087 mle=1.4083 pcon=5.0453 forget=2.4551 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 250 total=8.9165 mle=1.4250 pcon=5.0442 forget=2.4472 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 300 total=8.9383 mle=1.5122 pcon=5.0431 forget=2.3830 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 10 it 350 total=8.7568 mle=1.2608 pcon=5.0423 forget=2.4537 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 11 it 10 total=8.9130 mle=1.4402 pcon=5.0411 forget=2.4317 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 60 total=8.8452 mle=1.3727 pcon=5.0402 forget=2.4323 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 110 total=8.8327 mle=1.3445 pcon=5.0394 forget=2.4488 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 160 total=8.7803 mle=1.2733 pcon=5.0383 forget=2.4688 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 210 total=8.7523 mle=1.2650 pcon=5.0375 forget=2.4498 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 260 total=8.7358 mle=1.2659 pcon=5.0365 forget=2.4333 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 310 total=8.8654 mle=1.3900 pcon=5.0357 forget=2.4397 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 11 it 360 total=8.8281 mle=1.3171 pcon=5.0350 forget=2.4759 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 12 it 20 total=8.8001 mle=1.3121 pcon=5.0338 forget=2.4542 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 70 total=8.8387 mle=1.3531 pcon=5.0329 forget=2.4527 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 120 total=8.7894 mle=1.3589 pcon=5.0318 forget=2.3987 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 170 total=8.8279 mle=1.3336 pcon=5.0308 forget=2.4635 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 220 total=8.7260 mle=1.2212 pcon=5.0303 forget=2.4746 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 270 total=8.7405 mle=1.2917 pcon=5.0297 forget=2.4191 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 320 total=8.8767 mle=1.4073 pcon=5.0291 forget=2.4404 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 12 it 370 total=8.8368 mle=1.3739 pcon=5.0284 forget=2.4345 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 13 it 30 total=8.7972 mle=1.3110 pcon=5.0278 forget=2.4583 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 80 total=8.8988 mle=1.4850 pcon=5.0271 forget=2.3868 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 130 total=8.7774 mle=1.3403 pcon=5.0261 forget=2.4110 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 28%|██▊       | 14/50 [02:34<06:31, 10.87s/it] 30%|███       | 15/50 [02:43<06:07, 10.50s/it] 32%|███▏      | 16/50 [02:53<05:50, 10.31s/it] 34%|███▍      | 17/50 [03:03<05:36, 10.20s/it] 36%|███▌      | 18/50 [03:14<05:30, 10.33s/it] 38%|███▊      | 19/50 [03:24<05:18, 10.29s/it] 40%|████      | 20/50 [03:34<05:09, 10.32s/it][loss] ep 13 it 180 total=8.8215 mle=1.3600 pcon=5.0248 forget=2.4368 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 230 total=8.6711 mle=1.3022 pcon=5.0238 forget=2.3451 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 280 total=8.6540 mle=1.2102 pcon=5.0226 forget=2.4212 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 330 total=8.7333 mle=1.3932 pcon=5.0214 forget=2.3186 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 13 it 380 total=8.5909 mle=1.3266 pcon=5.0201 forget=2.2442 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 14 it 40 total=8.6005 mle=1.3311 pcon=5.0183 forget=2.2510 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 90 total=8.5551 mle=1.3663 pcon=5.0169 forget=2.1720 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 140 total=8.5102 mle=1.3799 pcon=5.0154 forget=2.1149 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 190 total=8.4343 mle=1.3673 pcon=5.0141 forget=2.0529 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 240 total=8.4696 mle=1.3813 pcon=5.0129 forget=2.0754 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 290 total=8.3916 mle=1.4350 pcon=5.0124 forget=1.9441 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 14 it 340 total=8.3587 mle=1.3928 pcon=5.0124 forget=1.9535 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 15 it 0 total=8.4285 mle=1.4743 pcon=5.0125 forget=1.9416 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 50 total=8.3541 mle=1.3859 pcon=5.0133 forget=1.9549 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 100 total=8.3798 mle=1.4717 pcon=5.0145 forget=1.8935 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 150 total=8.4162 mle=1.4766 pcon=5.0158 forget=1.9239 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 200 total=8.3362 mle=1.4059 pcon=5.0172 forget=1.9131 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 250 total=8.4153 mle=1.4451 pcon=5.0187 forget=1.9516 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 300 total=8.4600 mle=1.5064 pcon=5.0203 forget=1.9333 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 15 it 350 total=8.4660 mle=1.5114 pcon=5.0221 forget=1.9324 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 16 it 10 total=8.4468 mle=1.4330 pcon=5.0239 forget=1.9900 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 60 total=8.3997 mle=1.4180 pcon=5.0259 forget=1.9558 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 110 total=8.4086 mle=1.3845 pcon=5.0277 forget=1.9964 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 160 total=8.4658 mle=1.4110 pcon=5.0299 forget=2.0249 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 210 total=8.4882 mle=1.4207 pcon=5.0320 forget=2.0355 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 260 total=8.6916 mle=1.5791 pcon=5.0337 forget=2.0788 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 310 total=8.6572 mle=1.5740 pcon=5.0354 forget=2.0478 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 16 it 360 total=8.6023 mle=1.5262 pcon=5.0372 forget=2.0389 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 20 total=8.5758 mle=1.5527 pcon=5.0393 forget=1.9838 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 70 total=8.6508 mle=1.5555 pcon=5.0413 forget=2.0540 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 120 total=8.4757 mle=1.4187 pcon=5.0427 forget=2.0143 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 170 total=8.5848 mle=1.4736 pcon=5.0442 forget=2.0670 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 220 total=8.6118 mle=1.5708 pcon=5.0454 forget=1.9956 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 270 total=8.5576 mle=1.5343 pcon=5.0466 forget=1.9767 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 320 total=8.5106 mle=1.5206 pcon=5.0477 forget=1.9423 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 17 it 370 total=8.5234 mle=1.4830 pcon=5.0483 forget=1.9921 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 30 total=8.4386 mle=1.4370 pcon=5.0490 forget=1.9526 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 80 total=8.6190 mle=1.6303 pcon=5.0495 forget=1.9392 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 130 total=8.5599 mle=1.5447 pcon=5.0498 forget=1.9654 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 180 total=8.4070 mle=1.5023 pcon=5.0499 forget=1.8549 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 230 total=8.4666 mle=1.5029 pcon=5.0496 forget=1.9141 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 280 total=8.4450 mle=1.5207 pcon=5.0493 forget=1.8751 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 330 total=8.4432 mle=1.5157 pcon=5.0487 forget=1.8787 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 18 it 380 total=8.3252 mle=1.4364 pcon=5.0484 forget=1.8404 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 40 total=8.3105 mle=1.4502 pcon=5.0476 forget=1.8127 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 90 total=8.3826 mle=1.5383 pcon=5.0466 forget=1.7978 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 140 total=8.2668 mle=1.4041 pcon=5.0454 forget=1.8172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 190 total=8.2822 mle=1.4720 pcon=5.0442 forget=1.7659 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 240 total=8.2823 mle=1.4484 pcon=5.0429 forget=1.7910 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 290 total=8.3094 mle=1.5425 pcon=5.0420 forget=1.7249 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 19 it 340 total=8.2751 mle=1.4735 pcon=5.0410 forget=1.7606 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 20 it 0 total=8.2330 mle=1.4576 pcon=5.0397 forget=1.7357 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 50 total=8.2036 mle=1.4364 pcon=5.0382 forget=1.7290 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 100 total=8.1771 mle=1.4488 pcon=5.0366 forget=1.6917 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 150 total=8.1916 mle=1.4787 pcon=5.0350 forget=1.6778 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 200 total=8.2687 mle=1.5567 pcon=5.0334 forget=1.6786 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 42%|████▏     | 21/50 [03:45<05:05, 10.52s/it] 44%|████▍     | 22/50 [03:57<05:02, 10.79s/it] 46%|████▌     | 23/50 [04:08<04:58, 11.07s/it] 48%|████▊     | 24/50 [04:20<04:51, 11.21s/it] 50%|█████     | 25/50 [04:31<04:38, 11.13s/it] 52%|█████▏    | 26/50 [04:43<04:30, 11.29s/it] 54%|█████▍    | 27/50 [04:53<04:15, 11.13s/it] 56%|█████▌    | 28/50 [05:04<04:02, 11.02s/it][loss] ep 20 it 250 total=8.1718 mle=1.4824 pcon=5.0318 forget=1.6575 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 300 total=8.2804 mle=1.6118 pcon=5.0303 forget=1.6383 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 20 it 350 total=8.1908 mle=1.5288 pcon=5.0286 forget=1.6334 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 21 it 10 total=8.0147 mle=1.3799 pcon=5.0266 forget=1.6082 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 60 total=8.1447 mle=1.5000 pcon=5.0248 forget=1.6199 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 110 total=8.1566 mle=1.4993 pcon=5.0230 forget=1.6344 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 160 total=8.1380 mle=1.4941 pcon=5.0211 forget=1.6228 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 210 total=8.0632 mle=1.4150 pcon=5.0192 forget=1.6289 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 260 total=8.1923 mle=1.5711 pcon=5.0177 forget=1.6035 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 310 total=8.1952 mle=1.5202 pcon=5.0159 forget=1.6592 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 21 it 360 total=8.1111 mle=1.4775 pcon=5.0143 forget=1.6193 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-lambda0.2-epochgrid_runs-planB_adapter
[loss] ep 22 it 20 total=8.1316 mle=1.4902 pcon=5.0129 forget=1.6284 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 70 total=8.1170 mle=1.4618 pcon=5.0113 forget=1.6439 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 120 total=8.2126 mle=1.5176 pcon=5.0099 forget=1.6851 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 170 total=8.1127 mle=1.4604 pcon=5.0084 forget=1.6439 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 220 total=8.2117 mle=1.5296 pcon=5.0074 forget=1.6747 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 270 total=8.1782 mle=1.4737 pcon=5.0065 forget=1.6981 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 320 total=8.0885 mle=1.4125 pcon=5.0056 forget=1.6704 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 22 it 370 total=8.2792 mle=1.5843 pcon=5.0047 forget=1.6902 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 30 total=8.2203 mle=1.5025 pcon=5.0042 forget=1.7136 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 80 total=8.2087 mle=1.4623 pcon=5.0036 forget=1.7429 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 130 total=8.3165 mle=1.5728 pcon=5.0031 forget=1.7407 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 180 total=8.3024 mle=1.5372 pcon=5.0026 forget=1.7626 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 230 total=8.3080 mle=1.5133 pcon=5.0026 forget=1.7921 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 280 total=8.3773 mle=1.5887 pcon=5.0026 forget=1.7860 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 330 total=8.3257 mle=1.5134 pcon=5.0025 forget=1.8098 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 23 it 380 total=8.2750 mle=1.4466 pcon=5.0027 forget=1.8257 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 40 total=8.3179 mle=1.4452 pcon=5.0031 forget=1.8695 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 90 total=8.4642 mle=1.6029 pcon=5.0033 forget=1.8580 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 140 total=8.3461 mle=1.4706 pcon=5.0037 forget=1.8718 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 190 total=8.3453 mle=1.4500 pcon=5.0044 forget=1.8909 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 240 total=8.4030 mle=1.5086 pcon=5.0051 forget=1.8893 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 290 total=8.3977 mle=1.4973 pcon=5.0056 forget=1.8948 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 24 it 340 total=8.3978 mle=1.4850 pcon=5.0061 forget=1.9067 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 0 total=8.4335 mle=1.5261 pcon=5.0067 forget=1.9007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 50 total=8.4737 mle=1.5595 pcon=5.0073 forget=1.9068 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 100 total=8.4988 mle=1.5561 pcon=5.0079 forget=1.9349 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 150 total=8.4640 mle=1.4940 pcon=5.0087 forget=1.9613 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 200 total=8.5050 mle=1.5349 pcon=5.0096 forget=1.9605 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 250 total=8.4672 mle=1.4850 pcon=5.0105 forget=1.9717 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 300 total=8.4863 mle=1.5108 pcon=5.0114 forget=1.9641 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 25 it 350 total=8.5335 mle=1.5427 pcon=5.0120 forget=1.9788 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 10 total=8.4834 mle=1.4840 pcon=5.0128 forget=1.9866 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 60 total=8.4859 mle=1.5067 pcon=5.0134 forget=1.9658 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 110 total=8.5045 mle=1.4928 pcon=5.0142 forget=1.9975 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 160 total=8.4320 mle=1.4346 pcon=5.0147 forget=1.9827 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 210 total=8.5342 mle=1.5305 pcon=5.0153 forget=1.9883 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 260 total=8.4702 mle=1.4645 pcon=5.0164 forget=1.9893 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 310 total=8.6114 mle=1.6154 pcon=5.0169 forget=1.9791 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 26 it 360 total=8.6090 mle=1.6154 pcon=5.0176 forget=1.9760 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 20 total=8.4807 mle=1.4621 pcon=5.0183 forget=2.0004 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 70 total=8.5139 mle=1.5172 pcon=5.0187 forget=1.9781 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 120 total=8.4845 mle=1.4640 pcon=5.0194 forget=2.0011 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 170 total=8.4749 mle=1.4660 pcon=5.0200 forget=1.9888 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 220 total=8.4808 mle=1.4731 pcon=5.0208 forget=1.9869 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 270 total=8.5267 mle=1.5047 pcon=5.0212 forget=2.0009 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 320 total=8.5155 mle=1.4942 pcon=5.0216 forget=1.9997 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 27 it 370 total=8.3986 mle=1.3876 pcon=5.0221 forget=1.9889 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 30 total=8.4897 mle=1.4663 pcon=5.0227 forget=2.0007 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 58%|█████▊    | 29/50 [05:15<03:49, 10.94s/it] 60%|██████    | 30/50 [05:26<03:39, 10.98s/it] 62%|██████▏   | 31/50 [05:37<03:29, 11.04s/it] 64%|██████▍   | 32/50 [05:49<03:21, 11.19s/it] 66%|██████▌   | 33/50 [06:00<03:09, 11.17s/it] 68%|██████▊   | 34/50 [06:11<02:57, 11.12s/it] 70%|███████   | 35/50 [06:22<02:47, 11.20s/it] 72%|███████▏  | 36/50 [06:34<02:38, 11.30s/it][loss] ep 28 it 80 total=8.5414 mle=1.5626 pcon=5.0229 forget=1.9560 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 130 total=8.3976 mle=1.4009 pcon=5.0234 forget=1.9733 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 180 total=8.3906 mle=1.3763 pcon=5.0232 forget=1.9911 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 230 total=8.4391 mle=1.4482 pcon=5.0235 forget=1.9674 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 280 total=8.4737 mle=1.4731 pcon=5.0236 forget=1.9770 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 330 total=8.3709 mle=1.3955 pcon=5.0235 forget=1.9519 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 28 it 380 total=8.3966 mle=1.4208 pcon=5.0232 forget=1.9525 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 40 total=8.5149 mle=1.5504 pcon=5.0230 forget=1.9415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 90 total=8.4556 mle=1.4644 pcon=5.0227 forget=1.9686 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 140 total=8.3471 mle=1.3810 pcon=5.0224 forget=1.9437 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 190 total=8.5035 mle=1.5245 pcon=5.0218 forget=1.9572 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 240 total=8.5013 mle=1.5320 pcon=5.0214 forget=1.9478 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 290 total=8.3785 mle=1.4114 pcon=5.0204 forget=1.9466 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 29 it 340 total=8.4163 mle=1.4508 pcon=5.0200 forget=1.9455 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 0 total=8.3835 mle=1.4292 pcon=5.0193 forget=1.9350 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 50 total=8.4294 mle=1.4942 pcon=5.0182 forget=1.9170 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 100 total=8.4130 mle=1.4698 pcon=5.0176 forget=1.9256 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 150 total=8.4028 mle=1.4391 pcon=5.0167 forget=1.9470 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 200 total=8.3815 mle=1.4487 pcon=5.0157 forget=1.9171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 250 total=8.4151 mle=1.5092 pcon=5.0148 forget=1.8911 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 300 total=8.4370 mle=1.5124 pcon=5.0134 forget=1.9112 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 30 it 350 total=8.4527 mle=1.4977 pcon=5.0120 forget=1.9430 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 10 total=8.4089 mle=1.4931 pcon=5.0106 forget=1.9052 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 60 total=8.3300 mle=1.4035 pcon=5.0093 forget=1.9171 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 110 total=8.3824 mle=1.4819 pcon=5.0080 forget=1.8926 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 160 total=8.3771 mle=1.4451 pcon=5.0063 forget=1.9258 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 210 total=8.5288 mle=1.5877 pcon=5.0046 forget=1.9366 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 260 total=8.3573 mle=1.4047 pcon=5.0030 forget=1.9496 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 310 total=8.3410 mle=1.4385 pcon=5.0010 forget=1.9014 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 31 it 360 total=8.2938 mle=1.3976 pcon=4.9997 forget=1.8965 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 20 total=8.3345 mle=1.3750 pcon=4.9982 forget=1.9613 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 70 total=8.3480 mle=1.4439 pcon=4.9965 forget=1.9075 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 120 total=8.2994 mle=1.3815 pcon=4.9944 forget=1.9235 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 170 total=8.3431 mle=1.4103 pcon=4.9925 forget=1.9403 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 220 total=8.3546 mle=1.4744 pcon=4.9903 forget=1.8898 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 270 total=8.3217 mle=1.4161 pcon=4.9882 forget=1.9174 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 320 total=8.4026 mle=1.4579 pcon=4.9858 forget=1.9590 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 32 it 370 total=8.2624 mle=1.3492 pcon=4.9838 forget=1.9294 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 30 total=8.4228 mle=1.5013 pcon=4.9817 forget=1.9398 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 80 total=8.4008 mle=1.4888 pcon=4.9794 forget=1.9326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 130 total=8.3739 mle=1.4581 pcon=4.9773 forget=1.9385 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 180 total=8.3581 mle=1.4760 pcon=4.9751 forget=1.9070 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 230 total=8.4059 mle=1.4843 pcon=4.9731 forget=1.9485 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 280 total=8.3556 mle=1.4730 pcon=4.9706 forget=1.9119 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 330 total=8.4814 mle=1.5345 pcon=4.9683 forget=1.9786 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 33 it 380 total=8.2844 mle=1.3876 pcon=4.9662 forget=1.9307 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 40 total=8.4069 mle=1.4829 pcon=4.9639 forget=1.9601 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 90 total=8.3672 mle=1.4846 pcon=4.9618 forget=1.9207 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 140 total=8.4410 mle=1.5067 pcon=4.9595 forget=1.9748 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 190 total=8.3283 mle=1.4351 pcon=4.9575 forget=1.9358 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 240 total=8.3919 mle=1.4774 pcon=4.9555 forget=1.9590 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 290 total=8.3343 mle=1.4383 pcon=4.9538 forget=1.9422 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 34 it 340 total=8.3650 mle=1.4311 pcon=4.9520 forget=1.9820 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 0 total=8.3399 mle=1.4124 pcon=4.9505 forget=1.9769 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 50 total=8.4224 mle=1.4920 pcon=4.9494 forget=1.9810 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 100 total=8.3757 mle=1.4785 pcon=4.9483 forget=1.9489 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 150 total=8.3403 mle=1.3844 pcon=4.9468 forget=2.0091 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 200 total=8.4238 mle=1.4646 pcon=4.9458 forget=2.0134 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 250 total=8.5282 mle=1.5260 pcon=4.9451 forget=2.0571 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 300 total=8.4703 mle=1.4853 pcon=4.9443 forget=2.0407 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 35 it 350 total=8.5512 mle=1.5644 pcon=4.9436 forget=2.0432 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 10 total=8.5258 mle=1.5320 pcon=4.9431 forget=2.0507 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 74%|███████▍  | 37/50 [06:45<02:26, 11.29s/it] 76%|███████▌  | 38/50 [06:56<02:16, 11.36s/it] 78%|███████▊  | 39/50 [07:08<02:05, 11.40s/it] 80%|████████  | 40/50 [07:19<01:54, 11.44s/it] 82%|████████▏ | 41/50 [07:31<01:42, 11.34s/it] 84%|████████▍ | 42/50 [07:42<01:31, 11.48s/it] 86%|████████▌ | 43/50 [07:54<01:21, 11.60s/it] 88%|████████▊ | 44/50 [08:06<01:09, 11.62s/it][loss] ep 36 it 60 total=8.5446 mle=1.5379 pcon=4.9426 forget=2.0640 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 110 total=8.6220 mle=1.5737 pcon=4.9427 forget=2.1056 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 160 total=8.5414 mle=1.5195 pcon=4.9426 forget=2.0793 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 210 total=8.6809 mle=1.6305 pcon=4.9425 forget=2.1080 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 260 total=8.6289 mle=1.5675 pcon=4.9427 forget=2.1187 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 310 total=8.4996 mle=1.3941 pcon=4.9431 forget=2.1623 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 36 it 360 total=8.5330 mle=1.4443 pcon=4.9441 forget=2.1446 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 20 total=8.6166 mle=1.4972 pcon=4.9447 forget=2.1748 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 70 total=8.6714 mle=1.5588 pcon=4.9454 forget=2.1672 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 120 total=8.6116 mle=1.4628 pcon=4.9460 forget=2.2028 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 170 total=8.6152 mle=1.4416 pcon=4.9473 forget=2.2263 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 220 total=8.6547 mle=1.5088 pcon=4.9481 forget=2.1979 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 270 total=8.6099 mle=1.4715 pcon=4.9493 forget=2.1891 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 320 total=8.6637 mle=1.4778 pcon=4.9506 forget=2.2353 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 37 it 370 total=8.6448 mle=1.4603 pcon=4.9519 forget=2.2326 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 30 total=8.6122 mle=1.3917 pcon=4.9534 forget=2.2671 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 80 total=8.7255 mle=1.4590 pcon=4.9546 forget=2.3118 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 130 total=8.6672 mle=1.4400 pcon=4.9559 forget=2.2713 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 180 total=8.7018 mle=1.4548 pcon=4.9573 forget=2.2897 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 230 total=8.6962 mle=1.4687 pcon=4.9588 forget=2.2687 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 280 total=8.8574 mle=1.5902 pcon=4.9604 forget=2.3068 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 330 total=8.9412 mle=1.6666 pcon=4.9619 forget=2.3127 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 38 it 380 total=8.7552 mle=1.4823 pcon=4.9633 forget=2.3096 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 40 total=8.8269 mle=1.5365 pcon=4.9648 forget=2.3256 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 90 total=8.7890 mle=1.4621 pcon=4.9664 forget=2.3606 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 140 total=8.8166 mle=1.5175 pcon=4.9680 forget=2.3311 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 190 total=8.7881 mle=1.4846 pcon=4.9696 forget=2.3339 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 240 total=8.7926 mle=1.4577 pcon=4.9713 forget=2.3636 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 290 total=8.7952 mle=1.4554 pcon=4.9728 forget=2.3670 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 39 it 340 total=8.8584 mle=1.5057 pcon=4.9746 forget=2.3781 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 0 total=8.7392 mle=1.3846 pcon=4.9761 forget=2.3785 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 50 total=8.7890 mle=1.4344 pcon=4.9776 forget=2.3771 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 100 total=8.7628 mle=1.3991 pcon=4.9792 forget=2.3845 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 150 total=8.8695 mle=1.4922 pcon=4.9808 forget=2.3965 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 200 total=8.8294 mle=1.4541 pcon=4.9823 forget=2.3929 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 250 total=8.9092 mle=1.5548 pcon=4.9837 forget=2.3707 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 300 total=8.8848 mle=1.4943 pcon=4.9853 forget=2.4053 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 40 it 350 total=8.7968 mle=1.4216 pcon=4.9867 forget=2.3886 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 10 total=8.9201 mle=1.5376 pcon=4.9880 forget=2.3945 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 60 total=8.8698 mle=1.4879 pcon=4.9899 forget=2.3921 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 110 total=8.8380 mle=1.4398 pcon=4.9913 forget=2.4070 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 160 total=8.7734 mle=1.3792 pcon=4.9924 forget=2.4018 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 210 total=8.7646 mle=1.4039 pcon=4.9937 forget=2.3670 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 260 total=8.8059 mle=1.4031 pcon=4.9951 forget=2.4077 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 310 total=8.8266 mle=1.4142 pcon=4.9967 forget=2.4158 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 41 it 360 total=8.7540 mle=1.3528 pcon=4.9982 forget=2.4030 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 20 total=8.8359 mle=1.4178 pcon=4.9996 forget=2.4185 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 70 total=8.8181 mle=1.3978 pcon=5.0009 forget=2.4193 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 120 total=8.8736 mle=1.4851 pcon=5.0020 forget=2.3866 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 170 total=8.8723 mle=1.4428 pcon=5.0034 forget=2.4261 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 220 total=8.7722 mle=1.3567 pcon=5.0045 forget=2.4111 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 270 total=8.8224 mle=1.3917 pcon=5.0054 forget=2.4253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 320 total=8.9374 mle=1.4988 pcon=5.0065 forget=2.4321 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 42 it 370 total=8.8880 mle=1.4814 pcon=5.0076 forget=2.3990 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 30 total=8.9176 mle=1.5319 pcon=5.0088 forget=2.3770 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 80 total=8.9172 mle=1.4783 pcon=5.0097 forget=2.4292 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 130 total=8.9875 mle=1.5363 pcon=5.0108 forget=2.4403 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 180 total=8.8791 mle=1.3887 pcon=5.0118 forget=2.4787 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 230 total=8.8125 mle=1.3288 pcon=5.0129 forget=2.4709 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 280 total=8.8401 mle=1.3685 pcon=5.0140 forget=2.4575 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 330 total=8.9209 mle=1.4721 pcon=5.0150 forget=2.4338 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 43 it 380 total=8.8993 mle=1.4464 pcon=5.0161 forget=2.4368 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
 90%|█████████ | 45/50 [08:18<00:58, 11.80s/it] 92%|█████████▏| 46/50 [08:31<00:49, 12.26s/it] 94%|█████████▍| 47/50 [08:45<00:37, 12.56s/it] 96%|█████████▌| 48/50 [08:58<00:25, 12.82s/it] 98%|█████████▊| 49/50 [09:10<00:12, 12.55s/it]100%|██████████| 50/50 [09:22<00:00, 12.35s/it]100%|██████████| 50/50 [09:22<00:00, 11.25s/it]
[loss] ep 44 it 40 total=8.9494 mle=1.4499 pcon=5.0170 forget=2.4825 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 90 total=8.9225 mle=1.3948 pcon=5.0179 forget=2.5098 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 140 total=8.9561 mle=1.4661 pcon=5.0190 forget=2.4711 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 190 total=8.9843 mle=1.5312 pcon=5.0198 forget=2.4333 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 240 total=8.9905 mle=1.4921 pcon=5.0207 forget=2.4777 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 290 total=8.9064 mle=1.4319 pcon=5.0217 forget=2.4528 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 44 it 340 total=8.9617 mle=1.4479 pcon=5.0227 forget=2.4912 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 0 total=8.8762 mle=1.3754 pcon=5.0235 forget=2.4773 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 50 total=8.9100 mle=1.3679 pcon=5.0242 forget=2.5179 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 100 total=9.0121 mle=1.5027 pcon=5.0253 forget=2.4841 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 150 total=8.9533 mle=1.4369 pcon=5.0261 forget=2.4903 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 200 total=8.9325 mle=1.3978 pcon=5.0266 forget=2.5081 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 250 total=8.9680 mle=1.4334 pcon=5.0273 forget=2.5072 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 300 total=8.9350 mle=1.4522 pcon=5.0281 forget=2.4547 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 45 it 350 total=8.8831 mle=1.3627 pcon=5.0294 forget=2.4911 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 10 total=8.8711 mle=1.3529 pcon=5.0304 forget=2.4879 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 60 total=9.0249 mle=1.4490 pcon=5.0311 forget=2.5448 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 110 total=8.8823 mle=1.3522 pcon=5.0319 forget=2.4982 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 160 total=9.0301 mle=1.4945 pcon=5.0324 forget=2.5032 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 210 total=8.9455 mle=1.3946 pcon=5.0329 forget=2.5180 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 260 total=9.0407 mle=1.5020 pcon=5.0336 forget=2.5051 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 310 total=8.8759 mle=1.3407 pcon=5.0342 forget=2.5010 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 46 it 360 total=8.9418 mle=1.4103 pcon=5.0349 forget=2.4966 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 20 total=9.0065 mle=1.4707 pcon=5.0356 forget=2.5002 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 70 total=8.9050 mle=1.3619 pcon=5.0362 forget=2.5069 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 120 total=9.0712 mle=1.4860 pcon=5.0370 forget=2.5483 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 170 total=9.0241 mle=1.4611 pcon=5.0375 forget=2.5255 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 220 total=8.9096 mle=1.3740 pcon=5.0380 forget=2.4976 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 270 total=9.1684 mle=1.5873 pcon=5.0385 forget=2.5426 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 320 total=8.9840 mle=1.4575 pcon=5.0392 forget=2.4873 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 47 it 370 total=8.9133 mle=1.3657 pcon=5.0396 forget=2.5080 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 30 total=9.1005 mle=1.5445 pcon=5.0402 forget=2.5158 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 80 total=9.0714 mle=1.5045 pcon=5.0409 forget=2.5260 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 130 total=8.9650 mle=1.3805 pcon=5.0418 forget=2.5427 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 180 total=9.0159 mle=1.4284 pcon=5.0422 forget=2.5453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 230 total=8.9870 mle=1.4130 pcon=5.0428 forget=2.5312 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 280 total=8.9260 mle=1.3433 pcon=5.0437 forget=2.5390 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 330 total=8.9456 mle=1.3589 pcon=5.0444 forget=2.5423 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 48 it 380 total=9.0963 mle=1.5051 pcon=5.0450 forget=2.5462 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 40 total=8.9148 mle=1.3264 pcon=5.0455 forget=2.5428 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 90 total=8.9824 mle=1.3838 pcon=5.0462 forget=2.5524 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 140 total=8.9061 mle=1.3382 pcon=5.0466 forget=2.5213 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 190 total=8.9748 mle=1.3877 pcon=5.0472 forget=2.5400 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 240 total=9.0481 mle=1.4615 pcon=5.0476 forget=2.5390 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 290 total=8.9719 mle=1.4215 pcon=5.0481 forget=2.5024 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 49 it 340 total=9.0006 mle=1.4100 pcon=5.0491 forget=2.5415 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[Full Grid] Completed. Manifest saved to: evaluation_results/CIFAR-100-resnet34-top5-palm-cache6-ema0.999--lambda0.2-epochgrid_runs.csv
