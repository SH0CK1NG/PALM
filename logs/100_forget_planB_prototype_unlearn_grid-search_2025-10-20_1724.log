nohup: ignoring input
[Full Grid] Searching over: lambdas(1.0 0.5 0.2 0.1) × lrs(0.001 0.0005 0.0001) × epochs(5 10 20 50)
[Run] lambda=1.0 lr=0.001 epochs=5
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:30<02:00, 30.01s/it] 40%|████      | 2/5 [00:55<01:21, 27.09s/it] 60%|██████    | 3/5 [01:20<00:52, 26.33s/it] 80%|████████  | 4/5 [01:45<00:25, 25.66s/it]100%|██████████| 5/5 [02:10<00:00, 25.66s/it]100%|██████████| 5/5 [02:10<00:00, 26.15s/it]
[loss] ep 0 it 0 total=13.7434 mle=1.5709 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8369 mle=1.5424 pcon=5.2879 forget=7.0066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8512 mle=1.7004 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0217 mle=1.8997 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8930 mle=1.7131 pcon=5.2670 forget=6.9129 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6543 mle=1.5020 pcon=5.2603 forget=6.8920 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7232 mle=1.5597 pcon=5.2540 forget=6.9095 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8764 mle=1.6809 pcon=5.2476 forget=6.9479 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=13.9621 mle=1.6716 pcon=5.2409 forget=7.0496 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7062 mle=1.6372 pcon=5.2346 forget=6.8344 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6914 mle=1.5166 pcon=5.2284 forget=6.9464 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9484 mle=1.7804 pcon=5.2224 forget=6.9456 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9067 mle=1.7648 pcon=5.2167 forget=6.9252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6564 mle=1.6051 pcon=5.2112 forget=6.8401 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8180 mle=1.7235 pcon=5.2056 forget=6.8889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9235 mle=1.7943 pcon=5.2003 forget=6.9290 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=13.6298 mle=1.5530 pcon=5.1950 forget=6.8818 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8627 mle=1.8881 pcon=5.1899 forget=6.7847 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8038 mle=1.7420 pcon=5.1846 forget=6.8771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5271 mle=1.5265 pcon=5.1796 forget=6.8210 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6861 mle=1.5858 pcon=5.1745 forget=6.9257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9021 mle=1.8840 pcon=5.1698 forget=6.8483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5393 mle=1.5911 pcon=5.1651 forget=6.7830 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7887 mle=1.8112 pcon=5.1604 forget=6.8171 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=13.7064 mle=1.7047 pcon=5.1558 forget=6.8458 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7121 mle=1.6713 pcon=5.1518 forget=6.8890 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7349 mle=1.7665 pcon=5.1474 forget=6.8210 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8713 mle=1.9269 pcon=5.1434 forget=6.8009 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.4988 mle=1.5695 pcon=5.1393 forget=6.7899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7426 mle=1.8359 pcon=5.1351 forget=6.7716 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.6962 mle=1.7028 pcon=5.1308 forget=6.8626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.6912 mle=1.6955 pcon=5.1272 forget=6.8685 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=13.4700 mle=1.6521 pcon=5.1233 forget=6.6946 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6125 mle=1.7319 pcon=5.1192 forget=6.7615 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5722 mle=1.6522 pcon=5.1155 forget=6.8045 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3640 mle=1.4534 pcon=5.1119 forget=6.7987 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6078 mle=1.7661 pcon=5.1079 forget=6.7338 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3132 mle=1.4892 pcon=5.1042 forget=6.7197 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7346 mle=1.8760 pcon=5.1006 forget=6.7581 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:04,  3.13it/s]  2%|▏         | 6/391 [00:00<00:22, 17.21it/s]  3%|▎         | 12/391 [00:00<00:13, 29.12it/s]  5%|▍         | 19/391 [00:00<00:09, 40.04it/s]  7%|▋         | 28/391 [00:00<00:06, 53.88it/s]  9%|▉         | 37/391 [00:00<00:05, 63.74it/s] 12%|█▏        | 46/391 [00:00<00:04, 70.81it/s] 14%|█▍        | 55/391 [00:01<00:04, 75.80it/s] 16%|█▌        | 63/391 [00:01<00:05, 63.78it/s] 18%|█▊        | 70/391 [00:01<00:05, 61.04it/s] 20%|█▉        | 77/391 [00:01<00:04, 63.17it/s] 22%|██▏       | 86/391 [00:01<00:04, 69.73it/s] 24%|██▍       | 95/391 [00:01<00:03, 74.80it/s] 27%|██▋       | 104/391 [00:01<00:03, 78.75it/s] 29%|██▉       | 113/391 [00:01<00:03, 77.58it/s] 31%|███       | 121/391 [00:02<00:04, 65.10it/s] 33%|███▎      | 128/391 [00:02<00:04, 60.94it/s] 35%|███▌      | 137/391 [00:02<00:03, 66.81it/s] 37%|███▋      | 146/391 [00:02<00:03, 72.08it/s] 40%|███▉      | 155/391 [00:02<00:03, 76.27it/s] 42%|████▏     | 164/391 [00:02<00:02, 79.24it/s] 44%|████▍     | 173/391 [00:02<00:03, 67.20it/s] 46%|████▋     | 181/391 [00:02<00:03, 61.74it/s] 48%|████▊     | 189/391 [00:03<00:03, 65.33it/s] 51%|█████     | 198/391 [00:03<00:02, 70.95it/s] 53%|█████▎    | 207/391 [00:03<00:02, 75.67it/s] 55%|█████▌    | 216/391 [00:03<00:02, 79.13it/s] 58%|█████▊    | 225/391 [00:03<00:02, 73.66it/s] 60%|█████▉    | 233/391 [00:03<00:02, 64.45it/s] 61%|██████▏   | 240/391 [00:03<00:02, 62.46it/s] 64%|██████▎   | 249/391 [00:03<00:02, 68.89it/s] 66%|██████▌   | 258/391 [00:03<00:01, 74.24it/s] 68%|██████▊   | 267/391 [00:04<00:01, 78.04it/s] 71%|███████   | 276/391 [00:04<00:01, 80.88it/s] 73%|███████▎  | 285/391 [00:04<00:01, 66.67it/s] 75%|███████▍  | 293/391 [00:04<00:01, 61.79it/s] 77%|███████▋  | 302/391 [00:04<00:01, 68.05it/s] 80%|███████▉  | 311/391 [00:04<00:01, 72.35it/s] 82%|████████▏ | 320/391 [00:04<00:00, 76.41it/s] 84%|████████▍ | 329/391 [00:04<00:00, 79.66it/s] 86%|████████▋ | 338/391 [00:05<00:00, 68.68it/s] 88%|████████▊ | 346/391 [00:05<00:00, 63.74it/s] 90%|█████████ | 353/391 [00:05<00:00, 64.96it/s] 93%|█████████▎| 362/391 [00:05<00:00, 70.84it/s] 95%|█████████▍| 371/391 [00:05<00:00, 74.99it/s] 97%|█████████▋| 380/391 [00:05<00:00, 78.03it/s] 99%|█████████▉| 389/391 [00:05<00:00, 81.32it/s]100%|██████████| 391/391 [00:05<00:00, 67.09it/s]
50000 images processed, 5.972806930541992 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:30,  2.57it/s]  8%|▊         | 6/79 [00:00<00:04, 14.97it/s] 15%|█▌        | 12/79 [00:00<00:02, 26.13it/s] 25%|██▌       | 20/79 [00:00<00:01, 40.26it/s] 37%|███▋      | 29/79 [00:00<00:00, 53.42it/s] 48%|████▊     | 38/79 [00:00<00:00, 63.01it/s] 59%|█████▉    | 47/79 [00:01<00:00, 69.66it/s] 70%|██████▉   | 55/79 [00:01<00:00, 66.67it/s] 80%|███████▉  | 63/79 [00:01<00:00, 61.47it/s] 89%|████████▊ | 70/79 [00:01<00:00, 59.44it/s]100%|██████████| 79/79 [00:01<00:00, 65.89it/s]100%|██████████| 79/79 [00:01<00:00, 50.93it/s]
10000 images processed, 1.5783956050872803 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:06,  3.05it/s]  3%|▎         | 7/204 [00:00<00:09, 19.74it/s]  7%|▋         | 15/204 [00:00<00:05, 37.56it/s] 12%|█▏        | 24/204 [00:00<00:03, 52.50it/s] 16%|█▌        | 33/204 [00:00<00:02, 62.81it/s] 20%|██        | 41/204 [00:00<00:02, 65.78it/s] 24%|██▍       | 49/204 [00:01<00:02, 55.87it/s] 27%|██▋       | 56/204 [00:01<00:02, 55.01it/s] 32%|███▏      | 65/204 [00:01<00:02, 62.91it/s] 36%|███▋      | 74/204 [00:01<00:01, 68.78it/s] 41%|████      | 83/204 [00:01<00:01, 73.28it/s] 45%|████▍     | 91/204 [00:01<00:01, 65.99it/s] 48%|████▊     | 98/204 [00:01<00:01, 60.91it/s] 51%|█████▏    | 105/204 [00:01<00:01, 60.75it/s] 56%|█████▌    | 114/204 [00:02<00:01, 67.11it/s] 60%|██████    | 123/204 [00:02<00:01, 72.43it/s] 65%|██████▍   | 132/204 [00:02<00:00, 76.43it/s] 69%|██████▊   | 140/204 [00:02<00:00, 71.95it/s] 73%|███████▎  | 148/204 [00:02<00:00, 62.04it/s] 76%|███████▌  | 155/204 [00:02<00:00, 60.20it/s] 80%|████████  | 164/204 [00:02<00:00, 66.58it/s] 85%|████████▍ | 173/204 [00:02<00:00, 71.88it/s] 89%|████████▉ | 182/204 [00:02<00:00, 75.92it/s] 93%|█████████▎| 190/204 [00:03<00:00, 69.67it/s] 97%|█████████▋| 198/204 [00:03<00:00, 62.93it/s]100%|██████████| 204/204 [00:03<00:00, 60.51it/s]
26032 images processed, 3.4452426433563232 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:47,  1.63it/s]  9%|▉         | 7/79 [00:00<00:05, 12.59it/s] 15%|█▌        | 12/79 [00:00<00:03, 20.29it/s] 22%|██▏       | 17/79 [00:00<00:02, 25.63it/s] 28%|██▊       | 22/79 [00:01<00:01, 30.04it/s] 34%|███▍      | 27/79 [00:01<00:01, 33.04it/s] 42%|████▏     | 33/79 [00:01<00:01, 38.21it/s] 52%|█████▏    | 41/79 [00:01<00:00, 40.81it/s] 58%|█████▊    | 46/79 [00:01<00:00, 42.32it/s] 65%|██████▍   | 51/79 [00:01<00:00, 43.55it/s] 72%|███████▏  | 57/79 [00:01<00:00, 41.20it/s] 78%|███████▊  | 62/79 [00:01<00:00, 41.44it/s] 85%|████████▍ | 67/79 [00:02<00:00, 42.37it/s] 92%|█████████▏| 73/79 [00:02<00:00, 43.87it/s]100%|██████████| 79/79 [00:02<00:00, 34.32it/s]
10000 images processed, 2.342430353164673 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:34,  2.23it/s]  9%|▉         | 7/79 [00:00<00:04, 15.93it/s] 19%|█▉        | 15/79 [00:00<00:01, 32.16it/s] 30%|███       | 24/79 [00:00<00:01, 46.70it/s] 42%|████▏     | 33/79 [00:00<00:00, 57.49it/s] 52%|█████▏    | 41/79 [00:00<00:00, 59.37it/s] 61%|██████    | 48/79 [00:01<00:00, 53.72it/s] 70%|██████▉   | 55/79 [00:01<00:00, 53.18it/s] 81%|████████  | 64/79 [00:01<00:00, 61.58it/s] 92%|█████████▏| 73/79 [00:01<00:00, 68.60it/s]100%|██████████| 79/79 [00:01<00:00, 50.84it/s]
10000 images processed, 1.6563470363616943 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:31,  2.19it/s] 11%|█▏        | 8/70 [00:00<00:03, 17.70it/s] 24%|██▍       | 17/70 [00:00<00:01, 35.13it/s] 37%|███▋      | 26/70 [00:00<00:00, 48.77it/s] 50%|█████     | 35/70 [00:00<00:00, 59.05it/s] 61%|██████▏   | 43/70 [00:01<00:00, 55.19it/s] 71%|███████▏  | 50/70 [00:01<00:00, 53.61it/s] 81%|████████▏ | 57/70 [00:01<00:00, 57.37it/s] 94%|█████████▍| 66/70 [00:01<00:00, 65.63it/s]100%|██████████| 70/70 [00:01<00:00, 48.46it/s]
8925 images processed, 1.479687213897705 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:38,  1.15it/s]  4%|▍         | 2/45 [00:01<00:21,  2.04it/s] 20%|██        | 9/45 [00:01<00:03, 10.29it/s] 24%|██▍       | 11/45 [00:01<00:04,  8.43it/s] 38%|███▊      | 17/45 [00:01<00:01, 14.91it/s] 44%|████▍     | 20/45 [00:02<00:02, 12.04it/s] 53%|█████▎    | 24/45 [00:02<00:01, 15.80it/s] 60%|██████    | 27/45 [00:02<00:01, 12.72it/s] 67%|██████▋   | 30/45 [00:02<00:01, 12.67it/s] 73%|███████▎  | 33/45 [00:03<00:00, 14.19it/s] 78%|███████▊  | 35/45 [00:03<00:00, 10.36it/s] 93%|█████████▎| 42/45 [00:04<00:00, 10.13it/s]100%|██████████| 45/45 [00:04<00:00, 10.85it/s]
5640 images processed, 4.1717848777771 seconds used

22.45780348777771
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.53  99.36
places365     67.96  81.19
LSUN          17.51  96.09
iSUN          72.25  81.77
dtd           37.91  91.39
forget        79.50  86.90
AVG           46.28  89.45
Retain-Acc: 0.7406
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
53.79726314544678
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=1.0 lr=0.001 epochs=10
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:30<04:32, 30.29s/it] 20%|██        | 2/10 [00:57<03:46, 28.34s/it] 30%|███       | 3/10 [01:22<03:09, 27.06s/it] 40%|████      | 4/10 [01:48<02:39, 26.62s/it] 50%|█████     | 5/10 [02:15<02:12, 26.59s/it] 60%|██████    | 6/10 [02:40<01:44, 26.10s/it] 70%|███████   | 7/10 [03:06<01:18, 26.10s/it] 80%|████████  | 8/10 [03:32<00:52, 26.06s/it][loss] ep 0 it 0 total=13.7435 mle=1.5710 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8370 mle=1.5424 pcon=5.2879 forget=7.0066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8513 mle=1.7005 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0218 mle=1.8998 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8931 mle=1.7132 pcon=5.2670 forget=6.9130 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6543 mle=1.5020 pcon=5.2603 forget=6.8919 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7233 mle=1.5597 pcon=5.2540 forget=6.9095 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8764 mle=1.6810 pcon=5.2476 forget=6.9478 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=13.9620 mle=1.6716 pcon=5.2409 forget=7.0496 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7062 mle=1.6371 pcon=5.2346 forget=6.8344 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6914 mle=1.5165 pcon=5.2284 forget=6.9464 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9483 mle=1.7804 pcon=5.2224 forget=6.9455 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9066 mle=1.7647 pcon=5.2167 forget=6.9252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6561 mle=1.6049 pcon=5.2112 forget=6.8400 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8182 mle=1.7237 pcon=5.2056 forget=6.8889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9235 mle=1.7943 pcon=5.2003 forget=6.9289 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=13.6297 mle=1.5530 pcon=5.1950 forget=6.8817 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8626 mle=1.8880 pcon=5.1899 forget=6.7847 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8040 mle=1.7421 pcon=5.1846 forget=6.8772 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5269 mle=1.5263 pcon=5.1796 forget=6.8210 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6861 mle=1.5858 pcon=5.1745 forget=6.9258 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9022 mle=1.8841 pcon=5.1698 forget=6.8483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5393 mle=1.5912 pcon=5.1651 forget=6.7830 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7884 mle=1.8109 pcon=5.1604 forget=6.8170 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=13.7066 mle=1.7049 pcon=5.1558 forget=6.8459 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7119 mle=1.6712 pcon=5.1518 forget=6.8890 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7348 mle=1.7665 pcon=5.1474 forget=6.8210 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8714 mle=1.9270 pcon=5.1434 forget=6.8010 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.4988 mle=1.5696 pcon=5.1393 forget=6.7899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7423 mle=1.8356 pcon=5.1351 forget=6.7716 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.6962 mle=1.7028 pcon=5.1308 forget=6.8626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.6910 mle=1.6954 pcon=5.1272 forget=6.8685 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=13.4700 mle=1.6521 pcon=5.1233 forget=6.6946 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6122 mle=1.7317 pcon=5.1192 forget=6.7613 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5720 mle=1.6521 pcon=5.1154 forget=6.8044 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3640 mle=1.4534 pcon=5.1119 forget=6.7987 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6078 mle=1.7661 pcon=5.1079 forget=6.7338 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3132 mle=1.4892 pcon=5.1042 forget=6.7198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7345 mle=1.8758 pcon=5.1006 forget=6.7581 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=13.6935 mle=1.8653 pcon=5.0970 forget=6.7311 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=13.5492 mle=1.8063 pcon=5.0931 forget=6.6498 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=13.4058 mle=1.5904 pcon=5.0897 forget=6.7257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=13.3915 mle=1.5923 pcon=5.0858 forget=6.7134 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=13.4120 mle=1.6921 pcon=5.0826 forget=6.6374 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=13.4546 mle=1.7801 pcon=5.0792 forget=6.5953 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=13.2877 mle=1.6034 pcon=5.0757 forget=6.6086 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=13.4639 mle=1.7401 pcon=5.0724 forget=6.6514 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=13.2318 mle=1.4103 pcon=5.0691 forget=6.7524 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=13.1497 mle=1.5273 pcon=5.0660 forget=6.5563 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=13.3006 mle=1.5943 pcon=5.0630 forget=6.6433 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=13.3710 mle=1.6816 pcon=5.0600 forget=6.6294 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=13.3247 mle=1.6310 pcon=5.0576 forget=6.6361 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=13.5427 mle=1.7993 pcon=5.0546 forget=6.6889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=13.3507 mle=1.5470 pcon=5.0521 forget=6.7515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=13.4036 mle=1.6704 pcon=5.0498 forget=6.6834 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=13.4034 mle=1.6511 pcon=5.0474 forget=6.7049 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=13.5264 mle=1.7660 pcon=5.0453 forget=6.7151 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=13.4397 mle=1.6174 pcon=5.0432 forget=6.7790 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=13.4470 mle=1.5966 pcon=5.0414 forget=6.8089 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=13.5856 mle=1.6447 pcon=5.0401 forget=6.9008 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=13.7965 mle=1.7799 pcon=5.0388 forget=6.9778 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=13.7836 mle=1.7537 pcon=5.0372 forget=6.9927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=13.6789 mle=1.6351 pcon=5.0361 forget=7.0077 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 30 total=13.7225 mle=1.5611 pcon=5.0352 forget=7.1262 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=13.7310 mle=1.5985 pcon=5.0343 forget=7.0982 nr=64 nf=64 protos=540 fproto_sim=NA
 90%|█████████ | 9/10 [03:59<00:26, 26.20s/it]100%|██████████| 10/10 [04:24<00:00, 26.03s/it]100%|██████████| 10/10 [04:24<00:00, 26.47s/it]
[loss] ep 8 it 130 total=13.6543 mle=1.4968 pcon=5.0331 forget=7.1243 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=13.8710 mle=1.7136 pcon=5.0322 forget=7.1252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=13.6505 mle=1.5296 pcon=5.0315 forget=7.0894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=13.7760 mle=1.7031 pcon=5.0302 forget=7.0427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=13.8857 mle=1.7306 pcon=5.0286 forget=7.1266 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=13.4633 mle=1.4732 pcon=5.0270 forget=6.9631 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 40 total=13.4103 mle=1.6206 pcon=5.0252 forget=6.7645 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=13.3209 mle=1.6541 pcon=5.0228 forget=6.6440 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=13.2958 mle=1.8050 pcon=5.0206 forget=6.4702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=12.7673 mle=1.5772 pcon=5.0179 forget=6.1722 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=12.6483 mle=1.5925 pcon=5.0147 forget=6.0411 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=12.5618 mle=1.5532 pcon=5.0109 forget=5.9977 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=12.5521 mle=1.5802 pcon=5.0074 forget=5.9644 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:04,  3.14it/s]  3%|▎         | 10/391 [00:00<00:12, 29.97it/s]  5%|▌         | 20/391 [00:00<00:07, 50.22it/s]  8%|▊         | 30/391 [00:00<00:05, 63.97it/s] 10%|█         | 40/391 [00:00<00:04, 73.55it/s] 13%|█▎        | 50/391 [00:00<00:04, 80.44it/s] 15%|█▌        | 60/391 [00:00<00:03, 85.16it/s] 18%|█▊        | 70/391 [00:01<00:03, 88.35it/s] 20%|██        | 80/391 [00:01<00:03, 84.28it/s] 23%|██▎       | 89/391 [00:01<00:03, 79.52it/s] 25%|██▌       | 98/391 [00:01<00:03, 82.24it/s] 27%|██▋       | 107/391 [00:01<00:03, 84.31it/s] 30%|██▉       | 116/391 [00:01<00:03, 85.68it/s] 32%|███▏      | 125/391 [00:01<00:03, 85.87it/s] 34%|███▍      | 134/391 [00:01<00:02, 86.51it/s] 37%|███▋      | 143/391 [00:02<00:03, 70.51it/s] 39%|███▊      | 151/391 [00:02<00:03, 64.39it/s] 41%|████      | 159/391 [00:02<00:03, 67.96it/s] 43%|████▎     | 168/391 [00:02<00:03, 73.34it/s] 45%|████▌     | 177/391 [00:02<00:02, 77.35it/s] 48%|████▊     | 186/391 [00:02<00:02, 80.61it/s] 50%|████▉     | 195/391 [00:02<00:02, 82.26it/s] 52%|█████▏    | 204/391 [00:02<00:02, 77.87it/s] 54%|█████▍    | 212/391 [00:02<00:02, 66.44it/s] 56%|█████▋    | 220/391 [00:03<00:02, 63.27it/s] 59%|█████▊    | 229/391 [00:03<00:02, 69.41it/s] 61%|██████    | 238/391 [00:03<00:02, 74.56it/s] 63%|██████▎   | 247/391 [00:03<00:01, 78.25it/s] 65%|██████▌   | 256/391 [00:03<00:01, 80.82it/s] 68%|██████▊   | 265/391 [00:03<00:01, 81.48it/s] 70%|███████   | 274/391 [00:03<00:01, 66.28it/s] 72%|███████▏  | 282/391 [00:03<00:01, 63.14it/s] 74%|███████▍  | 291/391 [00:04<00:01, 69.23it/s] 77%|███████▋  | 300/391 [00:04<00:01, 74.20it/s] 79%|███████▉  | 309/391 [00:04<00:01, 76.69it/s] 81%|████████▏ | 318/391 [00:04<00:00, 79.76it/s] 84%|████████▎ | 327/391 [00:04<00:00, 74.02it/s] 86%|████████▌ | 335/391 [00:04<00:00, 66.44it/s] 87%|████████▋ | 342/391 [00:04<00:00, 63.10it/s] 90%|████████▉ | 351/391 [00:04<00:00, 68.76it/s] 92%|█████████▏| 360/391 [00:04<00:00, 73.71it/s] 94%|█████████▍| 369/391 [00:05<00:00, 75.88it/s] 97%|█████████▋| 378/391 [00:05<00:00, 79.01it/s] 99%|█████████▉| 387/391 [00:05<00:00, 71.89it/s]100%|██████████| 391/391 [00:05<00:00, 71.69it/s]
50000 images processed, 5.6300883293151855 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:26,  2.98it/s]  9%|▉         | 7/79 [00:00<00:03, 18.95it/s] 20%|██        | 16/79 [00:00<00:01, 38.62it/s] 32%|███▏      | 25/79 [00:00<00:01, 52.97it/s] 43%|████▎     | 34/79 [00:00<00:00, 63.23it/s] 54%|█████▍    | 43/79 [00:00<00:00, 69.71it/s] 65%|██████▍   | 51/79 [00:01<00:00, 58.74it/s] 73%|███████▎  | 58/79 [00:01<00:00, 58.91it/s] 84%|████████▎ | 66/79 [00:01<00:00, 62.59it/s] 95%|█████████▍| 75/79 [00:01<00:00, 69.70it/s]100%|██████████| 79/79 [00:01<00:00, 54.97it/s]
10000 images processed, 1.4763529300689697 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:17,  2.63it/s]  4%|▍         | 9/204 [00:00<00:08, 23.03it/s]  9%|▉         | 18/204 [00:00<00:04, 40.70it/s] 13%|█▎        | 26/204 [00:00<00:03, 51.34it/s] 16%|█▌        | 33/204 [00:00<00:03, 54.99it/s] 20%|█▉        | 40/204 [00:00<00:03, 51.12it/s] 23%|██▎       | 46/204 [00:01<00:03, 51.18it/s] 27%|██▋       | 55/204 [00:01<00:02, 60.50it/s] 31%|███▏      | 64/204 [00:01<00:02, 67.52it/s] 36%|███▌      | 73/204 [00:01<00:01, 73.02it/s] 40%|████      | 82/204 [00:01<00:01, 77.02it/s] 44%|████▍     | 90/204 [00:01<00:01, 68.42it/s] 48%|████▊     | 98/204 [00:01<00:01, 58.43it/s] 52%|█████▏    | 106/204 [00:01<00:01, 62.78it/s] 56%|█████▋    | 115/204 [00:02<00:01, 68.58it/s] 61%|██████    | 124/204 [00:02<00:01, 73.40it/s] 65%|██████▌   | 133/204 [00:02<00:00, 77.42it/s] 70%|██████▉   | 142/204 [00:02<00:00, 65.26it/s] 74%|███████▎  | 150/204 [00:02<00:00, 59.28it/s] 78%|███████▊  | 159/204 [00:02<00:00, 65.08it/s] 82%|████████▏ | 168/204 [00:02<00:00, 70.23it/s] 87%|████████▋ | 177/204 [00:02<00:00, 74.17it/s] 91%|█████████ | 185/204 [00:03<00:00, 70.24it/s] 95%|█████████▍| 193/204 [00:03<00:00, 61.89it/s] 98%|█████████▊| 200/204 [00:03<00:00, 60.50it/s]100%|██████████| 204/204 [00:03<00:00, 60.05it/s]
26032 images processed, 3.454428195953369 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:50,  1.55it/s]  5%|▌         | 4/79 [00:00<00:11,  6.69it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.40it/s] 19%|█▉        | 15/79 [00:00<00:02, 24.45it/s] 28%|██▊       | 22/79 [00:01<00:01, 34.89it/s] 34%|███▍      | 27/79 [00:01<00:01, 34.02it/s] 42%|████▏     | 33/79 [00:01<00:01, 40.06it/s] 48%|████▊     | 38/79 [00:01<00:00, 41.75it/s] 57%|█████▋    | 45/79 [00:01<00:00, 48.48it/s] 65%|██████▍   | 51/79 [00:01<00:00, 41.84it/s] 72%|███████▏  | 57/79 [00:01<00:00, 45.52it/s] 80%|███████▉  | 63/79 [00:01<00:00, 48.90it/s] 89%|████████▊ | 70/79 [00:02<00:00, 52.89it/s] 96%|█████████▌| 76/79 [00:02<00:00, 45.74it/s]100%|██████████| 79/79 [00:02<00:00, 35.00it/s]
10000 images processed, 2.2919623851776123 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.01it/s]  9%|▉         | 7/79 [00:00<00:03, 19.88it/s] 20%|██        | 16/79 [00:00<00:01, 40.24it/s] 32%|███▏      | 25/79 [00:00<00:00, 54.42it/s] 43%|████▎     | 34/79 [00:00<00:00, 64.42it/s] 53%|█████▎    | 42/79 [00:00<00:00, 64.21it/s] 63%|██████▎   | 50/79 [00:01<00:00, 57.80it/s] 72%|███████▏  | 57/79 [00:01<00:00, 57.23it/s] 84%|████████▎ | 66/79 [00:01<00:00, 65.09it/s] 95%|█████████▍| 75/79 [00:01<00:00, 71.57it/s]100%|██████████| 79/79 [00:01<00:00, 56.24it/s]
10000 images processed, 1.4221501350402832 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:25,  2.67it/s] 14%|█▍        | 10/70 [00:00<00:02, 26.59it/s] 26%|██▌       | 18/70 [00:00<00:01, 40.69it/s] 36%|███▌      | 25/70 [00:00<00:01, 41.23it/s] 44%|████▍     | 31/70 [00:00<00:00, 44.43it/s] 56%|█████▌    | 39/70 [00:00<00:00, 53.13it/s] 69%|██████▊   | 48/70 [00:01<00:00, 62.34it/s] 81%|████████▏ | 57/70 [00:01<00:00, 69.69it/s] 94%|█████████▍| 66/70 [00:01<00:00, 75.21it/s]100%|██████████| 70/70 [00:01<00:00, 52.89it/s]
8925 images processed, 1.3553228378295898 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:37,  1.16it/s]  4%|▍         | 2/45 [00:01<00:18,  2.29it/s] 18%|█▊        | 8/45 [00:01<00:03, 11.48it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.90it/s] 36%|███▌      | 16/45 [00:01<00:02, 14.39it/s] 42%|████▏     | 19/45 [00:02<00:02, 12.02it/s] 49%|████▉     | 22/45 [00:02<00:01, 13.58it/s] 56%|█████▌    | 25/45 [00:02<00:01, 13.41it/s] 60%|██████    | 27/45 [00:02<00:01, 13.69it/s] 67%|██████▋   | 30/45 [00:02<00:01, 12.88it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.41it/s] 78%|███████▊  | 35/45 [00:03<00:00, 11.72it/s] 89%|████████▉ | 40/45 [00:03<00:00, 17.85it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.54it/s]100%|██████████| 45/45 [00:04<00:00, 10.89it/s]
5640 images processed, 4.163875579833984 seconds used

21.469862461090088
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.55  99.35
places365     68.17  81.15
LSUN          17.19  96.14
iSUN          71.75  81.93
dtd           38.33  91.32
forget        78.50  87.02
AVG           46.08  89.49
Retain-Acc: 0.7397
Forget-as-OOD (retain known vs forget novel):
  FPR: 78.50 AUROC: 87.02 AUIN: 98.38
62.5753128528595
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=1.0 lr=0.001 epochs=20
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:24<07:40, 24.24s/it] 10%|█         | 2/20 [00:47<07:03, 23.52s/it] 15%|█▌        | 3/20 [01:03<05:46, 20.37s/it] 20%|██        | 4/20 [01:21<05:07, 19.20s/it] 25%|██▌       | 5/20 [01:37<04:33, 18.23s/it] 30%|███       | 6/20 [01:58<04:28, 19.21s/it] 35%|███▌      | 7/20 [02:21<04:22, 20.22s/it] 40%|████      | 8/20 [02:37<03:47, 18.93s/it][loss] ep 0 it 0 total=13.7434 mle=1.5709 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8369 mle=1.5424 pcon=5.2879 forget=7.0066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8513 mle=1.7004 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0218 mle=1.8998 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8933 mle=1.7133 pcon=5.2670 forget=6.9130 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6543 mle=1.5021 pcon=5.2603 forget=6.8919 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7231 mle=1.5597 pcon=5.2540 forget=6.9094 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8765 mle=1.6810 pcon=5.2476 forget=6.9479 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=13.9621 mle=1.6716 pcon=5.2409 forget=7.0497 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7060 mle=1.6369 pcon=5.2346 forget=6.8345 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6915 mle=1.5166 pcon=5.2284 forget=6.9465 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9482 mle=1.7803 pcon=5.2224 forget=6.9455 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9066 mle=1.7646 pcon=5.2167 forget=6.9253 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6563 mle=1.6051 pcon=5.2112 forget=6.8401 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8181 mle=1.7236 pcon=5.2056 forget=6.8889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9235 mle=1.7943 pcon=5.2003 forget=6.9289 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=13.6296 mle=1.5529 pcon=5.1950 forget=6.8817 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8630 mle=1.8883 pcon=5.1899 forget=6.7848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8039 mle=1.7421 pcon=5.1846 forget=6.8771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5267 mle=1.5262 pcon=5.1796 forget=6.8210 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6858 mle=1.5856 pcon=5.1745 forget=6.9257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9022 mle=1.8841 pcon=5.1698 forget=6.8483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5395 mle=1.5914 pcon=5.1651 forget=6.7831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7885 mle=1.8111 pcon=5.1604 forget=6.8170 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=13.7066 mle=1.7049 pcon=5.1558 forget=6.8459 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7120 mle=1.6712 pcon=5.1518 forget=6.8890 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7349 mle=1.7665 pcon=5.1474 forget=6.8210 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8713 mle=1.9269 pcon=5.1434 forget=6.8010 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.4988 mle=1.5695 pcon=5.1393 forget=6.7900 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7424 mle=1.8358 pcon=5.1351 forget=6.7715 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.6962 mle=1.7028 pcon=5.1308 forget=6.8626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.6911 mle=1.6954 pcon=5.1272 forget=6.8685 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=13.4700 mle=1.6521 pcon=5.1233 forget=6.6946 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6124 mle=1.7318 pcon=5.1192 forget=6.7614 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5720 mle=1.6521 pcon=5.1155 forget=6.8044 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3638 mle=1.4534 pcon=5.1119 forget=6.7985 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6082 mle=1.7665 pcon=5.1079 forget=6.7338 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3135 mle=1.4894 pcon=5.1042 forget=6.7198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7345 mle=1.8758 pcon=5.1006 forget=6.7581 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=13.6935 mle=1.8654 pcon=5.0970 forget=6.7311 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=13.5491 mle=1.8062 pcon=5.0931 forget=6.6498 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=13.4057 mle=1.5905 pcon=5.0897 forget=6.7256 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=13.3916 mle=1.5924 pcon=5.0858 forget=6.7134 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=13.4119 mle=1.6920 pcon=5.0826 forget=6.6373 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=13.4547 mle=1.7801 pcon=5.0792 forget=6.5954 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=13.2879 mle=1.6036 pcon=5.0757 forget=6.6086 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=13.4637 mle=1.7399 pcon=5.0724 forget=6.6514 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=13.2320 mle=1.4104 pcon=5.0692 forget=6.7525 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=13.1497 mle=1.5272 pcon=5.0660 forget=6.5564 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=13.3005 mle=1.5942 pcon=5.0630 forget=6.6433 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=13.3710 mle=1.6815 pcon=5.0600 forget=6.6295 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=13.3248 mle=1.6311 pcon=5.0576 forget=6.6361 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=13.5428 mle=1.7993 pcon=5.0546 forget=6.6889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=13.3506 mle=1.5470 pcon=5.0521 forget=6.7515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=13.4035 mle=1.6704 pcon=5.0498 forget=6.6833 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=13.4032 mle=1.6509 pcon=5.0474 forget=6.7049 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=13.5265 mle=1.7660 pcon=5.0453 forget=6.7151 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=13.4398 mle=1.6174 pcon=5.0432 forget=6.7791 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=13.4467 mle=1.5964 pcon=5.0414 forget=6.8089 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=13.5855 mle=1.6446 pcon=5.0401 forget=6.9008 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=13.7964 mle=1.7800 pcon=5.0388 forget=6.9777 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=13.7837 mle=1.7538 pcon=5.0372 forget=6.9927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=13.6791 mle=1.6353 pcon=5.0361 forget=7.0077 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 30 total=13.7227 mle=1.5613 pcon=5.0352 forget=7.1262 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=13.7310 mle=1.5986 pcon=5.0343 forget=7.0981 nr=64 nf=64 protos=540 fproto_sim=NA
 45%|████▌     | 9/20 [02:53<03:20, 18.19s/it] 50%|█████     | 10/20 [03:10<02:55, 17.58s/it] 55%|█████▌    | 11/20 [03:30<02:45, 18.41s/it] 60%|██████    | 12/20 [03:53<02:37, 19.75s/it] 65%|██████▌   | 13/20 [04:09<02:11, 18.77s/it] 70%|███████   | 14/20 [04:26<01:48, 18.06s/it] 75%|███████▌  | 15/20 [04:42<01:27, 17.51s/it] 80%|████████  | 16/20 [05:02<01:12, 18.18s/it] 85%|████████▌ | 17/20 [05:25<00:59, 19.67s/it][loss] ep 8 it 130 total=13.6544 mle=1.4969 pcon=5.0331 forget=7.1244 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=13.8711 mle=1.7137 pcon=5.0322 forget=7.1252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=13.6504 mle=1.5295 pcon=5.0314 forget=7.0894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=13.7760 mle=1.7031 pcon=5.0302 forget=7.0427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=13.8858 mle=1.7306 pcon=5.0286 forget=7.1266 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=13.4634 mle=1.4733 pcon=5.0270 forget=6.9630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 40 total=13.4102 mle=1.6205 pcon=5.0252 forget=6.7645 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=13.3210 mle=1.6542 pcon=5.0228 forget=6.6439 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=13.2960 mle=1.8052 pcon=5.0206 forget=6.4702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=12.7674 mle=1.5773 pcon=5.0179 forget=6.1722 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=12.6482 mle=1.5923 pcon=5.0147 forget=6.0412 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=12.5620 mle=1.5533 pcon=5.0109 forget=5.9977 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=12.5520 mle=1.5801 pcon=5.0074 forget=5.9645 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=12.8019 mle=1.6823 pcon=5.0037 forget=6.1159 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=12.8263 mle=1.6226 pcon=5.0000 forget=6.2037 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=12.9459 mle=1.7457 pcon=4.9967 forget=6.2035 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=13.1367 mle=1.8063 pcon=4.9935 forget=6.3369 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=13.3381 mle=1.8845 pcon=4.9906 forget=6.4629 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=13.2327 mle=1.6570 pcon=4.9880 forget=6.5877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=13.3262 mle=1.6403 pcon=4.9857 forget=6.7003 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=13.4489 mle=1.6386 pcon=4.9835 forget=6.8269 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 10 total=13.5507 mle=1.5991 pcon=4.9815 forget=6.9702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=13.7403 mle=1.7200 pcon=4.9797 forget=7.0405 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=13.7619 mle=1.6310 pcon=4.9783 forget=7.1526 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=13.9553 mle=1.6967 pcon=4.9766 forget=7.2820 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=14.0705 mle=1.7386 pcon=4.9757 forget=7.3561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=13.9819 mle=1.6107 pcon=4.9751 forget=7.3961 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=14.3541 mle=1.9732 pcon=4.9741 forget=7.4068 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=14.2350 mle=1.7617 pcon=4.9734 forget=7.5000 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 20 total=14.0208 mle=1.5778 pcon=4.9726 forget=7.4704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=14.1911 mle=1.7525 pcon=4.9719 forget=7.4667 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=13.8952 mle=1.6182 pcon=4.9711 forget=7.3059 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=13.8861 mle=1.7429 pcon=4.9703 forget=7.1729 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=13.7453 mle=1.5935 pcon=4.9692 forget=7.1826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=13.6314 mle=1.5692 pcon=4.9683 forget=7.0938 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=13.6289 mle=1.5228 pcon=4.9668 forget=7.1394 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=13.6762 mle=1.8435 pcon=4.9649 forget=6.8678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 30 total=13.4089 mle=1.4763 pcon=4.9630 forget=6.9696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=13.2881 mle=1.4676 pcon=4.9608 forget=6.8596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=13.1603 mle=1.5211 pcon=4.9582 forget=6.6810 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=13.2375 mle=1.6231 pcon=4.9556 forget=6.6588 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=13.2112 mle=1.6662 pcon=4.9527 forget=6.5924 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=13.0827 mle=1.5381 pcon=4.9496 forget=6.5950 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=12.9866 mle=1.5917 pcon=4.9462 forget=6.4487 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=12.9376 mle=1.6009 pcon=4.9426 forget=6.3940 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 40 total=13.0408 mle=1.8053 pcon=4.9387 forget=6.2968 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=13.0136 mle=1.7812 pcon=4.9350 forget=6.2975 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=12.7593 mle=1.5859 pcon=4.9313 forget=6.2422 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=12.9364 mle=1.8104 pcon=4.9271 forget=6.1989 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=12.8402 mle=1.6760 pcon=4.9233 forget=6.2409 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=12.8019 mle=1.6479 pcon=4.9192 forget=6.2348 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=12.8790 mle=1.6966 pcon=4.9151 forget=6.2672 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=12.9670 mle=1.7507 pcon=4.9113 forget=6.3050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=12.7480 mle=1.5173 pcon=4.9071 forget=6.3235 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=12.8572 mle=1.5981 pcon=4.9034 forget=6.3558 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=13.0279 mle=1.7144 pcon=4.8997 forget=6.4138 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=12.9895 mle=1.6506 pcon=4.8962 forget=6.4426 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=13.4073 mle=2.0025 pcon=4.8928 forget=6.5120 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=13.0454 mle=1.5786 pcon=4.8897 forget=6.5771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=13.1112 mle=1.5615 pcon=4.8867 forget=6.6631 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 10 total=13.1545 mle=1.6269 pcon=4.8836 forget=6.6440 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=13.1194 mle=1.5388 pcon=4.8809 forget=6.6997 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=13.2099 mle=1.5819 pcon=4.8783 forget=6.7497 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=13.2898 mle=1.6038 pcon=4.8755 forget=6.8105 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=13.2893 mle=1.5981 pcon=4.8728 forget=6.8184 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=13.6051 mle=1.8661 pcon=4.8701 forget=6.8689 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=13.5350 mle=1.7644 pcon=4.8677 forget=6.9029 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=13.4140 mle=1.6171 pcon=4.8654 forget=6.9315 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 20 total=13.6477 mle=1.7874 pcon=4.8628 forget=6.9975 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=13.5966 mle=1.7702 pcon=4.8608 forget=6.9656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=13.5469 mle=1.6218 pcon=4.8586 forget=7.0665 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=13.5170 mle=1.5889 pcon=4.8562 forget=7.0718 nr=64 nf=64 protos=540 fproto_sim=NA
 90%|█████████ | 18/20 [05:41<00:37, 18.74s/it] 95%|█████████▌| 19/20 [05:58<00:18, 18.02s/it]100%|██████████| 20/20 [06:14<00:00, 17.51s/it]100%|██████████| 20/20 [06:14<00:00, 18.73s/it]
[loss] ep 17 it 220 total=13.5927 mle=1.6367 pcon=4.8541 forget=7.1019 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=13.4757 mle=1.5240 pcon=4.8524 forget=7.0993 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=13.8242 mle=1.8285 pcon=4.8505 forget=7.1452 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=13.7796 mle=1.7585 pcon=4.8483 forget=7.1728 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=13.5915 mle=1.5424 pcon=4.8468 forget=7.2023 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=13.7168 mle=1.6117 pcon=4.8450 forget=7.2602 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=13.7554 mle=1.6634 pcon=4.8432 forget=7.2489 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=13.6768 mle=1.5512 pcon=4.8415 forget=7.2841 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=13.8185 mle=1.6743 pcon=4.8397 forget=7.3044 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=13.8024 mle=1.6252 pcon=4.8381 forget=7.3391 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=13.8738 mle=1.7290 pcon=4.8366 forget=7.3082 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=13.9124 mle=1.6849 pcon=4.8353 forget=7.3922 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 40 total=13.8191 mle=1.5345 pcon=4.8339 forget=7.4507 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=14.0603 mle=1.6650 pcon=4.8326 forget=7.5627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=13.8673 mle=1.5673 pcon=4.8311 forget=7.4688 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=13.9735 mle=1.6694 pcon=4.8297 forget=7.4744 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=14.2350 mle=1.8788 pcon=4.8284 forget=7.5279 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=14.1604 mle=1.6749 pcon=4.8274 forget=7.6581 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=14.0022 mle=1.5506 pcon=4.8262 forget=7.6254 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:28,  2.63it/s]  2%|▏         | 8/391 [00:00<00:19, 19.86it/s]  4%|▍         | 15/391 [00:00<00:11, 32.87it/s]  6%|▌         | 24/391 [00:00<00:07, 47.79it/s]  8%|▊         | 33/391 [00:00<00:06, 59.24it/s] 11%|█         | 42/391 [00:00<00:05, 67.34it/s] 13%|█▎        | 51/391 [00:01<00:04, 73.43it/s] 15%|█▌        | 60/391 [00:01<00:05, 61.74it/s] 17%|█▋        | 68/391 [00:01<00:05, 58.84it/s] 20%|█▉        | 77/391 [00:01<00:04, 65.77it/s] 22%|██▏       | 86/391 [00:01<00:04, 71.44it/s] 24%|██▍       | 95/391 [00:01<00:03, 75.57it/s] 27%|██▋       | 104/391 [00:01<00:03, 79.38it/s] 29%|██▉       | 113/391 [00:01<00:03, 77.16it/s] 31%|███       | 121/391 [00:02<00:04, 65.15it/s] 33%|███▎      | 128/391 [00:02<00:04, 62.32it/s] 35%|███▌      | 137/391 [00:02<00:03, 67.68it/s] 37%|███▋      | 146/391 [00:02<00:03, 73.08it/s] 40%|███▉      | 155/391 [00:02<00:03, 77.29it/s] 42%|████▏     | 164/391 [00:02<00:02, 79.16it/s] 44%|████▍     | 173/391 [00:02<00:02, 74.07it/s] 46%|████▋     | 181/391 [00:02<00:03, 64.26it/s] 48%|████▊     | 188/391 [00:03<00:03, 62.44it/s] 50%|█████     | 197/391 [00:03<00:02, 69.02it/s] 53%|█████▎    | 206/391 [00:03<00:02, 74.26it/s] 55%|█████▍    | 214/391 [00:03<00:02, 75.64it/s] 57%|█████▋    | 223/391 [00:03<00:02, 77.77it/s] 59%|█████▉    | 231/391 [00:03<00:02, 65.26it/s] 61%|██████    | 238/391 [00:03<00:02, 60.48it/s] 63%|██████▎   | 247/391 [00:03<00:02, 66.61it/s] 65%|██████▌   | 256/391 [00:03<00:01, 72.39it/s] 68%|██████▊   | 265/391 [00:04<00:01, 76.84it/s] 70%|███████   | 274/391 [00:04<00:01, 80.08it/s] 72%|███████▏  | 283/391 [00:04<00:01, 82.06it/s] 75%|███████▍  | 292/391 [00:04<00:01, 66.98it/s] 77%|███████▋  | 300/391 [00:04<00:01, 62.94it/s] 79%|███████▉  | 308/391 [00:04<00:01, 66.98it/s] 81%|████████  | 317/391 [00:04<00:01, 72.00it/s] 83%|████████▎ | 326/391 [00:04<00:00, 76.46it/s] 86%|████████▌ | 335/391 [00:05<00:00, 79.88it/s] 88%|████████▊ | 344/391 [00:05<00:00, 82.40it/s] 90%|█████████ | 353/391 [00:05<00:00, 72.11it/s] 92%|█████████▏| 361/391 [00:05<00:00, 66.33it/s] 94%|█████████▍| 368/391 [00:05<00:00, 64.78it/s] 96%|█████████▋| 377/391 [00:05<00:00, 70.66it/s] 99%|█████████▊| 386/391 [00:05<00:00, 75.78it/s]100%|██████████| 391/391 [00:05<00:00, 67.19it/s]
50000 images processed, 5.908676624298096 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.11it/s]  9%|▉         | 7/79 [00:00<00:03, 20.48it/s] 20%|██        | 16/79 [00:00<00:01, 41.37it/s] 32%|███▏      | 25/79 [00:00<00:00, 54.81it/s] 43%|████▎     | 34/79 [00:00<00:00, 64.81it/s] 54%|█████▍    | 43/79 [00:00<00:00, 71.43it/s] 65%|██████▍   | 51/79 [00:01<00:00, 62.26it/s] 73%|███████▎  | 58/79 [00:01<00:00, 60.76it/s] 82%|████████▏ | 65/79 [00:01<00:00, 60.68it/s] 94%|█████████▎| 74/79 [00:01<00:00, 68.29it/s]100%|██████████| 79/79 [00:01<00:00, 56.19it/s]
10000 images processed, 1.4277687072753906 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:15,  2.69it/s]  4%|▍         | 9/204 [00:00<00:08, 23.81it/s]  9%|▉         | 18/204 [00:00<00:04, 41.12it/s] 13%|█▎        | 27/204 [00:00<00:03, 54.25it/s] 18%|█▊        | 36/204 [00:00<00:02, 62.30it/s] 22%|██▏       | 44/204 [00:00<00:02, 53.92it/s] 26%|██▌       | 53/204 [00:01<00:02, 62.36it/s] 30%|██▉       | 61/204 [00:01<00:02, 61.04it/s] 34%|███▍      | 70/204 [00:01<00:01, 67.67it/s] 39%|███▊      | 79/204 [00:01<00:01, 72.83it/s] 43%|████▎     | 88/204 [00:01<00:01, 76.41it/s] 48%|████▊     | 97/204 [00:01<00:01, 64.87it/s] 51%|█████▏    | 105/204 [00:01<00:01, 63.19it/s] 55%|█████▍    | 112/204 [00:01<00:01, 63.14it/s] 59%|█████▉    | 121/204 [00:02<00:01, 68.53it/s] 64%|██████▎   | 130/204 [00:02<00:01, 72.93it/s] 68%|██████▊   | 139/204 [00:02<00:00, 75.47it/s] 72%|███████▏  | 147/204 [00:02<00:00, 65.36it/s] 75%|███████▌  | 154/204 [00:02<00:00, 60.60it/s] 79%|███████▉  | 161/204 [00:02<00:00, 62.59it/s] 83%|████████▎ | 170/204 [00:02<00:00, 68.74it/s] 88%|████████▊ | 179/204 [00:02<00:00, 73.55it/s] 92%|█████████▏| 188/204 [00:03<00:00, 77.13it/s] 97%|█████████▋| 197/204 [00:03<00:00, 78.29it/s]100%|██████████| 204/204 [00:03<00:00, 61.98it/s]
26032 images processed, 3.3369140625 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:41,  1.87it/s] 11%|█▏        | 9/79 [00:00<00:04, 16.42it/s] 18%|█▊        | 14/79 [00:00<00:02, 22.56it/s] 24%|██▍       | 19/79 [00:00<00:02, 27.92it/s] 33%|███▎      | 26/79 [00:01<00:01, 36.88it/s] 42%|████▏     | 33/79 [00:01<00:01, 42.74it/s] 49%|████▉     | 39/79 [00:01<00:01, 39.84it/s] 56%|█████▌    | 44/79 [00:01<00:00, 40.09it/s] 66%|██████▌   | 52/79 [00:01<00:00, 39.03it/s] 73%|███████▎  | 58/79 [00:01<00:00, 43.07it/s] 80%|███████▉  | 63/79 [00:01<00:00, 42.33it/s] 86%|████████▌ | 68/79 [00:02<00:00, 37.82it/s] 94%|█████████▎| 74/79 [00:02<00:00, 41.44it/s]100%|██████████| 79/79 [00:02<00:00, 34.95it/s]
10000 images processed, 2.295792818069458 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:33,  2.32it/s]  9%|▉         | 7/79 [00:00<00:04, 16.05it/s] 18%|█▊        | 14/79 [00:00<00:02, 29.77it/s] 29%|██▉       | 23/79 [00:00<00:01, 44.62it/s] 38%|███▊      | 30/79 [00:00<00:00, 50.21it/s] 47%|████▋     | 37/79 [00:01<00:00, 47.37it/s] 54%|█████▍    | 43/79 [00:01<00:00, 48.42it/s] 66%|██████▌   | 52/79 [00:01<00:00, 58.05it/s] 77%|███████▋  | 61/79 [00:01<00:00, 65.48it/s] 89%|████████▊ | 70/79 [00:01<00:00, 71.55it/s]100%|██████████| 79/79 [00:01<00:00, 50.74it/s]
10000 images processed, 1.5767064094543457 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:21,  3.15it/s] 14%|█▍        | 10/70 [00:00<00:02, 29.36it/s] 24%|██▍       | 17/70 [00:00<00:01, 40.08it/s] 33%|███▎      | 23/70 [00:00<00:01, 40.94it/s] 41%|████▏     | 29/70 [00:00<00:00, 43.32it/s] 54%|█████▍    | 38/70 [00:00<00:00, 55.49it/s] 67%|██████▋   | 47/70 [00:01<00:00, 64.53it/s] 80%|████████  | 56/70 [00:01<00:00, 71.04it/s] 93%|█████████▎| 65/70 [00:01<00:00, 76.13it/s]100%|██████████| 70/70 [00:01<00:00, 55.02it/s]
8925 images processed, 1.3046116828918457 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:34,  1.26it/s]  4%|▍         | 2/45 [00:00<00:17,  2.49it/s] 20%|██        | 9/45 [00:01<00:03, 10.39it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.73it/s] 38%|███▊      | 17/45 [00:01<00:02, 13.61it/s] 42%|████▏     | 19/45 [00:01<00:02, 12.88it/s] 49%|████▉     | 22/45 [00:02<00:01, 15.28it/s] 56%|█████▌    | 25/45 [00:02<00:01, 15.28it/s] 60%|██████    | 27/45 [00:02<00:01, 14.98it/s] 67%|██████▋   | 30/45 [00:02<00:01, 14.29it/s] 73%|███████▎  | 33/45 [00:02<00:00, 12.55it/s] 78%|███████▊  | 35/45 [00:03<00:00, 12.60it/s] 89%|████████▉ | 40/45 [00:03<00:00, 15.34it/s] 93%|█████████▎| 42/45 [00:03<00:00, 10.04it/s]100%|██████████| 45/45 [00:03<00:00, 11.75it/s]
5640 images processed, 3.8533687591552734 seconds used

21.395451545715332
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.42  99.22
places365     72.26  79.69
LSUN          23.24  95.39
iSUN          73.49  81.09
dtd           41.47  90.44
forget        58.00  88.68
AVG           45.31  89.08
Retain-Acc: 0.7392
Forget-as-OOD (retain known vs forget novel):
  FPR: 58.00 AUROC: 88.68 AUIN: 98.56
11.668869733810425
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=1.0 lr=0.001 epochs=50
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:19<16:07, 19.75s/it]  4%|▍         | 2/50 [00:41<16:57, 21.19s/it]  6%|▌         | 3/50 [01:02<16:22, 20.90s/it]  8%|▊         | 4/50 [01:20<15:07, 19.72s/it] 10%|█         | 5/50 [01:38<14:21, 19.14s/it] 12%|█▏        | 6/50 [01:58<14:16, 19.47s/it] 14%|█▍        | 7/50 [02:23<15:11, 21.20s/it] 16%|█▌        | 8/50 [02:41<14:07, 20.17s/it][loss] ep 0 it 0 total=13.7434 mle=1.5709 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8369 mle=1.5424 pcon=5.2879 forget=7.0066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8512 mle=1.7004 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0218 mle=1.8997 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8932 mle=1.7132 pcon=5.2670 forget=6.9129 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6545 mle=1.5021 pcon=5.2603 forget=6.8920 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7233 mle=1.5598 pcon=5.2541 forget=6.9095 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8765 mle=1.6810 pcon=5.2476 forget=6.9479 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=13.9621 mle=1.6716 pcon=5.2409 forget=7.0496 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7061 mle=1.6371 pcon=5.2346 forget=6.8345 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6915 mle=1.5166 pcon=5.2284 forget=6.9465 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9482 mle=1.7802 pcon=5.2225 forget=6.9456 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9066 mle=1.7646 pcon=5.2167 forget=6.9254 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6563 mle=1.6051 pcon=5.2112 forget=6.8400 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8180 mle=1.7235 pcon=5.2056 forget=6.8889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9234 mle=1.7943 pcon=5.2003 forget=6.9289 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=13.6297 mle=1.5529 pcon=5.1950 forget=6.8817 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8629 mle=1.8883 pcon=5.1899 forget=6.7847 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8038 mle=1.7421 pcon=5.1847 forget=6.8771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5268 mle=1.5262 pcon=5.1796 forget=6.8209 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6860 mle=1.5859 pcon=5.1745 forget=6.9257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9021 mle=1.8839 pcon=5.1698 forget=6.8483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5395 mle=1.5912 pcon=5.1651 forget=6.7831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7886 mle=1.8111 pcon=5.1604 forget=6.8171 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=13.7066 mle=1.7048 pcon=5.1558 forget=6.8459 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7120 mle=1.6712 pcon=5.1518 forget=6.8890 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7351 mle=1.7666 pcon=5.1474 forget=6.8211 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8714 mle=1.9270 pcon=5.1434 forget=6.8010 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.4988 mle=1.5696 pcon=5.1393 forget=6.7899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7424 mle=1.8358 pcon=5.1351 forget=6.7716 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.6962 mle=1.7028 pcon=5.1308 forget=6.8626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.6910 mle=1.6953 pcon=5.1272 forget=6.8685 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=13.4699 mle=1.6521 pcon=5.1233 forget=6.6946 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6124 mle=1.7319 pcon=5.1192 forget=6.7614 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5721 mle=1.6522 pcon=5.1155 forget=6.8044 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3640 mle=1.4534 pcon=5.1119 forget=6.7987 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6081 mle=1.7664 pcon=5.1079 forget=6.7338 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3134 mle=1.4894 pcon=5.1042 forget=6.7198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7344 mle=1.8758 pcon=5.1006 forget=6.7580 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=13.6934 mle=1.8653 pcon=5.0970 forget=6.7311 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=13.5492 mle=1.8062 pcon=5.0931 forget=6.6498 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=13.4060 mle=1.5905 pcon=5.0897 forget=6.7258 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=13.3917 mle=1.5925 pcon=5.0858 forget=6.7134 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=13.4119 mle=1.6918 pcon=5.0826 forget=6.6374 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=13.4548 mle=1.7802 pcon=5.0792 forget=6.5954 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=13.2879 mle=1.6036 pcon=5.0757 forget=6.6086 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=13.4639 mle=1.7401 pcon=5.0724 forget=6.6514 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=13.2320 mle=1.4104 pcon=5.0692 forget=6.7524 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=13.1497 mle=1.5272 pcon=5.0660 forget=6.5564 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=13.3004 mle=1.5941 pcon=5.0630 forget=6.6433 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=13.3710 mle=1.6816 pcon=5.0600 forget=6.6295 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=13.3248 mle=1.6311 pcon=5.0576 forget=6.6361 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=13.5428 mle=1.7994 pcon=5.0546 forget=6.6889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=13.3506 mle=1.5469 pcon=5.0521 forget=6.7515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=13.4036 mle=1.6705 pcon=5.0498 forget=6.6833 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=13.4035 mle=1.6512 pcon=5.0474 forget=6.7049 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=13.5263 mle=1.7658 pcon=5.0454 forget=6.7151 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=13.4397 mle=1.6175 pcon=5.0432 forget=6.7791 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=13.4468 mle=1.5965 pcon=5.0415 forget=6.8089 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=13.5854 mle=1.6444 pcon=5.0401 forget=6.9009 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=13.7965 mle=1.7799 pcon=5.0388 forget=6.9778 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=13.7837 mle=1.7538 pcon=5.0372 forget=6.9927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=13.6791 mle=1.6352 pcon=5.0361 forget=7.0078 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 30 total=13.7227 mle=1.5612 pcon=5.0352 forget=7.1263 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=13.7311 mle=1.5986 pcon=5.0343 forget=7.0981 nr=64 nf=64 protos=540 fproto_sim=NA
 18%|█▊        | 9/50 [02:58<13:07, 19.20s/it] 20%|██        | 10/50 [03:15<12:23, 18.60s/it] 22%|██▏       | 11/50 [03:41<13:28, 20.72s/it] 24%|██▍       | 12/50 [03:59<12:39, 19.99s/it] 26%|██▌       | 13/50 [04:17<11:53, 19.28s/it] 28%|██▊       | 14/50 [04:35<11:28, 19.12s/it] 30%|███       | 15/50 [05:01<12:14, 21.00s/it] 32%|███▏      | 16/50 [05:21<11:46, 20.79s/it] 34%|███▍      | 17/50 [05:39<11:00, 20.01s/it][loss] ep 8 it 130 total=13.6543 mle=1.4970 pcon=5.0331 forget=7.1243 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=13.8711 mle=1.7137 pcon=5.0322 forget=7.1252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=13.6504 mle=1.5295 pcon=5.0315 forget=7.0894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=13.7776 mle=1.7047 pcon=5.0302 forget=7.0427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=13.8856 mle=1.7305 pcon=5.0286 forget=7.1266 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=13.4632 mle=1.4732 pcon=5.0270 forget=6.9630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 40 total=13.4103 mle=1.6206 pcon=5.0252 forget=6.7644 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=13.3204 mle=1.6538 pcon=5.0228 forget=6.6438 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=13.2960 mle=1.8052 pcon=5.0206 forget=6.4701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=12.7673 mle=1.5773 pcon=5.0179 forget=6.1721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=12.6482 mle=1.5924 pcon=5.0147 forget=6.0412 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=12.5617 mle=1.5531 pcon=5.0109 forget=5.9977 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=12.5522 mle=1.5803 pcon=5.0074 forget=5.9645 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=12.8019 mle=1.6823 pcon=5.0037 forget=6.1160 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=12.8263 mle=1.6225 pcon=5.0000 forget=6.2038 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=12.9458 mle=1.7456 pcon=4.9967 forget=6.2036 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=13.1367 mle=1.8062 pcon=4.9936 forget=6.3369 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=13.3379 mle=1.8844 pcon=4.9906 forget=6.4629 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=13.2324 mle=1.6569 pcon=4.9880 forget=6.5875 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=13.3247 mle=1.6389 pcon=4.9857 forget=6.7001 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=13.4485 mle=1.6385 pcon=4.9835 forget=6.8265 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 10 total=13.5494 mle=1.5986 pcon=4.9815 forget=6.9693 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=13.7391 mle=1.7201 pcon=4.9797 forget=7.0393 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=13.7589 mle=1.6306 pcon=4.9783 forget=7.1500 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=13.9513 mle=1.6965 pcon=4.9766 forget=7.2782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=14.0639 mle=1.7385 pcon=4.9756 forget=7.3498 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=13.9688 mle=1.6102 pcon=4.9750 forget=7.3836 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=14.3322 mle=1.9731 pcon=4.9741 forget=7.3851 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=14.2090 mle=1.7615 pcon=4.9733 forget=7.4742 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 20 total=13.9792 mle=1.5781 pcon=4.9725 forget=7.4286 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=14.1343 mle=1.7512 pcon=4.9718 forget=7.4114 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=13.8212 mle=1.6193 pcon=4.9709 forget=7.2310 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=13.8052 mle=1.7465 pcon=4.9700 forget=7.0887 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=13.6528 mle=1.6003 pcon=4.9686 forget=7.0839 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=13.5372 mle=1.5757 pcon=4.9675 forget=6.9940 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=13.5109 mle=1.5279 pcon=4.9657 forget=7.0172 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=13.5786 mle=1.8441 pcon=4.9636 forget=6.7709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 30 total=13.2971 mle=1.4809 pcon=4.9613 forget=6.8548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=13.1716 mle=1.4619 pcon=4.9587 forget=6.7510 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=13.0293 mle=1.5114 pcon=4.9556 forget=6.5622 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=13.0816 mle=1.6139 pcon=4.9525 forget=6.5152 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=13.0489 mle=1.6726 pcon=4.9491 forget=6.4272 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=12.8511 mle=1.5181 pcon=4.9452 forget=6.3878 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=12.7763 mle=1.5893 pcon=4.9410 forget=6.2460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=12.7379 mle=1.6245 pcon=4.9366 forget=6.1768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 40 total=12.8699 mle=1.8174 pcon=4.9318 forget=6.1207 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=12.8482 mle=1.7902 pcon=4.9273 forget=6.1307 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=12.6772 mle=1.6148 pcon=4.9228 forget=6.1395 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=12.8677 mle=1.8052 pcon=4.9181 forget=6.1444 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=12.8469 mle=1.7066 pcon=4.9137 forget=6.2267 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=12.8583 mle=1.6683 pcon=4.9092 forget=6.2809 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=12.9885 mle=1.7345 pcon=4.9047 forget=6.3492 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=13.0764 mle=1.7560 pcon=4.9006 forget=6.4198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=12.9316 mle=1.5503 pcon=4.8962 forget=6.4851 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=13.0452 mle=1.6035 pcon=4.8923 forget=6.5494 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=13.2336 mle=1.7152 pcon=4.8885 forget=6.6299 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=13.2141 mle=1.6585 pcon=4.8850 forget=6.6706 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=13.6087 mle=1.9907 pcon=4.8815 forget=6.7365 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=13.2655 mle=1.5988 pcon=4.8782 forget=6.7885 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=13.2861 mle=1.5795 pcon=4.8749 forget=6.8316 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 10 total=13.2445 mle=1.6127 pcon=4.8716 forget=6.7601 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=13.1896 mle=1.5487 pcon=4.8685 forget=6.7724 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=13.1648 mle=1.5611 pcon=4.8654 forget=6.7382 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=13.1379 mle=1.5396 pcon=4.8619 forget=6.7364 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=13.0695 mle=1.5728 pcon=4.8584 forget=6.6384 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=13.2767 mle=1.8244 pcon=4.8547 forget=6.5975 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=13.1906 mle=1.7821 pcon=4.8513 forget=6.5572 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=12.9470 mle=1.5975 pcon=4.8478 forget=6.5017 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 20 total=13.1323 mle=1.7944 pcon=4.8440 forget=6.4939 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=13.0009 mle=1.7595 pcon=4.8405 forget=6.4008 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=12.9079 mle=1.6386 pcon=4.8369 forget=6.4324 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=12.8142 mle=1.5823 pcon=4.8331 forget=6.3988 nr=64 nf=64 protos=540 fproto_sim=NA
 36%|███▌      | 18/50 [05:58<10:24, 19.51s/it] 38%|███▊      | 19/50 [06:22<10:51, 21.00s/it] 40%|████      | 20/50 [06:43<10:27, 20.92s/it] 42%|████▏     | 21/50 [07:02<09:47, 20.25s/it] 44%|████▍     | 22/50 [07:19<09:05, 19.48s/it] 46%|████▌     | 23/50 [07:44<09:26, 20.98s/it] 48%|████▊     | 24/50 [08:04<09:02, 20.88s/it] 50%|█████     | 25/50 [08:22<08:21, 20.05s/it] 52%|█████▏    | 26/50 [08:41<07:48, 19.53s/it][loss] ep 17 it 220 total=12.8356 mle=1.6167 pcon=4.8295 forget=6.3894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=12.6866 mle=1.5428 pcon=4.8263 forget=6.3174 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=12.9713 mle=1.8487 pcon=4.8229 forget=6.2996 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=12.9058 mle=1.7858 pcon=4.8194 forget=6.3007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=12.6669 mle=1.5766 pcon=4.8164 forget=6.2739 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=12.7999 mle=1.6477 pcon=4.8133 forget=6.3389 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=12.8755 mle=1.6837 pcon=4.8104 forget=6.3814 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=12.7158 mle=1.5736 pcon=4.8075 forget=6.3347 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=12.8673 mle=1.6913 pcon=4.8048 forget=6.3712 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=12.9001 mle=1.6871 pcon=4.8023 forget=6.4108 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=13.0244 mle=1.7756 pcon=4.7999 forget=6.4488 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=13.0665 mle=1.7505 pcon=4.7979 forget=6.5181 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 40 total=13.0099 mle=1.6523 pcon=4.7958 forget=6.5618 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=13.1039 mle=1.6671 pcon=4.7940 forget=6.6429 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=13.0370 mle=1.5947 pcon=4.7920 forget=6.6504 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=13.1249 mle=1.6908 pcon=4.7901 forget=6.6440 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=13.3320 mle=1.8520 pcon=4.7884 forget=6.6916 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=13.2636 mle=1.7412 pcon=4.7870 forget=6.7354 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=13.1753 mle=1.5999 pcon=4.7855 forget=6.7899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 0 total=13.2408 mle=1.6743 pcon=4.7839 forget=6.7826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=13.2603 mle=1.6981 pcon=4.7823 forget=6.7800 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=13.2741 mle=1.6989 pcon=4.7808 forget=6.7944 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=13.3533 mle=1.8135 pcon=4.7790 forget=6.7608 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=13.0863 mle=1.5798 pcon=4.7772 forget=6.7293 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=13.3954 mle=1.9118 pcon=4.7753 forget=6.7083 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=13.3321 mle=1.8301 pcon=4.7734 forget=6.7286 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=12.9657 mle=1.5462 pcon=4.7715 forget=6.6480 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 10 total=12.9285 mle=1.5276 pcon=4.7696 forget=6.6313 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=13.0644 mle=1.6726 pcon=4.7675 forget=6.6243 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=12.9054 mle=1.5730 pcon=4.7656 forget=6.5668 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=12.9129 mle=1.6263 pcon=4.7637 forget=6.5229 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=13.0013 mle=1.7307 pcon=4.7618 forget=6.5088 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=12.8788 mle=1.6609 pcon=4.7597 forget=6.4581 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=12.8288 mle=1.6455 pcon=4.7575 forget=6.4257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=12.7278 mle=1.5484 pcon=4.7555 forget=6.4240 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 20 total=12.7642 mle=1.6667 pcon=4.7533 forget=6.3442 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=12.7526 mle=1.7134 pcon=4.7512 forget=6.2880 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=12.6316 mle=1.6048 pcon=4.7489 forget=6.2778 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=12.6333 mle=1.6239 pcon=4.7469 forget=6.2625 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=12.5824 mle=1.6018 pcon=4.7449 forget=6.2358 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=12.4806 mle=1.5341 pcon=4.7427 forget=6.2038 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=12.6353 mle=1.7001 pcon=4.7407 forget=6.1944 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=12.8499 mle=1.9079 pcon=4.7385 forget=6.2034 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 23 it 30 total=12.4807 mle=1.5568 pcon=4.7368 forget=6.1871 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=12.6011 mle=1.6579 pcon=4.7348 forget=6.2085 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=12.5342 mle=1.6037 pcon=4.7328 forget=6.1977 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=12.6825 mle=1.7386 pcon=4.7308 forget=6.2130 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=12.6092 mle=1.6312 pcon=4.7291 forget=6.2488 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=12.6602 mle=1.6449 pcon=4.7273 forget=6.2880 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=12.6998 mle=1.6729 pcon=4.7256 forget=6.3012 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=12.6906 mle=1.6311 pcon=4.7240 forget=6.3355 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 24 it 40 total=12.6716 mle=1.5741 pcon=4.7226 forget=6.3749 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=12.8163 mle=1.6501 pcon=4.7213 forget=6.4449 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=12.8336 mle=1.6307 pcon=4.7199 forget=6.4830 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=12.8741 mle=1.6420 pcon=4.7188 forget=6.5133 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 240 total=12.7986 mle=1.5391 pcon=4.7176 forget=6.5420 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=13.0050 mle=1.7259 pcon=4.7166 forget=6.5625 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=12.9364 mle=1.6253 pcon=4.7156 forget=6.5956 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 0 total=12.9660 mle=1.6263 pcon=4.7146 forget=6.6251 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=13.1375 mle=1.7603 pcon=4.7139 forget=6.6634 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=13.0192 mle=1.6531 pcon=4.7130 forget=6.6531 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 150 total=13.1768 mle=1.7887 pcon=4.7123 forget=6.6759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 200 total=13.2101 mle=1.8095 pcon=4.7115 forget=6.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=13.1920 mle=1.7358 pcon=4.7107 forget=6.7455 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=13.0437 mle=1.6375 pcon=4.7099 forget=6.6964 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=12.9850 mle=1.5709 pcon=4.7094 forget=6.7047 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 10 total=13.0336 mle=1.6360 pcon=4.7087 forget=6.6889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=13.0109 mle=1.6011 pcon=4.7080 forget=6.7018 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=13.1122 mle=1.7277 pcon=4.7073 forget=6.6772 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=12.9206 mle=1.5659 pcon=4.7066 forget=6.6481 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=13.0353 mle=1.6727 pcon=4.7057 forget=6.6569 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=12.9548 mle=1.5641 pcon=4.7050 forget=6.6857 nr=64 nf=64 protos=540 fproto_sim=NA
 54%|█████▍    | 27/50 [09:05<08:01, 20.94s/it] 56%|█████▌    | 28/50 [09:26<07:43, 21.05s/it] 58%|█████▊    | 29/50 [09:44<07:03, 20.18s/it] 60%|██████    | 30/50 [10:04<06:36, 19.85s/it] 62%|██████▏   | 31/50 [10:29<06:46, 21.42s/it] 64%|██████▍   | 32/50 [10:50<06:23, 21.29s/it] 66%|██████▌   | 33/50 [11:08<05:45, 20.31s/it] 68%|██████▊   | 34/50 [11:26<05:15, 19.74s/it] 70%|███████   | 35/50 [11:50<05:15, 21.02s/it] 72%|███████▏  | 36/50 [12:13<05:00, 21.46s/it][loss] ep 26 it 310 total=13.2537 mle=1.9222 pcon=4.7042 forget=6.6273 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=12.9295 mle=1.6183 pcon=4.7033 forget=6.6079 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 20 total=12.9578 mle=1.6523 pcon=4.7026 forget=6.6029 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=12.8571 mle=1.5789 pcon=4.7016 forget=6.5766 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=12.8681 mle=1.5768 pcon=4.7007 forget=6.5906 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=12.8397 mle=1.5542 pcon=4.6998 forget=6.5857 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=12.7818 mle=1.5228 pcon=4.6989 forget=6.5601 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=12.8600 mle=1.6083 pcon=4.6979 forget=6.5537 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=12.9936 mle=1.7254 pcon=4.6971 forget=6.5711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=12.9803 mle=1.7244 pcon=4.6963 forget=6.5596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 30 total=12.8969 mle=1.6701 pcon=4.6956 forget=6.5312 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=12.9634 mle=1.7056 pcon=4.6949 forget=6.5629 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=12.8278 mle=1.6011 pcon=4.6942 forget=6.5325 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=12.8143 mle=1.5927 pcon=4.6934 forget=6.5281 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=12.6859 mle=1.4636 pcon=4.6924 forget=6.5298 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=12.8367 mle=1.5848 pcon=4.6915 forget=6.5603 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=13.0368 mle=1.8065 pcon=4.6906 forget=6.5397 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=12.9601 mle=1.7295 pcon=4.6897 forget=6.5409 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 40 total=12.8866 mle=1.6835 pcon=4.6887 forget=6.5144 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=12.8047 mle=1.5793 pcon=4.6878 forget=6.5377 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=12.8991 mle=1.6650 pcon=4.6870 forget=6.5471 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=12.8760 mle=1.6293 pcon=4.6863 forget=6.5604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=12.8331 mle=1.5784 pcon=4.6856 forget=6.5692 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=12.9693 mle=1.7318 pcon=4.6850 forget=6.5525 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=12.8336 mle=1.5805 pcon=4.6844 forget=6.5686 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 0 total=12.9087 mle=1.6378 pcon=4.6837 forget=6.5873 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=12.9496 mle=1.6582 pcon=4.6832 forget=6.6083 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=12.8613 mle=1.5682 pcon=4.6826 forget=6.6105 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=13.1354 mle=1.8480 pcon=4.6819 forget=6.6055 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=12.8510 mle=1.5066 pcon=4.6813 forget=6.6631 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=13.1723 mle=1.8048 pcon=4.6807 forget=6.6868 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=12.8487 mle=1.5108 pcon=4.6801 forget=6.6578 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=13.0586 mle=1.7212 pcon=4.6796 forget=6.6578 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 10 total=13.0001 mle=1.6605 pcon=4.6791 forget=6.6605 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=12.9135 mle=1.5658 pcon=4.6786 forget=6.6692 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=12.9803 mle=1.6067 pcon=4.6781 forget=6.6955 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=12.9401 mle=1.5698 pcon=4.6775 forget=6.6928 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=12.8992 mle=1.5774 pcon=4.6770 forget=6.6448 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 260 total=12.8991 mle=1.5145 pcon=4.6764 forget=6.7082 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=13.0363 mle=1.6179 pcon=4.6758 forget=6.7426 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=12.9417 mle=1.5465 pcon=4.6752 forget=6.7200 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 20 total=13.0938 mle=1.7056 pcon=4.6746 forget=6.7135 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=12.9427 mle=1.5837 pcon=4.6739 forget=6.6851 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=13.0316 mle=1.6060 pcon=4.6734 forget=6.7522 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 170 total=12.9398 mle=1.5574 pcon=4.6730 forget=6.7094 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=13.0599 mle=1.6847 pcon=4.6725 forget=6.7027 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=12.9266 mle=1.5566 pcon=4.6721 forget=6.6979 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=12.8716 mle=1.5083 pcon=4.6715 forget=6.6918 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=12.8414 mle=1.4802 pcon=4.6712 forget=6.6900 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 30 total=13.0300 mle=1.6414 pcon=4.6708 forget=6.7179 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=13.1812 mle=1.8805 pcon=4.6703 forget=6.6304 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=12.8685 mle=1.5253 pcon=4.6698 forget=6.6734 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=13.1098 mle=1.6934 pcon=4.6693 forget=6.7471 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=13.0231 mle=1.6378 pcon=4.6689 forget=6.7164 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=13.0182 mle=1.6666 pcon=4.6684 forget=6.6833 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=13.0263 mle=1.6981 pcon=4.6680 forget=6.6602 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 380 total=12.9949 mle=1.6209 pcon=4.6675 forget=6.7065 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 40 total=13.0674 mle=1.7306 pcon=4.6669 forget=6.6699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=12.8447 mle=1.5345 pcon=4.6663 forget=6.6439 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 140 total=12.9180 mle=1.6341 pcon=4.6659 forget=6.6180 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=13.0007 mle=1.6683 pcon=4.6655 forget=6.6669 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 240 total=12.9800 mle=1.6816 pcon=4.6650 forget=6.6334 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=12.9961 mle=1.6463 pcon=4.6644 forget=6.6853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=12.9730 mle=1.5812 pcon=4.6640 forget=6.7278 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 0 total=12.9181 mle=1.5991 pcon=4.6635 forget=6.6554 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 50 total=12.9600 mle=1.6252 pcon=4.6631 forget=6.6717 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 100 total=13.2101 mle=1.8274 pcon=4.6626 forget=6.7201 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 150 total=12.9737 mle=1.6563 pcon=4.6621 forget=6.6552 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 200 total=12.8988 mle=1.5491 pcon=4.6618 forget=6.6880 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 250 total=13.0204 mle=1.7146 pcon=4.6613 forget=6.6444 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 300 total=12.8509 mle=1.5700 pcon=4.6609 forget=6.6200 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 350 total=12.9994 mle=1.7266 pcon=4.6605 forget=6.6123 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 10 total=13.0301 mle=1.7191 pcon=4.6603 forget=6.6507 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 60 total=12.8941 mle=1.5718 pcon=4.6600 forget=6.6623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 110 total=12.8818 mle=1.5813 pcon=4.6597 forget=6.6408 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 160 total=12.9619 mle=1.6477 pcon=4.6594 forget=6.6548 nr=64 nf=64 protos=540 fproto_sim=NA
 74%|███████▍  | 37/50 [12:30<04:24, 20.37s/it] 76%|███████▌  | 38/50 [12:48<03:55, 19.61s/it] 78%|███████▊  | 39/50 [13:09<03:40, 20.02s/it] 80%|████████  | 40/50 [13:34<03:33, 21.31s/it] 82%|████████▏ | 41/50 [13:52<03:04, 20.49s/it] 84%|████████▍ | 42/50 [14:11<02:39, 19.99s/it] 86%|████████▌ | 43/50 [14:34<02:25, 20.79s/it] 88%|████████▊ | 44/50 [14:56<02:08, 21.40s/it] 90%|█████████ | 45/50 [15:14<01:41, 20.28s/it] 92%|█████████▏| 46/50 [15:31<01:17, 19.31s/it][loss] ep 36 it 210 total=12.8638 mle=1.5393 pcon=4.6591 forget=6.6654 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 260 total=12.9299 mle=1.5821 pcon=4.6590 forget=6.6888 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 310 total=13.0497 mle=1.6776 pcon=4.6586 forget=6.7135 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 360 total=12.9636 mle=1.6631 pcon=4.6583 forget=6.6422 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 20 total=12.8217 mle=1.5125 pcon=4.6579 forget=6.6513 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 70 total=13.1137 mle=1.7477 pcon=4.6577 forget=6.7083 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 120 total=12.9690 mle=1.6219 pcon=4.6574 forget=6.6896 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 170 total=12.9116 mle=1.6001 pcon=4.6572 forget=6.6543 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 220 total=13.0815 mle=1.7283 pcon=4.6568 forget=6.6965 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 270 total=12.9258 mle=1.6013 pcon=4.6565 forget=6.6680 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 320 total=12.9977 mle=1.6491 pcon=4.6562 forget=6.6923 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 370 total=13.0468 mle=1.6388 pcon=4.6560 forget=6.7519 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 30 total=13.0553 mle=1.6766 pcon=4.6560 forget=6.7228 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 80 total=12.8653 mle=1.4941 pcon=4.6559 forget=6.7153 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 130 total=12.9768 mle=1.6206 pcon=4.6557 forget=6.7005 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 180 total=12.8971 mle=1.5777 pcon=4.6555 forget=6.6639 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 230 total=12.8712 mle=1.5246 pcon=4.6553 forget=6.6913 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 280 total=13.0284 mle=1.7074 pcon=4.6552 forget=6.6658 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 330 total=12.9325 mle=1.6144 pcon=4.6549 forget=6.6632 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 380 total=12.8415 mle=1.5072 pcon=4.6549 forget=6.6795 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 40 total=13.0176 mle=1.6983 pcon=4.6549 forget=6.6644 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 90 total=13.0140 mle=1.6389 pcon=4.6548 forget=6.7204 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 140 total=12.9998 mle=1.6169 pcon=4.6547 forget=6.7282 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 190 total=12.7726 mle=1.4626 pcon=4.6546 forget=6.6554 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 240 total=13.0307 mle=1.6571 pcon=4.6545 forget=6.7191 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 290 total=12.8464 mle=1.5072 pcon=4.6544 forget=6.6848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 340 total=12.9593 mle=1.5858 pcon=4.6544 forget=6.7190 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 0 total=12.9146 mle=1.5728 pcon=4.6542 forget=6.6876 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 50 total=13.0100 mle=1.6114 pcon=4.6541 forget=6.7446 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 100 total=12.9477 mle=1.6268 pcon=4.6539 forget=6.6670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 150 total=12.8269 mle=1.4732 pcon=4.6538 forget=6.6999 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 200 total=13.1102 mle=1.7442 pcon=4.6537 forget=6.7122 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 250 total=13.2181 mle=1.8021 pcon=4.6536 forget=6.7623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 300 total=12.9206 mle=1.5628 pcon=4.6536 forget=6.7042 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 350 total=12.8384 mle=1.4631 pcon=4.6536 forget=6.7217 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 10 total=12.8845 mle=1.5125 pcon=4.6535 forget=6.7186 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 60 total=13.2611 mle=1.8495 pcon=4.6535 forget=6.7581 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 110 total=13.0329 mle=1.6221 pcon=4.6534 forget=6.7574 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 160 total=13.0539 mle=1.6114 pcon=4.6534 forget=6.7890 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 210 total=13.0892 mle=1.6876 pcon=4.6534 forget=6.7482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 260 total=12.9346 mle=1.5420 pcon=4.6536 forget=6.7390 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 310 total=12.9587 mle=1.5346 pcon=4.6537 forget=6.7704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 360 total=13.1299 mle=1.7124 pcon=4.6537 forget=6.7637 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 20 total=13.0726 mle=1.6483 pcon=4.6537 forget=6.7706 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 70 total=13.0216 mle=1.5859 pcon=4.6536 forget=6.7821 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 120 total=13.3028 mle=1.8522 pcon=4.6535 forget=6.7971 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 170 total=12.9924 mle=1.5854 pcon=4.6535 forget=6.7536 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 220 total=13.0951 mle=1.6504 pcon=4.6535 forget=6.7912 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 270 total=13.0995 mle=1.6730 pcon=4.6535 forget=6.7730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 320 total=13.1401 mle=1.6817 pcon=4.6536 forget=6.8048 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 370 total=12.9995 mle=1.5844 pcon=4.6536 forget=6.7615 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 30 total=13.0777 mle=1.6016 pcon=4.6536 forget=6.8225 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 80 total=13.0695 mle=1.6074 pcon=4.6535 forget=6.8086 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 130 total=13.1254 mle=1.6303 pcon=4.6535 forget=6.8416 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 180 total=13.0211 mle=1.5275 pcon=4.6535 forget=6.8402 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 230 total=13.0087 mle=1.5427 pcon=4.6534 forget=6.8126 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 280 total=13.0216 mle=1.5396 pcon=4.6534 forget=6.8286 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 330 total=13.0837 mle=1.5370 pcon=4.6533 forget=6.8935 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 380 total=12.9846 mle=1.4945 pcon=4.6532 forget=6.8369 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 40 total=13.2318 mle=1.7128 pcon=4.6532 forget=6.8658 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 90 total=13.0803 mle=1.5608 pcon=4.6531 forget=6.8663 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 140 total=13.1795 mle=1.6572 pcon=4.6531 forget=6.8692 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 190 total=13.0490 mle=1.5408 pcon=4.6531 forget=6.8550 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 240 total=13.1475 mle=1.5309 pcon=4.6531 forget=6.9635 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 290 total=13.3542 mle=1.8041 pcon=4.6531 forget=6.8970 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 340 total=13.4004 mle=1.8162 pcon=4.6532 forget=6.9311 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 0 total=13.1832 mle=1.5662 pcon=4.6531 forget=6.9638 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 50 total=13.1382 mle=1.5422 pcon=4.6531 forget=6.9429 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 100 total=13.1705 mle=1.5920 pcon=4.6530 forget=6.9255 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 150 total=13.1280 mle=1.5423 pcon=4.6530 forget=6.9327 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 200 total=13.1708 mle=1.5755 pcon=4.6530 forget=6.9423 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 250 total=13.2135 mle=1.6157 pcon=4.6529 forget=6.9448 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 300 total=13.1213 mle=1.6291 pcon=4.6528 forget=6.8394 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 350 total=13.0701 mle=1.5372 pcon=4.6528 forget=6.8801 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 10 total=13.1347 mle=1.5142 pcon=4.6528 forget=6.9676 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 60 total=13.1554 mle=1.5104 pcon=4.6528 forget=6.9922 nr=64 nf=64 protos=540 fproto_sim=NA
 94%|█████████▍| 47/50 [15:51<00:58, 19.53s/it] 96%|█████████▌| 48/50 [16:16<00:42, 21.21s/it] 98%|█████████▊| 49/50 [16:35<00:20, 20.54s/it]100%|██████████| 50/50 [16:54<00:00, 19.95s/it]100%|██████████| 50/50 [16:54<00:00, 20.29s/it]
[loss] ep 46 it 110 total=13.2516 mle=1.6442 pcon=4.6528 forget=6.9546 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 160 total=13.0637 mle=1.4685 pcon=4.6527 forget=6.9424 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 210 total=13.3169 mle=1.7087 pcon=4.6527 forget=6.9555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 260 total=13.2795 mle=1.6895 pcon=4.6526 forget=6.9373 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 310 total=13.1420 mle=1.5041 pcon=4.6527 forget=6.9852 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 360 total=13.3757 mle=1.7115 pcon=4.6527 forget=7.0115 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 20 total=13.1844 mle=1.5336 pcon=4.6525 forget=6.9983 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 70 total=13.0947 mle=1.5342 pcon=4.6525 forget=6.9080 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 120 total=13.3154 mle=1.6501 pcon=4.6524 forget=7.0129 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 170 total=13.2899 mle=1.5450 pcon=4.6522 forget=7.0927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 220 total=13.0794 mle=1.4633 pcon=4.6522 forget=6.9638 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 270 total=13.3513 mle=1.5673 pcon=4.6523 forget=7.1317 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 320 total=13.3673 mle=1.5685 pcon=4.6522 forget=7.1466 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 370 total=13.3270 mle=1.6144 pcon=4.6522 forget=7.0604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 30 total=13.3812 mle=1.5976 pcon=4.6523 forget=7.1313 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 80 total=13.3544 mle=1.5909 pcon=4.6523 forget=7.1112 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 130 total=13.3447 mle=1.6553 pcon=4.6523 forget=7.0371 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 180 total=13.4955 mle=1.6539 pcon=4.6523 forget=7.1893 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 230 total=13.4280 mle=1.6597 pcon=4.6523 forget=7.1160 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 280 total=13.2430 mle=1.5966 pcon=4.6523 forget=6.9941 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 330 total=13.3090 mle=1.5748 pcon=4.6523 forget=7.0819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 380 total=13.4090 mle=1.5293 pcon=4.6522 forget=7.2276 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 40 total=13.4163 mle=1.5481 pcon=4.6522 forget=7.2160 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 90 total=13.4408 mle=1.5892 pcon=4.6522 forget=7.1994 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 140 total=13.4305 mle=1.6138 pcon=4.6522 forget=7.1646 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 190 total=13.5700 mle=1.7733 pcon=4.6521 forget=7.1446 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 240 total=13.4591 mle=1.6598 pcon=4.6520 forget=7.1473 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 290 total=13.4165 mle=1.6303 pcon=4.6519 forget=7.1343 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 340 total=13.3354 mle=1.5266 pcon=4.6520 forget=7.1568 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:57,  3.32it/s]  2%|▏         | 7/391 [00:00<00:17, 21.43it/s]  4%|▎         | 14/391 [00:00<00:10, 36.49it/s]  6%|▌         | 23/391 [00:00<00:07, 52.41it/s]  8%|▊         | 32/391 [00:00<00:05, 63.17it/s] 10%|█         | 41/391 [00:00<00:04, 70.82it/s] 13%|█▎        | 49/391 [00:00<00:05, 63.55it/s] 14%|█▍        | 56/391 [00:01<00:05, 61.55it/s] 16%|█▌        | 63/391 [00:01<00:05, 59.35it/s] 18%|█▊        | 72/391 [00:01<00:04, 67.06it/s] 21%|██        | 81/391 [00:01<00:04, 72.77it/s] 23%|██▎       | 90/391 [00:01<00:03, 76.96it/s] 25%|██▌       | 99/391 [00:01<00:03, 80.25it/s] 28%|██▊       | 108/391 [00:01<00:04, 66.86it/s] 30%|██▉       | 116/391 [00:01<00:04, 64.30it/s] 31%|███▏      | 123/391 [00:02<00:04, 63.93it/s] 34%|███▍      | 132/391 [00:02<00:03, 69.25it/s] 36%|███▌      | 141/391 [00:02<00:03, 74.46it/s] 38%|███▊      | 150/391 [00:02<00:03, 76.69it/s] 40%|████      | 158/391 [00:02<00:03, 71.22it/s] 42%|████▏     | 166/391 [00:02<00:03, 64.84it/s] 44%|████▍     | 173/391 [00:02<00:03, 61.69it/s] 47%|████▋     | 182/391 [00:02<00:03, 68.50it/s] 49%|████▉     | 191/391 [00:02<00:02, 73.86it/s] 51%|█████     | 200/391 [00:03<00:02, 77.55it/s] 53%|█████▎    | 209/391 [00:03<00:02, 79.75it/s] 56%|█████▌    | 218/391 [00:03<00:02, 70.40it/s] 58%|█████▊    | 226/391 [00:03<00:02, 65.77it/s] 60%|█████▉    | 233/391 [00:03<00:02, 63.97it/s] 62%|██████▏   | 242/391 [00:03<00:02, 70.22it/s] 64%|██████▍   | 251/391 [00:03<00:01, 74.91it/s] 66%|██████▋   | 260/391 [00:03<00:01, 78.40it/s] 69%|██████▉   | 269/391 [00:04<00:01, 80.96it/s] 71%|███████   | 278/391 [00:04<00:01, 67.77it/s] 73%|███████▎  | 286/391 [00:04<00:01, 65.37it/s] 75%|███████▍  | 293/391 [00:04<00:01, 64.87it/s] 77%|███████▋  | 302/391 [00:04<00:01, 70.86it/s] 80%|███████▉  | 311/391 [00:04<00:01, 74.86it/s] 82%|████████▏ | 320/391 [00:04<00:00, 77.94it/s] 84%|████████▍ | 329/391 [00:04<00:00, 80.85it/s] 86%|████████▋ | 338/391 [00:05<00:00, 66.45it/s] 88%|████████▊ | 346/391 [00:05<00:00, 62.97it/s] 91%|█████████ | 354/391 [00:05<00:00, 66.85it/s] 93%|█████████▎| 363/391 [00:05<00:00, 72.35it/s] 95%|█████████▌| 372/391 [00:05<00:00, 76.57it/s] 97%|█████████▋| 381/391 [00:05<00:00, 79.27it/s]100%|██████████| 391/391 [00:05<00:00, 79.46it/s]100%|██████████| 391/391 [00:05<00:00, 68.07it/s]
50000 images processed, 5.820366144180298 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:27,  2.80it/s] 13%|█▎        | 10/79 [00:00<00:02, 27.06it/s] 24%|██▍       | 19/79 [00:00<00:01, 44.45it/s] 33%|███▎      | 26/79 [00:00<00:01, 46.50it/s] 42%|████▏     | 33/79 [00:00<00:00, 49.52it/s] 49%|████▉     | 39/79 [00:00<00:00, 49.89it/s] 61%|██████    | 48/79 [00:01<00:00, 59.93it/s] 72%|███████▏  | 57/79 [00:01<00:00, 67.77it/s] 84%|████████▎ | 66/79 [00:01<00:00, 73.71it/s] 95%|█████████▍| 75/79 [00:01<00:00, 78.31it/s]100%|██████████| 79/79 [00:01<00:00, 55.77it/s]
10000 images processed, 1.440523624420166 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:05,  3.12it/s]  5%|▍         | 10/204 [00:00<00:06, 29.17it/s]  9%|▉         | 19/204 [00:00<00:03, 46.99it/s] 14%|█▎        | 28/204 [00:00<00:02, 59.17it/s] 18%|█▊        | 37/204 [00:00<00:02, 67.16it/s] 22%|██▏       | 45/204 [00:00<00:02, 60.42it/s] 25%|██▌       | 52/204 [00:01<00:02, 57.61it/s] 29%|██▉       | 59/204 [00:01<00:02, 58.74it/s] 33%|███▎      | 68/204 [00:01<00:02, 65.05it/s] 38%|███▊      | 77/204 [00:01<00:01, 70.27it/s] 42%|████▏     | 85/204 [00:01<00:01, 72.53it/s] 46%|████▌     | 93/204 [00:01<00:01, 60.17it/s] 49%|████▉     | 100/204 [00:01<00:01, 58.24it/s] 53%|█████▎    | 109/204 [00:01<00:01, 65.42it/s] 57%|█████▋    | 117/204 [00:01<00:01, 68.84it/s] 62%|██████▏   | 126/204 [00:02<00:01, 73.51it/s] 66%|██████▌   | 134/204 [00:02<00:01, 64.16it/s] 69%|██████▉   | 141/204 [00:02<00:01, 58.32it/s] 73%|███████▎  | 149/204 [00:02<00:00, 62.35it/s] 77%|███████▋  | 158/204 [00:02<00:00, 68.18it/s] 82%|████████▏ | 167/204 [00:02<00:00, 72.18it/s] 86%|████████▋ | 176/204 [00:02<00:00, 75.61it/s] 90%|█████████ | 184/204 [00:03<00:00, 63.59it/s] 94%|█████████▎| 191/204 [00:03<00:00, 57.79it/s] 98%|█████████▊| 200/204 [00:03<00:00, 65.00it/s]100%|██████████| 204/204 [00:03<00:00, 61.31it/s]
26032 images processed, 3.380887746810913 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:39,  1.97it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.68it/s] 19%|█▉        | 15/79 [00:00<00:02, 24.15it/s] 25%|██▌       | 20/79 [00:00<00:02, 29.49it/s] 32%|███▏      | 25/79 [00:01<00:01, 28.35it/s] 41%|████      | 32/79 [00:01<00:01, 36.02it/s] 48%|████▊     | 38/79 [00:01<00:01, 40.64it/s] 54%|█████▍    | 43/79 [00:01<00:00, 36.52it/s] 62%|██████▏   | 49/79 [00:01<00:00, 41.18it/s] 68%|██████▊   | 54/79 [00:01<00:00, 43.32it/s] 77%|███████▋  | 61/79 [00:01<00:00, 49.53it/s] 85%|████████▍ | 67/79 [00:02<00:00, 42.21it/s] 92%|█████████▏| 73/79 [00:02<00:00, 44.34it/s]100%|██████████| 79/79 [00:02<00:00, 35.84it/s]
10000 images processed, 2.24520206451416 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.02it/s] 11%|█▏        | 9/79 [00:00<00:02, 25.22it/s] 18%|█▊        | 14/79 [00:00<00:02, 30.75it/s] 25%|██▌       | 20/79 [00:00<00:01, 37.02it/s] 35%|███▌      | 28/79 [00:00<00:01, 47.31it/s] 47%|████▋     | 37/79 [00:00<00:00, 58.64it/s] 58%|█████▊    | 46/79 [00:01<00:00, 67.03it/s] 70%|██████▉   | 55/79 [00:01<00:00, 71.81it/s] 81%|████████  | 64/79 [00:01<00:00, 74.30it/s] 91%|█████████ | 72/79 [00:01<00:00, 60.61it/s]100%|██████████| 79/79 [00:01<00:00, 59.85it/s]100%|██████████| 79/79 [00:01<00:00, 51.54it/s]
10000 images processed, 1.5579934120178223 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:26,  2.59it/s] 10%|█         | 7/70 [00:00<00:03, 17.21it/s] 17%|█▋        | 12/70 [00:00<00:02, 24.72it/s] 26%|██▌       | 18/70 [00:00<00:01, 33.77it/s] 34%|███▍      | 24/70 [00:00<00:01, 40.78it/s] 47%|████▋     | 33/70 [00:00<00:00, 54.21it/s] 60%|██████    | 42/70 [00:01<00:00, 62.86it/s] 73%|███████▎  | 51/70 [00:01<00:00, 69.94it/s] 84%|████████▍ | 59/70 [00:01<00:00, 68.22it/s] 96%|█████████▌| 67/70 [00:01<00:00, 61.15it/s]100%|██████████| 70/70 [00:01<00:00, 47.81it/s]
8925 images processed, 1.4976766109466553 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:46,  1.05s/it] 18%|█▊        | 8/45 [00:01<00:04,  9.18it/s] 29%|██▉       | 13/45 [00:01<00:03,  9.26it/s] 36%|███▌      | 16/45 [00:01<00:02, 11.10it/s] 42%|████▏     | 19/45 [00:02<00:02,  9.01it/s] 53%|█████▎    | 24/45 [00:02<00:01, 12.17it/s] 58%|█████▊    | 26/45 [00:02<00:02,  9.11it/s] 71%|███████   | 32/45 [00:03<00:00, 14.59it/s] 78%|███████▊  | 35/45 [00:03<00:01,  9.88it/s] 91%|█████████ | 41/45 [00:04<00:00, 10.89it/s]100%|██████████| 45/45 [00:04<00:00, 10.60it/s]
5640 images processed, 4.266132354736328 seconds used

22.01985263824463
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           5.29  98.85
places365     76.41  78.83
LSUN          28.52  94.81
iSUN          76.95  80.03
dtd           46.90  89.39
forget        46.20  91.25
AVG           46.71  88.86
Retain-Acc: 0.7362
Forget-as-OOD (retain known vs forget novel):
  FPR: 46.20 AUROC: 91.25 AUIN: 98.86
15.28959584236145
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=1.0 lr=0.0005 epochs=5
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:23<01:32, 23.11s/it] 40%|████      | 2/5 [00:47<01:12, 24.10s/it] 60%|██████    | 3/5 [01:10<00:46, 23.23s/it] 80%|████████  | 4/5 [01:29<00:21, 21.55s/it]100%|██████████| 5/5 [01:47<00:00, 20.56s/it]100%|██████████| 5/5 [01:47<00:00, 21.58s/it]
[loss] ep 0 it 0 total=13.7435 mle=1.5710 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8368 mle=1.5424 pcon=5.2879 forget=7.0065 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8514 mle=1.7005 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0218 mle=1.8998 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8930 mle=1.7130 pcon=5.2670 forget=6.9130 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6544 mle=1.5020 pcon=5.2603 forget=6.8921 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7233 mle=1.5596 pcon=5.2540 forget=6.9096 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8766 mle=1.6808 pcon=5.2476 forget=6.9481 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=13.9625 mle=1.6715 pcon=5.2409 forget=7.0501 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7065 mle=1.6372 pcon=5.2346 forget=6.8348 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6923 mle=1.5168 pcon=5.2285 forget=6.9470 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9491 mle=1.7802 pcon=5.2224 forget=6.9465 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9071 mle=1.7644 pcon=5.2167 forget=6.9260 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6575 mle=1.6053 pcon=5.2112 forget=6.8410 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8194 mle=1.7237 pcon=5.2056 forget=6.8901 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9247 mle=1.7943 pcon=5.2003 forget=6.9301 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=13.6311 mle=1.5530 pcon=5.1950 forget=6.8831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8652 mle=1.8884 pcon=5.1899 forget=6.7869 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8050 mle=1.7419 pcon=5.1847 forget=6.8785 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5291 mle=1.5263 pcon=5.1796 forget=6.8232 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6880 mle=1.5853 pcon=5.1745 forget=6.9281 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9050 mle=1.8836 pcon=5.1699 forget=6.8515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5437 mle=1.5923 pcon=5.1652 forget=6.7862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7932 mle=1.8113 pcon=5.1605 forget=6.8215 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=13.7127 mle=1.7057 pcon=5.1559 forget=6.8511 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7170 mle=1.6710 pcon=5.1519 forget=6.8941 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7410 mle=1.7668 pcon=5.1475 forget=6.8267 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8745 mle=1.9230 pcon=5.1435 forget=6.8079 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.5058 mle=1.5699 pcon=5.1395 forget=6.7965 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7494 mle=1.8359 pcon=5.1353 forget=6.7782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.7074 mle=1.7032 pcon=5.1311 forget=6.8731 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.7091 mle=1.6958 pcon=5.1275 forget=6.8857 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=13.4839 mle=1.6535 pcon=5.1236 forget=6.7067 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6257 mle=1.7294 pcon=5.1196 forget=6.7766 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5917 mle=1.6511 pcon=5.1159 forget=6.8247 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3891 mle=1.4536 pcon=5.1125 forget=6.8231 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6493 mle=1.7671 pcon=5.1086 forget=6.7737 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3557 mle=1.4911 pcon=5.1050 forget=6.7596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7882 mle=1.8722 pcon=5.1015 forget=6.8145 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:33,  2.55it/s]  2%|▏         | 8/391 [00:00<00:19, 19.70it/s]  3%|▎         | 13/391 [00:00<00:13, 27.23it/s]  5%|▌         | 20/391 [00:00<00:09, 37.46it/s]  7%|▋         | 26/391 [00:00<00:08, 43.09it/s]  9%|▉         | 35/391 [00:00<00:06, 55.46it/s] 11%|█▏        | 44/391 [00:01<00:05, 64.66it/s] 14%|█▎        | 53/391 [00:01<00:04, 71.56it/s] 16%|█▌        | 62/391 [00:01<00:04, 75.86it/s] 18%|█▊        | 70/391 [00:01<00:04, 73.20it/s] 20%|█▉        | 78/391 [00:01<00:04, 63.29it/s] 22%|██▏       | 85/391 [00:01<00:05, 60.67it/s] 24%|██▍       | 94/391 [00:01<00:04, 67.35it/s] 26%|██▋       | 103/391 [00:01<00:03, 72.70it/s] 29%|██▊       | 112/391 [00:01<00:03, 76.73it/s] 31%|███       | 121/391 [00:02<00:03, 79.61it/s] 33%|███▎      | 130/391 [00:02<00:03, 67.71it/s] 35%|███▌      | 138/391 [00:02<00:04, 61.28it/s] 37%|███▋      | 146/391 [00:02<00:03, 65.39it/s] 40%|███▉      | 155/391 [00:02<00:03, 71.15it/s] 42%|████▏     | 164/391 [00:02<00:02, 75.70it/s] 44%|████▍     | 173/391 [00:02<00:02, 77.83it/s] 47%|████▋     | 182/391 [00:02<00:02, 73.18it/s] 49%|████▊     | 190/391 [00:03<00:03, 65.26it/s] 50%|█████     | 197/391 [00:03<00:03, 62.09it/s] 53%|█████▎    | 206/391 [00:03<00:02, 68.04it/s] 55%|█████▍    | 215/391 [00:03<00:02, 73.23it/s] 57%|█████▋    | 224/391 [00:03<00:02, 77.40it/s] 60%|█████▉    | 233/391 [00:03<00:01, 80.34it/s] 62%|██████▏   | 242/391 [00:03<00:01, 74.76it/s] 64%|██████▍   | 250/391 [00:03<00:02, 66.36it/s] 66%|██████▌   | 257/391 [00:04<00:02, 62.64it/s] 68%|██████▊   | 266/391 [00:04<00:01, 69.08it/s] 70%|███████   | 275/391 [00:04<00:01, 72.82it/s] 73%|███████▎  | 284/391 [00:04<00:01, 76.30it/s] 75%|███████▍  | 293/391 [00:04<00:01, 79.12it/s] 77%|███████▋  | 302/391 [00:04<00:01, 67.85it/s] 79%|███████▉  | 310/391 [00:04<00:01, 64.49it/s] 81%|████████  | 317/391 [00:04<00:01, 64.40it/s] 83%|████████▎ | 326/391 [00:05<00:00, 70.51it/s] 86%|████████▌ | 335/391 [00:05<00:00, 73.79it/s] 88%|████████▊ | 344/391 [00:05<00:00, 77.62it/s] 90%|█████████ | 352/391 [00:05<00:00, 76.20it/s] 92%|█████████▏| 360/391 [00:05<00:00, 64.33it/s] 94%|█████████▍| 367/391 [00:05<00:00, 61.88it/s] 96%|█████████▌| 376/391 [00:05<00:00, 67.74it/s] 98%|█████████▊| 385/391 [00:05<00:00, 73.46it/s]100%|██████████| 391/391 [00:05<00:00, 65.79it/s]
50000 images processed, 6.033405065536499 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.04it/s]  9%|▉         | 7/79 [00:00<00:03, 20.05it/s] 20%|██        | 16/79 [00:00<00:01, 40.55it/s] 32%|███▏      | 25/79 [00:00<00:00, 54.29it/s] 43%|████▎     | 34/79 [00:00<00:00, 64.12it/s] 54%|█████▍    | 43/79 [00:00<00:00, 71.08it/s] 65%|██████▍   | 51/79 [00:00<00:00, 64.18it/s] 75%|███████▍  | 59/79 [00:01<00:00, 62.86it/s] 84%|████████▎ | 66/79 [00:01<00:00, 60.33it/s] 95%|█████████▍| 75/79 [00:01<00:00, 67.89it/s]100%|██████████| 79/79 [00:01<00:00, 55.84it/s]
10000 images processed, 1.4427618980407715 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:17,  2.63it/s]  4%|▍         | 9/204 [00:00<00:08, 22.88it/s]  7%|▋         | 14/204 [00:00<00:06, 28.80it/s] 10%|▉         | 20/204 [00:00<00:05, 36.36it/s] 13%|█▎        | 26/204 [00:00<00:04, 41.98it/s] 17%|█▋        | 35/204 [00:00<00:03, 54.54it/s] 22%|██▏       | 44/204 [00:01<00:02, 63.59it/s] 25%|██▌       | 52/204 [00:01<00:02, 68.19it/s] 29%|██▉       | 60/204 [00:01<00:02, 58.13it/s] 33%|███▎      | 67/204 [00:01<00:02, 55.91it/s] 37%|███▋      | 75/204 [00:01<00:02, 61.80it/s] 41%|████      | 84/204 [00:01<00:01, 67.59it/s] 46%|████▌     | 93/204 [00:01<00:01, 72.27it/s] 50%|████▉     | 101/204 [00:01<00:01, 71.79it/s] 53%|█████▎    | 109/204 [00:02<00:01, 61.64it/s] 57%|█████▋    | 116/204 [00:02<00:01, 59.20it/s] 61%|██████▏   | 125/204 [00:02<00:01, 66.15it/s] 66%|██████▌   | 134/204 [00:02<00:00, 71.66it/s] 70%|███████   | 143/204 [00:02<00:00, 76.03it/s] 75%|███████▍  | 152/204 [00:02<00:00, 79.28it/s] 79%|███████▉  | 161/204 [00:02<00:00, 66.11it/s] 83%|████████▎ | 169/204 [00:02<00:00, 60.60it/s] 87%|████████▋ | 178/204 [00:03<00:00, 66.79it/s] 92%|█████████▏| 187/204 [00:03<00:00, 71.86it/s] 96%|█████████▌| 196/204 [00:03<00:00, 75.95it/s]100%|██████████| 204/204 [00:03<00:00, 60.72it/s]
26032 images processed, 3.410891056060791 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:43,  1.79it/s]  9%|▉         | 7/79 [00:00<00:05, 13.43it/s] 15%|█▌        | 12/79 [00:00<00:03, 21.24it/s] 23%|██▎       | 18/79 [00:00<00:02, 30.10it/s] 32%|███▏      | 25/79 [00:01<00:01, 32.88it/s] 41%|████      | 32/79 [00:01<00:01, 39.79it/s] 47%|████▋     | 37/79 [00:01<00:00, 42.10it/s] 53%|█████▎    | 42/79 [00:01<00:00, 37.33it/s] 61%|██████    | 48/79 [00:01<00:00, 40.55it/s] 70%|██████▉   | 55/79 [00:01<00:00, 47.62it/s] 77%|███████▋  | 61/79 [00:01<00:00, 40.36it/s] 85%|████████▍ | 67/79 [00:01<00:00, 43.04it/s] 92%|█████████▏| 73/79 [00:02<00:00, 46.18it/s]100%|██████████| 79/79 [00:02<00:00, 36.48it/s]
10000 images processed, 2.2005553245544434 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:30,  2.53it/s]  9%|▉         | 7/79 [00:00<00:04, 17.71it/s] 19%|█▉        | 15/79 [00:00<00:01, 34.12it/s] 30%|███       | 24/79 [00:00<00:01, 48.97it/s] 42%|████▏     | 33/79 [00:00<00:00, 59.98it/s] 53%|█████▎    | 42/79 [00:00<00:00, 67.50it/s] 63%|██████▎   | 50/79 [00:01<00:00, 62.07it/s] 72%|███████▏  | 57/79 [00:01<00:00, 57.52it/s] 81%|████████  | 64/79 [00:01<00:00, 58.75it/s] 92%|█████████▏| 73/79 [00:01<00:00, 66.61it/s]100%|██████████| 79/79 [00:01<00:00, 53.14it/s]
10000 images processed, 1.5097565650939941 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:30,  2.24it/s]  9%|▊         | 6/70 [00:00<00:04, 13.75it/s] 17%|█▋        | 12/70 [00:00<00:02, 24.54it/s] 30%|███       | 21/70 [00:00<00:01, 41.15it/s] 43%|████▎     | 30/70 [00:00<00:00, 53.92it/s] 56%|█████▌    | 39/70 [00:00<00:00, 63.38it/s] 69%|██████▊   | 48/70 [00:01<00:00, 70.32it/s] 80%|████████  | 56/70 [00:01<00:00, 66.06it/s] 91%|█████████▏| 64/70 [00:01<00:00, 60.96it/s]100%|██████████| 70/70 [00:01<00:00, 47.10it/s]
8925 images processed, 1.518017292022705 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:41,  1.06it/s]  4%|▍         | 2/45 [00:01<00:20,  2.08it/s] 20%|██        | 9/45 [00:01<00:03,  9.08it/s] 24%|██▍       | 11/45 [00:01<00:03, 10.13it/s] 31%|███       | 14/45 [00:01<00:02, 12.92it/s] 38%|███▊      | 17/45 [00:02<00:02, 11.41it/s] 47%|████▋     | 21/45 [00:02<00:01, 15.45it/s] 53%|█████▎    | 24/45 [00:02<00:01, 15.83it/s] 58%|█████▊    | 26/45 [00:02<00:01, 12.11it/s] 67%|██████▋   | 30/45 [00:02<00:01, 13.49it/s] 73%|███████▎  | 33/45 [00:03<00:01, 10.99it/s] 78%|███████▊  | 35/45 [00:03<00:00, 11.64it/s] 91%|█████████ | 41/45 [00:03<00:00, 13.31it/s] 96%|█████████▌| 43/45 [00:04<00:00, 11.00it/s]100%|██████████| 45/45 [00:04<00:00, 10.96it/s]
5640 images processed, 4.133220195770264 seconds used

22.090440034866333
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.88  81.19
LSUN          17.54  96.08
iSUN          72.34  81.72
dtd           37.89  91.39
forget        79.50  86.90
AVG           46.28  89.44
Retain-Acc: 0.7406
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
21.745462656021118
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=1.0 lr=0.0005 epochs=10
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:27<04:11, 27.95s/it] 20%|██        | 2/10 [00:46<02:59, 22.45s/it] 30%|███       | 3/10 [01:04<02:23, 20.57s/it] 40%|████      | 4/10 [01:28<02:11, 21.91s/it] 50%|█████     | 5/10 [01:51<01:51, 22.23s/it] 60%|██████    | 6/10 [02:10<01:24, 21.16s/it] 70%|███████   | 7/10 [02:30<01:01, 20.66s/it] 80%|████████  | 8/10 [02:57<00:45, 22.77s/it][loss] ep 0 it 0 total=13.7434 mle=1.5709 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8369 mle=1.5424 pcon=5.2879 forget=7.0066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8514 mle=1.7004 pcon=5.2809 forget=6.8701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0217 mle=1.8997 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8931 mle=1.7129 pcon=5.2670 forget=6.9131 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6544 mle=1.5020 pcon=5.2603 forget=6.8921 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7232 mle=1.5596 pcon=5.2540 forget=6.9096 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8767 mle=1.6809 pcon=5.2476 forget=6.9481 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=13.9625 mle=1.6715 pcon=5.2409 forget=7.0501 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7064 mle=1.6371 pcon=5.2346 forget=6.8348 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6921 mle=1.5168 pcon=5.2285 forget=6.9469 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9492 mle=1.7803 pcon=5.2224 forget=6.9464 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9073 mle=1.7646 pcon=5.2167 forget=6.9260 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6574 mle=1.6052 pcon=5.2112 forget=6.8409 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8195 mle=1.7237 pcon=5.2056 forget=6.8901 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9247 mle=1.7943 pcon=5.2003 forget=6.9301 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=13.6311 mle=1.5530 pcon=5.1950 forget=6.8830 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8650 mle=1.8881 pcon=5.1899 forget=6.7870 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8051 mle=1.7420 pcon=5.1847 forget=6.8784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5294 mle=1.5265 pcon=5.1796 forget=6.8232 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6881 mle=1.5854 pcon=5.1745 forget=6.9281 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9052 mle=1.8838 pcon=5.1699 forget=6.8515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5437 mle=1.5923 pcon=5.1652 forget=6.7862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7932 mle=1.8112 pcon=5.1605 forget=6.8215 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=13.7127 mle=1.7058 pcon=5.1559 forget=6.8509 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7167 mle=1.6708 pcon=5.1519 forget=6.8940 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7411 mle=1.7669 pcon=5.1475 forget=6.8268 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8699 mle=1.9184 pcon=5.1435 forget=6.8080 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.5058 mle=1.5699 pcon=5.1395 forget=6.7965 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7497 mle=1.8362 pcon=5.1353 forget=6.7782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.7073 mle=1.7031 pcon=5.1311 forget=6.8732 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.7089 mle=1.6957 pcon=5.1275 forget=6.8857 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=13.4839 mle=1.6535 pcon=5.1237 forget=6.7067 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6255 mle=1.7292 pcon=5.1196 forget=6.7767 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5918 mle=1.6512 pcon=5.1159 forget=6.8246 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3891 mle=1.4536 pcon=5.1125 forget=6.8230 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6493 mle=1.7670 pcon=5.1086 forget=6.7737 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3558 mle=1.4912 pcon=5.1050 forget=6.7596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7882 mle=1.8722 pcon=5.1015 forget=6.8144 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=13.7431 mle=1.8672 pcon=5.0982 forget=6.7777 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=13.6147 mle=1.8005 pcon=5.0946 forget=6.7196 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=13.4659 mle=1.5851 pcon=5.0914 forget=6.7895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=13.4643 mle=1.5947 pcon=5.0879 forget=6.7818 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=13.4908 mle=1.6917 pcon=5.0849 forget=6.7142 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=13.5359 mle=1.7749 pcon=5.0819 forget=6.6791 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=13.4047 mle=1.5974 pcon=5.0788 forget=6.7285 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=13.5452 mle=1.7451 pcon=5.0758 forget=6.7243 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=13.3085 mle=1.3998 pcon=5.0729 forget=6.8358 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=13.2168 mle=1.5322 pcon=5.0700 forget=6.6145 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=13.4179 mle=1.5903 pcon=5.0673 forget=6.7604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=13.4137 mle=1.6675 pcon=5.0644 forget=6.6818 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=13.3609 mle=1.6311 pcon=5.0620 forget=6.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=13.5973 mle=1.8162 pcon=5.0589 forget=6.7221 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=13.3616 mle=1.5450 pcon=5.0564 forget=6.7602 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=13.3962 mle=1.6732 pcon=5.0537 forget=6.6693 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=13.3042 mle=1.6300 pcon=5.0508 forget=6.6234 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=13.3900 mle=1.7452 pcon=5.0481 forget=6.5967 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=13.2667 mle=1.6108 pcon=5.0453 forget=6.6106 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=13.1941 mle=1.5654 pcon=5.0426 forget=6.5861 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=13.3478 mle=1.6449 pcon=5.0402 forget=6.6627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=13.5278 mle=1.7902 pcon=5.0377 forget=6.6999 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=13.4457 mle=1.7320 pcon=5.0348 forget=6.6789 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=13.2733 mle=1.6253 pcon=5.0324 forget=6.6156 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 90%|█████████ | 9/10 [03:16<00:21, 21.58s/it]100%|██████████| 10/10 [03:35<00:00, 20.70s/it]100%|██████████| 10/10 [03:35<00:00, 21.53s/it]
[loss] ep 8 it 30 total=13.2569 mle=1.5430 pcon=5.0301 forget=6.6838 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=13.2589 mle=1.5774 pcon=5.0277 forget=6.6538 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=13.1650 mle=1.5041 pcon=5.0252 forget=6.6357 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=13.3486 mle=1.7123 pcon=5.0229 forget=6.6134 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=13.1491 mle=1.5289 pcon=5.0210 forget=6.5992 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=13.2951 mle=1.6906 pcon=5.0188 forget=6.5858 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=13.4825 mle=1.7407 pcon=5.0164 forget=6.7253 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=13.1435 mle=1.4824 pcon=5.0143 forget=6.6468 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=13.2487 mle=1.6330 pcon=5.0123 forget=6.6035 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=13.3710 mle=1.6719 pcon=5.0101 forget=6.6889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=13.6086 mle=1.8198 pcon=5.0086 forget=6.7801 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=13.2774 mle=1.5804 pcon=5.0071 forget=6.6898 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=13.3692 mle=1.5859 pcon=5.0057 forget=6.7776 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=13.3708 mle=1.5609 pcon=5.0040 forget=6.8059 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=13.3539 mle=1.5535 pcon=5.0028 forget=6.7976 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:46,  2.34it/s]  2%|▏         | 6/391 [00:00<00:27, 13.99it/s]  3%|▎         | 13/391 [00:00<00:13, 27.19it/s]  5%|▍         | 19/391 [00:00<00:10, 35.49it/s]  7%|▋         | 28/391 [00:00<00:07, 50.02it/s]  9%|▉         | 37/391 [00:00<00:05, 60.92it/s] 12%|█▏        | 46/391 [00:01<00:05, 68.99it/s] 14%|█▍        | 55/391 [00:01<00:04, 74.92it/s] 17%|█▋        | 65/391 [00:01<00:04, 79.81it/s] 19%|█▉        | 74/391 [00:01<00:03, 82.70it/s] 21%|██        | 83/391 [00:01<00:04, 66.60it/s] 23%|██▎       | 91/391 [00:01<00:04, 63.30it/s] 26%|██▌       | 100/391 [00:01<00:04, 68.72it/s] 28%|██▊       | 110/391 [00:01<00:03, 74.55it/s] 30%|███       | 119/391 [00:02<00:03, 78.53it/s] 33%|███▎      | 128/391 [00:02<00:03, 81.30it/s] 35%|███▌      | 137/391 [00:02<00:03, 83.32it/s] 37%|███▋      | 146/391 [00:02<00:02, 83.12it/s] 40%|███▉      | 155/391 [00:02<00:03, 67.21it/s] 42%|████▏     | 163/391 [00:02<00:03, 63.79it/s] 44%|████▍     | 172/391 [00:02<00:03, 69.87it/s] 46%|████▋     | 181/391 [00:02<00:02, 74.70it/s] 49%|████▊     | 190/391 [00:02<00:02, 76.15it/s] 51%|█████     | 199/391 [00:03<00:02, 79.01it/s] 53%|█████▎    | 208/391 [00:03<00:02, 75.76it/s] 55%|█████▌    | 216/391 [00:03<00:02, 65.30it/s] 57%|█████▋    | 223/391 [00:03<00:02, 62.27it/s] 59%|█████▉    | 232/391 [00:03<00:02, 68.83it/s] 62%|██████▏   | 241/391 [00:03<00:02, 73.97it/s] 64%|██████▍   | 250/391 [00:03<00:01, 77.94it/s] 66%|██████▌   | 259/391 [00:03<00:01, 80.89it/s] 69%|██████▊   | 268/391 [00:04<00:01, 82.97it/s] 71%|███████   | 277/391 [00:04<00:01, 74.40it/s] 73%|███████▎  | 285/391 [00:04<00:01, 67.22it/s] 75%|███████▍  | 293/391 [00:04<00:01, 64.80it/s] 77%|███████▋  | 302/391 [00:04<00:01, 70.80it/s] 80%|███████▉  | 311/391 [00:04<00:01, 75.55it/s] 82%|████████▏ | 320/391 [00:04<00:00, 79.15it/s] 84%|████████▍ | 329/391 [00:04<00:00, 81.45it/s] 86%|████████▋ | 338/391 [00:04<00:00, 83.20it/s] 89%|████████▊ | 347/391 [00:05<00:00, 69.47it/s] 91%|█████████ | 355/391 [00:05<00:00, 62.84it/s] 93%|█████████▎| 364/391 [00:05<00:00, 67.80it/s] 95%|█████████▌| 373/391 [00:05<00:00, 73.07it/s] 98%|█████████▊| 382/391 [00:05<00:00, 76.91it/s]100%|██████████| 391/391 [00:05<00:00, 77.96it/s]100%|██████████| 391/391 [00:05<00:00, 68.19it/s]
50000 images processed, 5.847063302993774 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.04it/s]  9%|▉         | 7/79 [00:00<00:03, 20.14it/s] 20%|██        | 16/79 [00:00<00:01, 40.89it/s] 32%|███▏      | 25/79 [00:00<00:00, 55.19it/s] 43%|████▎     | 34/79 [00:00<00:00, 64.77it/s] 54%|█████▍    | 43/79 [00:00<00:00, 71.41it/s] 65%|██████▍   | 51/79 [00:00<00:00, 67.97it/s] 75%|███████▍  | 59/79 [00:01<00:00, 61.84it/s] 84%|████████▎ | 66/79 [00:01<00:00, 59.28it/s] 95%|█████████▍| 75/79 [00:01<00:00, 67.05it/s]100%|██████████| 79/79 [00:01<00:00, 56.06it/s]
10000 images processed, 1.4329533576965332 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:17,  2.63it/s]  3%|▎         | 7/204 [00:00<00:10, 18.18it/s]  7%|▋         | 15/204 [00:00<00:05, 35.41it/s] 12%|█▏        | 24/204 [00:00<00:03, 49.88it/s] 16%|█▌        | 33/204 [00:00<00:02, 60.24it/s] 20%|██        | 41/204 [00:00<00:02, 57.36it/s] 24%|██▎       | 48/204 [00:01<00:02, 55.87it/s] 27%|██▋       | 55/204 [00:01<00:02, 56.38it/s] 31%|███▏      | 64/204 [00:01<00:02, 64.22it/s] 36%|███▌      | 73/204 [00:01<00:01, 69.88it/s] 40%|████      | 82/204 [00:01<00:01, 74.29it/s] 44%|████▍     | 90/204 [00:01<00:01, 70.93it/s] 48%|████▊     | 98/204 [00:01<00:01, 61.89it/s] 51%|█████▏    | 105/204 [00:01<00:01, 59.54it/s] 56%|█████▌    | 114/204 [00:02<00:01, 66.56it/s] 60%|██████    | 123/204 [00:02<00:01, 72.06it/s] 65%|██████▍   | 132/204 [00:02<00:00, 76.11it/s] 69%|██████▉   | 141/204 [00:02<00:00, 77.51it/s] 73%|███████▎  | 149/204 [00:02<00:00, 62.65it/s] 76%|███████▋  | 156/204 [00:02<00:00, 60.94it/s] 81%|████████  | 165/204 [00:02<00:00, 66.88it/s] 85%|████████▌ | 174/204 [00:02<00:00, 71.78it/s] 90%|████████▉ | 183/204 [00:02<00:00, 75.60it/s] 94%|█████████▍| 192/204 [00:03<00:00, 77.05it/s] 98%|█████████▊| 200/204 [00:03<00:00, 62.57it/s]100%|██████████| 204/204 [00:03<00:00, 60.65it/s]
26032 images processed, 3.4136247634887695 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:45,  1.70it/s]  9%|▉         | 7/79 [00:00<00:05, 12.68it/s] 14%|█▍        | 11/79 [00:00<00:03, 17.38it/s] 20%|██        | 16/79 [00:00<00:02, 23.40it/s] 27%|██▋       | 21/79 [00:01<00:02, 28.97it/s] 33%|███▎      | 26/79 [00:01<00:01, 33.95it/s] 43%|████▎     | 34/79 [00:01<00:01, 39.57it/s] 53%|█████▎    | 42/79 [00:01<00:00, 42.64it/s] 59%|█████▉    | 47/79 [00:01<00:00, 43.29it/s] 66%|██████▌   | 52/79 [00:01<00:00, 44.54it/s] 72%|███████▏  | 57/79 [00:01<00:00, 43.24it/s] 78%|███████▊  | 62/79 [00:02<00:00, 35.96it/s] 86%|████████▌ | 68/79 [00:02<00:00, 40.57it/s] 92%|█████████▏| 73/79 [00:02<00:00, 41.41it/s] 99%|█████████▊| 78/79 [00:02<00:00, 38.32it/s]100%|██████████| 79/79 [00:02<00:00, 32.69it/s]
10000 images processed, 2.4574224948883057 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:34,  2.26it/s] 11%|█▏        | 9/79 [00:00<00:03, 21.17it/s] 23%|██▎       | 18/79 [00:00<00:01, 38.35it/s] 34%|███▍      | 27/79 [00:00<00:01, 51.30it/s] 44%|████▍     | 35/79 [00:00<00:00, 56.67it/s] 54%|█████▍    | 43/79 [00:01<00:00, 52.54it/s] 63%|██████▎   | 50/79 [00:01<00:00, 52.89it/s] 75%|███████▍  | 59/79 [00:01<00:00, 61.19it/s] 86%|████████▌ | 68/79 [00:01<00:00, 67.88it/s] 97%|█████████▋| 77/79 [00:01<00:00, 73.11it/s]100%|██████████| 79/79 [00:01<00:00, 52.52it/s]
10000 images processed, 1.526841402053833 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:28,  2.46it/s]  9%|▊         | 6/70 [00:00<00:04, 14.14it/s] 17%|█▋        | 12/70 [00:00<00:02, 24.86it/s] 30%|███       | 21/70 [00:00<00:01, 40.99it/s] 43%|████▎     | 30/70 [00:00<00:00, 53.57it/s] 56%|█████▌    | 39/70 [00:00<00:00, 63.11it/s] 69%|██████▊   | 48/70 [00:01<00:00, 68.77it/s] 80%|████████  | 56/70 [00:01<00:00, 65.02it/s] 91%|█████████▏| 64/70 [00:01<00:00, 58.84it/s]100%|██████████| 70/70 [00:01<00:00, 47.29it/s]
8925 images processed, 1.5238926410675049 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:36,  1.22it/s]  4%|▍         | 2/45 [00:01<00:19,  2.25it/s] 18%|█▊        | 8/45 [00:01<00:03, 11.17it/s] 24%|██▍       | 11/45 [00:01<00:04,  8.19it/s] 31%|███       | 14/45 [00:01<00:02, 10.88it/s] 40%|████      | 18/45 [00:02<00:02, 10.03it/s] 51%|█████     | 23/45 [00:02<00:01, 14.87it/s] 58%|█████▊    | 26/45 [00:02<00:01, 10.56it/s] 67%|██████▋   | 30/45 [00:02<00:01, 13.85it/s] 76%|███████▌  | 34/45 [00:03<00:01,  9.31it/s] 87%|████████▋ | 39/45 [00:03<00:00, 13.02it/s] 93%|█████████▎| 42/45 [00:04<00:00,  8.93it/s]100%|██████████| 45/45 [00:04<00:00, 10.03it/s]
5640 images processed, 4.51054048538208 seconds used

22.548826456069946
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.81  81.20
LSUN          17.44  96.10
iSUN          72.20  81.79
dtd           37.82  91.38
forget        79.50  86.91
AVG           46.22  89.46
Retain-Acc: 0.7403
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.91 AUIN: 98.36
24.714258909225464
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=1.0 lr=0.0005 epochs=20
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:25<07:55, 25.05s/it] 10%|█         | 2/20 [00:49<07:22, 24.59s/it] 15%|█▌        | 3/20 [01:07<06:07, 21.64s/it] 20%|██        | 4/20 [01:26<05:26, 20.43s/it] 25%|██▌       | 5/20 [01:49<05:23, 21.54s/it] 30%|███       | 6/20 [02:12<05:08, 22.02s/it] 35%|███▌      | 7/20 [02:30<04:31, 20.88s/it] 40%|████      | 8/20 [02:49<04:01, 20.11s/it][loss] ep 0 it 0 total=13.7435 mle=1.5710 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8368 mle=1.5424 pcon=5.2879 forget=7.0065 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8514 mle=1.7005 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0218 mle=1.8998 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8930 mle=1.7130 pcon=5.2670 forget=6.9130 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6544 mle=1.5020 pcon=5.2603 forget=6.8921 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7233 mle=1.5596 pcon=5.2540 forget=6.9096 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8766 mle=1.6808 pcon=5.2476 forget=6.9481 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=13.9625 mle=1.6715 pcon=5.2409 forget=7.0501 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7065 mle=1.6372 pcon=5.2346 forget=6.8348 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6923 mle=1.5168 pcon=5.2285 forget=6.9470 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9491 mle=1.7802 pcon=5.2224 forget=6.9465 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9071 mle=1.7644 pcon=5.2167 forget=6.9260 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6575 mle=1.6053 pcon=5.2112 forget=6.8410 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8194 mle=1.7237 pcon=5.2056 forget=6.8901 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9247 mle=1.7943 pcon=5.2003 forget=6.9301 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=13.6311 mle=1.5530 pcon=5.1950 forget=6.8831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8652 mle=1.8884 pcon=5.1899 forget=6.7869 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8050 mle=1.7419 pcon=5.1847 forget=6.8785 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5291 mle=1.5263 pcon=5.1796 forget=6.8232 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6880 mle=1.5853 pcon=5.1745 forget=6.9281 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9050 mle=1.8836 pcon=5.1699 forget=6.8515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5437 mle=1.5923 pcon=5.1652 forget=6.7862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7932 mle=1.8113 pcon=5.1605 forget=6.8215 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=13.7127 mle=1.7057 pcon=5.1559 forget=6.8511 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7170 mle=1.6710 pcon=5.1519 forget=6.8941 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7410 mle=1.7668 pcon=5.1475 forget=6.8267 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8745 mle=1.9230 pcon=5.1435 forget=6.8079 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.5058 mle=1.5699 pcon=5.1395 forget=6.7965 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7494 mle=1.8359 pcon=5.1353 forget=6.7782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.7074 mle=1.7032 pcon=5.1311 forget=6.8731 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.7091 mle=1.6958 pcon=5.1275 forget=6.8857 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=13.4839 mle=1.6535 pcon=5.1236 forget=6.7067 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6257 mle=1.7294 pcon=5.1196 forget=6.7766 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5917 mle=1.6511 pcon=5.1159 forget=6.8247 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3891 mle=1.4536 pcon=5.1125 forget=6.8231 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6493 mle=1.7671 pcon=5.1086 forget=6.7737 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3557 mle=1.4911 pcon=5.1050 forget=6.7596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7882 mle=1.8722 pcon=5.1015 forget=6.8145 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=13.7431 mle=1.8671 pcon=5.0982 forget=6.7778 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=13.6147 mle=1.8005 pcon=5.0946 forget=6.7196 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=13.4661 mle=1.5853 pcon=5.0914 forget=6.7895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=13.4641 mle=1.5945 pcon=5.0879 forget=6.7817 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=13.4909 mle=1.6917 pcon=5.0849 forget=6.7142 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=13.5356 mle=1.7746 pcon=5.0819 forget=6.6791 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=13.4045 mle=1.5973 pcon=5.0788 forget=6.7285 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=13.5452 mle=1.7451 pcon=5.0758 forget=6.7243 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=13.3085 mle=1.3998 pcon=5.0729 forget=6.8358 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=13.2169 mle=1.5323 pcon=5.0700 forget=6.6145 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=13.4180 mle=1.5904 pcon=5.0673 forget=6.7604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=13.4137 mle=1.6674 pcon=5.0644 forget=6.6819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=13.3603 mle=1.6305 pcon=5.0620 forget=6.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=13.5974 mle=1.8164 pcon=5.0589 forget=6.7221 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=13.3616 mle=1.5452 pcon=5.0564 forget=6.7601 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=13.3960 mle=1.6731 pcon=5.0537 forget=6.6693 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=13.3042 mle=1.6300 pcon=5.0508 forget=6.6234 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=13.3902 mle=1.7454 pcon=5.0481 forget=6.5967 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=13.2668 mle=1.6110 pcon=5.0453 forget=6.6106 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=13.1941 mle=1.5654 pcon=5.0426 forget=6.5861 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=13.3478 mle=1.6450 pcon=5.0402 forget=6.6627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=13.5277 mle=1.7902 pcon=5.0377 forget=6.6999 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=13.4458 mle=1.7321 pcon=5.0348 forget=6.6789 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=13.2733 mle=1.6253 pcon=5.0324 forget=6.6156 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 45%|████▌     | 9/20 [03:15<04:00, 21.82s/it] 50%|█████     | 10/20 [03:35<03:34, 21.43s/it] 55%|█████▌    | 11/20 [03:54<03:04, 20.51s/it] 60%|██████    | 12/20 [04:13<02:40, 20.05s/it] 65%|██████▌   | 13/20 [04:38<02:31, 21.64s/it] 70%|███████   | 14/20 [04:58<02:07, 21.24s/it] 75%|███████▌  | 15/20 [05:16<01:41, 20.33s/it] 80%|████████  | 16/20 [05:36<01:20, 20.23s/it] 85%|████████▌ | 17/20 [06:02<01:05, 21.89s/it][loss] ep 8 it 30 total=13.2571 mle=1.5431 pcon=5.0301 forget=6.6839 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=13.2588 mle=1.5773 pcon=5.0277 forget=6.6538 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=13.1649 mle=1.5041 pcon=5.0252 forget=6.6356 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=13.3487 mle=1.7124 pcon=5.0229 forget=6.6135 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=13.1491 mle=1.5289 pcon=5.0210 forget=6.5992 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=13.2949 mle=1.6904 pcon=5.0187 forget=6.5858 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=13.4826 mle=1.7408 pcon=5.0164 forget=6.7253 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=13.1434 mle=1.4823 pcon=5.0143 forget=6.6467 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=13.2488 mle=1.6330 pcon=5.0123 forget=6.6035 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=13.3711 mle=1.6721 pcon=5.0101 forget=6.6889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=13.6086 mle=1.8198 pcon=5.0086 forget=6.7802 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=13.2771 mle=1.5802 pcon=5.0071 forget=6.6898 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=13.3713 mle=1.5880 pcon=5.0057 forget=6.7776 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=13.3707 mle=1.5609 pcon=5.0040 forget=6.8058 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=13.3539 mle=1.5535 pcon=5.0028 forget=6.7976 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 0 total=13.4740 mle=1.6136 pcon=5.0014 forget=6.8590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=13.5131 mle=1.5983 pcon=5.0000 forget=6.9147 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=13.6374 mle=1.7387 pcon=4.9990 forget=6.8998 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=13.7345 mle=1.8519 pcon=4.9980 forget=6.8845 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=13.7860 mle=1.8494 pcon=4.9970 forget=6.9396 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=13.5427 mle=1.5787 pcon=4.9962 forget=6.9677 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=13.5363 mle=1.5640 pcon=4.9956 forget=6.9767 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=13.7729 mle=1.6568 pcon=4.9947 forget=7.1214 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 10 total=13.6441 mle=1.5906 pcon=4.9938 forget=7.0597 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=13.6174 mle=1.6362 pcon=4.9929 forget=6.9883 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=13.5534 mle=1.5641 pcon=4.9922 forget=6.9971 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=13.7107 mle=1.6592 pcon=4.9909 forget=7.0606 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=13.8063 mle=1.7758 pcon=4.9901 forget=7.0405 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=13.6742 mle=1.6107 pcon=4.9895 forget=7.0740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=14.0401 mle=1.9858 pcon=4.9883 forget=7.0660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=13.5733 mle=1.6494 pcon=4.9872 forget=6.9367 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 20 total=13.6299 mle=1.6167 pcon=4.9859 forget=7.0273 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=13.6640 mle=1.6869 pcon=4.9846 forget=6.9925 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=13.5748 mle=1.5463 pcon=4.9833 forget=7.0452 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=13.6352 mle=1.7388 pcon=4.9821 forget=6.9142 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=13.4651 mle=1.5825 pcon=4.9806 forget=6.9021 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=13.4660 mle=1.5256 pcon=4.9795 forget=6.9609 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=13.3042 mle=1.5050 pcon=4.9781 forget=6.8211 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=13.6901 mle=1.8450 pcon=4.9765 forget=6.8686 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 30 total=13.1571 mle=1.4232 pcon=4.9751 forget=6.7589 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=13.3255 mle=1.4851 pcon=4.9734 forget=6.8670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=13.2221 mle=1.5542 pcon=4.9715 forget=6.6963 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=13.0854 mle=1.5436 pcon=4.9700 forget=6.5717 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=13.2462 mle=1.6671 pcon=4.9683 forget=6.6108 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=12.9891 mle=1.5742 pcon=4.9664 forget=6.4485 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=13.1276 mle=1.5815 pcon=4.9645 forget=6.5816 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=12.9529 mle=1.5682 pcon=4.9626 forget=6.4221 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=13.1886 mle=1.7699 pcon=4.9605 forget=6.4582 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=13.2426 mle=1.7860 pcon=4.9589 forget=6.4977 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=13.0839 mle=1.6015 pcon=4.9575 forget=6.5248 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=13.3823 mle=1.8579 pcon=4.9556 forget=6.5688 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=13.0852 mle=1.6254 pcon=4.9542 forget=6.5055 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=13.2320 mle=1.6879 pcon=4.9527 forget=6.5913 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=13.2826 mle=1.6723 pcon=4.9513 forget=6.6590 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=13.3565 mle=1.6782 pcon=4.9501 forget=6.7281 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=13.2028 mle=1.4978 pcon=4.9489 forget=6.7561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=13.3589 mle=1.5744 pcon=4.9479 forget=6.8367 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=13.6266 mle=1.6969 pcon=4.9469 forget=6.9827 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=13.4664 mle=1.5796 pcon=4.9462 forget=6.9406 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=13.8696 mle=1.9174 pcon=4.9455 forget=7.0068 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=13.5971 mle=1.5650 pcon=4.9451 forget=7.0871 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=13.6603 mle=1.5695 pcon=4.9446 forget=7.1461 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 10 total=13.7712 mle=1.5820 pcon=4.9442 forget=7.2450 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=13.6539 mle=1.5001 pcon=4.9440 forget=7.2098 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=13.7343 mle=1.5279 pcon=4.9440 forget=7.2624 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=13.9883 mle=1.7077 pcon=4.9437 forget=7.3369 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=13.8872 mle=1.5556 pcon=4.9434 forget=7.3882 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=14.2524 mle=1.9228 pcon=4.9432 forget=7.3864 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=14.2314 mle=1.8180 pcon=4.9431 forget=7.4703 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=14.0002 mle=1.5964 pcon=4.9431 forget=7.4607 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 20 total=14.2105 mle=1.7418 pcon=4.9428 forget=7.5259 nr=64 nf=64 protos=540 fproto_sim=NA
 90%|█████████ | 18/20 [06:20<00:41, 20.70s/it] 95%|█████████▌| 19/20 [06:38<00:19, 19.81s/it]100%|██████████| 20/20 [06:59<00:00, 20.26s/it]100%|██████████| 20/20 [06:59<00:00, 20.98s/it]
[loss] ep 17 it 70 total=14.2452 mle=1.7919 pcon=4.9432 forget=7.5101 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=14.0863 mle=1.5775 pcon=4.9430 forget=7.5658 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=14.1504 mle=1.6274 pcon=4.9427 forget=7.5803 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=14.2380 mle=1.6754 pcon=4.9426 forget=7.6201 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=14.0271 mle=1.4660 pcon=4.9430 forget=7.6180 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=14.3942 mle=1.8155 pcon=4.9430 forget=7.6356 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=14.2099 mle=1.7215 pcon=4.9427 forget=7.5456 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=14.1504 mle=1.5951 pcon=4.9431 forget=7.6121 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=14.3539 mle=1.6834 pcon=4.9431 forget=7.7273 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=14.1857 mle=1.6428 pcon=4.9430 forget=7.5998 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=14.1599 mle=1.5454 pcon=4.9430 forget=7.6714 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=14.1561 mle=1.6062 pcon=4.9430 forget=7.6068 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=14.2507 mle=1.6672 pcon=4.9430 forget=7.6405 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=14.3835 mle=1.7707 pcon=4.9430 forget=7.6697 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=14.3387 mle=1.6666 pcon=4.9432 forget=7.7288 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 40 total=14.1054 mle=1.4570 pcon=4.9434 forget=7.7050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=14.4354 mle=1.6869 pcon=4.9438 forget=7.8047 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=14.2798 mle=1.5585 pcon=4.9437 forget=7.7776 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=14.2548 mle=1.6518 pcon=4.9435 forget=7.6595 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=14.6201 mle=1.8880 pcon=4.9435 forget=7.7887 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=14.4636 mle=1.5637 pcon=4.9437 forget=7.9562 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=14.4616 mle=1.6035 pcon=4.9439 forget=7.9142 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:12,  2.93it/s]  2%|▏         | 8/391 [00:00<00:17, 22.03it/s]  4%|▍         | 17/391 [00:00<00:09, 41.54it/s]  7%|▋         | 26/391 [00:00<00:06, 55.20it/s]  9%|▉         | 35/391 [00:00<00:05, 65.07it/s] 11%|█▏        | 44/391 [00:00<00:04, 71.78it/s] 14%|█▎        | 53/391 [00:01<00:05, 61.65it/s] 16%|█▌        | 61/391 [00:01<00:05, 60.77it/s] 17%|█▋        | 68/391 [00:01<00:05, 62.84it/s] 20%|█▉        | 77/391 [00:01<00:04, 69.19it/s] 22%|██▏       | 86/391 [00:01<00:04, 74.20it/s] 24%|██▍       | 95/391 [00:01<00:03, 77.93it/s] 27%|██▋       | 104/391 [00:01<00:03, 79.14it/s] 29%|██▉       | 113/391 [00:01<00:03, 71.13it/s] 31%|███       | 121/391 [00:02<00:04, 65.81it/s] 33%|███▎      | 128/391 [00:02<00:04, 62.98it/s] 35%|███▌      | 137/391 [00:02<00:03, 68.43it/s] 37%|███▋      | 146/391 [00:02<00:03, 72.94it/s] 40%|███▉      | 155/391 [00:02<00:03, 76.31it/s] 42%|████▏     | 163/391 [00:02<00:03, 72.89it/s] 44%|████▎     | 171/391 [00:02<00:03, 63.79it/s] 46%|████▌     | 178/391 [00:02<00:03, 61.18it/s] 48%|████▊     | 187/391 [00:02<00:03, 67.84it/s] 50%|█████     | 196/391 [00:03<00:02, 72.12it/s] 52%|█████▏    | 205/391 [00:03<00:02, 75.47it/s] 55%|█████▍    | 214/391 [00:03<00:02, 77.70it/s] 57%|█████▋    | 222/391 [00:03<00:02, 65.15it/s] 59%|█████▊    | 229/391 [00:03<00:02, 61.99it/s] 60%|██████    | 236/391 [00:03<00:02, 63.81it/s] 63%|██████▎   | 245/391 [00:03<00:02, 69.76it/s] 65%|██████▍   | 254/391 [00:03<00:01, 73.38it/s] 67%|██████▋   | 263/391 [00:04<00:01, 76.87it/s] 69%|██████▉   | 271/391 [00:04<00:01, 64.96it/s] 71%|███████   | 278/391 [00:04<00:01, 60.47it/s] 73%|███████▎  | 286/391 [00:04<00:01, 64.31it/s] 75%|███████▌  | 295/391 [00:04<00:01, 70.16it/s] 78%|███████▊  | 304/391 [00:04<00:01, 74.23it/s] 80%|████████  | 313/391 [00:04<00:01, 77.13it/s] 82%|████████▏ | 321/391 [00:04<00:00, 77.23it/s] 84%|████████▍ | 329/391 [00:05<00:00, 64.84it/s] 86%|████████▌ | 336/391 [00:05<00:00, 60.77it/s] 88%|████████▊ | 344/391 [00:05<00:00, 65.18it/s] 90%|█████████ | 353/391 [00:05<00:00, 70.91it/s] 93%|█████████▎| 362/391 [00:05<00:00, 74.98it/s] 95%|█████████▍| 371/391 [00:05<00:00, 77.97it/s] 97%|█████████▋| 379/391 [00:05<00:00, 76.08it/s] 99%|█████████▉| 387/391 [00:05<00:00, 64.19it/s]100%|██████████| 391/391 [00:05<00:00, 66.01it/s]
50000 images processed, 6.0415637493133545 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:33,  2.32it/s] 13%|█▎        | 10/79 [00:00<00:02, 23.77it/s] 24%|██▍       | 19/79 [00:00<00:01, 40.51it/s] 35%|███▌      | 28/79 [00:00<00:00, 53.23it/s] 46%|████▌     | 36/79 [00:00<00:00, 50.68it/s] 54%|█████▍    | 43/79 [00:01<00:00, 51.83it/s] 63%|██████▎   | 50/79 [00:01<00:00, 55.64it/s] 75%|███████▍  | 59/79 [00:01<00:00, 63.89it/s] 86%|████████▌ | 68/79 [00:01<00:00, 70.34it/s] 97%|█████████▋| 77/79 [00:01<00:00, 75.51it/s]100%|██████████| 79/79 [00:01<00:00, 53.02it/s]
10000 images processed, 1.5231842994689941 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:55,  1.75it/s]  4%|▍         | 9/204 [00:00<00:11, 17.41it/s]  9%|▉         | 18/204 [00:00<00:05, 33.15it/s] 13%|█▎        | 27/204 [00:00<00:03, 46.34it/s] 18%|█▊        | 36/204 [00:00<00:02, 56.13it/s] 22%|██▏       | 44/204 [00:01<00:03, 49.99it/s] 25%|██▌       | 51/204 [00:01<00:02, 51.18it/s] 29%|██▉       | 60/204 [00:01<00:02, 58.82it/s] 34%|███▍      | 69/204 [00:01<00:02, 64.79it/s] 38%|███▊      | 77/204 [00:01<00:02, 62.57it/s] 41%|████      | 84/204 [00:01<00:02, 58.22it/s] 45%|████▍     | 91/204 [00:01<00:01, 56.65it/s] 49%|████▉     | 100/204 [00:02<00:01, 64.05it/s] 53%|█████▎    | 109/204 [00:02<00:01, 68.99it/s] 58%|█████▊    | 118/204 [00:02<00:01, 73.69it/s] 62%|██████▏   | 127/204 [00:02<00:00, 77.29it/s] 66%|██████▌   | 135/204 [00:02<00:01, 67.16it/s] 70%|███████   | 143/204 [00:02<00:01, 60.87it/s] 74%|███████▍  | 151/204 [00:02<00:00, 64.50it/s] 78%|███████▊  | 160/204 [00:02<00:00, 69.61it/s] 83%|████████▎ | 169/204 [00:03<00:00, 73.80it/s] 87%|████████▋ | 178/204 [00:03<00:00, 77.37it/s] 92%|█████████▏| 187/204 [00:03<00:00, 79.09it/s] 96%|█████████▌| 196/204 [00:03<00:00, 62.86it/s]100%|█████████▉| 203/204 [00:03<00:00, 60.56it/s]100%|██████████| 204/204 [00:03<00:00, 57.08it/s]
26032 images processed, 3.613478422164917 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:44,  1.74it/s]  9%|▉         | 7/79 [00:00<00:05, 13.09it/s] 14%|█▍        | 11/79 [00:00<00:03, 18.92it/s] 22%|██▏       | 17/79 [00:00<00:02, 26.27it/s] 28%|██▊       | 22/79 [00:01<00:01, 31.85it/s] 34%|███▍      | 27/79 [00:01<00:01, 36.24it/s] 44%|████▍     | 35/79 [00:01<00:00, 46.68it/s] 52%|█████▏    | 41/79 [00:01<00:00, 45.26it/s] 59%|█████▉    | 47/79 [00:01<00:00, 44.66it/s] 67%|██████▋   | 53/79 [00:01<00:00, 48.25it/s] 77%|███████▋  | 61/79 [00:01<00:00, 55.86it/s] 85%|████████▍ | 67/79 [00:01<00:00, 48.73it/s] 92%|█████████▏| 73/79 [00:02<00:00, 48.13it/s]100%|██████████| 79/79 [00:02<00:00, 49.87it/s]100%|██████████| 79/79 [00:02<00:00, 37.20it/s]
10000 images processed, 2.1554598808288574 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:22,  3.48it/s] 11%|█▏        | 9/79 [00:00<00:02, 28.65it/s] 23%|██▎       | 18/79 [00:00<00:01, 47.95it/s] 34%|███▍      | 27/79 [00:00<00:00, 60.52it/s] 46%|████▌     | 36/79 [00:00<00:00, 68.86it/s] 56%|█████▌    | 44/79 [00:00<00:00, 68.30it/s] 66%|██████▌   | 52/79 [00:00<00:00, 59.04it/s] 75%|███████▍  | 59/79 [00:01<00:00, 56.60it/s] 86%|████████▌ | 68/79 [00:01<00:00, 64.63it/s] 97%|█████████▋| 77/79 [00:01<00:00, 71.02it/s]100%|██████████| 79/79 [00:01<00:00, 58.68it/s]
10000 images processed, 1.3677730560302734 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:23,  2.96it/s] 14%|█▍        | 10/70 [00:00<00:02, 28.56it/s] 27%|██▋       | 19/70 [00:00<00:01, 46.34it/s] 40%|████      | 28/70 [00:00<00:00, 58.68it/s] 53%|█████▎    | 37/70 [00:00<00:00, 66.93it/s] 64%|██████▍   | 45/70 [00:00<00:00, 56.36it/s] 74%|███████▍  | 52/70 [00:01<00:00, 54.69it/s] 87%|████████▋ | 61/70 [00:01<00:00, 62.91it/s]100%|██████████| 70/70 [00:01<00:00, 67.95it/s]100%|██████████| 70/70 [00:01<00:00, 54.21it/s]
8925 images processed, 1.3303501605987549 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:42,  1.02it/s]  4%|▍         | 2/45 [00:01<00:20,  2.12it/s] 20%|██        | 9/45 [00:01<00:03,  9.58it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.43it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.48it/s] 42%|████▏     | 19/45 [00:02<00:01, 13.07it/s] 53%|█████▎    | 24/45 [00:02<00:01, 16.49it/s] 58%|█████▊    | 26/45 [00:02<00:01, 12.63it/s] 71%|███████   | 32/45 [00:02<00:00, 16.49it/s] 76%|███████▌  | 34/45 [00:03<00:01, 11.00it/s] 89%|████████▉ | 40/45 [00:03<00:00, 14.82it/s] 93%|█████████▎| 42/45 [00:03<00:00, 10.24it/s]100%|██████████| 45/45 [00:04<00:00, 11.19it/s]
5640 images processed, 4.04181170463562 seconds used

21.79651165008545
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.51  99.36
places365     67.79  81.19
LSUN          17.12  96.14
iSUN          71.69  81.88
dtd           37.87  91.35
forget        78.90  86.96
AVG           45.98  89.48
Retain-Acc: 0.7396
Forget-as-OOD (retain known vs forget novel):
  FPR: 78.90 AUROC: 86.96 AUIN: 98.37
24.093549728393555
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=1.0 lr=0.0005 epochs=50
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:30<24:47, 30.36s/it]  4%|▍         | 2/50 [00:49<19:08, 23.94s/it]  6%|▌         | 3/50 [01:08<16:53, 21.57s/it]  8%|▊         | 4/50 [01:32<17:13, 22.48s/it] 10%|█         | 5/50 [01:55<17:07, 22.84s/it] 12%|█▏        | 6/50 [02:15<15:50, 21.61s/it] 14%|█▍        | 7/50 [02:34<14:52, 20.76s/it] 16%|█▌        | 8/50 [03:00<15:42, 22.44s/it][loss] ep 0 it 0 total=13.7435 mle=1.5710 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8368 mle=1.5424 pcon=5.2879 forget=7.0065 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8514 mle=1.7005 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0218 mle=1.8998 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8930 mle=1.7130 pcon=5.2670 forget=6.9130 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6544 mle=1.5020 pcon=5.2603 forget=6.8921 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7233 mle=1.5596 pcon=5.2540 forget=6.9096 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8766 mle=1.6808 pcon=5.2476 forget=6.9481 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=13.9625 mle=1.6715 pcon=5.2409 forget=7.0501 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7065 mle=1.6372 pcon=5.2346 forget=6.8348 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6923 mle=1.5168 pcon=5.2285 forget=6.9470 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9491 mle=1.7802 pcon=5.2224 forget=6.9465 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9071 mle=1.7644 pcon=5.2167 forget=6.9260 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6575 mle=1.6053 pcon=5.2112 forget=6.8410 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8194 mle=1.7237 pcon=5.2056 forget=6.8901 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9247 mle=1.7943 pcon=5.2003 forget=6.9301 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=13.6311 mle=1.5530 pcon=5.1950 forget=6.8831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8652 mle=1.8884 pcon=5.1899 forget=6.7869 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8050 mle=1.7419 pcon=5.1847 forget=6.8785 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5291 mle=1.5263 pcon=5.1796 forget=6.8232 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6880 mle=1.5853 pcon=5.1745 forget=6.9281 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9050 mle=1.8836 pcon=5.1699 forget=6.8515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5437 mle=1.5923 pcon=5.1652 forget=6.7862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7932 mle=1.8113 pcon=5.1605 forget=6.8215 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=13.7127 mle=1.7057 pcon=5.1559 forget=6.8511 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7170 mle=1.6710 pcon=5.1519 forget=6.8941 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7410 mle=1.7668 pcon=5.1475 forget=6.8267 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8745 mle=1.9230 pcon=5.1435 forget=6.8079 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.5058 mle=1.5699 pcon=5.1395 forget=6.7965 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7494 mle=1.8359 pcon=5.1353 forget=6.7782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.7074 mle=1.7032 pcon=5.1311 forget=6.8731 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.7091 mle=1.6958 pcon=5.1275 forget=6.8857 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=13.4839 mle=1.6535 pcon=5.1236 forget=6.7067 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6257 mle=1.7294 pcon=5.1196 forget=6.7766 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5917 mle=1.6511 pcon=5.1159 forget=6.8247 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3891 mle=1.4536 pcon=5.1125 forget=6.8231 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6493 mle=1.7671 pcon=5.1086 forget=6.7737 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3557 mle=1.4911 pcon=5.1050 forget=6.7596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7882 mle=1.8722 pcon=5.1015 forget=6.8145 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=13.7431 mle=1.8671 pcon=5.0982 forget=6.7778 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=13.6147 mle=1.8005 pcon=5.0946 forget=6.7196 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=13.4661 mle=1.5853 pcon=5.0914 forget=6.7895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=13.4641 mle=1.5945 pcon=5.0879 forget=6.7817 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=13.4909 mle=1.6917 pcon=5.0849 forget=6.7142 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=13.5356 mle=1.7746 pcon=5.0819 forget=6.6791 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=13.4045 mle=1.5973 pcon=5.0788 forget=6.7285 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=13.5452 mle=1.7451 pcon=5.0758 forget=6.7243 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=13.3085 mle=1.3998 pcon=5.0729 forget=6.8358 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=13.2169 mle=1.5323 pcon=5.0700 forget=6.6145 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=13.4180 mle=1.5904 pcon=5.0673 forget=6.7604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=13.4137 mle=1.6674 pcon=5.0644 forget=6.6819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=13.3603 mle=1.6305 pcon=5.0620 forget=6.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=13.5974 mle=1.8164 pcon=5.0589 forget=6.7221 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=13.3616 mle=1.5452 pcon=5.0564 forget=6.7601 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=13.3960 mle=1.6731 pcon=5.0537 forget=6.6693 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=13.3042 mle=1.6300 pcon=5.0508 forget=6.6234 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=13.3902 mle=1.7454 pcon=5.0481 forget=6.5967 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=13.2668 mle=1.6110 pcon=5.0453 forget=6.6106 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=13.1941 mle=1.5654 pcon=5.0426 forget=6.5861 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=13.3478 mle=1.6450 pcon=5.0402 forget=6.6627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=13.5277 mle=1.7902 pcon=5.0377 forget=6.6999 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=13.4458 mle=1.7321 pcon=5.0348 forget=6.6789 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=13.2733 mle=1.6253 pcon=5.0324 forget=6.6156 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 18%|█▊        | 9/50 [03:22<15:12, 22.27s/it] 20%|██        | 10/50 [03:41<14:14, 21.37s/it] 22%|██▏       | 11/50 [04:01<13:42, 21.10s/it] 24%|██▍       | 12/50 [04:28<14:29, 22.89s/it] 26%|██▌       | 13/50 [04:47<13:23, 21.70s/it] 28%|██▊       | 14/50 [05:06<12:30, 20.84s/it] 30%|███       | 15/50 [05:29<12:28, 21.39s/it] 32%|███▏      | 16/50 [05:54<12:40, 22.37s/it] 34%|███▍      | 17/50 [06:13<11:45, 21.38s/it][loss] ep 8 it 30 total=13.2571 mle=1.5431 pcon=5.0301 forget=6.6839 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=13.2588 mle=1.5773 pcon=5.0277 forget=6.6538 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=13.1649 mle=1.5041 pcon=5.0252 forget=6.6356 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=13.3487 mle=1.7124 pcon=5.0229 forget=6.6135 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=13.1491 mle=1.5289 pcon=5.0210 forget=6.5992 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=13.2949 mle=1.6904 pcon=5.0187 forget=6.5858 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=13.4826 mle=1.7408 pcon=5.0164 forget=6.7253 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=13.1434 mle=1.4823 pcon=5.0143 forget=6.6467 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=13.2488 mle=1.6330 pcon=5.0123 forget=6.6035 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=13.3711 mle=1.6721 pcon=5.0101 forget=6.6889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=13.6086 mle=1.8198 pcon=5.0086 forget=6.7802 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=13.2771 mle=1.5802 pcon=5.0071 forget=6.6898 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=13.3713 mle=1.5880 pcon=5.0057 forget=6.7776 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=13.3707 mle=1.5609 pcon=5.0040 forget=6.8058 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=13.3539 mle=1.5535 pcon=5.0028 forget=6.7976 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 0 total=13.4740 mle=1.6136 pcon=5.0014 forget=6.8590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=13.5130 mle=1.5982 pcon=5.0000 forget=6.9147 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=13.6374 mle=1.7387 pcon=4.9990 forget=6.8998 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=13.7344 mle=1.8519 pcon=4.9980 forget=6.8845 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=13.7859 mle=1.8493 pcon=4.9970 forget=6.9396 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=13.5424 mle=1.5785 pcon=4.9962 forget=6.9676 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=13.5366 mle=1.5645 pcon=4.9956 forget=6.9766 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=13.7727 mle=1.6569 pcon=4.9947 forget=7.1210 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 10 total=13.6435 mle=1.5906 pcon=4.9938 forget=7.0591 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=13.6162 mle=1.6359 pcon=4.9929 forget=6.9874 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=13.5519 mle=1.5642 pcon=4.9922 forget=6.9955 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=13.7082 mle=1.6592 pcon=4.9909 forget=7.0582 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=13.8029 mle=1.7757 pcon=4.9901 forget=7.0371 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=13.6682 mle=1.6102 pcon=4.9895 forget=7.0685 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=14.0346 mle=1.9876 pcon=4.9883 forget=7.0587 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=13.5632 mle=1.6496 pcon=4.9871 forget=6.9264 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 20 total=13.6175 mle=1.6166 pcon=4.9858 forget=7.0151 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=13.6466 mle=1.6858 pcon=4.9845 forget=6.9764 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=13.5529 mle=1.5455 pcon=4.9831 forget=7.0243 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=13.6109 mle=1.7390 pcon=4.9818 forget=6.8900 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=13.4325 mle=1.5825 pcon=4.9803 forget=6.8697 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=13.4238 mle=1.5251 pcon=4.9791 forget=6.9195 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=13.2413 mle=1.5042 pcon=4.9776 forget=6.7595 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=13.6107 mle=1.8437 pcon=4.9759 forget=6.7910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 30 total=13.0468 mle=1.4226 pcon=4.9743 forget=6.6499 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=13.1847 mle=1.4854 pcon=4.9725 forget=6.7268 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=13.0660 mle=1.5515 pcon=4.9704 forget=6.5441 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=12.9279 mle=1.5445 pcon=4.9686 forget=6.4149 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=13.0995 mle=1.6738 pcon=4.9666 forget=6.4591 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=12.8588 mle=1.5789 pcon=4.9645 forget=6.3154 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=13.0354 mle=1.5839 pcon=4.9624 forget=6.4891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=12.9115 mle=1.5652 pcon=4.9605 forget=6.3858 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=13.1941 mle=1.7803 pcon=4.9583 forget=6.4555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=13.2739 mle=1.8034 pcon=4.9568 forget=6.5137 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=13.1803 mle=1.6308 pcon=4.9555 forget=6.5939 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=13.4828 mle=1.8478 pcon=4.9539 forget=6.6811 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=13.2741 mle=1.6330 pcon=4.9529 forget=6.6883 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=13.4362 mle=1.6935 pcon=4.9517 forget=6.7910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=13.5003 mle=1.6796 pcon=4.9508 forget=6.8699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 0 total=13.6084 mle=1.6897 pcon=4.9502 forget=6.9685 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=13.5154 mle=1.5206 pcon=4.9495 forget=7.0453 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=13.6572 mle=1.5953 pcon=4.9492 forget=7.1127 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=13.8581 mle=1.6977 pcon=4.9490 forget=7.2114 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=13.7329 mle=1.5756 pcon=4.9489 forget=7.2085 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=14.1684 mle=1.9275 pcon=4.9488 forget=7.2920 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=13.8703 mle=1.5669 pcon=4.9491 forget=7.3543 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=13.9299 mle=1.5775 pcon=4.9492 forget=7.4032 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 10 total=13.8361 mle=1.5752 pcon=4.9493 forget=7.3116 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=13.7332 mle=1.4956 pcon=4.9496 forget=7.2881 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=13.7045 mle=1.5322 pcon=4.9498 forget=7.2225 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=13.8453 mle=1.7013 pcon=4.9496 forget=7.1944 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=13.5709 mle=1.5568 pcon=4.9493 forget=7.0648 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=13.9245 mle=1.9219 pcon=4.9487 forget=7.0539 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=13.6132 mle=1.8002 pcon=4.9482 forget=6.8648 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=13.3302 mle=1.5737 pcon=4.9474 forget=6.8092 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 20 total=13.4372 mle=1.7471 pcon=4.9461 forget=6.7440 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=13.3170 mle=1.8077 pcon=4.9454 forget=6.5638 nr=64 nf=64 protos=540 fproto_sim=NA
 36%|███▌      | 18/50 [06:31<10:59, 20.62s/it] 38%|███▊      | 19/50 [06:59<11:40, 22.59s/it] 40%|████      | 20/50 [07:20<11:06, 22.21s/it] 42%|████▏     | 21/50 [07:40<10:22, 21.48s/it] 44%|████▍     | 22/50 [08:01<10:01, 21.49s/it] 46%|████▌     | 23/50 [08:28<10:25, 23.18s/it] 48%|████▊     | 24/50 [08:47<09:28, 21.86s/it] 50%|█████     | 25/50 [09:06<08:46, 21.06s/it] 52%|█████▏    | 26/50 [09:32<08:56, 22.33s/it][loss] ep 17 it 120 total=13.0964 mle=1.5709 pcon=4.9440 forget=6.5815 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=13.1052 mle=1.6247 pcon=4.9423 forget=6.5382 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=13.1566 mle=1.6631 pcon=4.9407 forget=6.5528 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=12.8258 mle=1.4598 pcon=4.9396 forget=6.4263 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=13.1486 mle=1.8344 pcon=4.9380 forget=6.3761 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=13.0791 mle=1.7623 pcon=4.9362 forget=6.3805 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=12.8587 mle=1.6031 pcon=4.9350 forget=6.3207 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=13.0867 mle=1.6943 pcon=4.9334 forget=6.4590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=12.9947 mle=1.6579 pcon=4.9317 forget=6.4051 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=12.7970 mle=1.5481 pcon=4.9301 forget=6.3188 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=12.8684 mle=1.5921 pcon=4.9286 forget=6.3477 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=12.9664 mle=1.6658 pcon=4.9271 forget=6.3734 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=13.0895 mle=1.8112 pcon=4.9257 forget=6.3526 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=13.0570 mle=1.7114 pcon=4.9245 forget=6.4211 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 19 it 40 total=12.8791 mle=1.4764 pcon=4.9233 forget=6.4793 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=13.1517 mle=1.7339 pcon=4.9223 forget=6.4955 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=12.9460 mle=1.5887 pcon=4.9209 forget=6.4364 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=13.0267 mle=1.6622 pcon=4.9195 forget=6.4450 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=13.2413 mle=1.8916 pcon=4.9182 forget=6.4314 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=13.0117 mle=1.5877 pcon=4.9172 forget=6.5068 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=12.9938 mle=1.6187 pcon=4.9161 forget=6.4589 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 0 total=12.9824 mle=1.6692 pcon=4.9148 forget=6.3985 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=12.9857 mle=1.6993 pcon=4.9135 forget=6.3729 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=13.0980 mle=1.7138 pcon=4.9124 forget=6.4718 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=13.1293 mle=1.8490 pcon=4.9109 forget=6.3694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=12.9363 mle=1.6375 pcon=4.9093 forget=6.3895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=13.3157 mle=1.9882 pcon=4.9077 forget=6.4198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=13.0716 mle=1.7475 pcon=4.9062 forget=6.4180 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=12.7937 mle=1.4472 pcon=4.9049 forget=6.4416 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 10 total=12.7644 mle=1.4411 pcon=4.9035 forget=6.4198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=13.0225 mle=1.7085 pcon=4.9024 forget=6.4117 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=12.9682 mle=1.5425 pcon=4.9013 forget=6.5245 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=12.9970 mle=1.6199 pcon=4.9004 forget=6.4767 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=13.2854 mle=1.7900 pcon=4.8994 forget=6.5961 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=13.1752 mle=1.6760 pcon=4.8984 forget=6.6008 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=13.1781 mle=1.6649 pcon=4.8972 forget=6.6160 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=13.2068 mle=1.6221 pcon=4.8963 forget=6.6884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 20 total=13.3021 mle=1.7282 pcon=4.8953 forget=6.6786 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=13.3879 mle=1.7922 pcon=4.8943 forget=6.7014 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=13.2305 mle=1.5521 pcon=4.8933 forget=6.7851 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=13.3627 mle=1.6466 pcon=4.8925 forget=6.8236 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=13.3588 mle=1.6737 pcon=4.8919 forget=6.7932 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=13.2444 mle=1.5134 pcon=4.8911 forget=6.8399 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=13.3991 mle=1.6823 pcon=4.8905 forget=6.8264 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=13.8591 mle=2.0606 pcon=4.8895 forget=6.9089 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 30 total=13.3274 mle=1.5294 pcon=4.8892 forget=6.9088 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=13.5603 mle=1.7533 pcon=4.8882 forget=6.9188 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=13.3694 mle=1.5565 pcon=4.8871 forget=6.9258 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=13.5440 mle=1.7535 pcon=4.8860 forget=6.9045 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=13.4794 mle=1.6755 pcon=4.8849 forget=6.9189 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=13.4739 mle=1.5785 pcon=4.8835 forget=7.0119 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=13.5031 mle=1.6484 pcon=4.8821 forget=6.9726 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=13.3347 mle=1.5214 pcon=4.8806 forget=6.9327 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 40 total=13.3898 mle=1.5397 pcon=4.8792 forget=6.9710 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=13.3285 mle=1.6353 pcon=4.8775 forget=6.8157 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=13.2600 mle=1.5528 pcon=4.8756 forget=6.8315 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=13.3362 mle=1.6589 pcon=4.8738 forget=6.8035 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 240 total=13.1809 mle=1.5669 pcon=4.8718 forget=6.7423 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=13.4010 mle=1.8270 pcon=4.8696 forget=6.7043 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=13.2154 mle=1.6777 pcon=4.8673 forget=6.6704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 0 total=13.2162 mle=1.6191 pcon=4.8647 forget=6.7324 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=13.2874 mle=1.7961 pcon=4.8625 forget=6.6289 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=13.1635 mle=1.6404 pcon=4.8599 forget=6.6632 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 150 total=13.2462 mle=1.8364 pcon=4.8577 forget=6.5520 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 200 total=13.3120 mle=1.8171 pcon=4.8550 forget=6.6399 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=13.2745 mle=1.7439 pcon=4.8523 forget=6.6783 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=13.0036 mle=1.6341 pcon=4.8497 forget=6.5198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=12.9382 mle=1.5858 pcon=4.8474 forget=6.5051 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 10 total=12.9287 mle=1.5595 pcon=4.8448 forget=6.5243 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=12.9554 mle=1.5598 pcon=4.8423 forget=6.5533 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=13.1312 mle=1.8067 pcon=4.8398 forget=6.4847 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=12.8236 mle=1.5472 pcon=4.8372 forget=6.4392 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=13.0064 mle=1.6635 pcon=4.8345 forget=6.5084 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=12.8719 mle=1.5404 pcon=4.8322 forget=6.4993 nr=64 nf=64 protos=540 fproto_sim=NA
 54%|█████▍    | 27/50 [09:54<08:34, 22.36s/it] 56%|█████▌    | 28/50 [10:13<07:49, 21.34s/it] 58%|█████▊    | 29/50 [10:34<07:23, 21.14s/it] 60%|██████    | 30/50 [11:01<07:38, 22.94s/it] 62%|██████▏   | 31/50 [11:20<06:56, 21.92s/it] 64%|██████▍   | 32/50 [11:40<06:20, 21.13s/it] 66%|██████▌   | 33/50 [12:06<06:26, 22.71s/it] 68%|██████▊   | 34/50 [12:29<06:03, 22.71s/it] 70%|███████   | 35/50 [12:47<05:22, 21.49s/it][loss] ep 26 it 310 total=13.2273 mle=1.9268 pcon=4.8299 forget=6.4707 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=12.9021 mle=1.5990 pcon=4.8273 forget=6.4758 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 20 total=12.9998 mle=1.6796 pcon=4.8251 forget=6.4952 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=12.9205 mle=1.5971 pcon=4.8226 forget=6.5008 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=12.9417 mle=1.5842 pcon=4.8203 forget=6.5372 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=12.9071 mle=1.5356 pcon=4.8180 forget=6.5534 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=12.9084 mle=1.5320 pcon=4.8158 forget=6.5606 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=12.9940 mle=1.6405 pcon=4.8135 forget=6.5400 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=13.1435 mle=1.7478 pcon=4.8115 forget=6.5842 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=13.1323 mle=1.7277 pcon=4.8095 forget=6.5951 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 30 total=13.0791 mle=1.7043 pcon=4.8077 forget=6.5671 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=13.1732 mle=1.7413 pcon=4.8059 forget=6.6260 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=13.0170 mle=1.5988 pcon=4.8042 forget=6.6140 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=12.9696 mle=1.5764 pcon=4.8024 forget=6.5908 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=12.8064 mle=1.4125 pcon=4.8003 forget=6.5936 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=13.0231 mle=1.5863 pcon=4.7984 forget=6.6384 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=13.2464 mle=1.8061 pcon=4.7963 forget=6.6440 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=13.1338 mle=1.7217 pcon=4.7944 forget=6.6177 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 40 total=13.1160 mle=1.7575 pcon=4.7924 forget=6.5661 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=12.8689 mle=1.5733 pcon=4.7904 forget=6.5052 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=13.0017 mle=1.6447 pcon=4.7887 forget=6.5683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=13.0474 mle=1.6524 pcon=4.7869 forget=6.6081 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=12.9211 mle=1.5643 pcon=4.7853 forget=6.5716 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=13.2077 mle=1.8381 pcon=4.7837 forget=6.5858 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=12.9187 mle=1.5837 pcon=4.7821 forget=6.5529 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 0 total=13.0264 mle=1.6881 pcon=4.7805 forget=6.5579 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=13.0912 mle=1.7041 pcon=4.7790 forget=6.6081 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=12.9432 mle=1.5435 pcon=4.7774 forget=6.6223 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=13.0777 mle=1.7864 pcon=4.7757 forget=6.5157 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=12.9181 mle=1.5082 pcon=4.7741 forget=6.6357 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=13.1760 mle=1.8000 pcon=4.7725 forget=6.6036 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=12.8493 mle=1.5203 pcon=4.7710 forget=6.5581 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=13.1703 mle=1.8312 pcon=4.7696 forget=6.5694 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 31 it 10 total=12.9002 mle=1.6439 pcon=4.7682 forget=6.4881 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=12.8489 mle=1.5629 pcon=4.7669 forget=6.5191 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=12.9035 mle=1.6139 pcon=4.7655 forget=6.5242 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=12.8387 mle=1.5731 pcon=4.7640 forget=6.5015 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=12.8091 mle=1.5998 pcon=4.7626 forget=6.4467 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 260 total=12.7895 mle=1.5108 pcon=4.7611 forget=6.5177 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=12.8864 mle=1.6038 pcon=4.7597 forget=6.5229 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=12.7927 mle=1.5574 pcon=4.7582 forget=6.4771 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 32 it 20 total=12.9515 mle=1.7584 pcon=4.7568 forget=6.4363 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=12.7904 mle=1.5917 pcon=4.7553 forget=6.4434 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=12.8153 mle=1.5547 pcon=4.7540 forget=6.5066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 170 total=12.8052 mle=1.5595 pcon=4.7529 forget=6.4929 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=12.9176 mle=1.6889 pcon=4.7517 forget=6.4770 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=12.7977 mle=1.5776 pcon=4.7505 forget=6.4696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=12.6714 mle=1.5168 pcon=4.7493 forget=6.4054 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=12.6390 mle=1.4670 pcon=4.7483 forget=6.4237 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 33 it 30 total=12.8558 mle=1.6444 pcon=4.7472 forget=6.4642 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=13.1136 mle=1.9478 pcon=4.7461 forget=6.4197 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=12.6607 mle=1.4785 pcon=4.7450 forget=6.4371 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=12.9971 mle=1.7876 pcon=4.7439 forget=6.4656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=12.8152 mle=1.6186 pcon=4.7429 forget=6.4537 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=12.8654 mle=1.6563 pcon=4.7419 forget=6.4672 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=12.8547 mle=1.6672 pcon=4.7410 forget=6.4466 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 380 total=12.8226 mle=1.5927 pcon=4.7400 forget=6.4899 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 34 it 40 total=12.8844 mle=1.6942 pcon=4.7387 forget=6.4515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=12.6731 mle=1.4801 pcon=4.7376 forget=6.4554 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 140 total=12.8028 mle=1.6377 pcon=4.7367 forget=6.4284 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=12.8875 mle=1.6729 pcon=4.7358 forget=6.4789 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 240 total=12.9509 mle=1.7282 pcon=4.7347 forget=6.4879 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=12.8487 mle=1.6199 pcon=4.7337 forget=6.4951 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=12.8736 mle=1.6178 pcon=4.7328 forget=6.5231 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 0 total=12.8837 mle=1.6374 pcon=4.7318 forget=6.5145 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 50 total=12.9061 mle=1.6092 pcon=4.7310 forget=6.5659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 100 total=13.1398 mle=1.8287 pcon=4.7300 forget=6.5811 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 150 total=12.9035 mle=1.6303 pcon=4.7292 forget=6.5441 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 200 total=12.8542 mle=1.5775 pcon=4.7284 forget=6.5483 nr=64 nf=64 protos=540 fproto_sim=NA
 72%|███████▏  | 36/50 [13:08<04:56, 21.18s/it] 74%|███████▍  | 37/50 [13:36<05:00, 23.15s/it] 76%|███████▌  | 38/50 [13:55<04:24, 22.00s/it] 78%|███████▊  | 39/50 [14:15<03:54, 21.31s/it] 80%|████████  | 40/50 [14:40<03:44, 22.49s/it] 82%|████████▏ | 41/50 [15:02<03:22, 22.49s/it] 84%|████████▍ | 42/50 [15:21<02:50, 21.37s/it] 86%|████████▌ | 43/50 [15:42<02:28, 21.16s/it] 88%|████████▊ | 44/50 [16:10<02:18, 23.12s/it] 90%|█████████ | 45/50 [16:29<01:49, 21.96s/it][loss] ep 35 it 250 total=13.0221 mle=1.7296 pcon=4.7276 forget=6.5650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 300 total=12.8069 mle=1.5246 pcon=4.7268 forget=6.5555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 350 total=13.0119 mle=1.7401 pcon=4.7260 forget=6.5457 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 10 total=13.0784 mle=1.7630 pcon=4.7253 forget=6.5901 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 60 total=12.8849 mle=1.5734 pcon=4.7248 forget=6.5868 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 110 total=12.9269 mle=1.5874 pcon=4.7240 forget=6.6155 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 160 total=12.9684 mle=1.6322 pcon=4.7233 forget=6.6128 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 210 total=12.8538 mle=1.4959 pcon=4.7227 forget=6.6351 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 260 total=12.9733 mle=1.5727 pcon=4.7223 forget=6.6783 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 310 total=13.1324 mle=1.6787 pcon=4.7215 forget=6.7322 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 360 total=13.0100 mle=1.6316 pcon=4.7208 forget=6.6576 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 20 total=12.9371 mle=1.5201 pcon=4.7201 forget=6.6969 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 70 total=13.1967 mle=1.7271 pcon=4.7195 forget=6.7501 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 120 total=13.0094 mle=1.5643 pcon=4.7189 forget=6.7262 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 170 total=12.9654 mle=1.5641 pcon=4.7182 forget=6.6831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 220 total=13.2085 mle=1.7300 pcon=4.7174 forget=6.7611 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 270 total=13.1217 mle=1.6364 pcon=4.7168 forget=6.7684 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 320 total=13.0818 mle=1.5838 pcon=4.7162 forget=6.7818 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 370 total=13.1820 mle=1.6056 pcon=4.7157 forget=6.8607 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 30 total=13.1855 mle=1.6578 pcon=4.7154 forget=6.8124 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 80 total=13.0285 mle=1.5079 pcon=4.7149 forget=6.8058 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 130 total=13.1568 mle=1.6311 pcon=4.7144 forget=6.8113 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 180 total=13.0879 mle=1.6011 pcon=4.7139 forget=6.7729 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 230 total=13.0662 mle=1.5362 pcon=4.7133 forget=6.8166 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 280 total=13.2001 mle=1.7024 pcon=4.7129 forget=6.7848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 330 total=13.0943 mle=1.5686 pcon=4.7122 forget=6.8134 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 380 total=12.9461 mle=1.4575 pcon=4.7119 forget=6.7767 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 40 total=13.2823 mle=1.7577 pcon=4.7116 forget=6.8130 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 90 total=13.2628 mle=1.6558 pcon=4.7111 forget=6.8959 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 140 total=13.1408 mle=1.6286 pcon=4.7107 forget=6.8016 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 190 total=13.0053 mle=1.4812 pcon=4.7102 forget=6.8139 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 240 total=13.2527 mle=1.6684 pcon=4.7098 forget=6.8745 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 290 total=12.9999 mle=1.4831 pcon=4.7093 forget=6.8074 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 340 total=13.1512 mle=1.5783 pcon=4.7090 forget=6.8640 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 0 total=13.1015 mle=1.5486 pcon=4.7084 forget=6.8445 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 50 total=13.1915 mle=1.6059 pcon=4.7079 forget=6.8776 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 100 total=13.0966 mle=1.5897 pcon=4.7072 forget=6.7997 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 150 total=13.0041 mle=1.4858 pcon=4.7069 forget=6.8115 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 200 total=13.2790 mle=1.7668 pcon=4.7063 forget=6.8059 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 250 total=13.3942 mle=1.7814 pcon=4.7059 forget=6.9069 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 300 total=13.0269 mle=1.5146 pcon=4.7055 forget=6.8067 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 350 total=13.0304 mle=1.4280 pcon=4.7052 forget=6.8973 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 10 total=13.0476 mle=1.5061 pcon=4.7048 forget=6.8367 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 60 total=13.4282 mle=1.8460 pcon=4.7046 forget=6.8776 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 110 total=13.2070 mle=1.5815 pcon=4.7040 forget=6.9215 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 160 total=13.1276 mle=1.5751 pcon=4.7038 forget=6.8488 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 210 total=13.2541 mle=1.6437 pcon=4.7034 forget=6.9070 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 260 total=13.0719 mle=1.5312 pcon=4.7033 forget=6.8374 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 310 total=13.0824 mle=1.5045 pcon=4.7032 forget=6.8747 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 360 total=13.2620 mle=1.6688 pcon=4.7029 forget=6.8902 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 20 total=13.2188 mle=1.6685 pcon=4.7026 forget=6.8477 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 70 total=13.2031 mle=1.6214 pcon=4.7022 forget=6.8796 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 120 total=13.5190 mle=1.9425 pcon=4.7018 forget=6.8747 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 170 total=13.1581 mle=1.6024 pcon=4.7015 forget=6.8542 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 220 total=13.1359 mle=1.6279 pcon=4.7013 forget=6.8068 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 270 total=13.1687 mle=1.6954 pcon=4.7010 forget=6.7724 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 320 total=13.2994 mle=1.7043 pcon=4.7008 forget=6.8943 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 370 total=13.1225 mle=1.6080 pcon=4.7006 forget=6.8139 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 30 total=13.2950 mle=1.6142 pcon=4.7003 forget=6.9805 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 80 total=13.1760 mle=1.5593 pcon=4.7000 forget=6.9166 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 130 total=13.3125 mle=1.6718 pcon=4.6998 forget=6.9409 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 180 total=13.2618 mle=1.5380 pcon=4.6995 forget=7.0242 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 230 total=13.0234 mle=1.5534 pcon=4.6992 forget=6.7708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 280 total=13.1123 mle=1.5523 pcon=4.6990 forget=6.8610 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 330 total=13.1533 mle=1.5347 pcon=4.6986 forget=6.9199 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 380 total=12.9691 mle=1.4799 pcon=4.6983 forget=6.7908 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 40 total=13.3541 mle=1.7037 pcon=4.6981 forget=6.9524 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 90 total=13.1785 mle=1.5548 pcon=4.6978 forget=6.9259 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 140 total=13.3428 mle=1.6364 pcon=4.6976 forget=7.0087 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 190 total=13.1782 mle=1.5292 pcon=4.6975 forget=6.9515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 240 total=13.1770 mle=1.5541 pcon=4.6973 forget=6.9256 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 290 total=13.3462 mle=1.7592 pcon=4.6972 forget=6.8899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 340 total=13.5139 mle=1.8434 pcon=4.6970 forget=6.9735 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 0 total=13.2811 mle=1.5426 pcon=4.6967 forget=7.0417 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 50 total=13.1570 mle=1.5747 pcon=4.6966 forget=6.8857 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 100 total=13.2583 mle=1.5648 pcon=4.6963 forget=6.9972 nr=64 nf=64 protos=540 fproto_sim=NA
 92%|█████████▏| 46/50 [16:48<01:24, 21.07s/it] 94%|█████████▍| 47/50 [17:13<01:06, 22.31s/it] 96%|█████████▌| 48/50 [17:36<00:45, 22.51s/it] 98%|█████████▊| 49/50 [17:55<00:21, 21.40s/it]100%|██████████| 50/50 [18:14<00:00, 20.87s/it]100%|██████████| 50/50 [18:14<00:00, 21.90s/it]
[loss] ep 45 it 150 total=13.0415 mle=1.5174 pcon=4.6961 forget=6.8280 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 200 total=13.2149 mle=1.5856 pcon=4.6960 forget=6.9333 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 250 total=13.1849 mle=1.5350 pcon=4.6958 forget=6.9541 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 300 total=13.2279 mle=1.6713 pcon=4.6954 forget=6.8612 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 350 total=12.9881 mle=1.5021 pcon=4.6953 forget=6.7908 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 10 total=13.1263 mle=1.5154 pcon=4.6952 forget=6.9157 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 60 total=13.2920 mle=1.5179 pcon=4.6950 forget=7.0790 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 110 total=13.4265 mle=1.6538 pcon=4.6949 forget=7.0778 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 160 total=13.0255 mle=1.4587 pcon=4.6947 forget=6.8722 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 210 total=13.2591 mle=1.7165 pcon=4.6945 forget=6.8481 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 260 total=13.1761 mle=1.6753 pcon=4.6943 forget=6.8065 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 310 total=13.3167 mle=1.5516 pcon=4.6943 forget=7.0708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 360 total=13.4547 mle=1.7069 pcon=4.6941 forget=7.0537 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 20 total=13.1475 mle=1.5364 pcon=4.6939 forget=6.9173 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 70 total=13.0247 mle=1.5481 pcon=4.6937 forget=6.7828 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 120 total=13.3294 mle=1.6746 pcon=4.6935 forget=6.9613 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 170 total=13.3081 mle=1.5375 pcon=4.6932 forget=7.0774 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 220 total=12.9368 mle=1.4474 pcon=4.6931 forget=6.7962 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 270 total=13.4186 mle=1.6010 pcon=4.6931 forget=7.1245 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 320 total=13.5014 mle=1.5589 pcon=4.6929 forget=7.2495 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 370 total=13.3795 mle=1.6292 pcon=4.6928 forget=7.0574 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 30 total=13.4625 mle=1.6047 pcon=4.6928 forget=7.1649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 80 total=13.4603 mle=1.6141 pcon=4.6927 forget=7.1535 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 130 total=13.1635 mle=1.6395 pcon=4.6926 forget=6.8314 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 180 total=13.6951 mle=1.6444 pcon=4.6926 forget=7.3581 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 230 total=13.2920 mle=1.6095 pcon=4.6926 forget=6.9899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 280 total=13.0354 mle=1.5671 pcon=4.6925 forget=6.7758 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 330 total=13.0947 mle=1.5794 pcon=4.6924 forget=6.8229 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 380 total=13.4053 mle=1.5430 pcon=4.6922 forget=7.1702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 40 total=13.5208 mle=1.5407 pcon=4.6921 forget=7.2880 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 90 total=13.3829 mle=1.5758 pcon=4.6921 forget=7.1151 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 140 total=13.3498 mle=1.6197 pcon=4.6920 forget=7.0380 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 190 total=13.5352 mle=1.7414 pcon=4.6917 forget=7.1021 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 240 total=13.2123 mle=1.6483 pcon=4.6916 forget=6.8724 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 290 total=13.3290 mle=1.6099 pcon=4.6914 forget=7.0277 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 340 total=13.1752 mle=1.4895 pcon=4.6915 forget=6.9942 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:55,  3.39it/s]  3%|▎         | 10/391 [00:00<00:12, 31.40it/s]  5%|▍         | 19/391 [00:00<00:07, 49.56it/s]  7%|▋         | 28/391 [00:00<00:05, 61.47it/s]  9%|▉         | 36/391 [00:00<00:06, 55.21it/s] 11%|█         | 43/391 [00:00<00:06, 55.06it/s] 13%|█▎        | 50/391 [00:01<00:05, 58.79it/s] 15%|█▌        | 59/391 [00:01<00:04, 66.56it/s] 17%|█▋        | 68/391 [00:01<00:04, 72.49it/s] 20%|█▉        | 77/391 [00:01<00:04, 76.93it/s] 22%|██▏       | 86/391 [00:01<00:03, 80.19it/s] 24%|██▍       | 95/391 [00:01<00:04, 67.32it/s] 26%|██▋       | 103/391 [00:01<00:04, 61.84it/s] 29%|██▊       | 112/391 [00:01<00:04, 67.06it/s] 31%|███       | 121/391 [00:01<00:03, 72.33it/s] 33%|███▎      | 130/391 [00:02<00:03, 75.58it/s] 36%|███▌      | 139/391 [00:02<00:03, 76.94it/s] 38%|███▊      | 147/391 [00:02<00:03, 65.06it/s] 39%|███▉      | 154/391 [00:02<00:03, 61.09it/s] 42%|████▏     | 163/391 [00:02<00:03, 66.93it/s] 44%|████▍     | 172/391 [00:02<00:03, 72.44it/s] 46%|████▋     | 181/391 [00:02<00:02, 76.58it/s] 49%|████▊     | 190/391 [00:02<00:02, 79.85it/s] 51%|█████     | 199/391 [00:03<00:02, 78.84it/s] 53%|█████▎    | 208/391 [00:03<00:02, 65.54it/s] 55%|█████▌    | 216/391 [00:03<00:02, 63.05it/s] 58%|█████▊    | 225/391 [00:03<00:02, 69.03it/s] 60%|█████▉    | 234/391 [00:03<00:02, 73.99it/s] 62%|██████▏   | 243/391 [00:03<00:01, 77.54it/s] 64%|██████▍   | 252/391 [00:03<00:01, 80.37it/s] 67%|██████▋   | 261/391 [00:03<00:01, 67.73it/s] 69%|██████▉   | 269/391 [00:04<00:01, 61.62it/s] 71%|███████   | 278/391 [00:04<00:01, 67.11it/s] 73%|███████▎  | 287/391 [00:04<00:01, 72.37it/s] 76%|███████▌  | 296/391 [00:04<00:01, 76.35it/s] 78%|███████▊  | 305/391 [00:04<00:01, 79.66it/s] 80%|████████  | 314/391 [00:04<00:01, 67.10it/s] 82%|████████▏ | 322/391 [00:04<00:01, 61.85it/s] 85%|████████▍ | 331/391 [00:04<00:00, 67.33it/s] 87%|████████▋ | 340/391 [00:05<00:00, 72.38it/s] 89%|████████▉ | 349/391 [00:05<00:00, 76.12it/s] 91%|█████████▏| 357/391 [00:05<00:00, 73.41it/s] 93%|█████████▎| 365/391 [00:05<00:00, 63.05it/s] 95%|█████████▌| 372/391 [00:05<00:00, 60.01it/s] 97%|█████████▋| 381/391 [00:05<00:00, 66.72it/s]100%|██████████| 391/391 [00:05<00:00, 71.77it/s]100%|██████████| 391/391 [00:05<00:00, 67.24it/s]
50000 images processed, 5.910570859909058 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:24,  3.21it/s] 10%|█         | 8/79 [00:00<00:02, 24.22it/s] 22%|██▏       | 17/79 [00:00<00:01, 44.12it/s] 33%|███▎      | 26/79 [00:00<00:00, 57.54it/s] 44%|████▍     | 35/79 [00:00<00:00, 66.50it/s] 54%|█████▍    | 43/79 [00:00<00:00, 68.15it/s] 65%|██████▍   | 51/79 [00:01<00:00, 59.15it/s] 73%|███████▎  | 58/79 [00:01<00:00, 57.52it/s] 85%|████████▍ | 67/79 [00:01<00:00, 65.37it/s] 96%|█████████▌| 76/79 [00:01<00:00, 71.79it/s]100%|██████████| 79/79 [00:01<00:00, 57.35it/s]
10000 images processed, 1.3981420993804932 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:25,  2.38it/s]  4%|▍         | 8/204 [00:00<00:10, 18.79it/s]  6%|▋         | 13/204 [00:00<00:07, 25.59it/s]  9%|▉         | 18/204 [00:00<00:06, 30.73it/s] 13%|█▎        | 26/204 [00:00<00:04, 43.58it/s] 17%|█▋        | 35/204 [00:00<00:03, 55.39it/s] 22%|██▏       | 44/204 [00:01<00:02, 64.32it/s] 26%|██▌       | 53/204 [00:01<00:02, 70.53it/s] 30%|██▉       | 61/204 [00:01<00:02, 58.96it/s] 33%|███▎      | 68/204 [00:01<00:02, 56.96it/s] 38%|███▊      | 77/204 [00:01<00:01, 63.61it/s] 42%|████▏     | 86/204 [00:01<00:01, 69.46it/s] 47%|████▋     | 95/204 [00:01<00:01, 73.96it/s] 51%|█████     | 104/204 [00:01<00:01, 76.90it/s] 55%|█████▍    | 112/204 [00:02<00:01, 62.22it/s] 58%|█████▊    | 119/204 [00:02<00:01, 59.97it/s] 63%|██████▎   | 128/204 [00:02<00:01, 66.44it/s] 67%|██████▋   | 137/204 [00:02<00:00, 71.78it/s] 72%|███████▏  | 146/204 [00:02<00:00, 75.80it/s] 75%|███████▌  | 154/204 [00:02<00:00, 76.40it/s] 79%|███████▉  | 162/204 [00:02<00:00, 62.30it/s] 83%|████████▎ | 169/204 [00:02<00:00, 60.12it/s] 87%|████████▋ | 178/204 [00:03<00:00, 66.74it/s] 92%|█████████▏| 187/204 [00:03<00:00, 71.60it/s] 96%|█████████▌| 196/204 [00:03<00:00, 75.77it/s]100%|██████████| 204/204 [00:03<00:00, 60.12it/s]
26032 images processed, 3.453490734100342 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:43,  1.81it/s]  8%|▊         | 6/79 [00:00<00:06, 11.25it/s] 15%|█▌        | 12/79 [00:00<00:03, 21.46it/s] 24%|██▍       | 19/79 [00:00<00:01, 32.09it/s] 32%|███▏      | 25/79 [00:01<00:01, 34.89it/s] 38%|███▊      | 30/79 [00:01<00:01, 36.83it/s] 46%|████▌     | 36/79 [00:01<00:01, 40.48it/s] 56%|█████▌    | 44/79 [00:01<00:00, 49.18it/s] 63%|██████▎   | 50/79 [00:01<00:00, 45.67it/s] 70%|██████▉   | 55/79 [00:01<00:00, 44.91it/s] 77%|███████▋  | 61/79 [00:01<00:00, 48.16it/s] 87%|████████▋ | 69/79 [00:01<00:00, 50.31it/s] 95%|█████████▍| 75/79 [00:02<00:00, 48.00it/s]100%|██████████| 79/79 [00:02<00:00, 37.20it/s]
10000 images processed, 2.1608738899230957 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:33,  2.31it/s]  8%|▊         | 6/79 [00:00<00:05, 13.51it/s] 14%|█▍        | 11/79 [00:00<00:03, 22.52it/s] 22%|██▏       | 17/79 [00:00<00:01, 31.80it/s] 33%|███▎      | 26/79 [00:00<00:01, 47.01it/s] 44%|████▍     | 35/79 [00:00<00:00, 58.21it/s] 56%|█████▌    | 44/79 [00:01<00:00, 66.46it/s] 66%|██████▌   | 52/79 [00:01<00:00, 65.52it/s] 76%|███████▌  | 60/79 [00:01<00:00, 58.94it/s] 85%|████████▍ | 67/79 [00:01<00:00, 58.08it/s] 96%|█████████▌| 76/79 [00:01<00:00, 65.77it/s]100%|██████████| 79/79 [00:01<00:00, 48.78it/s]
10000 images processed, 1.6456184387207031 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:24,  2.87it/s] 14%|█▍        | 10/70 [00:00<00:02, 27.77it/s] 23%|██▎       | 16/70 [00:00<00:01, 33.29it/s] 31%|███▏      | 22/70 [00:00<00:01, 39.51it/s] 40%|████      | 28/70 [00:00<00:00, 43.47it/s] 53%|█████▎    | 37/70 [00:00<00:00, 55.64it/s] 66%|██████▌   | 46/70 [00:01<00:00, 64.78it/s] 79%|███████▊  | 55/70 [00:01<00:00, 70.84it/s] 90%|█████████ | 63/70 [00:01<00:00, 64.35it/s]100%|██████████| 70/70 [00:01<00:00, 60.36it/s]100%|██████████| 70/70 [00:01<00:00, 49.54it/s]
8925 images processed, 1.462965488433838 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:39,  1.10it/s]  4%|▍         | 2/45 [00:01<00:24,  1.75it/s] 18%|█▊        | 8/45 [00:01<00:04,  9.08it/s] 24%|██▍       | 11/45 [00:01<00:04,  6.93it/s] 36%|███▌      | 16/45 [00:02<00:02, 11.39it/s] 42%|████▏     | 19/45 [00:02<00:02,  8.98it/s] 53%|█████▎    | 24/45 [00:02<00:01, 12.45it/s] 60%|██████    | 27/45 [00:03<00:01,  9.47it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.60it/s] 78%|███████▊  | 35/45 [00:04<00:01,  8.84it/s] 93%|█████████▎| 42/45 [00:04<00:00,  9.62it/s]100%|██████████| 45/45 [00:04<00:00,  9.24it/s]
5640 images processed, 4.892956733703613 seconds used

22.902638912200928
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.85  98.93
places365     76.92  78.31
LSUN          27.55  94.87
iSUN          77.24  80.34
dtd           45.60  89.56
forget        42.20  91.26
AVG           45.73  88.88
Retain-Acc: 0.7390
Forget-as-OOD (retain known vs forget novel):
  FPR: 42.20 AUROC: 91.26 AUIN: 98.86
24.224658012390137
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=1.0 lr=0.0001 epochs=5
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:26<01:44, 26.21s/it] 40%|████      | 2/5 [00:44<01:04, 21.60s/it] 60%|██████    | 3/5 [01:03<00:40, 20.20s/it] 80%|████████  | 4/5 [01:30<00:22, 22.84s/it]100%|██████████| 5/5 [01:48<00:00, 21.37s/it]100%|██████████| 5/5 [01:48<00:00, 21.76s/it]
[loss] ep 0 it 0 total=13.7429 mle=1.5705 pcon=5.2950 forget=6.8774 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8367 mle=1.5423 pcon=5.2879 forget=7.0065 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8516 mle=1.7008 pcon=5.2809 forget=6.8699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0214 mle=1.8994 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8933 mle=1.7131 pcon=5.2670 forget=6.9131 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6546 mle=1.5020 pcon=5.2603 forget=6.8922 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7237 mle=1.5599 pcon=5.2541 forget=6.9097 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8771 mle=1.6811 pcon=5.2476 forget=6.9484 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=13.9627 mle=1.6716 pcon=5.2409 forget=7.0502 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7068 mle=1.6372 pcon=5.2346 forget=6.8351 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6922 mle=1.5165 pcon=5.2285 forget=6.9472 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9500 mle=1.7806 pcon=5.2225 forget=6.9469 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9078 mle=1.7645 pcon=5.2167 forget=6.9267 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6583 mle=1.6055 pcon=5.2112 forget=6.8416 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8204 mle=1.7237 pcon=5.2056 forget=6.8911 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9256 mle=1.7945 pcon=5.2003 forget=6.9308 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=13.6323 mle=1.5530 pcon=5.1951 forget=6.8843 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8665 mle=1.8884 pcon=5.1899 forget=6.7882 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8057 mle=1.7416 pcon=5.1847 forget=6.8793 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5310 mle=1.5265 pcon=5.1797 forget=6.8248 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6900 mle=1.5858 pcon=5.1746 forget=6.9297 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9072 mle=1.8838 pcon=5.1699 forget=6.8534 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5460 mle=1.5924 pcon=5.1652 forget=6.7884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7962 mle=1.8115 pcon=5.1606 forget=6.8241 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=13.7160 mle=1.7061 pcon=5.1560 forget=6.8539 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7194 mle=1.6711 pcon=5.1519 forget=6.8964 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7435 mle=1.7666 pcon=5.1476 forget=6.8293 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8733 mle=1.9183 pcon=5.1436 forget=6.8114 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.5095 mle=1.5702 pcon=5.1396 forget=6.7997 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7529 mle=1.8362 pcon=5.1354 forget=6.7813 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.7113 mle=1.7029 pcon=5.1312 forget=6.8771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.7152 mle=1.6959 pcon=5.1276 forget=6.8917 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=13.4887 mle=1.6542 pcon=5.1238 forget=6.7107 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6309 mle=1.7299 pcon=5.1198 forget=6.7812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5972 mle=1.6502 pcon=5.1161 forget=6.8308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3949 mle=1.4535 pcon=5.1127 forget=6.8287 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6587 mle=1.7671 pcon=5.1088 forget=6.7827 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3639 mle=1.4912 pcon=5.1053 forget=6.7674 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7995 mle=1.8716 pcon=5.1019 forget=6.8260 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:11,  2.97it/s]  3%|▎         | 10/391 [00:00<00:13, 28.46it/s]  5%|▍         | 19/391 [00:00<00:08, 46.37it/s]  7%|▋         | 28/391 [00:00<00:06, 58.83it/s]  9%|▉         | 37/391 [00:00<00:05, 67.39it/s] 12%|█▏        | 46/391 [00:00<00:04, 73.39it/s] 14%|█▍        | 55/391 [00:00<00:04, 77.84it/s] 16%|█▋        | 64/391 [00:01<00:04, 80.90it/s] 19%|█▊        | 73/391 [00:01<00:03, 83.01it/s] 21%|██        | 82/391 [00:01<00:03, 84.57it/s] 23%|██▎       | 91/391 [00:01<00:03, 85.67it/s] 26%|██▌       | 100/391 [00:01<00:03, 86.00it/s] 28%|██▊       | 109/391 [00:01<00:03, 86.44it/s] 30%|███       | 118/391 [00:01<00:03, 86.75it/s] 32%|███▏      | 127/391 [00:01<00:03, 87.10it/s] 35%|███▍      | 136/391 [00:01<00:02, 87.09it/s] 37%|███▋      | 145/391 [00:01<00:02, 87.37it/s] 39%|███▉      | 154/391 [00:02<00:02, 87.49it/s] 42%|████▏     | 163/391 [00:02<00:02, 87.69it/s] 44%|████▍     | 172/391 [00:02<00:02, 87.70it/s] 46%|████▋     | 181/391 [00:02<00:02, 88.12it/s] 49%|████▊     | 190/391 [00:02<00:02, 88.18it/s] 51%|█████     | 199/391 [00:02<00:02, 88.06it/s] 53%|█████▎    | 208/391 [00:02<00:02, 87.92it/s] 55%|█████▌    | 217/391 [00:02<00:01, 88.03it/s] 58%|█████▊    | 226/391 [00:02<00:01, 87.94it/s] 60%|██████    | 235/391 [00:03<00:01, 88.08it/s] 62%|██████▏   | 244/391 [00:03<00:01, 88.13it/s] 65%|██████▍   | 253/391 [00:03<00:01, 87.93it/s] 67%|██████▋   | 262/391 [00:03<00:01, 87.95it/s] 69%|██████▉   | 271/391 [00:03<00:01, 86.97it/s] 72%|███████▏  | 280/391 [00:03<00:01, 86.68it/s] 74%|███████▍  | 289/391 [00:03<00:01, 86.27it/s] 76%|███████▌  | 298/391 [00:03<00:01, 86.73it/s] 79%|███████▊  | 307/391 [00:03<00:00, 87.09it/s] 81%|████████  | 316/391 [00:03<00:00, 87.16it/s] 83%|████████▎ | 325/391 [00:04<00:00, 87.34it/s] 85%|████████▌ | 334/391 [00:04<00:00, 87.49it/s] 88%|████████▊ | 343/391 [00:04<00:00, 87.33it/s] 90%|█████████ | 352/391 [00:04<00:00, 87.33it/s] 92%|█████████▏| 361/391 [00:04<00:00, 87.43it/s] 95%|█████████▍| 370/391 [00:04<00:00, 87.67it/s] 97%|█████████▋| 379/391 [00:04<00:00, 87.43it/s] 99%|█████████▉| 388/391 [00:04<00:00, 88.13it/s]100%|██████████| 391/391 [00:04<00:00, 81.47it/s]
50000 images processed, 4.902028560638428 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:27,  2.85it/s] 13%|█▎        | 10/79 [00:00<00:02, 27.70it/s] 24%|██▍       | 19/79 [00:00<00:01, 44.65it/s] 35%|███▌      | 28/79 [00:00<00:00, 56.95it/s] 47%|████▋     | 37/79 [00:00<00:00, 65.33it/s] 58%|█████▊    | 46/79 [00:00<00:00, 71.05it/s] 70%|██████▉   | 55/79 [00:00<00:00, 75.88it/s] 81%|████████  | 64/79 [00:01<00:00, 79.16it/s] 92%|█████████▏| 73/79 [00:01<00:00, 82.12it/s]100%|██████████| 79/79 [00:01<00:00, 62.45it/s]
10000 images processed, 1.2882096767425537 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:26,  2.36it/s]  5%|▍         | 10/204 [00:00<00:08, 23.82it/s]  9%|▉         | 19/204 [00:00<00:04, 40.51it/s] 14%|█▎        | 28/204 [00:00<00:03, 53.11it/s] 18%|█▊        | 37/204 [00:00<00:02, 62.35it/s] 23%|██▎       | 46/204 [00:00<00:02, 69.14it/s] 27%|██▋       | 55/204 [00:01<00:02, 73.66it/s] 31%|███▏      | 64/204 [00:01<00:01, 77.26it/s] 36%|███▌      | 73/204 [00:01<00:01, 79.27it/s] 40%|████      | 82/204 [00:01<00:01, 81.41it/s] 45%|████▍     | 91/204 [00:01<00:01, 82.96it/s] 49%|████▉     | 100/204 [00:01<00:01, 83.96it/s] 53%|█████▎    | 109/204 [00:01<00:01, 84.71it/s] 58%|█████▊    | 118/204 [00:01<00:01, 85.13it/s] 62%|██████▏   | 127/204 [00:01<00:00, 85.33it/s] 67%|██████▋   | 136/204 [00:01<00:00, 86.33it/s] 71%|███████   | 145/204 [00:02<00:00, 86.41it/s] 75%|███████▌  | 154/204 [00:02<00:00, 86.21it/s] 80%|███████▉  | 163/204 [00:02<00:00, 86.17it/s] 84%|████████▍ | 172/204 [00:02<00:00, 84.76it/s] 89%|████████▊ | 181/204 [00:02<00:00, 85.31it/s] 93%|█████████▎| 190/204 [00:02<00:00, 85.65it/s] 98%|█████████▊| 199/204 [00:02<00:00, 86.47it/s]100%|██████████| 204/204 [00:02<00:00, 72.94it/s]
26032 images processed, 2.844310760498047 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:44,  1.77it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.75it/s] 22%|██▏       | 17/79 [00:00<00:02, 25.40it/s] 32%|███▏      | 25/79 [00:01<00:01, 32.27it/s] 42%|████▏     | 33/79 [00:01<00:01, 36.98it/s] 52%|█████▏    | 41/79 [00:01<00:00, 39.35it/s] 62%|██████▏   | 49/79 [00:01<00:00, 44.45it/s] 72%|███████▏  | 57/79 [00:01<00:00, 45.17it/s] 82%|████████▏ | 65/79 [00:01<00:00, 45.92it/s] 92%|█████████▏| 73/79 [00:02<00:00, 46.06it/s]100%|██████████| 79/79 [00:02<00:00, 37.41it/s]
10000 images processed, 2.148528575897217 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:33,  2.34it/s] 13%|█▎        | 10/79 [00:00<00:02, 23.96it/s] 24%|██▍       | 19/79 [00:00<00:01, 40.80it/s] 35%|███▌      | 28/79 [00:00<00:00, 53.50it/s] 47%|████▋     | 37/79 [00:00<00:00, 62.81it/s] 58%|█████▊    | 46/79 [00:00<00:00, 69.40it/s] 70%|██████▉   | 55/79 [00:01<00:00, 74.44it/s] 81%|████████  | 64/79 [00:01<00:00, 78.24it/s] 92%|█████████▏| 73/79 [00:01<00:00, 81.13it/s]100%|██████████| 79/79 [00:01<00:00, 59.86it/s]
10000 images processed, 1.3409581184387207 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:27,  2.47it/s] 14%|█▍        | 10/70 [00:00<00:02, 24.93it/s] 27%|██▋       | 19/70 [00:00<00:01, 41.96it/s] 40%|████      | 28/70 [00:00<00:00, 54.11it/s] 53%|█████▎    | 37/70 [00:00<00:00, 63.42it/s] 67%|██████▋   | 47/70 [00:00<00:00, 71.55it/s] 80%|████████  | 56/70 [00:01<00:00, 75.36it/s] 93%|█████████▎| 65/70 [00:01<00:00, 79.06it/s]100%|██████████| 70/70 [00:01<00:00, 57.93it/s]
8925 images processed, 1.2408440113067627 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:42,  1.02it/s]  4%|▍         | 2/45 [00:01<00:20,  2.09it/s] 20%|██        | 9/45 [00:01<00:04,  8.87it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.63it/s] 31%|███       | 14/45 [00:01<00:02, 10.55it/s] 38%|███▊      | 17/45 [00:02<00:02, 12.76it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.46it/s] 49%|████▉     | 22/45 [00:02<00:02, 11.16it/s] 58%|█████▊    | 26/45 [00:02<00:01, 11.89it/s] 67%|██████▋   | 30/45 [00:03<00:01, 11.16it/s] 76%|███████▌  | 34/45 [00:03<00:01, 10.95it/s] 91%|█████████ | 41/45 [00:03<00:00, 17.87it/s] 98%|█████████▊| 44/45 [00:04<00:00, 10.49it/s]100%|██████████| 45/45 [00:04<00:00, 10.16it/s]
5640 images processed, 4.452004671096802 seconds used

20.058444499969482
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.87  81.18
LSUN          17.60  96.07
iSUN          72.34  81.68
dtd           37.87  91.39
forget        79.50  86.90
AVG           46.28  89.43
Retain-Acc: 0.7406
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
23.683862924575806
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=1.0 lr=0.0001 epochs=10
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:21<03:11, 21.25s/it] 20%|██        | 2/10 [00:41<02:43, 20.45s/it] 30%|███       | 3/10 [01:06<02:39, 22.74s/it] 40%|████      | 4/10 [01:24<02:04, 20.81s/it] 50%|█████     | 5/10 [01:42<01:39, 19.92s/it] 60%|██████    | 6/10 [02:03<01:20, 20.07s/it] 70%|███████   | 7/10 [02:28<01:05, 21.73s/it] 80%|████████  | 8/10 [02:45<00:40, 20.29s/it][loss] ep 0 it 0 total=13.7434 mle=1.5709 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8368 mle=1.5423 pcon=5.2879 forget=7.0066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8513 mle=1.7004 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0218 mle=1.8998 pcon=5.2738 forget=6.8481 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8933 mle=1.7131 pcon=5.2670 forget=6.9132 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6545 mle=1.5019 pcon=5.2603 forget=6.8922 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7233 mle=1.5596 pcon=5.2541 forget=6.9096 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8769 mle=1.6809 pcon=5.2476 forget=6.9484 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=13.9628 mle=1.6716 pcon=5.2409 forget=7.0503 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7067 mle=1.6371 pcon=5.2346 forget=6.8350 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6924 mle=1.5165 pcon=5.2285 forget=6.9474 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9496 mle=1.7801 pcon=5.2225 forget=6.9470 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9079 mle=1.7646 pcon=5.2167 forget=6.9266 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6582 mle=1.6054 pcon=5.2112 forget=6.8415 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8208 mle=1.7241 pcon=5.2056 forget=6.8910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9253 mle=1.7943 pcon=5.2003 forget=6.9307 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=13.6320 mle=1.5528 pcon=5.1951 forget=6.8841 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8666 mle=1.8883 pcon=5.1899 forget=6.7884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8058 mle=1.7417 pcon=5.1847 forget=6.8794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5308 mle=1.5264 pcon=5.1797 forget=6.8248 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6901 mle=1.5858 pcon=5.1746 forget=6.9297 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9070 mle=1.8836 pcon=5.1699 forget=6.8534 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5461 mle=1.5926 pcon=5.1652 forget=6.7883 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7961 mle=1.8115 pcon=5.1606 forget=6.8241 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=13.7163 mle=1.7063 pcon=5.1560 forget=6.8540 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7193 mle=1.6708 pcon=5.1519 forget=6.8966 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7438 mle=1.7668 pcon=5.1476 forget=6.8295 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8735 mle=1.9184 pcon=5.1436 forget=6.8115 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.5095 mle=1.5702 pcon=5.1396 forget=6.7997 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7529 mle=1.8363 pcon=5.1354 forget=6.7811 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.7116 mle=1.7033 pcon=5.1312 forget=6.8771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.7151 mle=1.6959 pcon=5.1276 forget=6.8915 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=13.4887 mle=1.6542 pcon=5.1238 forget=6.7107 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6309 mle=1.7298 pcon=5.1198 forget=6.7813 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5972 mle=1.6503 pcon=5.1161 forget=6.8308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3953 mle=1.4538 pcon=5.1127 forget=6.8288 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6591 mle=1.7676 pcon=5.1088 forget=6.7827 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3640 mle=1.4914 pcon=5.1053 forget=6.7673 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7998 mle=1.8720 pcon=5.1018 forget=6.8259 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=13.7525 mle=1.8672 pcon=5.0985 forget=6.7867 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=13.6252 mle=1.7998 pcon=5.0949 forget=6.7305 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=13.4765 mle=1.5838 pcon=5.0918 forget=6.8008 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=13.4770 mle=1.5945 pcon=5.0883 forget=6.7942 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=13.5060 mle=1.6915 pcon=5.0854 forget=6.7291 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=13.5504 mle=1.7734 pcon=5.0824 forget=6.6945 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=13.4314 mle=1.5965 pcon=5.0794 forget=6.7555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=13.5685 mle=1.7471 pcon=5.0765 forget=6.7449 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=13.3358 mle=1.4013 pcon=5.0737 forget=6.8608 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=13.2501 mle=1.5357 pcon=5.0709 forget=6.6435 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=13.4678 mle=1.5916 pcon=5.0683 forget=6.8079 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=13.4618 mle=1.6657 pcon=5.0655 forget=6.7306 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=13.4029 mle=1.6304 pcon=5.0633 forget=6.7092 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=13.6641 mle=1.8233 pcon=5.0604 forget=6.7804 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=13.4130 mle=1.5352 pcon=5.0581 forget=6.8198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=13.4687 mle=1.6750 pcon=5.0556 forget=6.7381 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=13.3821 mle=1.6308 pcon=5.0530 forget=6.6983 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=13.4714 mle=1.7449 pcon=5.0506 forget=6.6759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=13.3863 mle=1.6271 pcon=5.0481 forget=6.7111 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=13.2729 mle=1.5820 pcon=5.0458 forget=6.6451 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=13.4598 mle=1.6545 pcon=5.0438 forget=6.7615 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=13.6462 mle=1.7958 pcon=5.0417 forget=6.8086 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=13.5803 mle=1.7420 pcon=5.0393 forget=6.7990 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=13.4099 mle=1.6406 pcon=5.0373 forget=6.7320 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 90%|█████████ | 9/10 [03:03<00:19, 19.58s/it]100%|██████████| 10/10 [03:23<00:00, 19.77s/it]100%|██████████| 10/10 [03:23<00:00, 20.37s/it]
[loss] ep 8 it 30 total=13.3746 mle=1.5371 pcon=5.0354 forget=6.8022 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=13.3671 mle=1.5904 pcon=5.0335 forget=6.7433 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=13.3201 mle=1.5004 pcon=5.0313 forget=6.7883 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=13.4417 mle=1.7036 pcon=5.0294 forget=6.7087 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=13.2591 mle=1.5309 pcon=5.0279 forget=6.7002 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=13.3598 mle=1.6776 pcon=5.0260 forget=6.6563 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=13.6167 mle=1.7565 pcon=5.0239 forget=6.8363 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=13.1773 mle=1.4891 pcon=5.0220 forget=6.6662 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=13.2317 mle=1.6365 pcon=5.0201 forget=6.5751 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=13.3678 mle=1.6693 pcon=5.0180 forget=6.6805 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=13.6155 mle=1.8421 pcon=5.0166 forget=6.7569 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=13.2796 mle=1.5783 pcon=5.0150 forget=6.6863 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=13.2831 mle=1.5673 pcon=5.0135 forget=6.7024 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=13.3052 mle=1.5599 pcon=5.0116 forget=6.7337 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=13.2497 mle=1.5582 pcon=5.0102 forget=6.6814 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:06,  3.09it/s]  3%|▎         | 10/391 [00:00<00:12, 29.41it/s]  4%|▍         | 17/391 [00:00<00:09, 39.58it/s]  6%|▌         | 23/391 [00:00<00:08, 41.74it/s]  8%|▊         | 30/391 [00:00<00:07, 46.62it/s] 10%|▉         | 39/391 [00:00<00:06, 57.26it/s] 12%|█▏        | 48/391 [00:01<00:05, 65.92it/s] 15%|█▍        | 57/391 [00:01<00:04, 72.26it/s] 17%|█▋        | 66/391 [00:01<00:04, 76.94it/s] 19%|█▉        | 75/391 [00:01<00:04, 76.31it/s] 21%|██        | 83/391 [00:01<00:04, 64.21it/s] 23%|██▎       | 90/391 [00:01<00:04, 61.42it/s] 25%|██▌       | 99/391 [00:01<00:04, 67.60it/s] 28%|██▊       | 108/391 [00:01<00:03, 73.12it/s] 30%|██▉       | 117/391 [00:01<00:03, 77.37it/s] 32%|███▏      | 126/391 [00:02<00:03, 80.49it/s] 35%|███▍      | 135/391 [00:02<00:03, 82.60it/s] 37%|███▋      | 144/391 [00:02<00:03, 67.26it/s] 39%|███▉      | 152/391 [00:02<00:03, 62.84it/s] 41%|████      | 161/391 [00:02<00:03, 68.45it/s] 43%|████▎     | 170/391 [00:02<00:03, 73.66it/s] 46%|████▌     | 179/391 [00:02<00:02, 77.42it/s] 48%|████▊     | 188/391 [00:02<00:02, 80.40it/s] 50%|█████     | 197/391 [00:03<00:02, 80.94it/s] 53%|█████▎    | 206/391 [00:03<00:02, 66.06it/s] 55%|█████▍    | 214/391 [00:03<00:02, 62.75it/s] 57%|█████▋    | 223/391 [00:03<00:02, 68.61it/s] 59%|█████▉    | 232/391 [00:03<00:02, 73.31it/s] 62%|██████▏   | 241/391 [00:03<00:01, 77.21it/s] 64%|██████▍   | 250/391 [00:03<00:01, 80.20it/s] 66%|██████▌   | 259/391 [00:03<00:01, 72.79it/s] 68%|██████▊   | 267/391 [00:04<00:01, 65.41it/s] 70%|███████   | 274/391 [00:04<00:01, 63.24it/s] 72%|███████▏  | 283/391 [00:04<00:01, 69.60it/s] 75%|███████▍  | 292/391 [00:04<00:01, 74.64it/s] 77%|███████▋  | 301/391 [00:04<00:01, 78.53it/s] 79%|███████▉  | 310/391 [00:04<00:00, 81.38it/s] 82%|████████▏ | 319/391 [00:04<00:00, 77.95it/s] 84%|████████▎ | 327/391 [00:04<00:00, 66.00it/s] 85%|████████▌ | 334/391 [00:05<00:00, 62.11it/s] 88%|████████▊ | 343/391 [00:05<00:00, 68.78it/s] 90%|█████████ | 352/391 [00:05<00:00, 73.97it/s] 92%|█████████▏| 361/391 [00:05<00:00, 77.94it/s] 95%|█████████▍| 370/391 [00:05<00:00, 80.92it/s] 97%|█████████▋| 379/391 [00:05<00:00, 74.43it/s] 99%|█████████▉| 387/391 [00:05<00:00, 66.22it/s]100%|██████████| 391/391 [00:05<00:00, 67.30it/s]
50000 images processed, 5.913675546646118 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:29,  2.62it/s] 13%|█▎        | 10/79 [00:00<00:02, 25.86it/s] 24%|██▍       | 19/79 [00:00<00:01, 43.24it/s] 35%|███▌      | 28/79 [00:00<00:00, 55.87it/s] 47%|████▋     | 37/79 [00:00<00:00, 63.80it/s] 57%|█████▋    | 45/79 [00:00<00:00, 56.72it/s] 66%|██████▌   | 52/79 [00:01<00:00, 56.44it/s] 76%|███████▌  | 60/79 [00:01<00:00, 61.11it/s] 87%|████████▋ | 69/79 [00:01<00:00, 68.38it/s] 99%|█████████▊| 78/79 [00:01<00:00, 74.12it/s]100%|██████████| 79/79 [00:01<00:00, 55.10it/s]
10000 images processed, 1.4549951553344727 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:19,  2.54it/s]  5%|▍         | 10/204 [00:00<00:07, 25.50it/s]  9%|▉         | 19/204 [00:00<00:04, 42.48it/s] 14%|█▎        | 28/204 [00:00<00:03, 55.01it/s] 18%|█▊        | 37/204 [00:00<00:02, 61.30it/s] 22%|██▏       | 45/204 [00:01<00:02, 54.44it/s] 25%|██▌       | 52/204 [00:01<00:02, 54.46it/s] 30%|██▉       | 61/204 [00:01<00:02, 62.54it/s] 34%|███▍      | 70/204 [00:01<00:01, 68.89it/s] 39%|███▊      | 79/204 [00:01<00:01, 73.83it/s] 43%|████▎     | 88/204 [00:01<00:01, 76.76it/s] 48%|████▊     | 97/204 [00:01<00:01, 61.94it/s] 51%|█████     | 104/204 [00:01<00:01, 59.96it/s] 55%|█████▌    | 113/204 [00:01<00:01, 66.55it/s] 60%|█████▉    | 122/204 [00:02<00:01, 71.69it/s] 64%|██████▍   | 131/204 [00:02<00:00, 75.75it/s] 69%|██████▊   | 140/204 [00:02<00:00, 78.32it/s] 73%|███████▎  | 149/204 [00:02<00:00, 62.89it/s] 76%|███████▋  | 156/204 [00:02<00:00, 60.59it/s] 81%|████████  | 165/204 [00:02<00:00, 66.98it/s] 85%|████████▌ | 174/204 [00:02<00:00, 72.11it/s] 90%|████████▉ | 183/204 [00:02<00:00, 76.10it/s] 94%|█████████▍| 192/204 [00:03<00:00, 79.30it/s] 99%|█████████▊| 201/204 [00:03<00:00, 64.83it/s]100%|██████████| 204/204 [00:03<00:00, 61.73it/s]
26032 images processed, 3.3458094596862793 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:35,  2.18it/s]  9%|▉         | 7/79 [00:00<00:04, 15.21it/s] 20%|██        | 16/79 [00:00<00:01, 32.70it/s] 28%|██▊       | 22/79 [00:00<00:01, 31.56it/s] 35%|███▌      | 28/79 [00:00<00:01, 37.59it/s] 42%|████▏     | 33/79 [00:01<00:01, 40.33it/s] 49%|████▉     | 39/79 [00:01<00:00, 44.79it/s] 57%|█████▋    | 45/79 [00:01<00:00, 40.52it/s] 63%|██████▎   | 50/79 [00:01<00:00, 42.18it/s] 71%|███████   | 56/79 [00:01<00:00, 46.33it/s] 78%|███████▊  | 62/79 [00:01<00:00, 43.60it/s] 85%|████████▍ | 67/79 [00:01<00:00, 43.22it/s] 91%|█████████ | 72/79 [00:01<00:00, 44.32it/s]100%|██████████| 79/79 [00:02<00:00, 38.77it/s]
10000 images processed, 2.0745654106140137 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:33,  2.33it/s]  8%|▊         | 6/79 [00:00<00:05, 14.11it/s] 19%|█▉        | 15/79 [00:00<00:01, 33.48it/s] 30%|███       | 24/79 [00:00<00:01, 48.37it/s] 42%|████▏     | 33/79 [00:00<00:00, 59.40it/s] 53%|█████▎    | 42/79 [00:00<00:00, 67.58it/s] 65%|██████▍   | 51/79 [00:01<00:00, 73.28it/s] 76%|███████▌  | 60/79 [00:01<00:00, 62.84it/s] 86%|████████▌ | 68/79 [00:01<00:00, 58.14it/s] 97%|█████████▋| 77/79 [00:01<00:00, 64.88it/s]100%|██████████| 79/79 [00:01<00:00, 52.12it/s]
10000 images processed, 1.543341875076294 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:24,  2.82it/s] 13%|█▎        | 9/70 [00:00<00:02, 24.36it/s] 26%|██▌       | 18/70 [00:00<00:01, 42.60it/s] 39%|███▊      | 27/70 [00:00<00:00, 55.56it/s] 51%|█████▏    | 36/70 [00:00<00:00, 64.62it/s] 64%|██████▍   | 45/70 [00:00<00:00, 68.52it/s] 76%|███████▌  | 53/70 [00:01<00:00, 58.24it/s] 86%|████████▌ | 60/70 [00:01<00:00, 56.69it/s] 99%|█████████▊| 69/70 [00:01<00:00, 64.75it/s]100%|██████████| 70/70 [00:01<00:00, 52.88it/s]
8925 images processed, 1.3533141613006592 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:34,  1.28it/s]  4%|▍         | 2/45 [00:00<00:18,  2.34it/s] 18%|█▊        | 8/45 [00:01<00:03, 11.61it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.50it/s] 31%|███       | 14/45 [00:01<00:02, 10.99it/s] 40%|████      | 18/45 [00:01<00:02, 11.95it/s] 49%|████▉     | 22/45 [00:02<00:01, 13.01it/s] 58%|█████▊    | 26/45 [00:02<00:01, 14.62it/s] 67%|██████▋   | 30/45 [00:02<00:01, 13.23it/s] 76%|███████▌  | 34/45 [00:03<00:00, 13.87it/s] 87%|████████▋ | 39/45 [00:03<00:00, 16.61it/s] 93%|█████████▎| 42/45 [00:03<00:00, 11.65it/s]100%|██████████| 45/45 [00:03<00:00, 13.77it/s]100%|██████████| 45/45 [00:03<00:00, 11.67it/s]
5640 images processed, 3.878858804702759 seconds used

21.173986434936523
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.89  81.18
LSUN          17.54  96.08
iSUN          72.38  81.70
dtd           37.89  91.39
forget        79.50  86.90
AVG           46.29  89.44
Retain-Acc: 0.7406
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
18.445475578308105
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=1.0 lr=0.0001 epochs=20
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:29<09:24, 29.70s/it] 10%|█         | 2/20 [00:48<06:59, 23.32s/it] 15%|█▌        | 3/20 [01:06<05:56, 20.99s/it] 20%|██        | 4/20 [01:27<05:36, 21.05s/it] 25%|██▌       | 5/20 [01:52<05:35, 22.35s/it] 30%|███       | 6/20 [02:10<04:53, 20.94s/it] 35%|███▌      | 7/20 [02:29<04:22, 20.20s/it] 40%|████      | 8/20 [02:52<04:14, 21.24s/it][loss] ep 0 it 0 total=13.7435 mle=1.5710 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8370 mle=1.5426 pcon=5.2879 forget=7.0066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8514 mle=1.7005 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0217 mle=1.8997 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8932 mle=1.7132 pcon=5.2670 forget=6.9131 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6546 mle=1.5021 pcon=5.2603 forget=6.8922 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7233 mle=1.5596 pcon=5.2540 forget=6.9096 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8769 mle=1.6809 pcon=5.2476 forget=6.9484 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=13.9627 mle=1.6716 pcon=5.2409 forget=7.0503 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7067 mle=1.6371 pcon=5.2346 forget=6.8350 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6923 mle=1.5165 pcon=5.2285 forget=6.9474 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9498 mle=1.7802 pcon=5.2225 forget=6.9471 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9078 mle=1.7646 pcon=5.2167 forget=6.9265 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6581 mle=1.6054 pcon=5.2112 forget=6.8415 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8208 mle=1.7241 pcon=5.2056 forget=6.8911 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9253 mle=1.7942 pcon=5.2003 forget=6.9308 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=13.6321 mle=1.5528 pcon=5.1950 forget=6.8843 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8666 mle=1.8884 pcon=5.1899 forget=6.7883 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8057 mle=1.7417 pcon=5.1847 forget=6.8793 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5308 mle=1.5264 pcon=5.1797 forget=6.8248 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6901 mle=1.5858 pcon=5.1746 forget=6.9297 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9071 mle=1.8836 pcon=5.1699 forget=6.8535 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5461 mle=1.5926 pcon=5.1652 forget=6.7883 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7961 mle=1.8115 pcon=5.1605 forget=6.8241 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=13.7160 mle=1.7061 pcon=5.1560 forget=6.8540 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7192 mle=1.6709 pcon=5.1519 forget=6.8964 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7436 mle=1.7666 pcon=5.1476 forget=6.8295 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8735 mle=1.9185 pcon=5.1436 forget=6.8114 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.5093 mle=1.5701 pcon=5.1396 forget=6.7996 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7526 mle=1.8361 pcon=5.1354 forget=6.7811 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.7116 mle=1.7033 pcon=5.1312 forget=6.8771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.7153 mle=1.6961 pcon=5.1276 forget=6.8916 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=13.4886 mle=1.6542 pcon=5.1238 forget=6.7106 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6309 mle=1.7297 pcon=5.1198 forget=6.7815 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5970 mle=1.6500 pcon=5.1161 forget=6.8308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3951 mle=1.4536 pcon=5.1127 forget=6.8288 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6591 mle=1.7676 pcon=5.1088 forget=6.7827 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3639 mle=1.4914 pcon=5.1053 forget=6.7673 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7997 mle=1.8720 pcon=5.1018 forget=6.8259 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=13.7525 mle=1.8673 pcon=5.0985 forget=6.7867 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=13.6250 mle=1.7997 pcon=5.0949 forget=6.7304 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=13.4767 mle=1.5840 pcon=5.0918 forget=6.8009 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=13.4780 mle=1.5956 pcon=5.0883 forget=6.7940 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=13.5058 mle=1.6913 pcon=5.0854 forget=6.7291 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=13.5504 mle=1.7735 pcon=5.0824 forget=6.6945 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=13.4315 mle=1.5966 pcon=5.0794 forget=6.7555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=13.5683 mle=1.7468 pcon=5.0765 forget=6.7449 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=13.3357 mle=1.4013 pcon=5.0737 forget=6.8607 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=13.2503 mle=1.5358 pcon=5.0709 forget=6.6435 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=13.4680 mle=1.5918 pcon=5.0682 forget=6.8080 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=13.4616 mle=1.6655 pcon=5.0655 forget=6.7305 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=13.4031 mle=1.6306 pcon=5.0633 forget=6.7092 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=13.6642 mle=1.8234 pcon=5.0604 forget=6.7804 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=13.4130 mle=1.5352 pcon=5.0581 forget=6.8197 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=13.4687 mle=1.6751 pcon=5.0556 forget=6.7381 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=13.3821 mle=1.6307 pcon=5.0530 forget=6.6984 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=13.4714 mle=1.7449 pcon=5.0506 forget=6.6759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=13.3861 mle=1.6269 pcon=5.0481 forget=6.7111 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=13.2731 mle=1.5821 pcon=5.0458 forget=6.6451 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=13.4599 mle=1.6545 pcon=5.0438 forget=6.7615 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=13.6462 mle=1.7958 pcon=5.0417 forget=6.8087 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=13.5804 mle=1.7420 pcon=5.0393 forget=6.7991 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=13.4098 mle=1.6407 pcon=5.0372 forget=6.7319 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 45%|████▌     | 9/20 [03:15<03:58, 21.66s/it] 50%|█████     | 10/20 [03:33<03:23, 20.39s/it] 55%|█████▌    | 11/20 [03:50<02:55, 19.47s/it] 60%|██████    | 12/20 [04:15<02:48, 21.04s/it] 65%|██████▌   | 13/20 [04:36<02:27, 21.06s/it] 70%|███████   | 14/20 [04:54<02:01, 20.21s/it] 75%|███████▌  | 15/20 [05:13<01:38, 19.73s/it] 80%|████████  | 16/20 [05:39<01:27, 21.76s/it][loss] ep 8 it 30 total=13.3746 mle=1.5371 pcon=5.0354 forget=6.8022 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=13.3672 mle=1.5905 pcon=5.0335 forget=6.7433 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=13.3201 mle=1.5004 pcon=5.0313 forget=6.7883 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=13.4417 mle=1.7036 pcon=5.0294 forget=6.7087 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=13.2589 mle=1.5308 pcon=5.0279 forget=6.7002 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=13.3598 mle=1.6776 pcon=5.0260 forget=6.6562 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=13.6170 mle=1.7567 pcon=5.0239 forget=6.8364 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=13.1775 mle=1.4893 pcon=5.0220 forget=6.6662 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=13.2316 mle=1.6364 pcon=5.0201 forget=6.5751 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=13.3680 mle=1.6695 pcon=5.0180 forget=6.6805 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=13.6155 mle=1.8421 pcon=5.0166 forget=6.7569 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=13.2796 mle=1.5783 pcon=5.0150 forget=6.6862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=13.2832 mle=1.5674 pcon=5.0135 forget=6.7024 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=13.3052 mle=1.5600 pcon=5.0116 forget=6.7336 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=13.2498 mle=1.5581 pcon=5.0102 forget=6.6815 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=13.3480 mle=1.6071 pcon=5.0084 forget=6.7325 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=13.3770 mle=1.5901 pcon=5.0067 forget=6.7801 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=13.4452 mle=1.7541 pcon=5.0052 forget=6.6858 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=13.5032 mle=1.8520 pcon=5.0038 forget=6.6474 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=13.5155 mle=1.8596 pcon=5.0024 forget=6.6535 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=13.2730 mle=1.5827 pcon=5.0011 forget=6.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=13.1808 mle=1.5623 pcon=4.9999 forget=6.6187 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=13.4556 mle=1.6674 pcon=4.9985 forget=6.7897 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=13.3184 mle=1.5754 pcon=4.9971 forget=6.7459 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=13.2703 mle=1.6217 pcon=4.9957 forget=6.6529 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=13.1818 mle=1.5687 pcon=4.9945 forget=6.6187 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=13.3361 mle=1.6655 pcon=4.9928 forget=6.6777 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=13.4911 mle=1.7817 pcon=4.9917 forget=6.7177 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=13.2990 mle=1.6034 pcon=4.9908 forget=6.7048 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=13.6517 mle=1.9684 pcon=4.9894 forget=6.6938 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=13.2440 mle=1.6694 pcon=4.9882 forget=6.5864 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 12 it 20 total=13.2766 mle=1.6201 pcon=4.9868 forget=6.6696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=13.3727 mle=1.6955 pcon=4.9856 forget=6.6916 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=13.2768 mle=1.5475 pcon=4.9844 forget=6.7449 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=13.3447 mle=1.7430 pcon=4.9833 forget=6.6185 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=13.2164 mle=1.5973 pcon=4.9820 forget=6.6371 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=13.2395 mle=1.5344 pcon=4.9812 forget=6.7240 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=13.1741 mle=1.5184 pcon=4.9801 forget=6.6757 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=13.5663 mle=1.8647 pcon=4.9790 forget=6.7226 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=13.0845 mle=1.4301 pcon=4.9780 forget=6.6764 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=13.3128 mle=1.4944 pcon=4.9768 forget=6.8416 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=13.2188 mle=1.5695 pcon=4.9756 forget=6.6737 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=13.1638 mle=1.5512 pcon=4.9747 forget=6.6378 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=13.3089 mle=1.6448 pcon=4.9738 forget=6.6903 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=13.1651 mle=1.5838 pcon=4.9727 forget=6.6085 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=13.2390 mle=1.5878 pcon=4.9717 forget=6.6795 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=13.1790 mle=1.5762 pcon=4.9707 forget=6.6321 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=13.3224 mle=1.7665 pcon=4.9695 forget=6.5864 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=13.4105 mle=1.7679 pcon=4.9688 forget=6.6737 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=13.2152 mle=1.6144 pcon=4.9683 forget=6.6325 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=13.4969 mle=1.8838 pcon=4.9672 forget=6.6459 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=13.2133 mle=1.6178 pcon=4.9664 forget=6.6291 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=13.2042 mle=1.6681 pcon=4.9655 forget=6.5706 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=13.2352 mle=1.6816 pcon=4.9646 forget=6.5889 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=13.3628 mle=1.6808 pcon=4.9638 forget=6.7182 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=13.0435 mle=1.4896 pcon=4.9628 forget=6.5910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=13.1682 mle=1.5559 pcon=4.9621 forget=6.6503 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=13.4080 mle=1.7271 pcon=4.9612 forget=6.7197 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=13.1564 mle=1.6147 pcon=4.9605 forget=6.5812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=13.4906 mle=1.8790 pcon=4.9596 forget=6.6520 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=13.2176 mle=1.5926 pcon=4.9590 forget=6.6661 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=13.1656 mle=1.5417 pcon=4.9582 forget=6.6657 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 85%|████████▌ | 17/20 [05:57<01:02, 20.72s/it] 90%|█████████ | 18/20 [06:15<00:39, 19.74s/it] 95%|█████████▌| 19/20 [06:33<00:19, 19.37s/it]100%|██████████| 20/20 [06:59<00:00, 21.35s/it]100%|██████████| 20/20 [06:59<00:00, 20.99s/it]
[loss] ep 16 it 10 total=13.2043 mle=1.5872 pcon=4.9574 forget=6.6597 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=13.0506 mle=1.4959 pcon=4.9567 forget=6.5980 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=13.0753 mle=1.5130 pcon=4.9561 forget=6.6061 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=13.2334 mle=1.7079 pcon=4.9552 forget=6.5703 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=13.0742 mle=1.5769 pcon=4.9543 forget=6.5429 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=13.4570 mle=1.9452 pcon=4.9534 forget=6.5583 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=13.3504 mle=1.7862 pcon=4.9527 forget=6.6115 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=13.1549 mle=1.6098 pcon=4.9520 forget=6.5931 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 17 it 20 total=13.3660 mle=1.7677 pcon=4.9510 forget=6.6473 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=13.3211 mle=1.8245 pcon=4.9506 forget=6.5460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=13.1411 mle=1.5761 pcon=4.9497 forget=6.6154 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=13.1350 mle=1.5782 pcon=4.9487 forget=6.6080 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=13.2673 mle=1.6656 pcon=4.9479 forget=6.6539 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=13.0374 mle=1.4638 pcon=4.9477 forget=6.6259 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=13.3409 mle=1.8039 pcon=4.9471 forget=6.5899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=13.2534 mle=1.7463 pcon=4.9461 forget=6.5610 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=13.0648 mle=1.5831 pcon=4.9460 forget=6.5358 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=13.2655 mle=1.6934 pcon=4.9454 forget=6.6268 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=13.2366 mle=1.6612 pcon=4.9447 forget=6.6306 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=13.1172 mle=1.5498 pcon=4.9442 forget=6.6232 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=13.0965 mle=1.5914 pcon=4.9437 forget=6.5614 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=13.2336 mle=1.6862 pcon=4.9432 forget=6.6042 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=13.3203 mle=1.8018 pcon=4.9426 forget=6.5759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=13.2748 mle=1.6992 pcon=4.9424 forget=6.6333 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 19 it 40 total=13.0482 mle=1.4680 pcon=4.9422 forget=6.6381 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=13.3286 mle=1.7014 pcon=4.9421 forget=6.6851 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=13.1523 mle=1.6074 pcon=4.9416 forget=6.6034 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=13.1252 mle=1.6396 pcon=4.9410 forget=6.5447 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=13.3770 mle=1.8518 pcon=4.9406 forget=6.5847 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=13.1548 mle=1.5530 pcon=4.9404 forget=6.6614 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=13.3040 mle=1.6001 pcon=4.9402 forget=6.7637 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:03,  3.17it/s]  3%|▎         | 10/391 [00:00<00:12, 30.01it/s]  5%|▍         | 19/391 [00:00<00:07, 48.15it/s]  7%|▋         | 28/391 [00:00<00:06, 60.47it/s]  9%|▉         | 37/391 [00:00<00:05, 67.76it/s] 12%|█▏        | 46/391 [00:00<00:04, 73.75it/s] 14%|█▍        | 55/391 [00:00<00:04, 78.09it/s] 17%|█▋        | 65/391 [00:01<00:03, 82.14it/s] 19%|█▉        | 74/391 [00:01<00:03, 83.91it/s] 21%|██        | 83/391 [00:01<00:03, 85.43it/s] 24%|██▎       | 92/391 [00:01<00:03, 84.92it/s] 26%|██▌       | 101/391 [00:01<00:03, 85.74it/s] 28%|██▊       | 110/391 [00:01<00:03, 86.45it/s] 30%|███       | 119/391 [00:01<00:03, 85.36it/s] 33%|███▎      | 128/391 [00:01<00:03, 85.14it/s] 35%|███▌      | 137/391 [00:01<00:02, 85.06it/s] 37%|███▋      | 146/391 [00:01<00:02, 84.93it/s] 40%|███▉      | 155/391 [00:02<00:02, 85.52it/s] 42%|████▏     | 164/391 [00:02<00:02, 86.48it/s] 44%|████▍     | 173/391 [00:02<00:02, 85.42it/s] 47%|████▋     | 182/391 [00:02<00:02, 86.35it/s] 49%|████▉     | 191/391 [00:02<00:02, 86.44it/s] 51%|█████     | 200/391 [00:02<00:02, 87.15it/s] 53%|█████▎    | 209/391 [00:02<00:02, 87.55it/s] 56%|█████▌    | 218/391 [00:02<00:01, 87.44it/s] 58%|█████▊    | 227/391 [00:02<00:01, 87.22it/s] 60%|██████    | 236/391 [00:03<00:01, 87.73it/s] 63%|██████▎   | 246/391 [00:03<00:01, 88.87it/s] 65%|██████▌   | 255/391 [00:03<00:01, 88.64it/s] 68%|██████▊   | 264/391 [00:03<00:01, 88.64it/s] 70%|██████▉   | 273/391 [00:03<00:01, 88.48it/s] 72%|███████▏  | 282/391 [00:03<00:01, 88.49it/s] 74%|███████▍  | 291/391 [00:03<00:01, 88.70it/s] 77%|███████▋  | 300/391 [00:03<00:01, 88.77it/s] 79%|███████▉  | 309/391 [00:03<00:00, 88.85it/s] 81%|████████▏ | 318/391 [00:03<00:00, 87.73it/s] 84%|████████▎ | 327/391 [00:04<00:00, 86.98it/s] 86%|████████▌ | 336/391 [00:04<00:00, 87.17it/s] 88%|████████▊ | 345/391 [00:04<00:00, 87.68it/s] 91%|█████████ | 354/391 [00:04<00:00, 87.85it/s] 93%|█████████▎| 363/391 [00:04<00:00, 87.98it/s] 95%|█████████▌| 372/391 [00:04<00:00, 87.76it/s] 97%|█████████▋| 381/391 [00:04<00:00, 85.27it/s]100%|█████████▉| 390/391 [00:04<00:00, 86.62it/s]100%|██████████| 391/391 [00:04<00:00, 81.56it/s]
50000 images processed, 4.876374006271362 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:22,  3.41it/s] 11%|█▏        | 9/79 [00:00<00:02, 27.81it/s] 23%|██▎       | 18/79 [00:00<00:01, 46.98it/s] 34%|███▍      | 27/79 [00:00<00:00, 59.83it/s] 46%|████▌     | 36/79 [00:00<00:00, 68.13it/s] 57%|█████▋    | 45/79 [00:00<00:00, 74.08it/s] 68%|██████▊   | 54/79 [00:00<00:00, 77.72it/s] 80%|███████▉  | 63/79 [00:01<00:00, 80.71it/s] 91%|█████████ | 72/79 [00:01<00:00, 83.16it/s]100%|██████████| 79/79 [00:01<00:00, 65.19it/s]
10000 images processed, 1.2347204685211182 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<00:52,  3.89it/s]  5%|▍         | 10/204 [00:00<00:05, 33.47it/s]  9%|▉         | 19/204 [00:00<00:03, 50.74it/s] 14%|█▎        | 28/204 [00:00<00:02, 62.36it/s] 18%|█▊        | 37/204 [00:00<00:02, 69.32it/s] 23%|██▎       | 46/204 [00:00<00:02, 74.37it/s] 27%|██▋       | 55/204 [00:00<00:01, 77.78it/s] 31%|███▏      | 64/204 [00:01<00:01, 80.39it/s] 36%|███▌      | 73/204 [00:01<00:01, 81.67it/s] 40%|████      | 82/204 [00:01<00:01, 82.95it/s] 45%|████▍     | 91/204 [00:01<00:01, 83.86it/s] 49%|████▉     | 100/204 [00:01<00:01, 84.85it/s] 53%|█████▎    | 109/204 [00:01<00:01, 85.43it/s] 58%|█████▊    | 118/204 [00:01<00:01, 83.92it/s] 62%|██████▏   | 127/204 [00:01<00:00, 84.69it/s] 67%|██████▋   | 136/204 [00:01<00:00, 85.16it/s] 71%|███████   | 145/204 [00:01<00:00, 85.42it/s] 75%|███████▌  | 154/204 [00:02<00:00, 85.96it/s] 80%|███████▉  | 163/204 [00:02<00:00, 85.90it/s] 84%|████████▍ | 172/204 [00:02<00:00, 86.04it/s] 89%|████████▊ | 181/204 [00:02<00:00, 85.18it/s] 93%|█████████▎| 190/204 [00:02<00:00, 85.58it/s] 98%|█████████▊| 199/204 [00:02<00:00, 86.39it/s]100%|██████████| 204/204 [00:02<00:00, 77.39it/s]
26032 images processed, 2.6813478469848633 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:36,  2.13it/s] 11%|█▏        | 9/79 [00:00<00:03, 18.10it/s] 22%|██▏       | 17/79 [00:00<00:02, 28.96it/s] 32%|███▏      | 25/79 [00:00<00:01, 36.58it/s] 42%|████▏     | 33/79 [00:01<00:01, 41.81it/s] 52%|█████▏    | 41/79 [00:01<00:00, 43.66it/s] 61%|██████    | 48/79 [00:01<00:00, 49.02it/s] 68%|██████▊   | 54/79 [00:01<00:00, 46.42it/s] 76%|███████▌  | 60/79 [00:01<00:00, 44.70it/s] 82%|████████▏ | 65/79 [00:01<00:00, 44.17it/s] 92%|█████████▏| 73/79 [00:01<00:00, 45.45it/s]100%|██████████| 79/79 [00:01<00:00, 39.92it/s]
10000 images processed, 2.014153242111206 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:31,  2.47it/s] 13%|█▎        | 10/79 [00:00<00:02, 25.09it/s] 24%|██▍       | 19/79 [00:00<00:01, 42.15it/s] 35%|███▌      | 28/79 [00:00<00:00, 54.83it/s] 47%|████▋     | 37/79 [00:00<00:00, 63.98it/s] 58%|█████▊    | 46/79 [00:00<00:00, 70.52it/s] 70%|██████▉   | 55/79 [00:01<00:00, 75.25it/s] 81%|████████  | 64/79 [00:01<00:00, 78.44it/s] 92%|█████████▏| 73/79 [00:01<00:00, 81.21it/s]100%|██████████| 79/79 [00:01<00:00, 61.00it/s]
10000 images processed, 1.3162057399749756 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:26,  2.58it/s] 14%|█▍        | 10/70 [00:00<00:02, 25.91it/s] 27%|██▋       | 19/70 [00:00<00:01, 43.36it/s] 40%|████      | 28/70 [00:00<00:00, 55.92it/s] 53%|█████▎    | 37/70 [00:00<00:00, 64.87it/s] 66%|██████▌   | 46/70 [00:00<00:00, 71.25it/s] 79%|███████▊  | 55/70 [00:01<00:00, 76.00it/s] 91%|█████████▏| 64/70 [00:01<00:00, 79.57it/s]100%|██████████| 70/70 [00:01<00:00, 59.18it/s]
8925 images processed, 1.2152378559112549 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:42,  1.05it/s] 16%|█▌        | 7/45 [00:01<00:04,  8.61it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.55it/s] 36%|███▌      | 16/45 [00:01<00:02, 10.83it/s] 40%|████      | 18/45 [00:02<00:02, 10.12it/s] 53%|█████▎    | 24/45 [00:02<00:01, 11.56it/s] 58%|█████▊    | 26/45 [00:02<00:01, 11.36it/s] 71%|███████   | 32/45 [00:03<00:00, 13.63it/s] 76%|███████▌  | 34/45 [00:03<00:01,  9.70it/s] 91%|█████████ | 41/45 [00:04<00:00, 10.86it/s]100%|██████████| 45/45 [00:04<00:00, 10.86it/s]
5640 images processed, 4.1676976680755615 seconds used

19.18843913078308
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.51  99.36
places365     67.91  81.18
LSUN          17.50  96.09
iSUN          72.35  81.72
dtd           37.89  91.39
forget        79.50  86.90
AVG           46.28  89.44
Retain-Acc: 0.7406
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
24.449015855789185
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=1.0 lr=0.0001 epochs=50
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=1.0, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:21<17:38, 21.60s/it]  4%|▍         | 2/50 [00:40<16:07, 20.16s/it]  6%|▌         | 3/50 [01:05<17:33, 22.41s/it]  8%|▊         | 4/50 [01:26<16:43, 21.81s/it] 10%|█         | 5/50 [01:44<15:23, 20.52s/it] 12%|█▏        | 6/50 [02:05<14:57, 20.41s/it] 14%|█▍        | 7/50 [02:32<16:08, 22.53s/it] 16%|█▌        | 8/50 [02:50<14:52, 21.25s/it][loss] ep 0 it 0 total=13.7435 mle=1.5710 pcon=5.2950 forget=6.8775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=13.8370 mle=1.5426 pcon=5.2879 forget=7.0066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=13.8514 mle=1.7005 pcon=5.2809 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=14.0217 mle=1.8997 pcon=5.2738 forget=6.8482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=13.8932 mle=1.7132 pcon=5.2670 forget=6.9131 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=13.6546 mle=1.5021 pcon=5.2603 forget=6.8922 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=13.7233 mle=1.5596 pcon=5.2540 forget=6.9096 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=13.8769 mle=1.6809 pcon=5.2476 forget=6.9484 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=13.9627 mle=1.6715 pcon=5.2409 forget=7.0503 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=13.7067 mle=1.6371 pcon=5.2346 forget=6.8350 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=13.6924 mle=1.5166 pcon=5.2285 forget=6.9474 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=13.9496 mle=1.7802 pcon=5.2224 forget=6.9470 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=13.9078 mle=1.7646 pcon=5.2167 forget=6.9265 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=13.6582 mle=1.6054 pcon=5.2112 forget=6.8415 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=13.8209 mle=1.7242 pcon=5.2056 forget=6.8911 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=13.9253 mle=1.7944 pcon=5.2003 forget=6.9307 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=13.6320 mle=1.5528 pcon=5.1951 forget=6.8842 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=13.8667 mle=1.8884 pcon=5.1899 forget=6.7883 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=13.8056 mle=1.7415 pcon=5.1847 forget=6.8794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=13.5310 mle=1.5265 pcon=5.1797 forget=6.8248 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=13.6901 mle=1.5858 pcon=5.1746 forget=6.9297 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=13.9071 mle=1.8837 pcon=5.1699 forget=6.8535 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=13.5460 mle=1.5926 pcon=5.1652 forget=6.7882 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=13.7962 mle=1.8116 pcon=5.1605 forget=6.8241 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=13.7162 mle=1.7062 pcon=5.1560 forget=6.8540 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=13.7193 mle=1.6709 pcon=5.1519 forget=6.8964 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=13.7439 mle=1.7668 pcon=5.1476 forget=6.8295 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=13.8736 mle=1.9185 pcon=5.1436 forget=6.8115 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=13.5093 mle=1.5701 pcon=5.1396 forget=6.7996 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=13.7529 mle=1.8363 pcon=5.1354 forget=6.7812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=13.7116 mle=1.7033 pcon=5.1312 forget=6.8771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=13.7154 mle=1.6961 pcon=5.1276 forget=6.8917 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=13.4887 mle=1.6542 pcon=5.1238 forget=6.7107 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=13.6311 mle=1.7299 pcon=5.1198 forget=6.7814 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=13.5970 mle=1.6500 pcon=5.1161 forget=6.8308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=13.3951 mle=1.4537 pcon=5.1127 forget=6.8287 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=13.6591 mle=1.7677 pcon=5.1088 forget=6.7826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=13.3639 mle=1.4913 pcon=5.1053 forget=6.7673 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=13.7997 mle=1.8720 pcon=5.1018 forget=6.8259 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=13.7525 mle=1.8674 pcon=5.0985 forget=6.7866 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=13.6250 mle=1.7997 pcon=5.0949 forget=6.7304 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=13.4767 mle=1.5840 pcon=5.0918 forget=6.8009 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=13.4780 mle=1.5956 pcon=5.0883 forget=6.7940 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=13.5058 mle=1.6914 pcon=5.0854 forget=6.7290 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=13.5505 mle=1.7735 pcon=5.0825 forget=6.6945 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=13.4314 mle=1.5965 pcon=5.0794 forget=6.7555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=13.5683 mle=1.7468 pcon=5.0765 forget=6.7449 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=13.3357 mle=1.4012 pcon=5.0737 forget=6.8608 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=13.2503 mle=1.5359 pcon=5.0709 forget=6.6435 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=13.4679 mle=1.5916 pcon=5.0683 forget=6.8080 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=13.4616 mle=1.6654 pcon=5.0655 forget=6.7306 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=13.4028 mle=1.6303 pcon=5.0633 forget=6.7092 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=13.6642 mle=1.8233 pcon=5.0604 forget=6.7805 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=13.4129 mle=1.5352 pcon=5.0581 forget=6.8197 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=13.4688 mle=1.6751 pcon=5.0556 forget=6.7381 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=13.3821 mle=1.6307 pcon=5.0530 forget=6.6984 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=13.4715 mle=1.7450 pcon=5.0506 forget=6.6759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=13.3862 mle=1.6271 pcon=5.0481 forget=6.7111 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=13.2730 mle=1.5821 pcon=5.0458 forget=6.6451 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=13.4598 mle=1.6544 pcon=5.0438 forget=6.7615 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=13.6462 mle=1.7958 pcon=5.0417 forget=6.8086 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=13.5803 mle=1.7420 pcon=5.0393 forget=6.7990 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=13.4096 mle=1.6405 pcon=5.0373 forget=6.7319 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 18%|█▊        | 9/50 [03:08<13:54, 20.36s/it] 20%|██        | 10/50 [03:33<14:23, 21.59s/it] 22%|██▏       | 11/50 [03:56<14:18, 22.01s/it] 24%|██▍       | 12/50 [04:14<13:15, 20.93s/it] 26%|██▌       | 13/50 [04:34<12:36, 20.44s/it] 28%|██▊       | 14/50 [05:01<13:28, 22.47s/it] 30%|███       | 15/50 [05:19<12:18, 21.10s/it] 32%|███▏      | 16/50 [05:37<11:33, 20.40s/it][loss] ep 8 it 30 total=13.3746 mle=1.5371 pcon=5.0354 forget=6.8021 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=13.3672 mle=1.5905 pcon=5.0335 forget=6.7433 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=13.3200 mle=1.5003 pcon=5.0313 forget=6.7884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=13.4417 mle=1.7036 pcon=5.0294 forget=6.7087 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=13.2590 mle=1.5309 pcon=5.0279 forget=6.7002 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=13.3599 mle=1.6776 pcon=5.0260 forget=6.6563 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=13.6167 mle=1.7565 pcon=5.0239 forget=6.8364 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=13.1774 mle=1.4892 pcon=5.0220 forget=6.6662 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=13.2316 mle=1.6365 pcon=5.0201 forget=6.5751 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=13.3679 mle=1.6694 pcon=5.0180 forget=6.6805 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=13.6154 mle=1.8420 pcon=5.0166 forget=6.7569 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=13.2797 mle=1.5784 pcon=5.0150 forget=6.6863 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=13.2831 mle=1.5673 pcon=5.0135 forget=6.7024 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=13.3051 mle=1.5598 pcon=5.0116 forget=6.7336 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=13.2497 mle=1.5582 pcon=5.0102 forget=6.6813 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=13.3482 mle=1.6073 pcon=5.0084 forget=6.7325 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=13.3770 mle=1.5902 pcon=5.0067 forget=6.7800 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=13.4451 mle=1.7541 pcon=5.0052 forget=6.6858 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=13.5031 mle=1.8519 pcon=5.0038 forget=6.6474 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=13.5155 mle=1.8596 pcon=5.0024 forget=6.6535 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=13.2729 mle=1.5826 pcon=5.0011 forget=6.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=13.1808 mle=1.5623 pcon=4.9999 forget=6.6186 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=13.4556 mle=1.6674 pcon=4.9985 forget=6.7897 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=13.3183 mle=1.5754 pcon=4.9971 forget=6.7458 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=13.2701 mle=1.6216 pcon=4.9957 forget=6.6528 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=13.1817 mle=1.5687 pcon=4.9945 forget=6.6185 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=13.3360 mle=1.6656 pcon=4.9928 forget=6.6776 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=13.4912 mle=1.7821 pcon=4.9917 forget=6.7175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=13.2989 mle=1.6036 pcon=4.9908 forget=6.7045 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=13.6515 mle=1.9687 pcon=4.9894 forget=6.6934 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=13.2434 mle=1.6693 pcon=4.9882 forget=6.5859 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 12 it 20 total=13.2762 mle=1.6204 pcon=4.9868 forget=6.6691 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=13.3719 mle=1.6956 pcon=4.9856 forget=6.6907 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=13.2760 mle=1.5477 pcon=4.9843 forget=6.7439 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=13.3460 mle=1.7454 pcon=4.9832 forget=6.6174 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=13.2150 mle=1.5972 pcon=4.9819 forget=6.6359 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=13.2376 mle=1.5345 pcon=4.9812 forget=6.7220 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=13.1713 mle=1.5180 pcon=4.9800 forget=6.6732 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=13.5631 mle=1.8646 pcon=4.9789 forget=6.7196 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=13.0809 mle=1.4299 pcon=4.9779 forget=6.6732 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=13.3094 mle=1.4943 pcon=4.9768 forget=6.8384 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=13.2142 mle=1.5691 pcon=4.9755 forget=6.6696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=13.1580 mle=1.5510 pcon=4.9746 forget=6.6324 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=13.3022 mle=1.6446 pcon=4.9737 forget=6.6839 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=13.1581 mle=1.5841 pcon=4.9726 forget=6.6015 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=13.2309 mle=1.5871 pcon=4.9715 forget=6.6723 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=13.1696 mle=1.5757 pcon=4.9705 forget=6.6234 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=13.3136 mle=1.7665 pcon=4.9693 forget=6.5778 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=13.4002 mle=1.7680 pcon=4.9685 forget=6.6637 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=13.2024 mle=1.6141 pcon=4.9680 forget=6.6203 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=13.4815 mle=1.8842 pcon=4.9669 forget=6.6304 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=13.1981 mle=1.6181 pcon=4.9661 forget=6.6139 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=13.1853 mle=1.6670 pcon=4.9651 forget=6.5532 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=13.2197 mle=1.6815 pcon=4.9641 forget=6.5740 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=13.3492 mle=1.6877 pcon=4.9633 forget=6.6982 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=13.0236 mle=1.4884 pcon=4.9622 forget=6.5730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=13.1483 mle=1.5639 pcon=4.9614 forget=6.6229 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=13.3746 mle=1.7236 pcon=4.9605 forget=6.6905 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=13.1287 mle=1.6129 pcon=4.9596 forget=6.5562 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=13.4597 mle=1.8765 pcon=4.9587 forget=6.6245 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=13.1869 mle=1.5916 pcon=4.9579 forget=6.6374 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=13.1277 mle=1.5386 pcon=4.9571 forget=6.6320 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 34%|███▍      | 17/50 [06:00<11:35, 21.07s/it] 36%|███▌      | 18/50 [06:24<11:39, 21.85s/it] 38%|███▊      | 19/50 [06:43<10:53, 21.09s/it] 40%|████      | 20/50 [07:01<10:08, 20.30s/it] 42%|████▏     | 21/50 [07:29<10:51, 22.46s/it] 44%|████▍     | 22/50 [07:48<09:59, 21.42s/it] 46%|████▌     | 23/50 [08:06<09:09, 20.34s/it] 48%|████▊     | 24/50 [08:27<08:57, 20.69s/it] 50%|█████     | 25/50 [08:52<09:08, 21.96s/it][loss] ep 16 it 10 total=13.1677 mle=1.5870 pcon=4.9561 forget=6.6245 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=13.0212 mle=1.4980 pcon=4.9553 forget=6.5679 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=13.0375 mle=1.5125 pcon=4.9546 forget=6.5704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=13.2128 mle=1.7118 pcon=4.9536 forget=6.5473 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=13.0452 mle=1.5795 pcon=4.9525 forget=6.5132 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=13.4201 mle=1.9432 pcon=4.9515 forget=6.5255 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=13.3146 mle=1.7838 pcon=4.9507 forget=6.5801 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=13.1312 mle=1.6044 pcon=4.9498 forget=6.5769 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 17 it 20 total=13.3300 mle=1.7661 pcon=4.9486 forget=6.6153 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=13.2840 mle=1.8220 pcon=4.9481 forget=6.5140 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=13.1026 mle=1.5743 pcon=4.9470 forget=6.5812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=13.0983 mle=1.5824 pcon=4.9458 forget=6.5701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=13.2352 mle=1.6654 pcon=4.9448 forget=6.6250 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=13.0026 mle=1.4578 pcon=4.9445 forget=6.6003 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=13.3189 mle=1.8080 pcon=4.9436 forget=6.5672 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=13.2351 mle=1.7515 pcon=4.9426 forget=6.5410 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=13.0309 mle=1.5815 pcon=4.9422 forget=6.5072 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=13.2614 mle=1.6876 pcon=4.9415 forget=6.6323 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=13.2217 mle=1.6606 pcon=4.9406 forget=6.6205 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=13.0956 mle=1.5490 pcon=4.9399 forget=6.6067 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=13.0954 mle=1.5919 pcon=4.9392 forget=6.5642 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=13.2338 mle=1.6817 pcon=4.9386 forget=6.6135 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=13.3247 mle=1.8142 pcon=4.9379 forget=6.5727 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=13.2709 mle=1.6995 pcon=4.9375 forget=6.6339 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 40 total=13.0469 mle=1.4653 pcon=4.9371 forget=6.6444 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=13.3364 mle=1.7017 pcon=4.9369 forget=6.6978 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=13.1747 mle=1.6014 pcon=4.9362 forget=6.6371 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=13.1653 mle=1.6416 pcon=4.9355 forget=6.5883 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=13.4001 mle=1.8571 pcon=4.9349 forget=6.6081 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=13.1804 mle=1.5500 pcon=4.9347 forget=6.6958 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=13.3269 mle=1.6062 pcon=4.9344 forget=6.7863 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 0 total=13.2315 mle=1.6585 pcon=4.9338 forget=6.6393 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=13.2293 mle=1.6682 pcon=4.9333 forget=6.6278 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=13.3592 mle=1.7303 pcon=4.9330 forget=6.6959 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=13.4068 mle=1.8361 pcon=4.9324 forget=6.6383 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=13.1885 mle=1.6568 pcon=4.9317 forget=6.6000 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=13.5797 mle=2.0204 pcon=4.9310 forget=6.6282 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=13.3062 mle=1.6597 pcon=4.9303 forget=6.7161 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=13.0017 mle=1.4021 pcon=4.9298 forget=6.6698 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 10 total=13.0485 mle=1.4088 pcon=4.9293 forget=6.7105 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=13.3598 mle=1.6762 pcon=4.9289 forget=6.7547 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=13.2111 mle=1.5065 pcon=4.9286 forget=6.7760 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=13.1348 mle=1.5626 pcon=4.9284 forget=6.6438 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=13.4706 mle=1.7678 pcon=4.9281 forget=6.7747 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=13.3514 mle=1.6764 pcon=4.9277 forget=6.7473 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=13.2924 mle=1.6220 pcon=4.9272 forget=6.7433 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=13.3873 mle=1.6584 pcon=4.9269 forget=6.8021 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 20 total=13.3392 mle=1.6651 pcon=4.9263 forget=6.7478 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=13.4361 mle=1.8114 pcon=4.9258 forget=6.6988 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=13.2582 mle=1.5249 pcon=4.9252 forget=6.8081 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=13.3439 mle=1.6465 pcon=4.9250 forget=6.7724 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=13.4077 mle=1.6784 pcon=4.9247 forget=6.8045 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=13.2666 mle=1.5169 pcon=4.9244 forget=6.8253 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=13.3160 mle=1.6378 pcon=4.9242 forget=6.7540 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=13.8088 mle=2.0733 pcon=4.9238 forget=6.8117 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 30 total=13.2751 mle=1.5603 pcon=4.9240 forget=6.7908 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=13.3898 mle=1.7022 pcon=4.9236 forget=6.7640 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=13.3555 mle=1.5563 pcon=4.9232 forget=6.8760 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=13.4223 mle=1.7517 pcon=4.9229 forget=6.7476 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=13.4070 mle=1.6695 pcon=4.9229 forget=6.8146 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=13.3312 mle=1.5642 pcon=4.9225 forget=6.8445 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=13.3587 mle=1.6663 pcon=4.9223 forget=6.7701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=13.3092 mle=1.5347 pcon=4.9219 forget=6.8526 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 40 total=13.2081 mle=1.5271 pcon=4.9219 forget=6.7591 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=13.3319 mle=1.6025 pcon=4.9218 forget=6.8077 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=13.2097 mle=1.4245 pcon=4.9216 forget=6.8636 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=13.4059 mle=1.6612 pcon=4.9216 forget=6.8230 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 240 total=13.3917 mle=1.6106 pcon=4.9215 forget=6.8596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=13.6042 mle=1.7929 pcon=4.9213 forget=6.8900 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=13.4601 mle=1.7282 pcon=4.9211 forget=6.8108 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 0 total=13.4606 mle=1.6748 pcon=4.9208 forget=6.8650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=13.6591 mle=1.8125 pcon=4.9209 forget=6.9257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=13.3312 mle=1.6311 pcon=4.9206 forget=6.7795 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 150 total=13.5570 mle=1.8145 pcon=4.9209 forget=6.8216 nr=64 nf=64 protos=540 fproto_sim=NA
 52%|█████▏    | 26/50 [09:11<08:23, 20.97s/it] 54%|█████▍    | 27/50 [09:29<07:41, 20.04s/it] 56%|█████▌    | 28/50 [09:53<07:49, 21.34s/it] 58%|█████▊    | 29/50 [10:15<07:31, 21.50s/it] 60%|██████    | 30/50 [10:34<06:54, 20.72s/it] 62%|██████▏   | 31/50 [10:54<06:29, 20.50s/it] 64%|██████▍   | 32/50 [11:20<06:39, 22.21s/it] 66%|██████▌   | 33/50 [11:39<05:59, 21.13s/it] 68%|██████▊   | 34/50 [11:57<05:23, 20.20s/it] 70%|███████   | 35/50 [12:20<05:14, 20.98s/it][loss] ep 25 it 200 total=13.6315 mle=1.8241 pcon=4.9207 forget=6.8867 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=13.5695 mle=1.7870 pcon=4.9205 forget=6.8620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=13.3479 mle=1.5934 pcon=4.9204 forget=6.8342 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=13.3910 mle=1.6048 pcon=4.9206 forget=6.8656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 10 total=13.3380 mle=1.5175 pcon=4.9206 forget=6.8999 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=13.4082 mle=1.5635 pcon=4.9206 forget=6.9241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=13.5130 mle=1.7717 pcon=4.9206 forget=6.8207 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=13.2161 mle=1.4614 pcon=4.9205 forget=6.8342 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=13.4754 mle=1.6664 pcon=4.9201 forget=6.8890 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=13.3322 mle=1.5311 pcon=4.9203 forget=6.8808 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 310 total=13.6874 mle=1.9204 pcon=4.9204 forget=6.8467 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=13.3601 mle=1.6096 pcon=4.9199 forget=6.8306 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 20 total=13.4939 mle=1.6880 pcon=4.9198 forget=6.8862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=13.3557 mle=1.5652 pcon=4.9195 forget=6.8710 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=13.3397 mle=1.5289 pcon=4.9193 forget=6.8915 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=13.4037 mle=1.5774 pcon=4.9189 forget=6.9073 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=13.3862 mle=1.5922 pcon=4.9188 forget=6.8753 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=13.4371 mle=1.6508 pcon=4.9185 forget=6.8678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=13.5102 mle=1.7266 pcon=4.9183 forget=6.8653 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=13.5970 mle=1.7682 pcon=4.9183 forget=6.9105 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 30 total=13.5079 mle=1.7086 pcon=4.9183 forget=6.8811 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=13.5452 mle=1.7517 pcon=4.9184 forget=6.8751 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=13.3751 mle=1.5443 pcon=4.9184 forget=6.9123 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=13.1943 mle=1.4687 pcon=4.9183 forget=6.8073 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=13.1346 mle=1.3872 pcon=4.9180 forget=6.8294 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=13.3482 mle=1.5336 pcon=4.9177 forget=6.8969 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=13.5828 mle=1.7793 pcon=4.9172 forget=6.8863 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=13.6004 mle=1.7494 pcon=4.9170 forget=6.9341 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 40 total=13.5550 mle=1.7556 pcon=4.9166 forget=6.8828 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=13.3121 mle=1.5985 pcon=4.9163 forget=6.7973 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=13.4444 mle=1.6581 pcon=4.9163 forget=6.8700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=13.3362 mle=1.5606 pcon=4.9161 forget=6.8595 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=13.2563 mle=1.4624 pcon=4.9160 forget=6.8779 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=13.5711 mle=1.8581 pcon=4.9161 forget=6.7970 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=13.4221 mle=1.6571 pcon=4.9162 forget=6.8488 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 0 total=13.3422 mle=1.6104 pcon=4.9162 forget=6.8157 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=13.4121 mle=1.6148 pcon=4.9163 forget=6.8810 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=13.2276 mle=1.4673 pcon=4.9161 forget=6.8442 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=13.6675 mle=1.9004 pcon=4.9159 forget=6.8513 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=13.3906 mle=1.4873 pcon=4.9158 forget=6.9875 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=13.5920 mle=1.7768 pcon=4.9156 forget=6.8995 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=13.2922 mle=1.4872 pcon=4.9156 forget=6.8894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=13.7901 mle=1.9764 pcon=4.9156 forget=6.8981 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 10 total=13.3572 mle=1.6022 pcon=4.9156 forget=6.8394 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=13.3702 mle=1.5936 pcon=4.9157 forget=6.8610 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=13.4311 mle=1.6176 pcon=4.9157 forget=6.8978 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=13.3436 mle=1.5795 pcon=4.9157 forget=6.8483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=13.2836 mle=1.5560 pcon=4.9157 forget=6.8119 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 260 total=13.1926 mle=1.4843 pcon=4.9154 forget=6.7929 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=13.3841 mle=1.5868 pcon=4.9154 forget=6.8818 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=13.2927 mle=1.5872 pcon=4.9153 forget=6.7902 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 20 total=13.4375 mle=1.7429 pcon=4.9151 forget=6.7795 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=13.3719 mle=1.6953 pcon=4.9147 forget=6.7620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=13.3425 mle=1.5112 pcon=4.9146 forget=6.9168 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 170 total=13.3682 mle=1.6014 pcon=4.9148 forget=6.8520 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=13.5474 mle=1.8485 pcon=4.9149 forget=6.7840 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=13.3077 mle=1.5863 pcon=4.9149 forget=6.8065 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=13.2070 mle=1.4986 pcon=4.9148 forget=6.7936 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=13.1678 mle=1.4319 pcon=4.9152 forget=6.8207 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 30 total=13.4522 mle=1.6787 pcon=4.9152 forget=6.8582 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=13.6984 mle=2.0413 pcon=4.9151 forget=6.7419 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=13.2598 mle=1.5161 pcon=4.9150 forget=6.8286 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=13.6422 mle=1.8426 pcon=4.9150 forget=6.8846 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=13.2801 mle=1.5419 pcon=4.9150 forget=6.8232 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=13.3264 mle=1.5860 pcon=4.9150 forget=6.8254 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=13.3368 mle=1.6402 pcon=4.9153 forget=6.7813 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 380 total=13.3265 mle=1.5376 pcon=4.9153 forget=6.8736 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 40 total=13.5198 mle=1.7800 pcon=4.9150 forget=6.8248 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=13.1322 mle=1.4062 pcon=4.9147 forget=6.8113 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 140 total=13.2246 mle=1.6006 pcon=4.9148 forget=6.7092 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=13.4002 mle=1.7227 pcon=4.9147 forget=6.7627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 240 total=13.5849 mle=1.8900 pcon=4.9145 forget=6.7803 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=13.3374 mle=1.6000 pcon=4.9141 forget=6.8233 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=13.3542 mle=1.5856 pcon=4.9141 forget=6.8545 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 0 total=13.4186 mle=1.7280 pcon=4.9140 forget=6.7765 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 50 total=13.1914 mle=1.5104 pcon=4.9140 forget=6.7669 nr=64 nf=64 protos=540 fproto_sim=NA
 72%|███████▏  | 36/50 [12:43<05:05, 21.84s/it] 74%|███████▍  | 37/50 [13:01<04:29, 20.70s/it] 76%|███████▌  | 38/50 [13:20<04:00, 20.02s/it] 78%|███████▊  | 39/50 [13:46<03:58, 21.73s/it] 80%|████████  | 40/50 [14:05<03:31, 21.14s/it] 82%|████████▏ | 41/50 [14:23<03:01, 20.14s/it] 84%|████████▍ | 42/50 [14:45<02:45, 20.64s/it] 86%|████████▌ | 43/50 [15:10<02:34, 22.02s/it] 88%|████████▊ | 44/50 [15:29<02:06, 21.03s/it] 90%|█████████ | 45/50 [15:47<01:41, 20.30s/it][loss] ep 35 it 100 total=13.5275 mle=1.8012 pcon=4.9136 forget=6.8127 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 150 total=13.3445 mle=1.6996 pcon=4.9136 forget=6.7313 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 200 total=13.4148 mle=1.6309 pcon=4.9136 forget=6.8703 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 250 total=13.4499 mle=1.7083 pcon=4.9133 forget=6.8282 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 300 total=13.1175 mle=1.4692 pcon=4.9132 forget=6.7351 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 350 total=13.4560 mle=1.7612 pcon=4.9131 forget=6.7816 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 10 total=13.6009 mle=1.8921 pcon=4.9131 forget=6.7957 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 60 total=13.1706 mle=1.5159 pcon=4.9132 forget=6.7414 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 110 total=13.3499 mle=1.6684 pcon=4.9131 forget=6.7684 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 160 total=13.3534 mle=1.6873 pcon=4.9129 forget=6.7532 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 210 total=13.1224 mle=1.4766 pcon=4.9131 forget=6.7328 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 260 total=13.3465 mle=1.5793 pcon=4.9134 forget=6.8538 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 310 total=13.5470 mle=1.7669 pcon=4.9131 forget=6.8670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 360 total=13.2223 mle=1.5137 pcon=4.9130 forget=6.7957 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 20 total=13.1469 mle=1.4775 pcon=4.9126 forget=6.7568 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 70 total=13.5258 mle=1.7744 pcon=4.9127 forget=6.8388 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 120 total=13.3291 mle=1.6117 pcon=4.9124 forget=6.8049 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 170 total=13.3364 mle=1.6815 pcon=4.9122 forget=6.7427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 220 total=13.5157 mle=1.7862 pcon=4.9118 forget=6.8176 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 270 total=13.2917 mle=1.6730 pcon=4.9118 forget=6.7070 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 320 total=13.3038 mle=1.6323 pcon=4.9115 forget=6.7599 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 370 total=13.4061 mle=1.6017 pcon=4.9116 forget=6.8929 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 30 total=13.5326 mle=1.7960 pcon=4.9117 forget=6.8249 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 80 total=13.1875 mle=1.5144 pcon=4.9118 forget=6.7613 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 130 total=13.3749 mle=1.6874 pcon=4.9117 forget=6.7757 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 180 total=13.3027 mle=1.6110 pcon=4.9116 forget=6.7801 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 230 total=13.2761 mle=1.5731 pcon=4.9115 forget=6.7914 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 280 total=13.3094 mle=1.6651 pcon=4.9115 forget=6.7328 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 330 total=13.3268 mle=1.6589 pcon=4.9111 forget=6.7568 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 380 total=13.2799 mle=1.5641 pcon=4.9112 forget=6.8046 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 40 total=13.5246 mle=1.8508 pcon=4.9115 forget=6.7622 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 90 total=13.4434 mle=1.7467 pcon=4.9115 forget=6.7852 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 140 total=13.3323 mle=1.6602 pcon=4.9116 forget=6.7606 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 190 total=13.1983 mle=1.5439 pcon=4.9115 forget=6.7428 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 240 total=13.4833 mle=1.8009 pcon=4.9115 forget=6.7710 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 290 total=13.2007 mle=1.4956 pcon=4.9115 forget=6.7936 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 340 total=13.3371 mle=1.6613 pcon=4.9115 forget=6.7643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 0 total=13.4534 mle=1.6816 pcon=4.9113 forget=6.8604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 50 total=13.3886 mle=1.6692 pcon=4.9111 forget=6.8083 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 100 total=13.2381 mle=1.6124 pcon=4.9108 forget=6.7149 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 150 total=13.1058 mle=1.4547 pcon=4.9109 forget=6.7402 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 200 total=13.4639 mle=1.7412 pcon=4.9107 forget=6.8120 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 250 total=13.6414 mle=1.9349 pcon=4.9107 forget=6.7959 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 300 total=13.1997 mle=1.4935 pcon=4.9109 forget=6.7952 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 350 total=13.0501 mle=1.4089 pcon=4.9110 forget=6.7301 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 10 total=13.1107 mle=1.5163 pcon=4.9111 forget=6.6833 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 60 total=13.7354 mle=1.9970 pcon=4.9112 forget=6.8272 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 110 total=13.3390 mle=1.6411 pcon=4.9110 forget=6.7868 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 160 total=13.1774 mle=1.4946 pcon=4.9113 forget=6.7716 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 210 total=13.2353 mle=1.5420 pcon=4.9112 forget=6.7820 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 260 total=13.1052 mle=1.4879 pcon=4.9115 forget=6.7057 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 310 total=13.1815 mle=1.5341 pcon=4.9118 forget=6.7356 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 360 total=13.3043 mle=1.6751 pcon=4.9120 forget=6.7173 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 20 total=13.3078 mle=1.6359 pcon=4.9120 forget=6.7599 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 70 total=13.3457 mle=1.6590 pcon=4.9119 forget=6.7747 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 120 total=13.7092 mle=1.9889 pcon=4.9117 forget=6.8085 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 170 total=13.2614 mle=1.5625 pcon=4.9118 forget=6.7870 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 220 total=13.3201 mle=1.6408 pcon=4.9120 forget=6.7673 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 270 total=13.4076 mle=1.7722 pcon=4.9120 forget=6.7234 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 320 total=13.3277 mle=1.6857 pcon=4.9122 forget=6.7298 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 370 total=13.3921 mle=1.7495 pcon=4.9123 forget=6.7302 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 30 total=13.3410 mle=1.5892 pcon=4.9123 forget=6.8395 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 80 total=13.2836 mle=1.5609 pcon=4.9125 forget=6.8103 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 130 total=13.4251 mle=1.6997 pcon=4.9125 forget=6.8128 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 180 total=13.2267 mle=1.4780 pcon=4.9127 forget=6.8360 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 230 total=13.2963 mle=1.6646 pcon=4.9126 forget=6.7191 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 280 total=13.1836 mle=1.5430 pcon=4.9127 forget=6.7279 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 330 total=13.2098 mle=1.5439 pcon=4.9126 forget=6.7533 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 380 total=13.1276 mle=1.5099 pcon=4.9126 forget=6.7050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 40 total=13.5383 mle=1.8060 pcon=4.9126 forget=6.8197 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 90 total=13.2268 mle=1.6036 pcon=4.9125 forget=6.7108 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 140 total=13.5043 mle=1.7807 pcon=4.9127 forget=6.8108 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 190 total=13.0262 mle=1.3721 pcon=4.9125 forget=6.7416 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 240 total=13.3891 mle=1.6007 pcon=4.9125 forget=6.8759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 290 total=13.4899 mle=1.8092 pcon=4.9124 forget=6.7683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 340 total=13.6289 mle=1.9641 pcon=4.9125 forget=6.7523 nr=64 nf=64 protos=540 fproto_sim=NA
 92%|█████████▏| 46/50 [16:12<01:25, 21.48s/it] 94%|█████████▍| 47/50 [16:34<01:05, 21.79s/it] 96%|█████████▌| 48/50 [16:53<00:41, 20.78s/it] 98%|█████████▊| 49/50 [17:12<00:20, 20.34s/it]100%|██████████| 50/50 [17:39<00:00, 22.42s/it]100%|██████████| 50/50 [17:39<00:00, 21.20s/it]
[loss] ep 45 it 0 total=13.2581 mle=1.5617 pcon=4.9123 forget=6.7840 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 50 total=13.2178 mle=1.6094 pcon=4.9123 forget=6.6961 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 100 total=13.2683 mle=1.6020 pcon=4.9122 forget=6.7541 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 150 total=13.2676 mle=1.5656 pcon=4.9121 forget=6.7899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 200 total=13.3203 mle=1.5883 pcon=4.9122 forget=6.8199 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 250 total=13.1364 mle=1.4703 pcon=4.9122 forget=6.7539 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 300 total=13.3453 mle=1.6910 pcon=4.9120 forget=6.7424 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 350 total=13.1541 mle=1.4971 pcon=4.9121 forget=6.7449 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 10 total=13.1937 mle=1.4948 pcon=4.9122 forget=6.7867 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 60 total=13.0967 mle=1.4412 pcon=4.9123 forget=6.7432 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 110 total=13.3783 mle=1.6940 pcon=4.9124 forget=6.7719 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 160 total=13.1206 mle=1.4856 pcon=4.9123 forget=6.7227 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 210 total=13.3666 mle=1.7368 pcon=4.9122 forget=6.7177 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 260 total=13.2892 mle=1.6842 pcon=4.9119 forget=6.6930 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 310 total=13.1752 mle=1.5003 pcon=4.9120 forget=6.7629 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 360 total=13.3468 mle=1.6708 pcon=4.9119 forget=6.7641 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 20 total=13.3193 mle=1.6158 pcon=4.9118 forget=6.7918 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 70 total=13.2634 mle=1.5768 pcon=4.9116 forget=6.7750 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 120 total=13.3284 mle=1.6266 pcon=4.9114 forget=6.7904 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 170 total=13.2896 mle=1.5198 pcon=4.9112 forget=6.8586 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 220 total=13.1520 mle=1.4376 pcon=4.9112 forget=6.8033 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 270 total=13.2749 mle=1.5795 pcon=4.9111 forget=6.7842 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 320 total=13.3067 mle=1.6114 pcon=4.9110 forget=6.7843 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 370 total=13.3462 mle=1.6943 pcon=4.9109 forget=6.7410 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 30 total=13.5339 mle=1.7351 pcon=4.9110 forget=6.8877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 80 total=13.3561 mle=1.6175 pcon=4.9111 forget=6.8275 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 130 total=13.3888 mle=1.7054 pcon=4.9112 forget=6.7722 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 180 total=13.3983 mle=1.7008 pcon=4.9112 forget=6.7862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 230 total=13.3825 mle=1.7676 pcon=4.9114 forget=6.7034 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 280 total=13.1274 mle=1.5038 pcon=4.9112 forget=6.7123 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 330 total=13.0949 mle=1.5241 pcon=4.9110 forget=6.6597 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 380 total=13.2760 mle=1.6121 pcon=4.9107 forget=6.7533 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 40 total=13.1971 mle=1.5418 pcon=4.9109 forget=6.7444 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 90 total=13.2564 mle=1.6224 pcon=4.9107 forget=6.7233 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 140 total=13.3183 mle=1.6181 pcon=4.9106 forget=6.7896 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 190 total=13.3860 mle=1.7395 pcon=4.9103 forget=6.7362 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 240 total=13.2880 mle=1.6790 pcon=4.9101 forget=6.6989 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 290 total=13.2785 mle=1.6751 pcon=4.9099 forget=6.6935 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 340 total=13.1113 mle=1.4710 pcon=4.9101 forget=6.7302 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:03,  3.15it/s]  3%|▎         | 10/391 [00:00<00:12, 29.73it/s]  5%|▍         | 19/391 [00:00<00:07, 47.56it/s]  7%|▋         | 28/391 [00:00<00:06, 59.61it/s]  9%|▉         | 37/391 [00:00<00:05, 67.96it/s] 12%|█▏        | 46/391 [00:00<00:04, 73.88it/s] 14%|█▍        | 55/391 [00:00<00:04, 77.61it/s] 16%|█▋        | 64/391 [00:01<00:04, 80.45it/s] 19%|█▊        | 73/391 [00:01<00:03, 82.52it/s] 21%|██        | 82/391 [00:01<00:03, 83.92it/s] 23%|██▎       | 91/391 [00:01<00:03, 85.05it/s] 26%|██▌       | 100/391 [00:01<00:03, 85.85it/s] 28%|██▊       | 109/391 [00:01<00:03, 86.26it/s] 30%|███       | 118/391 [00:01<00:03, 86.80it/s] 32%|███▏      | 127/391 [00:01<00:03, 86.90it/s] 35%|███▍      | 136/391 [00:01<00:02, 87.25it/s] 37%|███▋      | 145/391 [00:01<00:02, 87.62it/s] 39%|███▉      | 154/391 [00:02<00:02, 87.82it/s] 42%|████▏     | 163/391 [00:02<00:02, 87.84it/s] 44%|████▍     | 172/391 [00:02<00:02, 87.92it/s] 46%|████▋     | 181/391 [00:02<00:02, 87.91it/s] 49%|████▊     | 190/391 [00:02<00:02, 87.98it/s] 51%|█████     | 199/391 [00:02<00:02, 88.16it/s] 53%|█████▎    | 208/391 [00:02<00:02, 88.23it/s] 55%|█████▌    | 217/391 [00:02<00:01, 88.29it/s] 58%|█████▊    | 226/391 [00:02<00:01, 87.04it/s] 60%|██████    | 235/391 [00:02<00:01, 87.54it/s] 62%|██████▏   | 244/391 [00:03<00:01, 87.95it/s] 65%|██████▍   | 254/391 [00:03<00:01, 88.90it/s] 67%|██████▋   | 263/391 [00:03<00:01, 88.95it/s] 70%|██████▉   | 272/391 [00:03<00:01, 88.67it/s] 72%|███████▏  | 281/391 [00:03<00:01, 88.53it/s] 74%|███████▍  | 290/391 [00:03<00:01, 88.56it/s] 76%|███████▋  | 299/391 [00:03<00:01, 88.50it/s] 79%|███████▉  | 308/391 [00:03<00:00, 88.39it/s] 81%|████████  | 317/391 [00:03<00:00, 88.20it/s] 83%|████████▎ | 326/391 [00:04<00:00, 88.31it/s] 86%|████████▌ | 335/391 [00:04<00:00, 88.07it/s] 88%|████████▊ | 344/391 [00:04<00:00, 88.14it/s] 90%|█████████ | 353/391 [00:04<00:00, 88.08it/s] 93%|█████████▎| 362/391 [00:04<00:00, 88.07it/s] 95%|█████████▍| 371/391 [00:04<00:00, 88.30it/s] 97%|█████████▋| 380/391 [00:04<00:00, 88.26it/s]100%|█████████▉| 390/391 [00:04<00:00, 88.97it/s]100%|██████████| 391/391 [00:04<00:00, 82.14it/s]
50000 images processed, 4.8470234870910645 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:22,  3.45it/s] 13%|█▎        | 10/79 [00:00<00:02, 31.65it/s] 24%|██▍       | 19/79 [00:00<00:01, 49.44it/s] 35%|███▌      | 28/79 [00:00<00:00, 61.21it/s] 47%|████▋     | 37/79 [00:00<00:00, 68.97it/s] 58%|█████▊    | 46/79 [00:00<00:00, 74.24it/s] 70%|██████▉   | 55/79 [00:00<00:00, 78.00it/s] 81%|████████  | 64/79 [00:01<00:00, 80.59it/s] 92%|█████████▏| 73/79 [00:01<00:00, 82.94it/s]100%|██████████| 79/79 [00:01<00:00, 65.77it/s]
10000 images processed, 1.226449728012085 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:24,  2.40it/s]  5%|▍         | 10/204 [00:00<00:07, 24.54it/s]  9%|▉         | 19/204 [00:00<00:04, 41.11it/s] 14%|█▎        | 28/204 [00:00<00:03, 53.61it/s] 18%|█▊        | 37/204 [00:00<00:02, 62.84it/s] 23%|██▎       | 46/204 [00:00<00:02, 69.74it/s] 27%|██▋       | 55/204 [00:01<00:02, 74.46it/s] 31%|███▏      | 64/204 [00:01<00:01, 78.00it/s] 36%|███▌      | 73/204 [00:01<00:01, 80.24it/s] 40%|████      | 82/204 [00:01<00:01, 81.97it/s] 45%|████▍     | 91/204 [00:01<00:01, 83.32it/s] 49%|████▉     | 100/204 [00:01<00:01, 84.52it/s] 53%|█████▎    | 109/204 [00:01<00:01, 85.23it/s] 58%|█████▊    | 118/204 [00:01<00:01, 85.40it/s] 62%|██████▏   | 127/204 [00:01<00:00, 85.42it/s] 67%|██████▋   | 136/204 [00:01<00:00, 85.67it/s] 71%|███████   | 145/204 [00:02<00:00, 85.84it/s] 75%|███████▌  | 154/204 [00:02<00:00, 85.96it/s] 80%|███████▉  | 163/204 [00:02<00:00, 85.93it/s] 84%|████████▍ | 172/204 [00:02<00:00, 85.92it/s] 89%|████████▊ | 181/204 [00:02<00:00, 85.78it/s] 93%|█████████▎| 190/204 [00:02<00:00, 85.77it/s] 98%|█████████▊| 199/204 [00:02<00:00, 86.38it/s]100%|██████████| 204/204 [00:02<00:00, 73.43it/s]
26032 images processed, 2.8270180225372314 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:46,  1.69it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.18it/s] 22%|██▏       | 17/79 [00:00<00:02, 24.78it/s] 32%|███▏      | 25/79 [00:01<00:01, 31.75it/s] 42%|████▏     | 33/79 [00:01<00:01, 37.08it/s] 52%|█████▏    | 41/79 [00:01<00:00, 41.86it/s] 62%|██████▏   | 49/79 [00:01<00:00, 44.24it/s] 72%|███████▏  | 57/79 [00:01<00:00, 45.20it/s] 82%|████████▏ | 65/79 [00:01<00:00, 46.17it/s] 92%|█████████▏| 73/79 [00:02<00:00, 47.19it/s]100%|██████████| 79/79 [00:02<00:00, 37.33it/s]
10000 images processed, 2.1495041847229004 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:31,  2.44it/s] 13%|█▎        | 10/79 [00:00<00:02, 24.62it/s] 24%|██▍       | 19/79 [00:00<00:01, 41.62it/s] 35%|███▌      | 28/79 [00:00<00:00, 54.14it/s] 47%|████▋     | 37/79 [00:00<00:00, 63.32it/s] 58%|█████▊    | 46/79 [00:00<00:00, 69.57it/s] 70%|██████▉   | 55/79 [00:01<00:00, 74.62it/s] 81%|████████  | 64/79 [00:01<00:00, 78.32it/s] 92%|█████████▏| 73/79 [00:01<00:00, 81.24it/s]100%|██████████| 79/79 [00:01<00:00, 60.32it/s]
10000 images processed, 1.332486867904663 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:19,  3.48it/s] 14%|█▍        | 10/70 [00:00<00:01, 31.45it/s] 27%|██▋       | 19/70 [00:00<00:01, 49.25it/s] 40%|████      | 28/70 [00:00<00:00, 60.86it/s] 53%|█████▎    | 37/70 [00:00<00:00, 68.69it/s] 66%|██████▌   | 46/70 [00:00<00:00, 72.77it/s] 79%|███████▊  | 55/70 [00:00<00:00, 76.83it/s] 91%|█████████▏| 64/70 [00:01<00:00, 80.12it/s]100%|██████████| 70/70 [00:01<00:00, 63.59it/s]
8925 images processed, 1.1333942413330078 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:42,  1.04it/s]  7%|▋         | 3/45 [00:01<00:12,  3.45it/s] 20%|██        | 9/45 [00:01<00:04,  8.51it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.17it/s] 33%|███▎      | 15/45 [00:01<00:02, 13.70it/s] 40%|████      | 18/45 [00:02<00:02, 11.25it/s] 51%|█████     | 23/45 [00:02<00:01, 16.59it/s] 58%|█████▊    | 26/45 [00:02<00:01, 12.13it/s] 69%|██████▉   | 31/45 [00:02<00:00, 17.10it/s] 76%|███████▌  | 34/45 [00:03<00:01, 10.11it/s] 89%|████████▉ | 40/45 [00:03<00:00, 13.63it/s] 96%|█████████▌| 43/45 [00:04<00:00, 10.89it/s]100%|██████████| 45/45 [00:04<00:00, 10.94it/s]
5640 images processed, 4.136748313903809 seconds used

19.34698462486267
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.51  99.36
places365     67.94  81.19
LSUN          17.52  96.09
iSUN          72.32  81.72
dtd           37.89  91.39
forget        79.50  86.90
AVG           46.28  89.44
Retain-Acc: 0.7406
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
16.96532940864563
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl1.0-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.5 lr=0.001 epochs=5
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.5, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:21<01:26, 21.61s/it] 40%|████      | 2/5 [00:39<00:59, 19.67s/it] 60%|██████    | 3/5 [01:01<00:41, 20.65s/it] 80%|████████  | 4/5 [01:25<00:21, 21.80s/it]100%|██████████| 5/5 [01:44<00:00, 20.76s/it]100%|██████████| 5/5 [01:44<00:00, 20.84s/it]
[loss] ep 0 it 0 total=10.3049 mle=1.5711 pcon=5.2950 forget=3.4387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=10.3338 mle=1.5426 pcon=5.2879 forget=3.5033 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=10.4165 mle=1.7007 pcon=5.2809 forget=3.4349 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=10.5975 mle=1.8996 pcon=5.2738 forget=3.4241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=10.4363 mle=1.7128 pcon=5.2670 forget=3.4565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=10.2085 mle=1.5021 pcon=5.2603 forget=3.4460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=10.2685 mle=1.5597 pcon=5.2541 forget=3.4548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=10.4025 mle=1.6809 pcon=5.2476 forget=3.4740 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=10.4374 mle=1.6715 pcon=5.2409 forget=3.5251 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=10.2892 mle=1.6372 pcon=5.2346 forget=3.4174 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=10.2186 mle=1.5166 pcon=5.2285 forget=3.4735 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=10.4757 mle=1.7801 pcon=5.2224 forget=3.4731 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=10.4446 mle=1.7648 pcon=5.2167 forget=3.4630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=10.2371 mle=1.6055 pcon=5.2112 forget=3.4204 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=10.3746 mle=1.7239 pcon=5.2056 forget=3.4451 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=10.4595 mle=1.7942 pcon=5.2003 forget=3.4650 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=10.1898 mle=1.5532 pcon=5.1950 forget=3.4416 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=10.4715 mle=1.8882 pcon=5.1899 forget=3.3934 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=10.3658 mle=1.7419 pcon=5.1847 forget=3.4392 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=10.1175 mle=1.5263 pcon=5.1796 forget=3.4116 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=10.2242 mle=1.5855 pcon=5.1745 forget=3.4641 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=10.4798 mle=1.8842 pcon=5.1699 forget=3.4257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=10.1500 mle=1.5918 pcon=5.1652 forget=3.3931 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=10.3822 mle=1.8109 pcon=5.1605 forget=3.4108 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=10.2868 mle=1.7054 pcon=5.1559 forget=3.4255 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=10.2695 mle=1.6708 pcon=5.1518 forget=3.4469 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=10.3276 mle=1.7669 pcon=5.1474 forget=3.4132 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=10.4705 mle=1.9230 pcon=5.1435 forget=3.4040 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=10.1072 mle=1.5696 pcon=5.1394 forget=3.3982 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=10.3599 mle=1.8356 pcon=5.1352 forget=3.3891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=10.2707 mle=1.7033 pcon=5.1310 forget=3.4364 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=10.2654 mle=1.6953 pcon=5.1274 forget=3.4427 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=10.1301 mle=1.6533 pcon=5.1236 forget=3.3532 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=10.2370 mle=1.7294 pcon=5.1195 forget=3.3882 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=10.1791 mle=1.6512 pcon=5.1158 forget=3.4121 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=9.9773 mle=1.4536 pcon=5.1123 forget=3.4114 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=10.2617 mle=1.7667 pcon=5.1085 forget=3.3865 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=9.9754 mle=1.4912 pcon=5.1049 forget=3.3793 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=10.3800 mle=1.8718 pcon=5.1014 forget=3.4068 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:38,  2.46it/s]  3%|▎         | 10/391 [00:00<00:15, 25.04it/s]  5%|▍         | 19/391 [00:00<00:08, 42.23it/s]  7%|▋         | 28/391 [00:00<00:06, 54.99it/s] 10%|▉         | 38/391 [00:00<00:05, 65.71it/s] 12%|█▏        | 47/391 [00:00<00:04, 72.03it/s] 14%|█▍        | 56/391 [00:01<00:04, 76.86it/s] 17%|█▋        | 65/391 [00:01<00:04, 80.25it/s] 19%|█▉        | 74/391 [00:01<00:03, 82.16it/s] 21%|██        | 83/391 [00:01<00:03, 83.60it/s] 24%|██▎       | 92/391 [00:01<00:03, 84.81it/s] 26%|██▌       | 101/391 [00:01<00:03, 85.43it/s] 28%|██▊       | 110/391 [00:01<00:03, 85.96it/s] 30%|███       | 119/391 [00:01<00:03, 86.38it/s] 33%|███▎      | 128/391 [00:01<00:03, 86.64it/s] 35%|███▌      | 137/391 [00:01<00:02, 86.94it/s] 37%|███▋      | 146/391 [00:02<00:02, 86.93it/s] 40%|███▉      | 155/391 [00:02<00:02, 86.51it/s] 42%|████▏     | 164/391 [00:02<00:02, 86.77it/s] 44%|████▍     | 173/391 [00:02<00:02, 87.16it/s] 47%|████▋     | 182/391 [00:02<00:02, 87.45it/s] 49%|████▉     | 191/391 [00:02<00:02, 87.64it/s] 51%|█████     | 200/391 [00:02<00:02, 87.85it/s] 53%|█████▎    | 209/391 [00:02<00:02, 87.66it/s] 56%|█████▌    | 218/391 [00:02<00:01, 87.87it/s] 58%|█████▊    | 227/391 [00:02<00:01, 87.55it/s] 60%|██████    | 236/391 [00:03<00:01, 87.50it/s] 63%|██████▎   | 245/391 [00:03<00:01, 87.35it/s] 65%|██████▍   | 254/391 [00:03<00:01, 87.29it/s] 67%|██████▋   | 263/391 [00:03<00:01, 87.45it/s] 70%|██████▉   | 272/391 [00:03<00:01, 87.31it/s] 72%|███████▏  | 281/391 [00:03<00:01, 87.23it/s] 74%|███████▍  | 290/391 [00:03<00:01, 87.24it/s] 76%|███████▋  | 299/391 [00:03<00:01, 87.26it/s] 79%|███████▉  | 308/391 [00:03<00:00, 87.88it/s] 81%|████████  | 317/391 [00:04<00:00, 87.55it/s] 83%|████████▎ | 326/391 [00:04<00:00, 87.83it/s] 86%|████████▌ | 335/391 [00:04<00:00, 87.91it/s] 88%|████████▊ | 344/391 [00:04<00:00, 88.01it/s] 90%|█████████ | 353/391 [00:04<00:00, 88.12it/s] 93%|█████████▎| 362/391 [00:04<00:00, 88.40it/s] 95%|█████████▍| 371/391 [00:04<00:00, 88.65it/s] 97%|█████████▋| 380/391 [00:04<00:00, 87.95it/s] 99%|█████████▉| 389/391 [00:04<00:00, 88.40it/s]100%|██████████| 391/391 [00:04<00:00, 80.37it/s]
50000 images processed, 4.990447998046875 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:31,  2.46it/s] 13%|█▎        | 10/79 [00:00<00:02, 24.79it/s] 24%|██▍       | 19/79 [00:00<00:01, 41.76it/s] 35%|███▌      | 28/79 [00:00<00:00, 54.26it/s] 47%|████▋     | 37/79 [00:00<00:00, 63.24it/s] 58%|█████▊    | 46/79 [00:00<00:00, 69.89it/s] 70%|██████▉   | 55/79 [00:01<00:00, 74.74it/s] 81%|████████  | 64/79 [00:01<00:00, 78.36it/s] 92%|█████████▏| 73/79 [00:01<00:00, 81.45it/s]100%|██████████| 79/79 [00:01<00:00, 59.92it/s]
10000 images processed, 1.3435659408569336 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:28,  2.29it/s]  5%|▍         | 10/204 [00:00<00:08, 23.49it/s]  9%|▉         | 19/204 [00:00<00:04, 40.13it/s] 14%|█▎        | 28/204 [00:00<00:03, 52.80it/s] 18%|█▊        | 37/204 [00:00<00:02, 62.12it/s] 23%|██▎       | 46/204 [00:00<00:02, 69.16it/s] 27%|██▋       | 56/204 [00:01<00:01, 75.51it/s] 32%|███▏      | 65/204 [00:01<00:01, 78.56it/s] 36%|███▋      | 74/204 [00:01<00:01, 80.27it/s] 41%|████      | 83/204 [00:01<00:01, 82.04it/s] 45%|████▌     | 92/204 [00:01<00:01, 83.19it/s] 50%|████▉     | 101/204 [00:01<00:01, 84.04it/s] 54%|█████▍    | 110/204 [00:01<00:01, 84.68it/s] 58%|█████▊    | 119/204 [00:01<00:00, 85.14it/s] 63%|██████▎   | 128/204 [00:01<00:00, 85.52it/s] 67%|██████▋   | 137/204 [00:02<00:00, 85.89it/s] 72%|███████▏  | 146/204 [00:02<00:00, 85.96it/s] 76%|███████▌  | 155/204 [00:02<00:00, 85.96it/s] 80%|████████  | 164/204 [00:02<00:00, 85.85it/s] 85%|████████▍ | 173/204 [00:02<00:00, 85.99it/s] 89%|████████▉ | 182/204 [00:02<00:00, 86.21it/s] 94%|█████████▎| 191/204 [00:02<00:00, 86.48it/s] 98%|█████████▊| 200/204 [00:02<00:00, 87.16it/s]100%|██████████| 204/204 [00:02<00:00, 73.15it/s]
26032 images processed, 2.828975200653076 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:36,  2.14it/s] 10%|█         | 8/79 [00:00<00:03, 17.86it/s] 16%|█▋        | 13/79 [00:00<00:02, 22.02it/s] 24%|██▍       | 19/79 [00:00<00:02, 26.35it/s] 34%|███▍      | 27/79 [00:01<00:01, 31.09it/s] 44%|████▍     | 35/79 [00:01<00:01, 33.65it/s] 54%|█████▍    | 43/79 [00:01<00:00, 36.17it/s] 65%|██████▍   | 51/79 [00:01<00:00, 41.36it/s] 75%|███████▍  | 59/79 [00:01<00:00, 45.12it/s] 85%|████████▍ | 67/79 [00:01<00:00, 48.47it/s] 95%|█████████▍| 75/79 [00:02<00:00, 49.86it/s]100%|██████████| 79/79 [00:02<00:00, 36.92it/s]
10000 images processed, 2.1743288040161133 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:28,  2.73it/s] 13%|█▎        | 10/79 [00:00<00:02, 27.06it/s] 24%|██▍       | 19/79 [00:00<00:01, 44.34it/s] 35%|███▌      | 28/79 [00:00<00:00, 56.80it/s] 47%|████▋     | 37/79 [00:00<00:00, 65.35it/s] 58%|█████▊    | 46/79 [00:00<00:00, 71.80it/s] 70%|██████▉   | 55/79 [00:00<00:00, 76.36it/s] 81%|████████  | 64/79 [00:01<00:00, 79.71it/s] 92%|█████████▏| 73/79 [00:01<00:00, 82.31it/s]100%|██████████| 79/79 [00:01<00:00, 63.00it/s]
10000 images processed, 1.2769019603729248 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:27,  2.54it/s] 14%|█▍        | 10/70 [00:00<00:02, 25.59it/s] 27%|██▋       | 19/70 [00:00<00:01, 42.66it/s] 40%|████      | 28/70 [00:00<00:00, 55.28it/s] 53%|█████▎    | 37/70 [00:00<00:00, 64.51it/s] 66%|██████▌   | 46/70 [00:00<00:00, 71.08it/s] 79%|███████▊  | 55/70 [00:01<00:00, 75.80it/s] 91%|█████████▏| 64/70 [00:01<00:00, 79.49it/s]100%|██████████| 70/70 [00:01<00:00, 58.79it/s]
8925 images processed, 1.222975492477417 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:39,  1.11it/s]  4%|▍         | 2/45 [00:01<00:19,  2.25it/s] 20%|██        | 9/45 [00:01<00:04,  8.96it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.99it/s] 36%|███▌      | 16/45 [00:01<00:02, 13.89it/s] 40%|████      | 18/45 [00:02<00:02, 10.39it/s] 49%|████▉     | 22/45 [00:02<00:01, 13.86it/s] 53%|█████▎    | 24/45 [00:02<00:01, 12.94it/s] 58%|█████▊    | 26/45 [00:02<00:01, 11.20it/s] 69%|██████▉   | 31/45 [00:02<00:00, 15.10it/s] 73%|███████▎  | 33/45 [00:03<00:01,  9.13it/s] 84%|████████▍ | 38/45 [00:03<00:00, 13.93it/s] 91%|█████████ | 41/45 [00:04<00:00, 10.76it/s] 96%|█████████▌| 43/45 [00:04<00:00, 11.40it/s]100%|██████████| 45/45 [00:04<00:00, 10.70it/s]
5640 images processed, 4.231177091598511 seconds used

19.760273218154907
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.89  81.19
LSUN          17.51  96.09
iSUN          72.30  81.72
dtd           37.89  91.39
forget        79.50  86.90
AVG           46.27  89.44
Retain-Acc: 0.7407
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
20.53040838241577
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.5 lr=0.001 epochs=10
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.5, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:21<03:09, 21.00s/it] 20%|██        | 2/10 [00:46<03:07, 23.42s/it] 30%|███       | 3/10 [01:07<02:36, 22.29s/it] 40%|████      | 4/10 [01:25<02:04, 20.67s/it] 50%|█████     | 5/10 [01:44<01:41, 20.23s/it] 60%|██████    | 6/10 [02:10<01:28, 22.14s/it] 70%|███████   | 7/10 [02:29<01:03, 21.14s/it] 80%|████████  | 8/10 [02:47<00:40, 20.19s/it][loss] ep 0 it 0 total=10.3048 mle=1.5710 pcon=5.2950 forget=3.4387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=10.3336 mle=1.5424 pcon=5.2879 forget=3.5033 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=10.4164 mle=1.7005 pcon=5.2809 forget=3.4350 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=10.5974 mle=1.8996 pcon=5.2738 forget=3.4241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=10.4366 mle=1.7131 pcon=5.2670 forget=3.4565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=10.2083 mle=1.5020 pcon=5.2603 forget=3.4460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=10.2685 mle=1.5596 pcon=5.2540 forget=3.4548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=10.4026 mle=1.6809 pcon=5.2476 forget=3.4741 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=10.4375 mle=1.6716 pcon=5.2409 forget=3.5250 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=10.2888 mle=1.6368 pcon=5.2346 forget=3.4174 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=10.2184 mle=1.5166 pcon=5.2285 forget=3.4734 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=10.4761 mle=1.7804 pcon=5.2224 forget=3.4733 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=10.4442 mle=1.7645 pcon=5.2167 forget=3.4630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=10.2367 mle=1.6050 pcon=5.2112 forget=3.4204 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=10.3742 mle=1.7235 pcon=5.2056 forget=3.4450 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=10.4594 mle=1.7941 pcon=5.2003 forget=3.4650 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=10.1896 mle=1.5530 pcon=5.1950 forget=3.4416 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=10.4714 mle=1.8881 pcon=5.1899 forget=3.3934 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=10.3658 mle=1.7419 pcon=5.1847 forget=3.4392 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=10.1176 mle=1.5264 pcon=5.1796 forget=3.4116 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=10.2244 mle=1.5858 pcon=5.1745 forget=3.4640 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=10.4794 mle=1.8838 pcon=5.1699 forget=3.4257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=10.1501 mle=1.5919 pcon=5.1652 forget=3.3931 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=10.3823 mle=1.8111 pcon=5.1605 forget=3.4107 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=10.2865 mle=1.7051 pcon=5.1559 forget=3.4254 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=10.2696 mle=1.6709 pcon=5.1518 forget=3.4469 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=10.3275 mle=1.7668 pcon=5.1474 forget=3.4132 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=10.4703 mle=1.9229 pcon=5.1435 forget=3.4040 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=10.1073 mle=1.5697 pcon=5.1394 forget=3.3981 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=10.3599 mle=1.8356 pcon=5.1352 forget=3.3890 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=10.2707 mle=1.7033 pcon=5.1310 forget=3.4364 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=10.2652 mle=1.6951 pcon=5.1274 forget=3.4426 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=10.1302 mle=1.6534 pcon=5.1235 forget=3.3532 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=10.2368 mle=1.7292 pcon=5.1195 forget=3.3881 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=10.1790 mle=1.6510 pcon=5.1158 forget=3.4122 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=9.9772 mle=1.4535 pcon=5.1123 forget=3.4114 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=10.2616 mle=1.7667 pcon=5.1085 forget=3.3865 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=9.9753 mle=1.4910 pcon=5.1049 forget=3.3794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=10.3798 mle=1.8716 pcon=5.1014 forget=3.4068 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=10.3534 mle=1.8667 pcon=5.0980 forget=3.3887 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=10.2538 mle=1.8004 pcon=5.0944 forget=3.3590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=10.0707 mle=1.5853 pcon=5.0911 forget=3.3943 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=10.0722 mle=1.5944 pcon=5.0876 forget=3.3902 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=10.1321 mle=1.6914 pcon=5.0847 forget=3.3561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=10.1976 mle=1.7777 pcon=5.0816 forget=3.3383 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=10.0382 mle=1.5975 pcon=5.0784 forget=3.3623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=10.1811 mle=1.7448 pcon=5.0754 forget=3.3609 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=9.8879 mle=1.3991 pcon=5.0725 forget=3.4163 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=9.9058 mle=1.5310 pcon=5.0696 forget=3.3053 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=10.0349 mle=1.5902 pcon=5.0668 forget=3.3779 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=10.0685 mle=1.6677 pcon=5.0638 forget=3.3369 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=10.0215 mle=1.6293 pcon=5.0614 forget=3.3308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=10.2301 mle=1.8141 pcon=5.0582 forget=3.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=9.9796 mle=1.5466 pcon=5.0556 forget=3.3773 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=10.0578 mle=1.6718 pcon=5.0528 forget=3.3332 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=9.9896 mle=1.6306 pcon=5.0498 forget=3.3092 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=10.0894 mle=1.7447 pcon=5.0471 forget=3.2976 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=9.9560 mle=1.6080 pcon=5.0441 forget=3.3039 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=9.8956 mle=1.5600 pcon=5.0414 forget=3.2942 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=10.0127 mle=1.6415 pcon=5.0389 forget=3.3322 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=10.1745 mle=1.7869 pcon=5.0363 forget=3.3513 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=10.1079 mle=1.7318 pcon=5.0334 forget=3.3427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=9.9723 mle=1.6299 pcon=5.0308 forget=3.3115 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 90%|█████████ | 9/10 [03:07<00:19, 19.89s/it]100%|██████████| 10/10 [03:32<00:00, 21.76s/it]100%|██████████| 10/10 [03:32<00:00, 21.29s/it]
[loss] ep 8 it 30 total=9.9190 mle=1.5423 pcon=5.0284 forget=3.3482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=9.9351 mle=1.5748 pcon=5.0260 forget=3.3342 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=9.8493 mle=1.5033 pcon=5.0234 forget=3.3225 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=10.0493 mle=1.7137 pcon=5.0211 forget=3.3145 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=9.8558 mle=1.5290 pcon=5.0192 forget=3.3076 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=10.0136 mle=1.6961 pcon=5.0169 forget=3.3006 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=10.1238 mle=1.7367 pcon=5.0145 forget=3.3726 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=9.8276 mle=1.4793 pcon=5.0124 forget=3.3359 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=9.9539 mle=1.6300 pcon=5.0104 forget=3.3135 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=10.0340 mle=1.6685 pcon=5.0082 forget=3.3573 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=10.2205 mle=1.8096 pcon=5.0067 forget=3.4043 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=9.9539 mle=1.5893 pcon=5.0052 forget=3.3594 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=9.9956 mle=1.5874 pcon=5.0038 forget=3.4044 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=9.9831 mle=1.5610 pcon=5.0021 forget=3.4200 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=9.9686 mle=1.5521 pcon=5.0010 forget=3.4155 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:41,  3.85it/s]  3%|▎         | 10/391 [00:00<00:11, 34.23it/s]  5%|▍         | 19/391 [00:00<00:07, 52.75it/s]  7%|▋         | 28/391 [00:00<00:05, 64.42it/s]  9%|▉         | 37/391 [00:00<00:04, 71.90it/s] 12%|█▏        | 46/391 [00:00<00:04, 76.86it/s] 14%|█▍        | 55/391 [00:00<00:04, 80.43it/s] 16%|█▋        | 64/391 [00:00<00:03, 82.89it/s] 19%|█▊        | 73/391 [00:01<00:03, 84.67it/s] 21%|██        | 82/391 [00:01<00:03, 85.96it/s] 23%|██▎       | 91/391 [00:01<00:03, 86.85it/s] 26%|██▌       | 100/391 [00:01<00:03, 87.49it/s] 28%|██▊       | 109/391 [00:01<00:03, 87.95it/s] 30%|███       | 118/391 [00:01<00:03, 88.22it/s] 32%|███▏      | 127/391 [00:01<00:02, 88.25it/s] 35%|███▍      | 136/391 [00:01<00:02, 88.49it/s] 37%|███▋      | 145/391 [00:01<00:02, 88.67it/s] 39%|███▉      | 154/391 [00:01<00:02, 88.68it/s] 42%|████▏     | 163/391 [00:02<00:02, 88.81it/s] 44%|████▍     | 172/391 [00:02<00:02, 88.90it/s] 46%|████▋     | 181/391 [00:02<00:02, 88.73it/s] 49%|████▊     | 190/391 [00:02<00:02, 88.72it/s] 51%|█████     | 200/391 [00:02<00:02, 89.72it/s] 53%|█████▎    | 209/391 [00:02<00:02, 89.52it/s] 56%|█████▌    | 218/391 [00:02<00:01, 89.48it/s] 58%|█████▊    | 227/391 [00:02<00:01, 89.25it/s] 60%|██████    | 236/391 [00:02<00:01, 88.23it/s] 63%|██████▎   | 245/391 [00:03<00:01, 88.43it/s] 65%|██████▍   | 254/391 [00:03<00:01, 88.63it/s] 67%|██████▋   | 263/391 [00:03<00:01, 88.76it/s] 70%|██████▉   | 272/391 [00:03<00:01, 88.89it/s] 72%|███████▏  | 281/391 [00:03<00:01, 88.90it/s] 74%|███████▍  | 290/391 [00:03<00:01, 88.97it/s] 76%|███████▋  | 299/391 [00:03<00:01, 88.96it/s] 79%|███████▉  | 308/391 [00:03<00:00, 88.98it/s] 81%|████████  | 317/391 [00:03<00:00, 88.98it/s] 83%|████████▎ | 326/391 [00:03<00:00, 89.04it/s] 86%|████████▌ | 335/391 [00:04<00:00, 89.13it/s] 88%|████████▊ | 344/391 [00:04<00:00, 89.20it/s] 90%|█████████ | 353/391 [00:04<00:00, 89.14it/s] 93%|█████████▎| 362/391 [00:04<00:00, 89.17it/s] 95%|█████████▍| 371/391 [00:04<00:00, 88.36it/s] 97%|█████████▋| 380/391 [00:04<00:00, 88.23it/s] 99%|█████████▉| 389/391 [00:04<00:00, 88.64it/s]100%|██████████| 391/391 [00:04<00:00, 83.88it/s]
50000 images processed, 4.752561569213867 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:24,  3.20it/s] 13%|█▎        | 10/79 [00:00<00:02, 29.70it/s] 24%|██▍       | 19/79 [00:00<00:01, 47.57it/s] 35%|███▌      | 28/79 [00:00<00:00, 59.78it/s] 47%|████▋     | 37/79 [00:00<00:00, 67.53it/s] 58%|█████▊    | 46/79 [00:00<00:00, 73.49it/s] 70%|██████▉   | 55/79 [00:00<00:00, 77.73it/s] 81%|████████  | 64/79 [00:01<00:00, 80.68it/s] 92%|█████████▏| 73/79 [00:01<00:00, 82.35it/s]100%|██████████| 79/79 [00:01<00:00, 64.46it/s]
10000 images processed, 1.2516708374023438 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:11,  2.85it/s]  5%|▍         | 10/204 [00:00<00:07, 27.49it/s]  9%|▉         | 19/204 [00:00<00:04, 45.04it/s] 14%|█▎        | 28/204 [00:00<00:03, 56.65it/s] 18%|█▊        | 37/204 [00:00<00:02, 65.61it/s] 23%|██▎       | 46/204 [00:00<00:02, 71.90it/s] 27%|██▋       | 55/204 [00:00<00:01, 75.52it/s] 31%|███▏      | 64/204 [00:01<00:01, 78.79it/s] 36%|███▌      | 73/204 [00:01<00:01, 81.01it/s] 40%|████      | 82/204 [00:01<00:01, 82.70it/s] 45%|████▍     | 91/204 [00:01<00:01, 83.85it/s] 49%|████▉     | 100/204 [00:01<00:01, 84.56it/s] 53%|█████▎    | 109/204 [00:01<00:01, 85.21it/s] 58%|█████▊    | 118/204 [00:01<00:01, 85.80it/s] 62%|██████▏   | 127/204 [00:01<00:00, 86.12it/s] 67%|██████▋   | 136/204 [00:01<00:00, 86.11it/s] 71%|███████   | 145/204 [00:02<00:00, 86.11it/s] 75%|███████▌  | 154/204 [00:02<00:00, 86.26it/s] 80%|███████▉  | 163/204 [00:02<00:00, 86.48it/s] 84%|████████▍ | 172/204 [00:02<00:00, 85.69it/s] 89%|████████▊ | 181/204 [00:02<00:00, 86.09it/s] 93%|█████████▎| 190/204 [00:02<00:00, 85.54it/s] 98%|█████████▊| 199/204 [00:02<00:00, 86.34it/s]100%|██████████| 204/204 [00:02<00:00, 75.18it/s]
26032 images processed, 2.7556681632995605 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:36,  2.12it/s] 11%|█▏        | 9/79 [00:00<00:03, 17.70it/s] 22%|██▏       | 17/79 [00:00<00:02, 28.35it/s] 32%|███▏      | 25/79 [00:00<00:01, 35.27it/s] 42%|████▏     | 33/79 [00:01<00:01, 39.31it/s] 52%|█████▏    | 41/79 [00:01<00:00, 44.54it/s] 62%|██████▏   | 49/79 [00:01<00:00, 47.74it/s] 72%|███████▏  | 57/79 [00:01<00:00, 49.50it/s] 81%|████████  | 64/79 [00:01<00:00, 52.82it/s] 89%|████████▊ | 70/79 [00:01<00:00, 47.51it/s] 97%|█████████▋| 77/79 [00:01<00:00, 50.04it/s]100%|██████████| 79/79 [00:01<00:00, 40.45it/s]
10000 images processed, 1.9890034198760986 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:24,  3.12it/s] 13%|█▎        | 10/79 [00:00<00:02, 29.43it/s] 24%|██▍       | 19/79 [00:00<00:01, 46.87it/s] 35%|███▌      | 28/79 [00:00<00:00, 59.13it/s] 48%|████▊     | 38/79 [00:00<00:00, 68.99it/s] 59%|█████▉    | 47/79 [00:00<00:00, 74.46it/s] 71%|███████   | 56/79 [00:00<00:00, 78.37it/s] 82%|████████▏ | 65/79 [00:01<00:00, 80.75it/s] 94%|█████████▎| 74/79 [00:01<00:00, 83.13it/s]100%|██████████| 79/79 [00:01<00:00, 65.20it/s]
10000 images processed, 1.2410697937011719 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:22,  3.01it/s] 14%|█▍        | 10/70 [00:00<00:02, 28.79it/s] 27%|██▋       | 19/70 [00:00<00:01, 46.48it/s] 40%|████      | 28/70 [00:00<00:00, 58.66it/s] 53%|█████▎    | 37/70 [00:00<00:00, 66.71it/s] 66%|██████▌   | 46/70 [00:00<00:00, 72.44it/s] 79%|███████▊  | 55/70 [00:00<00:00, 76.80it/s] 91%|█████████▏| 64/70 [00:01<00:00, 80.31it/s]100%|██████████| 70/70 [00:01<00:00, 61.74it/s]
8925 images processed, 1.1625862121582031 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:38,  1.13it/s]  7%|▋         | 3/45 [00:00<00:11,  3.75it/s] 20%|██        | 9/45 [00:01<00:03,  9.87it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.21it/s] 31%|███       | 14/45 [00:01<00:02, 12.17it/s] 38%|███▊      | 17/45 [00:01<00:02, 13.24it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.84it/s] 49%|████▉     | 22/45 [00:02<00:01, 13.55it/s] 56%|█████▌    | 25/45 [00:02<00:01, 15.22it/s] 60%|██████    | 27/45 [00:02<00:01, 12.32it/s] 67%|██████▋   | 30/45 [00:02<00:01, 13.57it/s] 73%|███████▎  | 33/45 [00:03<00:00, 12.68it/s] 78%|███████▊  | 35/45 [00:03<00:00, 10.54it/s] 91%|█████████ | 41/45 [00:03<00:00, 16.53it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.17it/s]100%|██████████| 45/45 [00:04<00:00, 10.78it/s]
5640 images processed, 4.19916844367981 seconds used

19.018933057785034
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.91  81.20
LSUN          17.27  96.12
iSUN          72.13  81.76
dtd           37.82  91.37
forget        79.30  86.89
AVG           46.16  89.45
Retain-Acc: 0.7409
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.30 AUROC: 86.89 AUIN: 98.36
19.805618286132812
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.5 lr=0.001 epochs=20
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.5, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:25<08:06, 25.61s/it] 10%|█         | 2/20 [00:43<06:21, 21.19s/it] 15%|█▌        | 3/20 [01:03<05:47, 20.43s/it] 20%|██        | 4/20 [01:29<06:06, 22.91s/it] 25%|██▌       | 5/20 [01:48<05:17, 21.16s/it] 30%|███       | 6/20 [02:06<04:45, 20.42s/it] 35%|███▌      | 7/20 [02:30<04:39, 21.53s/it] 40%|████      | 8/20 [02:53<04:23, 21.94s/it][loss] ep 0 it 0 total=10.3048 mle=1.5710 pcon=5.2950 forget=3.4387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=10.3336 mle=1.5424 pcon=5.2879 forget=3.5033 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=10.4163 mle=1.7005 pcon=5.2809 forget=3.4350 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=10.5975 mle=1.8996 pcon=5.2738 forget=3.4241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=10.4366 mle=1.7130 pcon=5.2670 forget=3.4565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=10.2083 mle=1.5019 pcon=5.2603 forget=3.4460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=10.2684 mle=1.5596 pcon=5.2540 forget=3.4548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=10.4025 mle=1.6808 pcon=5.2476 forget=3.4741 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=10.4375 mle=1.6716 pcon=5.2409 forget=3.5250 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=10.2890 mle=1.6370 pcon=5.2346 forget=3.4174 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=10.2185 mle=1.5166 pcon=5.2285 forget=3.4734 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=10.4759 mle=1.7803 pcon=5.2224 forget=3.4732 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=10.4443 mle=1.7646 pcon=5.2167 forget=3.4630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=10.2369 mle=1.6052 pcon=5.2112 forget=3.4204 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=10.3744 mle=1.7237 pcon=5.2056 forget=3.4451 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=10.4595 mle=1.7942 pcon=5.2003 forget=3.4650 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=10.1895 mle=1.5529 pcon=5.1950 forget=3.4415 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=10.4715 mle=1.8882 pcon=5.1899 forget=3.3934 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=10.3657 mle=1.7418 pcon=5.1847 forget=3.4392 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=10.1175 mle=1.5263 pcon=5.1796 forget=3.4116 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=10.2244 mle=1.5857 pcon=5.1745 forget=3.4641 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=10.4795 mle=1.8840 pcon=5.1699 forget=3.4257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=10.1501 mle=1.5919 pcon=5.1652 forget=3.3931 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=10.3822 mle=1.8110 pcon=5.1605 forget=3.4107 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=10.2866 mle=1.7052 pcon=5.1559 forget=3.4254 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=10.2696 mle=1.6709 pcon=5.1518 forget=3.4469 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=10.3275 mle=1.7668 pcon=5.1474 forget=3.4133 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=10.4702 mle=1.9228 pcon=5.1435 forget=3.4040 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=10.1073 mle=1.5697 pcon=5.1394 forget=3.3982 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=10.3600 mle=1.8357 pcon=5.1352 forget=3.3890 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=10.2705 mle=1.7031 pcon=5.1310 forget=3.4364 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=10.2651 mle=1.6951 pcon=5.1274 forget=3.4426 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=10.1302 mle=1.6534 pcon=5.1236 forget=3.3532 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=10.2369 mle=1.7292 pcon=5.1195 forget=3.3881 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=10.1791 mle=1.6511 pcon=5.1158 forget=3.4122 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=9.9772 mle=1.4535 pcon=5.1123 forget=3.4114 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=10.2617 mle=1.7668 pcon=5.1085 forget=3.3864 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=9.9753 mle=1.4911 pcon=5.1049 forget=3.3794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=10.3798 mle=1.8717 pcon=5.1014 forget=3.4068 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=10.3532 mle=1.8665 pcon=5.0980 forget=3.3887 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=10.2538 mle=1.8005 pcon=5.0944 forget=3.3590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=10.0706 mle=1.5851 pcon=5.0911 forget=3.3943 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=10.0721 mle=1.5942 pcon=5.0876 forget=3.3903 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=10.1320 mle=1.6913 pcon=5.0847 forget=3.3561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=10.1945 mle=1.7745 pcon=5.0816 forget=3.3383 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=10.0383 mle=1.5976 pcon=5.0784 forget=3.3623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=10.1815 mle=1.7451 pcon=5.0755 forget=3.3610 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=9.8880 mle=1.3992 pcon=5.0725 forget=3.4163 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=9.9058 mle=1.5310 pcon=5.0696 forget=3.3053 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=10.0349 mle=1.5902 pcon=5.0668 forget=3.3779 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=10.0683 mle=1.6675 pcon=5.0638 forget=3.3369 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=10.0213 mle=1.6291 pcon=5.0614 forget=3.3307 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=10.2302 mle=1.8142 pcon=5.0583 forget=3.3578 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=9.9796 mle=1.5467 pcon=5.0556 forget=3.3773 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=10.0578 mle=1.6718 pcon=5.0528 forget=3.3332 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=9.9897 mle=1.6306 pcon=5.0499 forget=3.3092 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=10.0894 mle=1.7446 pcon=5.0471 forget=3.2976 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=9.9561 mle=1.6080 pcon=5.0442 forget=3.3039 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=9.8957 mle=1.5601 pcon=5.0414 forget=3.2943 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=10.0128 mle=1.6416 pcon=5.0390 forget=3.3322 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=10.1746 mle=1.7871 pcon=5.0363 forget=3.3513 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=10.1078 mle=1.7318 pcon=5.0334 forget=3.3427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=9.9721 mle=1.6296 pcon=5.0309 forget=3.3116 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 45%|████▌     | 9/20 [03:11<03:49, 20.82s/it] 50%|█████     | 10/20 [03:29<03:19, 19.95s/it] 55%|█████▌    | 11/20 [03:56<03:18, 22.07s/it] 60%|██████    | 12/20 [04:16<02:50, 21.28s/it] 65%|██████▌   | 13/20 [04:35<02:24, 20.69s/it] 70%|███████   | 14/20 [04:57<02:07, 21.18s/it] 75%|███████▌  | 15/20 [05:22<01:51, 22.27s/it] 80%|████████  | 16/20 [05:42<01:25, 21.39s/it] 85%|████████▌ | 17/20 [06:00<01:01, 20.54s/it][loss] ep 8 it 30 total=9.9187 mle=1.5421 pcon=5.0284 forget=3.3482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=9.9351 mle=1.5748 pcon=5.0260 forget=3.3343 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=9.8492 mle=1.5033 pcon=5.0235 forget=3.3225 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=10.0494 mle=1.7137 pcon=5.0211 forget=3.3145 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=9.8559 mle=1.5290 pcon=5.0192 forget=3.3077 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=10.0132 mle=1.6958 pcon=5.0169 forget=3.3006 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=10.1238 mle=1.7368 pcon=5.0145 forget=3.3725 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=9.8277 mle=1.4794 pcon=5.0124 forget=3.3359 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=9.9538 mle=1.6299 pcon=5.0104 forget=3.3135 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=10.0338 mle=1.6683 pcon=5.0082 forget=3.3573 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=10.2206 mle=1.8096 pcon=5.0067 forget=3.4043 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=9.9541 mle=1.5895 pcon=5.0052 forget=3.3594 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=9.9954 mle=1.5873 pcon=5.0038 forget=3.4044 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=9.9832 mle=1.5610 pcon=5.0022 forget=3.4200 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=9.9687 mle=1.5521 pcon=5.0010 forget=3.4156 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 0 total=10.0660 mle=1.6207 pcon=4.9996 forget=3.4457 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=10.0746 mle=1.6023 pcon=4.9983 forget=3.4739 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=10.2038 mle=1.7380 pcon=4.9972 forget=3.4685 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=10.2993 mle=1.8429 pcon=4.9964 forget=3.4600 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=10.3291 mle=1.8489 pcon=4.9954 forget=3.4848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=10.0827 mle=1.5885 pcon=4.9946 forget=3.4996 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=10.0674 mle=1.5694 pcon=4.9940 forget=3.5040 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=10.2231 mle=1.6558 pcon=4.9931 forget=3.5742 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 10 total=10.1007 mle=1.5704 pcon=4.9922 forget=3.5380 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=10.1256 mle=1.6374 pcon=4.9913 forget=3.4969 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=10.0572 mle=1.5684 pcon=4.9905 forget=3.4982 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=10.1826 mle=1.6615 pcon=4.9892 forget=3.5319 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=10.2695 mle=1.7665 pcon=4.9883 forget=3.5147 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=10.1276 mle=1.6136 pcon=4.9876 forget=3.5264 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=10.4864 mle=1.9746 pcon=4.9863 forget=3.5255 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=10.0908 mle=1.6697 pcon=4.9850 forget=3.4361 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 20 total=10.0749 mle=1.6085 pcon=4.9835 forget=3.4829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=10.1317 mle=1.6910 pcon=4.9821 forget=3.4586 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=10.0111 mle=1.5533 pcon=4.9806 forget=3.4771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=10.1093 mle=1.7440 pcon=4.9791 forget=3.3862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=9.8849 mle=1.5679 pcon=4.9774 forget=3.3396 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=9.8524 mle=1.5234 pcon=4.9760 forget=3.3529 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=9.6718 mle=1.4904 pcon=4.9740 forget=3.2074 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=10.0285 mle=1.8325 pcon=4.9718 forget=3.2241 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=9.5254 mle=1.4241 pcon=4.9696 forget=3.1318 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=9.6180 mle=1.4685 pcon=4.9670 forget=3.1825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=9.6051 mle=1.5271 pcon=4.9640 forget=3.1139 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=9.5754 mle=1.5489 pcon=4.9613 forget=3.0652 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=9.7354 mle=1.6682 pcon=4.9583 forget=3.1089 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=9.5537 mle=1.5530 pcon=4.9552 forget=3.0455 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=9.6948 mle=1.5692 pcon=4.9520 forget=3.1736 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=9.6238 mle=1.5505 pcon=4.9489 forget=3.1244 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=9.8902 mle=1.7727 pcon=4.9456 forget=3.1719 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=9.9269 mle=1.7798 pcon=4.9428 forget=3.2043 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=9.8038 mle=1.6155 pcon=4.9403 forget=3.2481 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=10.0556 mle=1.8118 pcon=4.9373 forget=3.3065 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=9.8496 mle=1.6277 pcon=4.9349 forget=3.2871 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=9.9646 mle=1.6724 pcon=4.9324 forget=3.3598 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=10.0071 mle=1.6824 pcon=4.9300 forget=3.3947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 0 total=10.0706 mle=1.6998 pcon=4.9279 forget=3.4429 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=9.9084 mle=1.5025 pcon=4.9256 forget=3.4803 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=10.0447 mle=1.5888 pcon=4.9236 forget=3.5323 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=10.1933 mle=1.6679 pcon=4.9218 forget=3.6036 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=10.1076 mle=1.5857 pcon=4.9201 forget=3.6018 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=10.5106 mle=1.9451 pcon=4.9185 forget=3.6471 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=10.1735 mle=1.5620 pcon=4.9172 forget=3.6944 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=10.2092 mle=1.5619 pcon=4.9159 forget=3.7315 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 10 total=10.2398 mle=1.5574 pcon=4.9146 forget=3.7678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=10.1904 mle=1.5092 pcon=4.9135 forget=3.7678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=10.2492 mle=1.5463 pcon=4.9126 forget=3.7904 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=10.3986 mle=1.6566 pcon=4.9113 forget=3.8308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=10.3453 mle=1.5748 pcon=4.9101 forget=3.8603 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=10.6834 mle=1.8910 pcon=4.9090 forget=3.8835 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=10.6153 mle=1.8282 pcon=4.9081 forget=3.8790 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=10.4176 mle=1.6182 pcon=4.9071 forget=3.8923 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 20 total=10.5662 mle=1.7243 pcon=4.9059 forget=3.9360 nr=64 nf=64 protos=540 fproto_sim=NA
 90%|█████████ | 18/20 [06:28<00:45, 22.70s/it] 95%|█████████▌| 19/20 [06:47<00:21, 21.59s/it]100%|██████████| 20/20 [07:06<00:00, 20.78s/it]100%|██████████| 20/20 [07:06<00:00, 21.31s/it]
[loss] ep 17 it 70 total=10.5709 mle=1.7590 pcon=4.9053 forget=3.9066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=10.4396 mle=1.5555 pcon=4.9043 forget=3.9798 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=10.4850 mle=1.5997 pcon=4.9031 forget=3.9823 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=10.5437 mle=1.6398 pcon=4.9022 forget=4.0018 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=10.3615 mle=1.4751 pcon=4.9016 forget=3.9848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=10.6995 mle=1.7763 pcon=4.9007 forget=4.0225 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=10.6322 mle=1.7145 pcon=4.8997 forget=4.0181 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=10.4575 mle=1.5326 pcon=4.8991 forget=4.0258 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=10.6240 mle=1.6327 pcon=4.8983 forget=4.0931 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=10.5386 mle=1.6109 pcon=4.8975 forget=4.0302 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=10.4924 mle=1.5563 pcon=4.8966 forget=4.0395 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=10.6075 mle=1.6412 pcon=4.8957 forget=4.0705 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=10.5539 mle=1.6231 pcon=4.8949 forget=4.0358 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=10.7022 mle=1.7406 pcon=4.8941 forget=4.0676 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=10.6650 mle=1.7061 pcon=4.8936 forget=4.0653 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 40 total=10.4135 mle=1.4593 pcon=4.8930 forget=4.0613 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=10.6838 mle=1.6624 pcon=4.8925 forget=4.1289 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=10.5157 mle=1.5266 pcon=4.8917 forget=4.0973 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=10.6457 mle=1.6626 pcon=4.8909 forget=4.0922 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=10.9475 mle=1.8884 pcon=4.8902 forget=4.1689 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=10.6943 mle=1.6318 pcon=4.8898 forget=4.1728 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=10.6635 mle=1.5518 pcon=4.8893 forget=4.2224 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:25,  2.69it/s]  3%|▎         | 10/391 [00:00<00:14, 26.69it/s]  4%|▍         | 16/391 [00:00<00:10, 34.90it/s]  6%|▌         | 22/391 [00:00<00:09, 38.88it/s]  7%|▋         | 28/391 [00:00<00:08, 42.99it/s]  9%|▉         | 37/391 [00:00<00:06, 54.22it/s] 12%|█▏        | 46/391 [00:01<00:05, 63.78it/s] 14%|█▍        | 55/391 [00:01<00:04, 70.77it/s] 16%|█▋        | 64/391 [00:01<00:04, 75.69it/s] 19%|█▊        | 73/391 [00:01<00:04, 76.28it/s] 21%|██        | 81/391 [00:01<00:04, 64.17it/s] 23%|██▎       | 88/391 [00:01<00:05, 60.38it/s] 25%|██▍       | 96/391 [00:01<00:04, 64.82it/s] 27%|██▋       | 105/391 [00:01<00:04, 70.81it/s] 29%|██▉       | 114/391 [00:01<00:03, 74.06it/s] 31%|███▏      | 123/391 [00:02<00:03, 77.47it/s] 34%|███▎      | 131/391 [00:02<00:03, 73.76it/s] 36%|███▌      | 139/391 [00:02<00:03, 64.50it/s] 37%|███▋      | 146/391 [00:02<00:04, 60.74it/s] 40%|███▉      | 155/391 [00:02<00:03, 67.69it/s] 42%|████▏     | 164/391 [00:02<00:03, 73.16it/s] 44%|████▍     | 173/391 [00:02<00:02, 76.82it/s] 47%|████▋     | 182/391 [00:02<00:02, 79.73it/s] 49%|████▉     | 191/391 [00:03<00:02, 78.00it/s] 51%|█████     | 199/391 [00:03<00:02, 64.77it/s] 53%|█████▎    | 206/391 [00:03<00:02, 61.80it/s] 55%|█████▍    | 215/391 [00:03<00:02, 68.21it/s] 57%|█████▋    | 223/391 [00:03<00:02, 71.19it/s] 59%|█████▉    | 232/391 [00:03<00:02, 75.81it/s] 62%|██████▏   | 241/391 [00:03<00:01, 78.70it/s] 64%|██████▍   | 250/391 [00:03<00:02, 65.87it/s] 66%|██████▌   | 258/391 [00:04<00:02, 61.20it/s] 68%|██████▊   | 267/391 [00:04<00:01, 66.55it/s] 71%|███████   | 276/391 [00:04<00:01, 71.51it/s] 73%|███████▎  | 285/391 [00:04<00:01, 75.32it/s] 75%|███████▌  | 294/391 [00:04<00:01, 78.84it/s] 77%|███████▋  | 303/391 [00:04<00:01, 69.19it/s] 80%|███████▉  | 311/391 [00:04<00:01, 61.85it/s] 82%|████████▏ | 319/391 [00:04<00:01, 64.84it/s] 84%|████████▍ | 328/391 [00:05<00:00, 70.61it/s] 86%|████████▌ | 337/391 [00:05<00:00, 73.96it/s] 88%|████████▊ | 346/391 [00:05<00:00, 77.79it/s] 91%|█████████ | 355/391 [00:05<00:00, 65.41it/s] 93%|█████████▎| 363/391 [00:05<00:00, 61.33it/s] 95%|█████████▌| 372/391 [00:05<00:00, 66.55it/s] 97%|█████████▋| 381/391 [00:05<00:00, 72.01it/s]100%|█████████▉| 390/391 [00:05<00:00, 76.49it/s]100%|██████████| 391/391 [00:05<00:00, 65.80it/s]
50000 images processed, 6.018564701080322 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:32,  2.39it/s]  8%|▊         | 6/79 [00:00<00:05, 14.21it/s] 15%|█▌        | 12/79 [00:00<00:02, 25.50it/s] 23%|██▎       | 18/79 [00:00<00:01, 34.46it/s] 34%|███▍      | 27/79 [00:00<00:01, 48.89it/s] 46%|████▌     | 36/79 [00:00<00:00, 58.91it/s] 56%|█████▌    | 44/79 [00:01<00:00, 63.80it/s] 66%|██████▌   | 52/79 [00:01<00:00, 56.79it/s] 75%|███████▍  | 59/79 [00:01<00:00, 55.63it/s] 86%|████████▌ | 68/79 [00:01<00:00, 62.59it/s] 97%|█████████▋| 77/79 [00:01<00:00, 69.37it/s]100%|██████████| 79/79 [00:01<00:00, 49.18it/s]
10000 images processed, 1.629277229309082 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:26,  2.36it/s]  3%|▎         | 6/204 [00:00<00:14, 13.90it/s]  5%|▌         | 11/204 [00:00<00:08, 22.97it/s] 10%|▉         | 20/204 [00:00<00:04, 40.13it/s] 14%|█▍        | 29/204 [00:00<00:03, 52.96it/s] 19%|█▊        | 38/204 [00:00<00:02, 62.45it/s] 23%|██▎       | 47/204 [00:01<00:02, 68.98it/s] 27%|██▋       | 55/204 [00:01<00:02, 57.75it/s] 30%|███       | 62/204 [00:01<00:02, 55.66it/s] 35%|███▍      | 71/204 [00:01<00:02, 63.06it/s] 39%|███▉      | 80/204 [00:01<00:01, 69.11it/s] 44%|████▎     | 89/204 [00:01<00:01, 72.84it/s] 48%|████▊     | 97/204 [00:01<00:01, 63.73it/s] 51%|█████     | 104/204 [00:02<00:01, 60.34it/s] 54%|█████▍    | 111/204 [00:02<00:01, 61.43it/s] 59%|█████▉    | 120/204 [00:02<00:01, 67.87it/s] 63%|██████▎   | 129/204 [00:02<00:01, 72.94it/s] 68%|██████▊   | 138/204 [00:02<00:00, 77.05it/s] 72%|███████▏  | 146/204 [00:02<00:00, 75.09it/s] 75%|███████▌  | 154/204 [00:02<00:00, 63.45it/s] 79%|███████▉  | 161/204 [00:02<00:00, 60.77it/s] 83%|████████▎ | 170/204 [00:02<00:00, 66.57it/s] 88%|████████▊ | 179/204 [00:03<00:00, 71.72it/s] 92%|█████████▏| 188/204 [00:03<00:00, 75.50it/s] 96%|█████████▌| 196/204 [00:03<00:00, 69.52it/s]100%|██████████| 204/204 [00:03<00:00, 61.87it/s]100%|██████████| 204/204 [00:03<00:00, 58.87it/s]
26032 images processed, 3.522184133529663 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:48,  1.62it/s] 11%|█▏        | 9/79 [00:00<00:05, 13.71it/s] 19%|█▉        | 15/79 [00:00<00:02, 21.70it/s] 27%|██▋       | 21/79 [00:01<00:02, 28.99it/s] 33%|███▎      | 26/79 [00:01<00:01, 29.29it/s] 42%|████▏     | 33/79 [00:01<00:01, 34.10it/s] 52%|█████▏    | 41/79 [00:01<00:00, 39.88it/s] 58%|█████▊    | 46/79 [00:01<00:00, 40.38it/s] 66%|██████▌   | 52/79 [00:01<00:00, 43.79it/s] 73%|███████▎  | 58/79 [00:01<00:00, 46.55it/s] 81%|████████  | 64/79 [00:01<00:00, 49.85it/s] 89%|████████▊ | 70/79 [00:02<00:00, 44.61it/s] 96%|█████████▌| 76/79 [00:02<00:00, 47.16it/s]100%|██████████| 79/79 [00:02<00:00, 34.58it/s]
10000 images processed, 2.324846029281616 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:34,  2.28it/s]  9%|▉         | 7/79 [00:00<00:04, 15.73it/s] 20%|██        | 16/79 [00:00<00:01, 33.99it/s] 32%|███▏      | 25/79 [00:00<00:01, 48.21it/s] 43%|████▎     | 34/79 [00:00<00:00, 58.49it/s] 53%|█████▎    | 42/79 [00:00<00:00, 64.35it/s] 63%|██████▎   | 50/79 [00:01<00:00, 54.93it/s] 72%|███████▏  | 57/79 [00:01<00:00, 54.18it/s] 84%|████████▎ | 66/79 [00:01<00:00, 62.11it/s] 95%|█████████▍| 75/79 [00:01<00:00, 69.06it/s]100%|██████████| 79/79 [00:01<00:00, 51.34it/s]
10000 images processed, 1.5577311515808105 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:26,  2.57it/s]  9%|▊         | 6/70 [00:00<00:04, 14.54it/s] 17%|█▋        | 12/70 [00:00<00:02, 26.31it/s] 27%|██▋       | 19/70 [00:00<00:01, 37.94it/s] 40%|████      | 28/70 [00:00<00:00, 51.66it/s] 53%|█████▎    | 37/70 [00:00<00:00, 61.96it/s] 66%|██████▌   | 46/70 [00:01<00:00, 69.07it/s] 77%|███████▋  | 54/70 [00:01<00:00, 63.29it/s] 87%|████████▋ | 61/70 [00:01<00:00, 60.29it/s] 97%|█████████▋| 68/70 [00:01<00:00, 59.02it/s]100%|██████████| 70/70 [00:01<00:00, 47.83it/s]
8925 images processed, 1.5014939308166504 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:35,  1.22it/s]  4%|▍         | 2/45 [00:01<00:20,  2.08it/s] 20%|██        | 9/45 [00:01<00:03, 11.42it/s] 27%|██▋       | 12/45 [00:01<00:03,  8.54it/s] 38%|███▊      | 17/45 [00:01<00:02, 13.31it/s] 44%|████▍     | 20/45 [00:02<00:02, 10.45it/s] 56%|█████▌    | 25/45 [00:02<00:01, 14.92it/s] 62%|██████▏   | 28/45 [00:02<00:01, 10.98it/s] 76%|███████▌  | 34/45 [00:03<00:01,  9.47it/s] 93%|█████████▎| 42/45 [00:04<00:00,  9.54it/s]100%|██████████| 45/45 [00:04<00:00,  9.96it/s]
5640 images processed, 4.539778232574463 seconds used

22.83896231651306
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.80  99.32
places365     68.97  80.75
LSUN          19.16  95.89
iSUN          72.37  81.33
dtd           39.27  90.99
forget        75.10  87.26
AVG           46.28  89.26
Retain-Acc: 0.7409
Forget-as-OOD (retain known vs forget novel):
  FPR: 75.10 AUROC: 87.26 AUIN: 98.40
20.364810943603516
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.5 lr=0.001 epochs=50
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.5, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:28<23:04, 28.26s/it]  4%|▍         | 2/50 [00:48<18:53, 23.61s/it]  6%|▌         | 3/50 [01:06<16:23, 20.93s/it]  8%|▊         | 4/50 [01:25<15:30, 20.23s/it] 10%|█         | 5/50 [01:51<16:43, 22.31s/it] 12%|█▏        | 6/50 [02:08<15:08, 20.65s/it] 14%|█▍        | 7/50 [02:26<14:02, 19.58s/it] 16%|█▌        | 8/50 [02:46<13:47, 19.71s/it][loss] ep 0 it 0 total=10.3047 mle=1.5709 pcon=5.2950 forget=3.4387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=10.3335 mle=1.5422 pcon=5.2879 forget=3.5033 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=10.4163 mle=1.7004 pcon=5.2809 forget=3.4350 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=10.5977 mle=1.8998 pcon=5.2738 forget=3.4241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=10.4366 mle=1.7130 pcon=5.2670 forget=3.4565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=10.2082 mle=1.5019 pcon=5.2603 forget=3.4460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=10.2685 mle=1.5597 pcon=5.2541 forget=3.4548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=10.4025 mle=1.6808 pcon=5.2476 forget=3.4740 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=10.4375 mle=1.6716 pcon=5.2409 forget=3.5250 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=10.2891 mle=1.6371 pcon=5.2346 forget=3.4174 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=10.2185 mle=1.5166 pcon=5.2284 forget=3.4734 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=10.4761 mle=1.7804 pcon=5.2225 forget=3.4733 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=10.4442 mle=1.7645 pcon=5.2167 forget=3.4630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=10.2367 mle=1.6050 pcon=5.2112 forget=3.4205 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=10.3743 mle=1.7236 pcon=5.2056 forget=3.4451 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=10.4595 mle=1.7942 pcon=5.2003 forget=3.4651 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=10.1895 mle=1.5529 pcon=5.1950 forget=3.4416 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=10.4713 mle=1.8880 pcon=5.1899 forget=3.3934 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=10.3658 mle=1.7419 pcon=5.1847 forget=3.4392 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=10.1176 mle=1.5264 pcon=5.1796 forget=3.4116 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=10.2245 mle=1.5859 pcon=5.1745 forget=3.4641 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=10.4795 mle=1.8839 pcon=5.1699 forget=3.4257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=10.1501 mle=1.5919 pcon=5.1652 forget=3.3931 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=10.3824 mle=1.8112 pcon=5.1605 forget=3.4107 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=10.2868 mle=1.7054 pcon=5.1559 forget=3.4254 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=10.2697 mle=1.6710 pcon=5.1518 forget=3.4469 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=10.3274 mle=1.7667 pcon=5.1474 forget=3.4133 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=10.4703 mle=1.9228 pcon=5.1435 forget=3.4040 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=10.1074 mle=1.5698 pcon=5.1394 forget=3.3982 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=10.3599 mle=1.8356 pcon=5.1352 forget=3.3891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=10.2706 mle=1.7031 pcon=5.1310 forget=3.4364 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=10.2651 mle=1.6951 pcon=5.1274 forget=3.4426 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=10.1302 mle=1.6535 pcon=5.1235 forget=3.3532 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=10.2368 mle=1.7291 pcon=5.1195 forget=3.3882 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=10.1789 mle=1.6509 pcon=5.1158 forget=3.4122 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=9.9771 mle=1.4534 pcon=5.1123 forget=3.4114 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=10.2616 mle=1.7667 pcon=5.1085 forget=3.3864 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=9.9752 mle=1.4909 pcon=5.1049 forget=3.3794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=10.3801 mle=1.8719 pcon=5.1014 forget=3.4068 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=10.3534 mle=1.8667 pcon=5.0980 forget=3.3887 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=10.2540 mle=1.8006 pcon=5.0944 forget=3.3590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=10.0708 mle=1.5854 pcon=5.0911 forget=3.3943 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=10.0721 mle=1.5942 pcon=5.0876 forget=3.3903 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=10.1322 mle=1.6915 pcon=5.0847 forget=3.3561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=10.1946 mle=1.7746 pcon=5.0816 forget=3.3383 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=10.0383 mle=1.5975 pcon=5.0784 forget=3.3623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=10.1813 mle=1.7450 pcon=5.0754 forget=3.3609 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=9.8879 mle=1.3991 pcon=5.0725 forget=3.4163 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=9.9059 mle=1.5310 pcon=5.0696 forget=3.3053 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=10.0349 mle=1.5902 pcon=5.0668 forget=3.3779 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=10.0681 mle=1.6674 pcon=5.0638 forget=3.3369 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=10.0214 mle=1.6292 pcon=5.0614 forget=3.3308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=10.2303 mle=1.8143 pcon=5.0583 forget=3.3578 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=9.9797 mle=1.5467 pcon=5.0556 forget=3.3773 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=10.0579 mle=1.6719 pcon=5.0528 forget=3.3332 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=9.9895 mle=1.6305 pcon=5.0498 forget=3.3092 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=10.0895 mle=1.7448 pcon=5.0471 forget=3.2976 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=9.9562 mle=1.6081 pcon=5.0442 forget=3.3039 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=9.8956 mle=1.5600 pcon=5.0414 forget=3.2942 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=10.0128 mle=1.6417 pcon=5.0390 forget=3.3322 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=10.1746 mle=1.7870 pcon=5.0363 forget=3.3513 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=10.1077 mle=1.7317 pcon=5.0334 forget=3.3426 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=9.9720 mle=1.6296 pcon=5.0309 forget=3.3115 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 18%|█▊        | 9/50 [03:11<14:43, 21.55s/it] 20%|██        | 10/50 [03:29<13:35, 20.39s/it] 22%|██▏       | 11/50 [03:47<12:50, 19.75s/it] 24%|██▍       | 12/50 [04:10<12:58, 20.48s/it] 26%|██▌       | 13/50 [04:33<13:05, 21.22s/it] 28%|██▊       | 14/50 [04:50<12:05, 20.15s/it] 30%|███       | 15/50 [05:08<11:19, 19.41s/it] 32%|███▏      | 16/50 [05:31<11:36, 20.47s/it] 34%|███▍      | 17/50 [05:54<11:38, 21.17s/it][loss] ep 8 it 30 total=9.9191 mle=1.5423 pcon=5.0285 forget=3.3483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=9.9351 mle=1.5748 pcon=5.0260 forget=3.3343 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=9.8495 mle=1.5035 pcon=5.0235 forget=3.3225 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=10.0493 mle=1.7136 pcon=5.0211 forget=3.3146 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=9.8559 mle=1.5290 pcon=5.0192 forget=3.3077 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=10.0132 mle=1.6958 pcon=5.0169 forget=3.3006 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=10.1238 mle=1.7367 pcon=5.0145 forget=3.3726 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=9.8279 mle=1.4796 pcon=5.0124 forget=3.3359 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=9.9539 mle=1.6300 pcon=5.0104 forget=3.3135 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=10.0340 mle=1.6685 pcon=5.0082 forget=3.3573 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=10.2206 mle=1.8095 pcon=5.0067 forget=3.4043 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=9.9541 mle=1.5895 pcon=5.0052 forget=3.3594 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=9.9955 mle=1.5873 pcon=5.0038 forget=3.4044 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=9.9831 mle=1.5609 pcon=5.0022 forget=3.4200 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=9.9687 mle=1.5521 pcon=5.0010 forget=3.4155 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 0 total=10.0662 mle=1.6208 pcon=4.9996 forget=3.4457 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=10.0744 mle=1.6022 pcon=4.9983 forget=3.4739 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=10.2035 mle=1.7377 pcon=4.9973 forget=3.4685 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=10.2995 mle=1.8431 pcon=4.9963 forget=3.4600 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=10.3291 mle=1.8490 pcon=4.9954 forget=3.4847 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=10.0825 mle=1.5884 pcon=4.9946 forget=3.4995 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=10.0670 mle=1.5692 pcon=4.9940 forget=3.5038 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=10.2227 mle=1.6556 pcon=4.9931 forget=3.5740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 10 total=10.1000 mle=1.5703 pcon=4.9922 forget=3.5375 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=10.1247 mle=1.6373 pcon=4.9913 forget=3.4961 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=10.0560 mle=1.5685 pcon=4.9905 forget=3.4970 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=10.1804 mle=1.6612 pcon=4.9892 forget=3.5300 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=10.2668 mle=1.7664 pcon=4.9882 forget=3.5122 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=10.1229 mle=1.6131 pcon=4.9875 forget=3.5223 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=10.4806 mle=1.9741 pcon=4.9862 forget=3.5203 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=10.0820 mle=1.6695 pcon=4.9849 forget=3.4276 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 20 total=10.0632 mle=1.6079 pcon=4.9834 forget=3.4719 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=10.1144 mle=1.6896 pcon=4.9819 forget=3.4429 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=9.9877 mle=1.5520 pcon=4.9804 forget=3.4553 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=10.0785 mle=1.7438 pcon=4.9788 forget=3.3559 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=9.8346 mle=1.5659 pcon=4.9769 forget=3.2918 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=9.7877 mle=1.5223 pcon=4.9753 forget=3.2901 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=9.5934 mle=1.4887 pcon=4.9730 forget=3.1317 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=9.9624 mle=1.8364 pcon=4.9705 forget=3.1555 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=9.4563 mle=1.4273 pcon=4.9678 forget=3.0612 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=9.5630 mle=1.4766 pcon=4.9647 forget=3.1217 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=9.5625 mle=1.5138 pcon=4.9613 forget=3.0873 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=9.5774 mle=1.5650 pcon=4.9581 forget=3.0544 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=9.7381 mle=1.6751 pcon=4.9547 forget=3.1084 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=9.5850 mle=1.5606 pcon=4.9513 forget=3.0732 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=9.7347 mle=1.5770 pcon=4.9478 forget=3.2099 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=9.6967 mle=1.5668 pcon=4.9446 forget=3.1854 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=9.9737 mle=1.7914 pcon=4.9412 forget=3.2411 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=10.0125 mle=1.7915 pcon=4.9383 forget=3.2827 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=9.9129 mle=1.6393 pcon=4.9357 forget=3.3380 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=10.1399 mle=1.8108 pcon=4.9327 forget=3.3964 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=9.9779 mle=1.6386 pcon=4.9302 forget=3.4091 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=10.0648 mle=1.6669 pcon=4.9277 forget=3.4703 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=10.1323 mle=1.7037 pcon=4.9252 forget=3.5034 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 0 total=10.1994 mle=1.7205 pcon=4.9229 forget=3.5560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=10.0299 mle=1.5104 pcon=4.9204 forget=3.5990 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=10.1385 mle=1.5897 pcon=4.9182 forget=3.6307 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=10.2535 mle=1.6776 pcon=4.9161 forget=3.6598 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=10.1873 mle=1.6077 pcon=4.9140 forget=3.6656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=10.5936 mle=1.9688 pcon=4.9119 forget=3.7130 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=10.2151 mle=1.5681 pcon=4.9099 forget=3.7371 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=10.2232 mle=1.5441 pcon=4.9080 forget=3.7711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 10 total=10.1572 mle=1.5614 pcon=4.9058 forget=3.6900 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=10.1327 mle=1.5191 pcon=4.9038 forget=3.7098 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=10.1330 mle=1.5485 pcon=4.9018 forget=3.6827 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=10.1495 mle=1.5833 pcon=4.8992 forget=3.6671 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=10.0679 mle=1.5547 pcon=4.8965 forget=3.6167 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=10.3894 mle=1.8720 pcon=4.8935 forget=3.6239 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=10.2003 mle=1.7845 pcon=4.8906 forget=3.5252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=9.9489 mle=1.5756 pcon=4.8876 forget=3.4857 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 20 total=10.0944 mle=1.7525 pcon=4.8840 forget=3.4579 nr=64 nf=64 protos=540 fproto_sim=NA
 36%|███▌      | 18/50 [06:12<10:47, 20.22s/it] 38%|███▊      | 19/50 [06:30<10:12, 19.76s/it] 40%|████      | 20/50 [06:56<10:50, 21.67s/it] 42%|████▏     | 21/50 [07:15<10:02, 20.76s/it] 44%|████▍     | 22/50 [07:33<09:19, 19.99s/it] 46%|████▌     | 23/50 [07:54<09:05, 20.20s/it] 48%|████▊     | 24/50 [08:19<09:20, 21.54s/it] 50%|█████     | 25/50 [08:37<08:30, 20.44s/it] 52%|█████▏    | 26/50 [08:55<07:53, 19.71s/it][loss] ep 17 it 70 total=10.0095 mle=1.7661 pcon=4.8808 forget=3.3627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=9.8267 mle=1.5831 pcon=4.8772 forget=3.3665 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=9.7595 mle=1.5499 pcon=4.8732 forget=3.3364 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=9.7900 mle=1.6068 pcon=4.8694 forget=3.3138 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=9.5934 mle=1.4871 pcon=4.8660 forget=3.2403 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=9.8464 mle=1.7929 pcon=4.8622 forget=3.1914 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=9.7759 mle=1.7363 pcon=4.8580 forget=3.1816 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=9.4907 mle=1.5025 pcon=4.8544 forget=3.1338 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=9.6095 mle=1.6064 pcon=4.8504 forget=3.1526 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=9.5933 mle=1.6324 pcon=4.8464 forget=3.1144 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=9.4129 mle=1.5266 pcon=4.8423 forget=3.0440 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=9.5452 mle=1.6699 pcon=4.8381 forget=3.0372 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=9.4906 mle=1.6209 pcon=4.8341 forget=3.0356 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=9.5720 mle=1.7134 pcon=4.8300 forget=3.0286 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=9.5395 mle=1.6663 pcon=4.8261 forget=3.0472 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 19 it 40 total=9.4737 mle=1.5761 pcon=4.8222 forget=3.0754 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=9.5088 mle=1.6055 pcon=4.8184 forget=3.0850 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=9.4644 mle=1.5697 pcon=4.8144 forget=3.0803 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=9.5385 mle=1.6389 pcon=4.8107 forget=3.0889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=9.7449 mle=1.8283 pcon=4.8071 forget=3.1095 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=9.6283 mle=1.6809 pcon=4.8039 forget=3.1436 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=9.4872 mle=1.5256 pcon=4.8006 forget=3.1610 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 20 it 0 total=9.6205 mle=1.6610 pcon=4.7974 forget=3.1620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=9.6545 mle=1.6810 pcon=4.7944 forget=3.1792 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=9.7186 mle=1.6894 pcon=4.7915 forget=3.2376 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=9.8210 mle=1.8083 pcon=4.7885 forget=3.2242 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=9.5899 mle=1.5577 pcon=4.7857 forget=3.2464 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=9.9384 mle=1.8900 pcon=4.7831 forget=3.2654 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=9.8444 mle=1.7732 pcon=4.7805 forget=3.2907 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=9.5837 mle=1.5059 pcon=4.7781 forget=3.2998 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 10 total=9.5766 mle=1.4860 pcon=4.7758 forget=3.3148 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=9.7331 mle=1.6357 pcon=4.7736 forget=3.3237 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=9.6934 mle=1.5618 pcon=4.7717 forget=3.3599 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=9.7122 mle=1.5887 pcon=4.7698 forget=3.3538 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=9.8886 mle=1.7236 pcon=4.7680 forget=3.3969 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=9.7926 mle=1.6298 pcon=4.7663 forget=3.3966 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=9.7451 mle=1.5802 pcon=4.7645 forget=3.4004 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=9.7433 mle=1.5447 pcon=4.7629 forget=3.4357 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 20 total=9.7811 mle=1.6029 pcon=4.7612 forget=3.4169 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=9.8398 mle=1.6645 pcon=4.7596 forget=3.4157 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=9.7875 mle=1.5680 pcon=4.7580 forget=3.4615 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=9.7894 mle=1.5741 pcon=4.7567 forget=3.4587 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=9.7740 mle=1.5773 pcon=4.7554 forget=3.4413 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=9.6954 mle=1.4910 pcon=4.7540 forget=3.4504 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=9.8914 mle=1.6936 pcon=4.7529 forget=3.4449 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=10.0913 mle=1.8681 pcon=4.7515 forget=3.4718 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 30 total=9.7261 mle=1.5246 pcon=4.7505 forget=3.4510 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=9.8349 mle=1.6372 pcon=4.7492 forget=3.4485 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=9.7926 mle=1.5895 pcon=4.7478 forget=3.4553 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=9.9043 mle=1.7281 pcon=4.7464 forget=3.4297 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=9.7633 mle=1.5803 pcon=4.7452 forget=3.4378 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=9.8188 mle=1.6111 pcon=4.7438 forget=3.4640 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=9.8188 mle=1.6263 pcon=4.7424 forget=3.4501 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=9.7776 mle=1.5876 pcon=4.7411 forget=3.4489 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 40 total=9.7256 mle=1.5459 pcon=4.7398 forget=3.4399 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=9.7920 mle=1.6306 pcon=4.7385 forget=3.4230 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=9.7513 mle=1.5949 pcon=4.7370 forget=3.4193 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=9.7798 mle=1.6379 pcon=4.7357 forget=3.4062 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 240 total=9.6337 mle=1.5100 pcon=4.7344 forget=3.3893 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=9.8324 mle=1.7146 pcon=4.7330 forget=3.3848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=9.7232 mle=1.6087 pcon=4.7316 forget=3.3828 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 0 total=9.7022 mle=1.5823 pcon=4.7301 forget=3.3898 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=9.8298 mle=1.7341 pcon=4.7289 forget=3.3668 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=9.6927 mle=1.6112 pcon=4.7274 forget=3.3541 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 150 total=9.8184 mle=1.7639 pcon=4.7261 forget=3.3284 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 200 total=9.8632 mle=1.7889 pcon=4.7246 forget=3.3497 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=9.8193 mle=1.7172 pcon=4.7231 forget=3.3790 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=9.6210 mle=1.5920 pcon=4.7216 forget=3.3074 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=9.5468 mle=1.5260 pcon=4.7205 forget=3.3003 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 10 total=9.6292 mle=1.6127 pcon=4.7191 forget=3.2973 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=9.5915 mle=1.5690 pcon=4.7178 forget=3.3048 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=9.6943 mle=1.7032 pcon=4.7164 forget=3.2747 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=9.5114 mle=1.5478 pcon=4.7150 forget=3.2486 nr=64 nf=64 protos=540 fproto_sim=NA
 54%|█████▍    | 27/50 [09:19<08:04, 21.04s/it] 56%|█████▌    | 28/50 [09:40<07:45, 21.18s/it] 58%|█████▊    | 29/50 [09:59<07:09, 20.46s/it] 60%|██████    | 30/50 [10:17<06:35, 19.78s/it] 62%|██████▏   | 31/50 [10:44<06:54, 21.80s/it] 64%|██████▍   | 32/50 [11:02<06:10, 20.60s/it] 66%|██████▌   | 33/50 [11:20<05:38, 19.94s/it] 68%|██████▊   | 34/50 [11:41<05:25, 20.35s/it] 70%|███████   | 35/50 [12:06<05:26, 21.76s/it] 72%|███████▏  | 36/50 [12:25<04:51, 20.85s/it][loss] ep 26 it 210 total=9.6221 mle=1.6443 pcon=4.7135 forget=3.2643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=9.4950 mle=1.5095 pcon=4.7123 forget=3.2733 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 310 total=9.8716 mle=1.9079 pcon=4.7110 forget=3.2527 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=9.5130 mle=1.5621 pcon=4.7096 forget=3.2413 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 20 total=9.5791 mle=1.6310 pcon=4.7083 forget=3.2398 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=9.5227 mle=1.5743 pcon=4.7069 forget=3.2416 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=9.5194 mle=1.5711 pcon=4.7055 forget=3.2428 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=9.4896 mle=1.5340 pcon=4.7042 forget=3.2513 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=9.4421 mle=1.5002 pcon=4.7029 forget=3.2390 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=9.5288 mle=1.5973 pcon=4.7016 forget=3.2300 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=9.6603 mle=1.7010 pcon=4.7005 forget=3.2588 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=9.6573 mle=1.6966 pcon=4.6994 forget=3.2614 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 30 total=9.6024 mle=1.6600 pcon=4.6984 forget=3.2439 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=9.6335 mle=1.6821 pcon=4.6975 forget=3.2540 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=9.5214 mle=1.5774 pcon=4.6966 forget=3.2475 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=9.5123 mle=1.5749 pcon=4.6956 forget=3.2419 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=9.3657 mle=1.4338 pcon=4.6944 forget=3.2375 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=9.5317 mle=1.5730 pcon=4.6934 forget=3.2653 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=9.6997 mle=1.7463 pcon=4.6922 forget=3.2612 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=9.6242 mle=1.6784 pcon=4.6912 forget=3.2546 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 40 total=9.6137 mle=1.6864 pcon=4.6901 forget=3.2372 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=9.4670 mle=1.5356 pcon=4.6891 forget=3.2424 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=9.5957 mle=1.6570 pcon=4.6882 forget=3.2505 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=9.5635 mle=1.6208 pcon=4.6873 forget=3.2553 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=9.5193 mle=1.5746 pcon=4.6866 forget=3.2582 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=9.6404 mle=1.7025 pcon=4.6859 forget=3.2521 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=9.5106 mle=1.5654 pcon=4.6852 forget=3.2600 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 0 total=9.5567 mle=1.6192 pcon=4.6844 forget=3.2532 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=9.5884 mle=1.6203 pcon=4.6838 forget=3.2843 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=9.5166 mle=1.5679 pcon=4.6831 forget=3.2656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=9.7084 mle=1.7680 pcon=4.6824 forget=3.2580 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=9.4759 mle=1.4879 pcon=4.6817 forget=3.3064 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=9.7553 mle=1.7863 pcon=4.6810 forget=3.2881 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=9.4523 mle=1.4817 pcon=4.6804 forget=3.2902 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=9.6969 mle=1.7352 pcon=4.6798 forget=3.2819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 10 total=9.5998 mle=1.6558 pcon=4.6792 forget=3.2648 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=9.4886 mle=1.5347 pcon=4.6787 forget=3.2752 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=9.5468 mle=1.5782 pcon=4.6782 forget=3.2904 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=9.5196 mle=1.5592 pcon=4.6776 forget=3.2828 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=9.5080 mle=1.5645 pcon=4.6770 forget=3.2665 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 260 total=9.4684 mle=1.5000 pcon=4.6764 forget=3.2921 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=9.6013 mle=1.6092 pcon=4.6758 forget=3.3163 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=9.5081 mle=1.5505 pcon=4.6752 forget=3.2824 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 20 total=9.6577 mle=1.7022 pcon=4.6747 forget=3.2809 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=9.5388 mle=1.5765 pcon=4.6741 forget=3.2882 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=9.5716 mle=1.5720 pcon=4.6735 forget=3.3261 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 170 total=9.5554 mle=1.5542 pcon=4.6732 forget=3.3280 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=9.6305 mle=1.6507 pcon=4.6728 forget=3.3070 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=9.5449 mle=1.5606 pcon=4.6723 forget=3.3120 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=9.4529 mle=1.4921 pcon=4.6719 forget=3.2889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=9.4609 mle=1.4680 pcon=4.6716 forget=3.3213 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 30 total=9.6290 mle=1.6220 pcon=4.6713 forget=3.3357 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=9.8367 mle=1.8734 pcon=4.6709 forget=3.2925 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=9.4979 mle=1.5033 pcon=4.6705 forget=3.3241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=9.7211 mle=1.7051 pcon=4.6701 forget=3.3459 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=9.6006 mle=1.6190 pcon=4.6697 forget=3.3119 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=9.6723 mle=1.6726 pcon=4.6693 forget=3.3304 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=9.6677 mle=1.6853 pcon=4.6690 forget=3.3134 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 380 total=9.6277 mle=1.5894 pcon=4.6685 forget=3.3697 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 40 total=9.7006 mle=1.7121 pcon=4.6680 forget=3.3205 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=9.5229 mle=1.5144 pcon=4.6675 forget=3.3409 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 140 total=9.5829 mle=1.6185 pcon=4.6672 forget=3.2972 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=9.6532 mle=1.6523 pcon=4.6668 forget=3.3340 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 240 total=9.6984 mle=1.6882 pcon=4.6664 forget=3.3439 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=9.6168 mle=1.6030 pcon=4.6659 forget=3.3479 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=9.6024 mle=1.5698 pcon=4.6655 forget=3.3671 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 0 total=9.6028 mle=1.5936 pcon=4.6651 forget=3.3442 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 50 total=9.6193 mle=1.5848 pcon=4.6647 forget=3.3698 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 100 total=9.8282 mle=1.7953 pcon=4.6642 forget=3.3687 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 150 total=9.6412 mle=1.6368 pcon=4.6638 forget=3.3406 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 200 total=9.5770 mle=1.5647 pcon=4.6634 forget=3.3489 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 250 total=9.7266 mle=1.7106 pcon=4.6630 forget=3.3531 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 300 total=9.5725 mle=1.5577 pcon=4.6625 forget=3.3523 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 350 total=9.7050 mle=1.7003 pcon=4.6622 forget=3.3425 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 10 total=9.7518 mle=1.7415 pcon=4.6618 forget=3.3484 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 60 total=9.5627 mle=1.5531 pcon=4.6616 forget=3.3480 nr=64 nf=64 protos=540 fproto_sim=NA
 74%|███████▍  | 37/50 [12:43<04:21, 20.10s/it] 76%|███████▌  | 38/50 [13:09<04:19, 21.64s/it] 78%|███████▊  | 39/50 [13:30<03:57, 21.62s/it] 80%|████████  | 40/50 [13:49<03:26, 20.69s/it] 82%|████████▏ | 41/50 [14:10<03:06, 20.77s/it] 84%|████████▍ | 42/50 [14:37<03:00, 22.62s/it] 86%|████████▌ | 43/50 [14:56<02:31, 21.64s/it] 88%|████████▊ | 44/50 [15:14<02:04, 20.69s/it] 90%|█████████ | 45/50 [15:40<01:50, 22.03s/it] 92%|█████████▏| 46/50 [16:02<01:28, 22.20s/it][loss] ep 36 it 110 total=9.5985 mle=1.5728 pcon=4.6612 forget=3.3645 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 160 total=9.6517 mle=1.6353 pcon=4.6608 forget=3.3555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 210 total=9.5364 mle=1.5098 pcon=4.6605 forget=3.3661 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 260 total=9.6072 mle=1.5797 pcon=4.6604 forget=3.3671 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 310 total=9.7345 mle=1.6700 pcon=4.6599 forget=3.4045 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 360 total=9.6665 mle=1.6399 pcon=4.6596 forget=3.3671 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 20 total=9.5351 mle=1.4962 pcon=4.6591 forget=3.3798 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 70 total=9.7254 mle=1.6626 pcon=4.6588 forget=3.4040 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 120 total=9.6624 mle=1.6138 pcon=4.6585 forget=3.3901 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 170 total=9.5878 mle=1.5671 pcon=4.6581 forget=3.3626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 220 total=9.7906 mle=1.7205 pcon=4.6577 forget=3.4124 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 270 total=9.6643 mle=1.6021 pcon=4.6573 forget=3.4050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 320 total=9.6881 mle=1.6205 pcon=4.6569 forget=3.4106 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 370 total=9.7288 mle=1.6135 pcon=4.6566 forget=3.4587 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 30 total=9.7534 mle=1.6745 pcon=4.6565 forget=3.4224 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 80 total=9.5450 mle=1.4796 pcon=4.6563 forget=3.4091 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 130 total=9.6647 mle=1.6023 pcon=4.6560 forget=3.4064 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 180 total=9.6182 mle=1.5680 pcon=4.6557 forget=3.3946 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 230 total=9.5893 mle=1.5168 pcon=4.6554 forget=3.4170 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 280 total=9.7435 mle=1.6843 pcon=4.6552 forget=3.4041 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 330 total=9.6458 mle=1.5899 pcon=4.6548 forget=3.4011 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 380 total=9.5385 mle=1.4945 pcon=4.6546 forget=3.3893 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 40 total=9.7525 mle=1.6981 pcon=4.6546 forget=3.3998 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 90 total=9.7357 mle=1.6240 pcon=4.6543 forget=3.4574 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 140 total=9.6737 mle=1.6022 pcon=4.6541 forget=3.4174 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 190 total=9.5158 mle=1.4603 pcon=4.6539 forget=3.4016 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 240 total=9.7215 mle=1.6288 pcon=4.6537 forget=3.4390 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 290 total=9.5445 mle=1.4829 pcon=4.6535 forget=3.4082 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 340 total=9.6629 mle=1.5794 pcon=4.6533 forget=3.4302 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 0 total=9.6213 mle=1.5477 pcon=4.6530 forget=3.4206 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 50 total=9.6978 mle=1.6013 pcon=4.6528 forget=3.4437 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 100 total=9.6651 mle=1.6110 pcon=4.6524 forget=3.4018 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 150 total=9.5086 mle=1.4494 pcon=4.6522 forget=3.4069 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 200 total=9.7883 mle=1.7350 pcon=4.6520 forget=3.4013 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 250 total=9.8979 mle=1.7845 pcon=4.6517 forget=3.4616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 300 total=9.5944 mle=1.5337 pcon=4.6516 forget=3.4091 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 350 total=9.5417 mle=1.4489 pcon=4.6514 forget=3.4414 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 10 total=9.5758 mle=1.5058 pcon=4.6513 forget=3.4187 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 60 total=9.9387 mle=1.8346 pcon=4.6512 forget=3.4529 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 110 total=9.6992 mle=1.5874 pcon=4.6509 forget=3.4609 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 160 total=9.6781 mle=1.5908 pcon=4.6509 forget=3.4364 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 210 total=9.7346 mle=1.6368 pcon=4.6508 forget=3.4470 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 260 total=9.5985 mle=1.5302 pcon=4.6508 forget=3.4175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 310 total=9.6147 mle=1.5203 pcon=4.6508 forget=3.4436 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 360 total=9.7605 mle=1.6760 pcon=4.6508 forget=3.4337 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 20 total=9.7330 mle=1.6462 pcon=4.6507 forget=3.4361 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 70 total=9.6905 mle=1.5911 pcon=4.6504 forget=3.4491 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 120 total=9.9492 mle=1.8563 pcon=4.6502 forget=3.4427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 170 total=9.6747 mle=1.5905 pcon=4.6501 forget=3.4340 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 220 total=9.6994 mle=1.6275 pcon=4.6500 forget=3.4219 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 270 total=9.7580 mle=1.6971 pcon=4.6499 forget=3.4110 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 320 total=9.7641 mle=1.6705 pcon=4.6499 forget=3.4438 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 370 total=9.6536 mle=1.5976 pcon=4.6498 forget=3.4061 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 30 total=9.7233 mle=1.6034 pcon=4.6497 forget=3.4702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 80 total=9.7075 mle=1.5989 pcon=4.6496 forget=3.4590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 130 total=9.7506 mle=1.6316 pcon=4.6494 forget=3.4695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 180 total=9.6396 mle=1.5042 pcon=4.6493 forget=3.4860 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 230 total=9.5774 mle=1.5229 pcon=4.6491 forget=3.4054 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 280 total=9.6074 mle=1.5264 pcon=4.6491 forget=3.4320 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 330 total=9.6351 mle=1.5291 pcon=4.6489 forget=3.4572 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 380 total=9.5518 mle=1.4842 pcon=4.6487 forget=3.4188 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 40 total=9.8136 mle=1.6821 pcon=4.6486 forget=3.4828 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 90 total=9.6436 mle=1.5398 pcon=4.6485 forget=3.4553 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 140 total=9.7818 mle=1.6480 pcon=4.6484 forget=3.4854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 190 total=9.5996 mle=1.5125 pcon=4.6483 forget=3.4387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 240 total=9.6724 mle=1.5363 pcon=4.6483 forget=3.4878 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 290 total=9.8889 mle=1.7911 pcon=4.6482 forget=3.4496 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 340 total=9.9125 mle=1.7878 pcon=4.6481 forget=3.4766 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 0 total=9.7003 mle=1.5515 pcon=4.6480 forget=3.5008 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 50 total=9.6188 mle=1.5420 pcon=4.6480 forget=3.4288 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 100 total=9.6755 mle=1.5475 pcon=4.6479 forget=3.4801 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 150 total=9.5944 mle=1.5272 pcon=4.6478 forget=3.4195 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 200 total=9.6804 mle=1.5615 pcon=4.6477 forget=3.4712 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 250 total=9.6342 mle=1.5420 pcon=4.6476 forget=3.4446 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 300 total=9.6939 mle=1.6325 pcon=4.6474 forget=3.4140 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 350 total=9.5764 mle=1.5220 pcon=4.6474 forget=3.4071 nr=64 nf=64 protos=540 fproto_sim=NA
 94%|█████████▍| 47/50 [16:22<01:04, 21.40s/it] 96%|█████████▌| 48/50 [16:42<00:42, 21.22s/it] 98%|█████████▊| 49/50 [17:10<00:23, 23.06s/it]100%|██████████| 50/50 [17:29<00:00, 21.78s/it]100%|██████████| 50/50 [17:29<00:00, 20.98s/it]
[loss] ep 46 it 10 total=9.5952 mle=1.4883 pcon=4.6473 forget=3.4595 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 60 total=9.6357 mle=1.5091 pcon=4.6472 forget=3.4794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 110 total=9.7688 mle=1.6380 pcon=4.6472 forget=3.4836 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 160 total=9.5413 mle=1.4596 pcon=4.6471 forget=3.4346 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 210 total=9.8060 mle=1.7304 pcon=4.6470 forget=3.4287 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 260 total=9.7101 mle=1.6680 pcon=4.6469 forget=3.3952 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 310 total=9.6705 mle=1.5218 pcon=4.6469 forget=3.5018 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 360 total=9.8230 mle=1.6961 pcon=4.6468 forget=3.4801 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 20 total=9.6363 mle=1.5312 pcon=4.6466 forget=3.4585 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 70 total=9.5824 mle=1.5340 pcon=4.6465 forget=3.4018 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 120 total=9.7573 mle=1.6624 pcon=4.6464 forget=3.4485 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 170 total=9.6689 mle=1.5281 pcon=4.6462 forget=3.4946 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 220 total=9.5139 mle=1.4526 pcon=4.6462 forget=3.4152 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 270 total=9.7253 mle=1.5589 pcon=4.6462 forget=3.5203 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 320 total=9.7468 mle=1.5496 pcon=4.6461 forget=3.5511 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 370 total=9.7590 mle=1.6389 pcon=4.6461 forget=3.4741 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 30 total=9.7742 mle=1.5812 pcon=4.6460 forget=3.5470 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 80 total=9.7716 mle=1.5873 pcon=4.6460 forget=3.5383 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 130 total=9.7022 mle=1.6528 pcon=4.6459 forget=3.4034 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 180 total=9.8818 mle=1.6481 pcon=4.6460 forget=3.5877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 230 total=9.7411 mle=1.6460 pcon=4.6459 forget=3.4492 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 280 total=9.5941 mle=1.5551 pcon=4.6459 forget=3.3932 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 330 total=9.6185 mle=1.5812 pcon=4.6458 forget=3.3915 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 380 total=9.6676 mle=1.5171 pcon=4.6457 forget=3.5048 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 40 total=9.7252 mle=1.5377 pcon=4.6456 forget=3.5419 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 90 total=9.7364 mle=1.5886 pcon=4.6457 forget=3.5021 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 140 total=9.7311 mle=1.6206 pcon=4.6457 forget=3.4649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 190 total=9.8534 mle=1.7259 pcon=4.6455 forget=3.4819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 240 total=9.7282 mle=1.6464 pcon=4.6454 forget=3.4365 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 290 total=9.6887 mle=1.5991 pcon=4.6452 forget=3.4443 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 340 total=9.6345 mle=1.5323 pcon=4.6453 forget=3.4569 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:09,  3.01it/s]  3%|▎         | 11/391 [00:00<00:12, 31.11it/s]  5%|▌         | 20/391 [00:00<00:07, 48.09it/s]  7%|▋         | 29/391 [00:00<00:06, 60.00it/s] 10%|▉         | 38/391 [00:00<00:05, 68.30it/s] 12%|█▏        | 47/391 [00:00<00:04, 73.98it/s] 14%|█▍        | 56/391 [00:00<00:04, 77.83it/s] 17%|█▋        | 65/391 [00:01<00:04, 80.83it/s] 19%|█▉        | 74/391 [00:01<00:03, 82.88it/s] 21%|██        | 83/391 [00:01<00:03, 84.58it/s] 24%|██▎       | 92/391 [00:01<00:03, 85.59it/s] 26%|██▌       | 101/391 [00:01<00:03, 86.36it/s] 28%|██▊       | 110/391 [00:01<00:03, 86.95it/s] 30%|███       | 119/391 [00:01<00:03, 87.26it/s] 33%|███▎      | 128/391 [00:01<00:03, 87.45it/s] 35%|███▌      | 137/391 [00:01<00:02, 87.67it/s] 37%|███▋      | 146/391 [00:01<00:02, 87.63it/s] 40%|███▉      | 155/391 [00:02<00:02, 87.82it/s] 42%|████▏     | 164/391 [00:02<00:02, 88.07it/s] 44%|████▍     | 173/391 [00:02<00:02, 88.12it/s] 47%|████▋     | 182/391 [00:02<00:02, 88.23it/s] 49%|████▉     | 191/391 [00:02<00:02, 88.32it/s] 51%|█████     | 200/391 [00:02<00:02, 88.25it/s] 53%|█████▎    | 209/391 [00:02<00:02, 88.17it/s] 56%|█████▌    | 218/391 [00:02<00:01, 88.17it/s] 58%|█████▊    | 227/391 [00:02<00:01, 88.02it/s] 60%|██████    | 236/391 [00:03<00:01, 87.93it/s] 63%|██████▎   | 245/391 [00:03<00:01, 88.07it/s] 65%|██████▍   | 254/391 [00:03<00:01, 88.19it/s] 67%|██████▋   | 263/391 [00:03<00:01, 88.66it/s] 70%|██████▉   | 272/391 [00:03<00:01, 88.50it/s] 72%|███████▏  | 281/391 [00:03<00:01, 88.39it/s] 74%|███████▍  | 290/391 [00:03<00:01, 88.38it/s] 76%|███████▋  | 299/391 [00:03<00:01, 88.33it/s] 79%|███████▉  | 308/391 [00:03<00:00, 88.23it/s] 81%|████████  | 317/391 [00:03<00:00, 88.19it/s] 83%|████████▎ | 326/391 [00:04<00:00, 88.38it/s] 86%|████████▌ | 335/391 [00:04<00:00, 88.12it/s] 88%|████████▊ | 344/391 [00:04<00:00, 88.11it/s] 90%|█████████ | 353/391 [00:04<00:00, 88.03it/s] 93%|█████████▎| 362/391 [00:04<00:00, 88.06it/s] 95%|█████████▍| 371/391 [00:04<00:00, 88.18it/s] 97%|█████████▋| 380/391 [00:04<00:00, 87.94it/s] 99%|█████████▉| 389/391 [00:04<00:00, 88.50it/s]100%|██████████| 391/391 [00:04<00:00, 82.01it/s]
50000 images processed, 4.8518970012664795 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.09it/s] 13%|█▎        | 10/79 [00:00<00:02, 29.29it/s] 24%|██▍       | 19/79 [00:00<00:01, 47.20it/s] 35%|███▌      | 28/79 [00:00<00:00, 59.44it/s] 47%|████▋     | 37/79 [00:00<00:00, 67.98it/s] 58%|█████▊    | 46/79 [00:00<00:00, 73.84it/s] 70%|██████▉   | 55/79 [00:00<00:00, 78.11it/s] 81%|████████  | 64/79 [00:01<00:00, 81.11it/s] 92%|█████████▏| 73/79 [00:01<00:00, 83.55it/s]100%|██████████| 79/79 [00:01<00:00, 64.67it/s]
10000 images processed, 1.2509734630584717 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:26,  2.35it/s]  5%|▍         | 10/204 [00:00<00:08, 23.80it/s]  9%|▉         | 19/204 [00:00<00:04, 40.48it/s] 14%|█▎        | 28/204 [00:00<00:03, 53.25it/s] 18%|█▊        | 37/204 [00:00<00:02, 62.09it/s] 23%|██▎       | 46/204 [00:00<00:02, 69.36it/s] 27%|██▋       | 55/204 [00:01<00:01, 74.64it/s] 31%|███▏      | 64/204 [00:01<00:01, 78.34it/s] 36%|███▌      | 73/204 [00:01<00:01, 80.98it/s] 40%|████      | 82/204 [00:01<00:01, 82.82it/s] 45%|████▍     | 91/204 [00:01<00:01, 84.04it/s] 49%|████▉     | 100/204 [00:01<00:01, 84.89it/s] 53%|█████▎    | 109/204 [00:01<00:01, 85.56it/s] 58%|█████▊    | 118/204 [00:01<00:00, 86.09it/s] 62%|██████▏   | 127/204 [00:01<00:00, 86.52it/s] 67%|██████▋   | 136/204 [00:01<00:00, 86.70it/s] 71%|███████   | 145/204 [00:02<00:00, 86.76it/s] 75%|███████▌  | 154/204 [00:02<00:00, 86.87it/s] 80%|███████▉  | 163/204 [00:02<00:00, 86.90it/s] 84%|████████▍ | 172/204 [00:02<00:00, 86.95it/s] 89%|████████▊ | 181/204 [00:02<00:00, 86.97it/s] 93%|█████████▎| 190/204 [00:02<00:00, 87.01it/s] 98%|█████████▊| 199/204 [00:02<00:00, 87.25it/s]100%|██████████| 204/204 [00:02<00:00, 73.63it/s]
26032 images processed, 2.819044828414917 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.94it/s] 11%|█▏        | 9/79 [00:00<00:04, 16.53it/s] 22%|██▏       | 17/79 [00:00<00:02, 26.47it/s] 32%|███▏      | 25/79 [00:01<00:01, 33.67it/s] 42%|████▏     | 33/79 [00:01<00:01, 38.82it/s] 52%|█████▏    | 41/79 [00:01<00:00, 41.64it/s] 62%|██████▏   | 49/79 [00:01<00:00, 43.78it/s] 72%|███████▏  | 57/79 [00:01<00:00, 45.05it/s] 82%|████████▏ | 65/79 [00:01<00:00, 46.11it/s] 92%|█████████▏| 73/79 [00:01<00:00, 46.77it/s]100%|██████████| 79/79 [00:02<00:00, 38.43it/s]
10000 images processed, 2.092564582824707 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:30,  2.57it/s] 13%|█▎        | 10/79 [00:00<00:02, 25.77it/s] 24%|██▍       | 19/79 [00:00<00:01, 42.98it/s] 35%|███▌      | 28/79 [00:00<00:00, 55.58it/s] 47%|████▋     | 37/79 [00:00<00:00, 64.65it/s] 58%|█████▊    | 46/79 [00:00<00:00, 71.14it/s] 70%|██████▉   | 55/79 [00:01<00:00, 75.79it/s] 81%|████████  | 64/79 [00:01<00:00, 79.08it/s] 92%|█████████▏| 73/79 [00:01<00:00, 81.76it/s]100%|██████████| 79/79 [00:01<00:00, 61.84it/s]
10000 images processed, 1.2978079319000244 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:27,  2.47it/s] 14%|█▍        | 10/70 [00:00<00:02, 25.12it/s] 27%|██▋       | 19/70 [00:00<00:01, 42.19it/s] 40%|████      | 28/70 [00:00<00:00, 54.68it/s] 53%|█████▎    | 37/70 [00:00<00:00, 63.91it/s] 66%|██████▌   | 46/70 [00:00<00:00, 69.39it/s] 79%|███████▊  | 55/70 [00:01<00:00, 74.08it/s] 91%|█████████▏| 64/70 [00:01<00:00, 78.06it/s]100%|██████████| 70/70 [00:01<00:00, 57.75it/s]
8925 images processed, 1.2465057373046875 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:41,  1.05it/s]  4%|▍         | 2/45 [00:01<00:20,  2.13it/s] 20%|██        | 9/45 [00:01<00:03,  9.15it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.91it/s] 31%|███       | 14/45 [00:01<00:03,  9.98it/s] 38%|███▊      | 17/45 [00:02<00:02, 12.68it/s] 42%|████▏     | 19/45 [00:02<00:02, 12.36it/s] 49%|████▉     | 22/45 [00:02<00:02, 10.22it/s] 58%|█████▊    | 26/45 [00:02<00:01, 13.32it/s] 67%|██████▋   | 30/45 [00:03<00:01, 10.38it/s] 76%|███████▌  | 34/45 [00:03<00:00, 11.63it/s] 84%|████████▍ | 38/45 [00:03<00:00, 14.46it/s] 93%|█████████▎| 42/45 [00:04<00:00,  9.99it/s]100%|██████████| 45/45 [00:04<00:00, 10.26it/s]
5640 images processed, 4.414930582046509 seconds used

19.742551565170288
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           5.64  98.76
places365     79.29  77.71
LSUN          33.14  93.90
iSUN          80.12  79.09
dtd           46.93  89.32
forget        46.50  90.93
AVG           48.60  88.29
Retain-Acc: 0.7381
Forget-as-OOD (retain known vs forget novel):
  FPR: 46.50 AUROC: 90.93 AUIN: 98.83
21.66651463508606
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.5 lr=0.0005 epochs=5
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.5, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:24<01:38, 24.70s/it] 40%|████      | 2/5 [00:49<01:14, 24.97s/it] 60%|██████    | 3/5 [01:07<00:42, 21.46s/it] 80%|████████  | 4/5 [01:24<00:19, 19.77s/it]100%|██████████| 5/5 [01:44<00:00, 19.93s/it]100%|██████████| 5/5 [01:44<00:00, 20.91s/it]
[loss] ep 0 it 0 total=10.3048 mle=1.5710 pcon=5.2950 forget=3.4387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=10.3337 mle=1.5424 pcon=5.2879 forget=3.5033 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=10.4164 mle=1.7005 pcon=5.2809 forget=3.4350 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=10.5977 mle=1.8998 pcon=5.2738 forget=3.4241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=10.4369 mle=1.7133 pcon=5.2670 forget=3.4566 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=10.2083 mle=1.5019 pcon=5.2603 forget=3.4461 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=10.2687 mle=1.5598 pcon=5.2540 forget=3.4549 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=10.4028 mle=1.6810 pcon=5.2476 forget=3.4741 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=10.4375 mle=1.6715 pcon=5.2409 forget=3.5251 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=10.2891 mle=1.6371 pcon=5.2346 forget=3.4175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=10.2186 mle=1.5166 pcon=5.2284 forget=3.4736 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=10.4762 mle=1.7804 pcon=5.2225 forget=3.4734 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=10.4444 mle=1.7646 pcon=5.2167 forget=3.4632 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=10.2373 mle=1.6054 pcon=5.2112 forget=3.4206 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=10.3748 mle=1.7238 pcon=5.2056 forget=3.4453 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=10.4596 mle=1.7940 pcon=5.2003 forget=3.4653 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=10.1899 mle=1.5529 pcon=5.1950 forget=3.4419 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=10.4720 mle=1.8882 pcon=5.1899 forget=3.3939 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=10.3660 mle=1.7418 pcon=5.1847 forget=3.4395 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=10.1181 mle=1.5264 pcon=5.1797 forget=3.4121 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=10.2247 mle=1.5856 pcon=5.1746 forget=3.4646 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=10.4801 mle=1.8838 pcon=5.1699 forget=3.4263 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=10.1512 mle=1.5923 pcon=5.1652 forget=3.3938 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=10.3834 mle=1.8114 pcon=5.1605 forget=3.4116 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=10.2881 mle=1.7057 pcon=5.1560 forget=3.4265 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=10.2705 mle=1.6708 pcon=5.1519 forget=3.4478 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=10.3287 mle=1.7670 pcon=5.1475 forget=3.4142 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=10.4674 mle=1.9186 pcon=5.1436 forget=3.4052 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=10.1087 mle=1.5699 pcon=5.1395 forget=3.3993 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=10.3615 mle=1.8361 pcon=5.1354 forget=3.3900 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=10.2722 mle=1.7031 pcon=5.1311 forget=3.4379 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=10.2679 mle=1.6955 pcon=5.1275 forget=3.4449 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=10.1324 mle=1.6540 pcon=5.1237 forget=3.3547 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=10.2393 mle=1.7297 pcon=5.1197 forget=3.3900 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=10.1811 mle=1.6506 pcon=5.1160 forget=3.4144 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=9.9795 mle=1.4535 pcon=5.1126 forget=3.4135 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=10.2652 mle=1.7666 pcon=5.1087 forget=3.3899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=9.9791 mle=1.4914 pcon=5.1052 forget=3.3825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=10.3851 mle=1.8721 pcon=5.1017 forget=3.4113 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:19,  2.80it/s]  2%|▏         | 7/391 [00:00<00:20, 18.68it/s]  3%|▎         | 13/391 [00:00<00:13, 28.90it/s]  6%|▌         | 22/391 [00:00<00:08, 45.36it/s]  8%|▊         | 31/391 [00:00<00:06, 57.71it/s] 10%|█         | 40/391 [00:00<00:05, 66.64it/s] 13%|█▎        | 49/391 [00:00<00:04, 73.12it/s] 15%|█▍        | 58/391 [00:01<00:04, 77.11it/s] 17%|█▋        | 67/391 [00:01<00:05, 63.69it/s] 19%|█▉        | 75/391 [00:01<00:05, 60.33it/s] 21%|██▏       | 84/391 [00:01<00:04, 67.16it/s] 24%|██▍       | 93/391 [00:01<00:04, 72.76it/s] 26%|██▌       | 102/391 [00:01<00:03, 76.58it/s] 28%|██▊       | 111/391 [00:01<00:03, 78.86it/s] 31%|███       | 120/391 [00:01<00:03, 80.09it/s] 33%|███▎      | 129/391 [00:02<00:04, 65.39it/s] 35%|███▌      | 137/391 [00:02<00:04, 62.71it/s] 37%|███▋      | 146/391 [00:02<00:03, 68.85it/s] 40%|███▉      | 155/391 [00:02<00:03, 73.63it/s] 42%|████▏     | 164/391 [00:02<00:02, 77.53it/s] 44%|████▍     | 173/391 [00:02<00:02, 79.95it/s] 47%|████▋     | 182/391 [00:02<00:02, 78.08it/s] 49%|████▊     | 190/391 [00:03<00:03, 65.48it/s] 50%|█████     | 197/391 [00:03<00:03, 62.30it/s] 53%|█████▎    | 206/391 [00:03<00:02, 68.93it/s] 55%|█████▍    | 215/391 [00:03<00:02, 74.15it/s] 57%|█████▋    | 223/391 [00:03<00:02, 75.72it/s] 59%|█████▉    | 232/391 [00:03<00:02, 78.69it/s] 62%|██████▏   | 241/391 [00:03<00:01, 76.90it/s] 64%|██████▎   | 249/391 [00:03<00:02, 64.78it/s] 65%|██████▌   | 256/391 [00:03<00:02, 62.13it/s] 68%|██████▊   | 265/391 [00:04<00:01, 67.23it/s] 70%|███████   | 274/391 [00:04<00:01, 72.76it/s] 72%|███████▏  | 283/391 [00:04<00:01, 76.64it/s] 75%|███████▍  | 292/391 [00:04<00:01, 79.24it/s] 77%|███████▋  | 301/391 [00:04<00:01, 78.27it/s] 79%|███████▉  | 309/391 [00:04<00:01, 65.64it/s] 81%|████████  | 316/391 [00:04<00:01, 61.72it/s] 83%|████████▎ | 324/391 [00:04<00:01, 66.04it/s] 85%|████████▌ | 333/391 [00:05<00:00, 71.80it/s] 87%|████████▋ | 342/391 [00:05<00:00, 75.83it/s] 90%|████████▉ | 351/391 [00:05<00:00, 79.01it/s] 92%|█████████▏| 360/391 [00:05<00:00, 81.39it/s] 94%|█████████▍| 369/391 [00:05<00:00, 71.95it/s] 96%|█████████▋| 377/391 [00:05<00:00, 65.76it/s] 98%|█████████▊| 384/391 [00:05<00:00, 63.75it/s]100%|██████████| 391/391 [00:05<00:00, 67.04it/s]
50000 images processed, 5.948231935501099 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:24,  3.21it/s]  9%|▉         | 7/79 [00:00<00:03, 20.44it/s] 19%|█▉        | 15/79 [00:00<00:01, 37.98it/s] 30%|███       | 24/79 [00:00<00:01, 52.67it/s] 42%|████▏     | 33/79 [00:00<00:00, 63.30it/s] 53%|█████▎    | 42/79 [00:00<00:00, 70.28it/s] 65%|██████▍   | 51/79 [00:00<00:00, 72.16it/s] 75%|███████▍  | 59/79 [00:01<00:00, 61.68it/s] 84%|████████▎ | 66/79 [00:01<00:00, 59.57it/s] 94%|█████████▎| 74/79 [00:01<00:00, 64.44it/s]100%|██████████| 79/79 [00:01<00:00, 55.44it/s]
10000 images processed, 1.454850435256958 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:04,  3.14it/s]  3%|▎         | 6/204 [00:00<00:11, 16.55it/s]  5%|▌         | 11/204 [00:00<00:07, 26.22it/s]  9%|▉         | 19/204 [00:00<00:04, 41.25it/s] 14%|█▎        | 28/204 [00:00<00:03, 54.87it/s] 18%|█▊        | 37/204 [00:00<00:02, 64.34it/s] 23%|██▎       | 46/204 [00:00<00:02, 71.11it/s] 27%|██▋       | 55/204 [00:01<00:01, 75.17it/s] 31%|███       | 63/204 [00:01<00:02, 60.59it/s] 34%|███▍      | 70/204 [00:01<00:02, 57.83it/s] 39%|███▊      | 79/204 [00:01<00:01, 65.04it/s] 43%|████▎     | 88/204 [00:01<00:01, 70.37it/s] 48%|████▊     | 97/204 [00:01<00:01, 74.82it/s] 52%|█████▏    | 106/204 [00:01<00:01, 77.52it/s] 56%|█████▋    | 115/204 [00:01<00:01, 78.94it/s] 61%|██████    | 124/204 [00:02<00:01, 64.17it/s] 64%|██████▍   | 131/204 [00:02<00:01, 61.53it/s] 69%|██████▊   | 140/204 [00:02<00:00, 67.92it/s] 73%|███████▎  | 149/204 [00:02<00:00, 72.73it/s] 77%|███████▋  | 158/204 [00:02<00:00, 76.10it/s] 82%|████████▏ | 167/204 [00:02<00:00, 78.68it/s] 86%|████████▋ | 176/204 [00:02<00:00, 72.12it/s] 90%|█████████ | 184/204 [00:02<00:00, 63.07it/s] 94%|█████████▎| 191/204 [00:03<00:00, 62.86it/s] 98%|█████████▊| 200/204 [00:03<00:00, 69.41it/s]100%|██████████| 204/204 [00:03<00:00, 62.77it/s]
26032 images processed, 3.296090841293335 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:37,  2.09it/s] 11%|█▏        | 9/79 [00:00<00:03, 18.45it/s] 18%|█▊        | 14/79 [00:00<00:02, 25.00it/s] 24%|██▍       | 19/79 [00:00<00:02, 29.95it/s] 32%|███▏      | 25/79 [00:00<00:01, 35.99it/s] 43%|████▎     | 34/79 [00:01<00:01, 43.14it/s] 53%|█████▎    | 42/79 [00:01<00:00, 43.95it/s] 59%|█████▉    | 47/79 [00:01<00:00, 44.86it/s] 68%|██████▊   | 54/79 [00:01<00:00, 50.13it/s] 76%|███████▌  | 60/79 [00:01<00:00, 44.28it/s] 82%|████████▏ | 65/79 [00:01<00:00, 43.64it/s] 89%|████████▊ | 70/79 [00:01<00:00, 44.95it/s]100%|██████████| 79/79 [00:02<00:00, 39.42it/s]
10000 images processed, 2.04726505279541 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:26,  2.95it/s] 13%|█▎        | 10/79 [00:00<00:02, 28.47it/s] 24%|██▍       | 19/79 [00:00<00:01, 46.07it/s] 35%|███▌      | 28/79 [00:00<00:00, 58.47it/s] 47%|████▋     | 37/79 [00:00<00:00, 67.02it/s] 57%|█████▋    | 45/79 [00:00<00:00, 59.71it/s] 66%|██████▌   | 52/79 [00:01<00:00, 56.05it/s] 76%|███████▌  | 60/79 [00:01<00:00, 60.53it/s] 87%|████████▋ | 69/79 [00:01<00:00, 67.61it/s] 99%|█████████▊| 78/79 [00:01<00:00, 73.44it/s]100%|██████████| 79/79 [00:01<00:00, 57.30it/s]
10000 images processed, 1.400824785232544 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:27,  2.55it/s] 14%|█▍        | 10/70 [00:00<00:02, 25.35it/s] 23%|██▎       | 16/70 [00:00<00:01, 33.44it/s] 31%|███▏      | 22/70 [00:00<00:01, 37.61it/s] 39%|███▊      | 27/70 [00:00<00:01, 39.24it/s] 51%|█████▏    | 36/70 [00:00<00:00, 52.12it/s] 64%|██████▍   | 45/70 [00:01<00:00, 61.76it/s] 77%|███████▋  | 54/70 [00:01<00:00, 69.06it/s] 90%|█████████ | 63/70 [00:01<00:00, 74.73it/s]100%|██████████| 70/70 [00:01<00:00, 51.66it/s]
8925 images processed, 1.3924994468688965 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:41,  1.06it/s] 11%|█         | 5/45 [00:01<00:06,  6.04it/s] 20%|██        | 9/45 [00:01<00:04,  8.18it/s] 27%|██▋       | 12/45 [00:01<00:03, 10.90it/s] 31%|███       | 14/45 [00:01<00:03, 10.09it/s] 38%|███▊      | 17/45 [00:01<00:02, 11.43it/s] 44%|████▍     | 20/45 [00:02<00:01, 13.56it/s] 49%|████▉     | 22/45 [00:02<00:01, 11.87it/s] 56%|█████▌    | 25/45 [00:02<00:01, 12.82it/s] 67%|██████▋   | 30/45 [00:03<00:01, 11.67it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.45it/s] 87%|████████▋ | 39/45 [00:03<00:00, 13.97it/s] 91%|█████████ | 41/45 [00:03<00:00, 13.26it/s] 96%|█████████▌| 43/45 [00:04<00:00, 11.45it/s]100%|██████████| 45/45 [00:04<00:00, 11.01it/s]
5640 images processed, 4.116386890411377 seconds used

21.28044366836548
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.91  81.18
LSUN          17.58  96.08
iSUN          72.37  81.70
dtd           37.91  91.39
forget        79.50  86.90
AVG           46.30  89.43
Retain-Acc: 0.7406
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
17.485175848007202
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.5 lr=0.0005 epochs=10
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.5, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:27<04:08, 27.60s/it] 20%|██        | 2/10 [00:47<03:04, 23.02s/it] 30%|███       | 3/10 [01:05<02:24, 20.59s/it] 40%|████      | 4/10 [01:24<02:01, 20.23s/it] 50%|█████     | 5/10 [01:49<01:49, 21.82s/it] 60%|██████    | 6/10 [02:07<01:22, 20.56s/it] 70%|███████   | 7/10 [02:24<00:58, 19.53s/it] 80%|████████  | 8/10 [02:43<00:38, 19.10s/it][loss] ep 0 it 0 total=10.3047 mle=1.5709 pcon=5.2950 forget=3.4387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=10.3338 mle=1.5425 pcon=5.2879 forget=3.5033 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=10.4163 mle=1.7004 pcon=5.2809 forget=3.4350 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=10.5976 mle=1.8997 pcon=5.2738 forget=3.4241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=10.4366 mle=1.7131 pcon=5.2670 forget=3.4565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=10.2084 mle=1.5020 pcon=5.2603 forget=3.4461 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=10.2686 mle=1.5598 pcon=5.2540 forget=3.4548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=10.4028 mle=1.6810 pcon=5.2476 forget=3.4741 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=10.4375 mle=1.6715 pcon=5.2409 forget=3.5251 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=10.2891 mle=1.6370 pcon=5.2346 forget=3.4175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=10.2187 mle=1.5166 pcon=5.2284 forget=3.4736 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=10.4761 mle=1.7802 pcon=5.2225 forget=3.4734 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=10.4444 mle=1.7645 pcon=5.2167 forget=3.4632 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=10.2372 mle=1.6053 pcon=5.2112 forget=3.4206 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=10.3747 mle=1.7238 pcon=5.2056 forget=3.4453 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=10.4597 mle=1.7941 pcon=5.2003 forget=3.4652 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=10.1899 mle=1.5530 pcon=5.1950 forget=3.4419 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=10.4718 mle=1.8880 pcon=5.1899 forget=3.3939 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=10.3660 mle=1.7418 pcon=5.1847 forget=3.4395 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=10.1181 mle=1.5264 pcon=5.1796 forget=3.4121 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=10.2248 mle=1.5857 pcon=5.1746 forget=3.4646 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=10.4802 mle=1.8839 pcon=5.1699 forget=3.4263 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=10.1513 mle=1.5924 pcon=5.1652 forget=3.3937 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=10.3832 mle=1.8111 pcon=5.1605 forget=3.4116 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=10.2880 mle=1.7056 pcon=5.1560 forget=3.4264 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=10.2704 mle=1.6707 pcon=5.1519 forget=3.4478 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=10.3286 mle=1.7668 pcon=5.1475 forget=3.4142 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=10.4671 mle=1.9184 pcon=5.1436 forget=3.4051 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=10.1087 mle=1.5699 pcon=5.1395 forget=3.3993 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=10.3614 mle=1.8360 pcon=5.1354 forget=3.3900 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=10.2724 mle=1.7033 pcon=5.1311 forget=3.4379 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=10.2680 mle=1.6956 pcon=5.1275 forget=3.4448 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=10.1323 mle=1.6539 pcon=5.1237 forget=3.3547 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=10.2392 mle=1.7296 pcon=5.1197 forget=3.3900 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=10.1812 mle=1.6508 pcon=5.1160 forget=3.4144 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=9.9795 mle=1.4534 pcon=5.1126 forget=3.4135 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=10.2654 mle=1.7668 pcon=5.1087 forget=3.3899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=9.9788 mle=1.4912 pcon=5.1052 forget=3.3825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=10.3850 mle=1.8720 pcon=5.1017 forget=3.4113 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=10.3578 mle=1.8673 pcon=5.0984 forget=3.3922 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=10.2586 mle=1.8001 pcon=5.0947 forget=3.3638 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=10.0753 mle=1.5846 pcon=5.0916 forget=3.3991 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=10.0789 mle=1.5951 pcon=5.0881 forget=3.3957 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=10.1396 mle=1.6916 pcon=5.0852 forget=3.3628 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=10.2020 mle=1.7739 pcon=5.0822 forget=3.3458 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=10.0509 mle=1.5966 pcon=5.0791 forget=3.3751 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=10.1927 mle=1.7460 pcon=5.0763 forget=3.3704 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=9.9023 mle=1.4007 pcon=5.0734 forget=3.4282 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=9.9250 mle=1.5349 pcon=5.0706 forget=3.3195 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=10.0593 mle=1.5914 pcon=5.0679 forget=3.4000 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=10.0928 mle=1.6660 pcon=5.0652 forget=3.3616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=10.0446 mle=1.6302 pcon=5.0629 forget=3.3515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=10.2682 mle=1.8220 pcon=5.0600 forget=3.3862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=10.0003 mle=1.5368 pcon=5.0576 forget=3.4059 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=10.0940 mle=1.6747 pcon=5.0551 forget=3.3642 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=10.0271 mle=1.6305 pcon=5.0524 forget=3.3441 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=10.1256 mle=1.7431 pcon=5.0500 forget=3.3325 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=10.0214 mle=1.6257 pcon=5.0475 forget=3.3482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=9.9426 mle=1.5801 pcon=5.0450 forget=3.3175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=10.0680 mle=1.6520 pcon=5.0430 forget=3.3730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=10.2329 mle=1.7972 pcon=5.0408 forget=3.3948 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=10.1684 mle=1.7416 pcon=5.0383 forget=3.3884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=10.0333 mle=1.6432 pcon=5.0362 forget=3.3538 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 90%|█████████ | 9/10 [03:07<00:20, 20.82s/it]100%|██████████| 10/10 [03:26<00:00, 20.15s/it]100%|██████████| 10/10 [03:26<00:00, 20.64s/it]
[loss] ep 8 it 30 total=9.9583 mle=1.5378 pcon=5.0342 forget=3.3862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=9.9789 mle=1.5880 pcon=5.0322 forget=3.3587 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=9.9091 mle=1.5057 pcon=5.0299 forget=3.3735 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=10.0665 mle=1.7032 pcon=5.0279 forget=3.3354 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=9.8866 mle=1.5304 pcon=5.0263 forget=3.3300 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=10.0077 mle=1.6772 pcon=5.0241 forget=3.3064 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=10.1695 mle=1.7566 pcon=5.0219 forget=3.3911 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=9.8137 mle=1.4846 pcon=5.0198 forget=3.3092 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=9.9168 mle=1.6331 pcon=5.0177 forget=3.2659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=10.0008 mle=1.6687 pcon=5.0154 forget=3.3167 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=10.1906 mle=1.8298 pcon=5.0136 forget=3.3471 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=9.8869 mle=1.5768 pcon=5.0118 forget=3.2983 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=9.8924 mle=1.5644 pcon=5.0100 forget=3.3180 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=9.8888 mle=1.5573 pcon=5.0078 forget=3.3238 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=9.8655 mle=1.5595 pcon=5.0060 forget=3.3000 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:42,  3.80it/s]  3%|▎         | 10/391 [00:00<00:11, 33.93it/s]  5%|▍         | 19/391 [00:00<00:07, 51.99it/s]  7%|▋         | 28/391 [00:00<00:05, 64.03it/s]  9%|▉         | 37/391 [00:00<00:04, 71.85it/s] 12%|█▏        | 46/391 [00:00<00:04, 76.82it/s] 14%|█▍        | 55/391 [00:00<00:04, 80.42it/s] 16%|█▋        | 64/391 [00:00<00:03, 82.87it/s] 19%|█▊        | 73/391 [00:01<00:03, 83.37it/s] 21%|██        | 82/391 [00:01<00:03, 83.42it/s] 23%|██▎       | 91/391 [00:01<00:03, 84.99it/s] 26%|██▌       | 100/391 [00:01<00:03, 85.72it/s] 28%|██▊       | 109/391 [00:01<00:03, 83.80it/s] 30%|███       | 118/391 [00:01<00:03, 84.10it/s] 32%|███▏      | 127/391 [00:01<00:03, 83.65it/s] 35%|███▍      | 136/391 [00:01<00:02, 85.10it/s] 37%|███▋      | 145/391 [00:01<00:02, 86.13it/s] 39%|███▉      | 154/391 [00:02<00:02, 86.80it/s] 42%|████▏     | 163/391 [00:02<00:02, 87.30it/s] 44%|████▍     | 172/391 [00:02<00:02, 87.52it/s] 46%|████▋     | 181/391 [00:02<00:02, 87.69it/s] 49%|████▊     | 190/391 [00:02<00:02, 86.73it/s] 51%|█████     | 199/391 [00:02<00:02, 87.02it/s] 53%|█████▎    | 208/391 [00:02<00:02, 87.54it/s] 55%|█████▌    | 217/391 [00:02<00:01, 87.86it/s] 58%|█████▊    | 226/391 [00:02<00:01, 88.12it/s] 60%|██████    | 235/391 [00:02<00:01, 88.02it/s] 62%|██████▏   | 244/391 [00:03<00:01, 88.27it/s] 65%|██████▍   | 253/391 [00:03<00:01, 88.36it/s] 67%|██████▋   | 262/391 [00:03<00:01, 88.60it/s] 69%|██████▉   | 271/391 [00:03<00:01, 88.78it/s] 72%|███████▏  | 280/391 [00:03<00:01, 88.93it/s] 74%|███████▍  | 289/391 [00:03<00:01, 89.12it/s] 76%|███████▌  | 298/391 [00:03<00:01, 88.27it/s] 79%|███████▊  | 307/391 [00:03<00:00, 87.27it/s] 81%|████████  | 316/391 [00:03<00:00, 87.00it/s] 83%|████████▎ | 325/391 [00:03<00:00, 86.94it/s] 85%|████████▌ | 334/391 [00:04<00:00, 86.39it/s] 88%|████████▊ | 343/391 [00:04<00:00, 86.69it/s] 90%|█████████ | 352/391 [00:04<00:00, 87.03it/s] 92%|█████████▏| 361/391 [00:04<00:00, 87.05it/s] 95%|█████████▍| 370/391 [00:04<00:00, 87.13it/s] 97%|█████████▋| 379/391 [00:04<00:00, 87.04it/s] 99%|█████████▉| 388/391 [00:04<00:00, 87.58it/s]100%|██████████| 391/391 [00:04<00:00, 82.42it/s]
50000 images processed, 4.867529630661011 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:29,  2.62it/s] 11%|█▏        | 9/79 [00:00<00:02, 23.65it/s] 23%|██▎       | 18/79 [00:00<00:01, 42.04it/s] 34%|███▍      | 27/79 [00:00<00:00, 55.14it/s] 46%|████▌     | 36/79 [00:00<00:00, 64.36it/s] 57%|█████▋    | 45/79 [00:00<00:00, 71.09it/s] 68%|██████▊   | 54/79 [00:00<00:00, 75.90it/s] 80%|███████▉  | 63/79 [00:01<00:00, 79.39it/s] 91%|█████████ | 72/79 [00:01<00:00, 82.22it/s]100%|██████████| 79/79 [00:01<00:00, 61.27it/s]
10000 images processed, 1.3154184818267822 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:18,  2.58it/s]  5%|▍         | 10/204 [00:00<00:07, 25.88it/s]  9%|▉         | 19/204 [00:00<00:04, 43.31it/s] 14%|█▎        | 28/204 [00:00<00:03, 55.36it/s] 18%|█▊        | 37/204 [00:00<00:02, 64.46it/s] 23%|██▎       | 46/204 [00:00<00:02, 71.13it/s] 27%|██▋       | 55/204 [00:01<00:01, 75.86it/s] 31%|███▏      | 64/204 [00:01<00:01, 79.18it/s] 36%|███▌      | 73/204 [00:01<00:01, 81.51it/s] 40%|████      | 82/204 [00:01<00:01, 82.63it/s] 45%|████▍     | 91/204 [00:01<00:01, 83.85it/s] 49%|████▉     | 100/204 [00:01<00:01, 84.65it/s] 53%|█████▎    | 109/204 [00:01<00:01, 84.84it/s] 58%|█████▊    | 118/204 [00:01<00:01, 85.12it/s] 62%|██████▏   | 127/204 [00:01<00:00, 85.32it/s] 67%|██████▋   | 136/204 [00:01<00:00, 85.53it/s] 71%|███████   | 145/204 [00:02<00:00, 85.62it/s] 75%|███████▌  | 154/204 [00:02<00:00, 85.49it/s] 80%|███████▉  | 163/204 [00:02<00:00, 85.99it/s] 84%|████████▍ | 172/204 [00:02<00:00, 86.02it/s] 89%|████████▊ | 181/204 [00:02<00:00, 86.48it/s] 93%|█████████▎| 190/204 [00:02<00:00, 86.52it/s] 98%|█████████▊| 200/204 [00:02<00:00, 87.97it/s]100%|██████████| 204/204 [00:02<00:00, 74.55it/s]
26032 images processed, 2.783989429473877 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:36,  2.15it/s] 11%|█▏        | 9/79 [00:00<00:03, 17.72it/s] 22%|██▏       | 17/79 [00:00<00:02, 27.69it/s] 32%|███▏      | 25/79 [00:00<00:01, 34.18it/s] 42%|████▏     | 33/79 [00:01<00:01, 39.91it/s] 49%|████▉     | 39/79 [00:01<00:00, 43.41it/s] 59%|█████▉    | 47/79 [00:01<00:00, 43.01it/s] 70%|██████▉   | 55/79 [00:01<00:00, 42.65it/s] 80%|███████▉  | 63/79 [00:01<00:00, 42.24it/s] 90%|████████▉ | 71/79 [00:01<00:00, 43.07it/s]100%|██████████| 79/79 [00:02<00:00, 38.46it/s]
10000 images processed, 2.086559295654297 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.03it/s] 13%|█▎        | 10/79 [00:00<00:02, 28.89it/s] 24%|██▍       | 19/79 [00:00<00:01, 46.42it/s] 35%|███▌      | 28/79 [00:00<00:00, 58.23it/s] 47%|████▋     | 37/79 [00:00<00:00, 66.35it/s] 58%|█████▊    | 46/79 [00:00<00:00, 72.30it/s] 70%|██████▉   | 55/79 [00:00<00:00, 76.30it/s] 81%|████████  | 64/79 [00:01<00:00, 78.20it/s] 92%|█████████▏| 73/79 [00:01<00:00, 81.25it/s]100%|██████████| 79/79 [00:01<00:00, 63.92it/s]
10000 images processed, 1.3210384845733643 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:23,  3.00it/s] 14%|█▍        | 10/70 [00:00<00:02, 28.62it/s] 27%|██▋       | 19/70 [00:00<00:01, 46.17it/s] 40%|████      | 28/70 [00:00<00:00, 58.08it/s] 53%|█████▎    | 37/70 [00:00<00:00, 66.67it/s] 67%|██████▋   | 47/70 [00:00<00:00, 73.97it/s] 80%|████████  | 56/70 [00:00<00:00, 78.03it/s] 93%|█████████▎| 65/70 [00:01<00:00, 81.18it/s]100%|██████████| 70/70 [00:01<00:00, 62.06it/s]
8925 images processed, 1.1594994068145752 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:36,  1.21it/s]  4%|▍         | 2/45 [00:00<00:17,  2.40it/s] 20%|██        | 9/45 [00:01<00:03,  9.86it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.48it/s] 31%|███       | 14/45 [00:01<00:02, 10.36it/s] 40%|████      | 18/45 [00:02<00:02, 12.18it/s] 49%|████▉     | 22/45 [00:02<00:02, 11.07it/s] 58%|█████▊    | 26/45 [00:02<00:01, 12.68it/s] 67%|██████▋   | 30/45 [00:03<00:01, 12.14it/s] 76%|███████▌  | 34/45 [00:03<00:01,  9.98it/s] 93%|█████████▎| 42/45 [00:04<00:00,  9.98it/s]100%|██████████| 45/45 [00:04<00:00, 10.22it/s]
5640 images processed, 4.4231483936309814 seconds used

19.748231887817383
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.51  99.36
places365     67.82  81.19
LSUN          17.42  96.11
iSUN          72.29  81.73
dtd           37.87  91.38
forget        79.50  86.89
AVG           46.24  89.44
Retain-Acc: 0.7407
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.89 AUIN: 98.36
48.24907469749451
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.5 lr=0.0005 epochs=20
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.5, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:27<08:34, 27.06s/it] 10%|█         | 2/20 [00:44<06:28, 21.59s/it] 15%|█▌        | 3/20 [01:03<05:40, 20.04s/it] 20%|██        | 4/20 [01:27<05:47, 21.71s/it] 25%|██▌       | 5/20 [01:48<05:20, 21.38s/it] 30%|███       | 6/20 [02:06<04:46, 20.49s/it] 35%|███▌      | 7/20 [02:27<04:25, 20.46s/it] 40%|████      | 8/20 [02:54<04:30, 22.58s/it][loss] ep 0 it 0 total=10.3048 mle=1.5710 pcon=5.2950 forget=3.4387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=10.3337 mle=1.5424 pcon=5.2879 forget=3.5033 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=10.4164 mle=1.7005 pcon=5.2809 forget=3.4350 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=10.5977 mle=1.8998 pcon=5.2738 forget=3.4241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=10.4367 mle=1.7132 pcon=5.2670 forget=3.4565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=10.2084 mle=1.5019 pcon=5.2603 forget=3.4461 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=10.2686 mle=1.5597 pcon=5.2540 forget=3.4548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=10.4027 mle=1.6809 pcon=5.2476 forget=3.4742 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=10.4375 mle=1.6715 pcon=5.2409 forget=3.5251 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=10.2892 mle=1.6371 pcon=5.2346 forget=3.4175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=10.2186 mle=1.5166 pcon=5.2285 forget=3.4736 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=10.4761 mle=1.7802 pcon=5.2225 forget=3.4734 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=10.4446 mle=1.7647 pcon=5.2167 forget=3.4632 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=10.2373 mle=1.6054 pcon=5.2112 forget=3.4206 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=10.3748 mle=1.7238 pcon=5.2056 forget=3.4453 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=10.4598 mle=1.7942 pcon=5.2003 forget=3.4653 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=10.1898 mle=1.5529 pcon=5.1950 forget=3.4419 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=10.4719 mle=1.8881 pcon=5.1899 forget=3.3939 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=10.3659 mle=1.7418 pcon=5.1847 forget=3.4395 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=10.1181 mle=1.5264 pcon=5.1797 forget=3.4121 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=10.2246 mle=1.5854 pcon=5.1746 forget=3.4646 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=10.4801 mle=1.8839 pcon=5.1699 forget=3.4263 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=10.1511 mle=1.5923 pcon=5.1652 forget=3.3937 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=10.3832 mle=1.8111 pcon=5.1605 forget=3.4116 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=10.2881 mle=1.7057 pcon=5.1560 forget=3.4265 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=10.2705 mle=1.6708 pcon=5.1519 forget=3.4478 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=10.3285 mle=1.7668 pcon=5.1475 forget=3.4142 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=10.4672 mle=1.9185 pcon=5.1436 forget=3.4051 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=10.1088 mle=1.5700 pcon=5.1395 forget=3.3992 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=10.3614 mle=1.8360 pcon=5.1354 forget=3.3900 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=10.2723 mle=1.7032 pcon=5.1311 forget=3.4379 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=10.2680 mle=1.6956 pcon=5.1275 forget=3.4449 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=10.1323 mle=1.6538 pcon=5.1237 forget=3.3547 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=10.2390 mle=1.7294 pcon=5.1197 forget=3.3899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=10.1813 mle=1.6508 pcon=5.1160 forget=3.4145 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=9.9796 mle=1.4536 pcon=5.1126 forget=3.4135 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=10.2650 mle=1.7664 pcon=5.1087 forget=3.3899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=9.9789 mle=1.4913 pcon=5.1052 forget=3.3825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=10.3851 mle=1.8720 pcon=5.1017 forget=3.4113 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=10.3576 mle=1.8671 pcon=5.0984 forget=3.3921 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=10.2582 mle=1.7997 pcon=5.0947 forget=3.3638 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=10.0750 mle=1.5844 pcon=5.0916 forget=3.3991 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=10.0788 mle=1.5950 pcon=5.0881 forget=3.3957 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=10.1395 mle=1.6915 pcon=5.0852 forget=3.3628 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=10.2018 mle=1.7738 pcon=5.0822 forget=3.3458 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=10.0509 mle=1.5966 pcon=5.0791 forget=3.3751 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=10.1928 mle=1.7461 pcon=5.0762 forget=3.3704 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=9.9024 mle=1.4008 pcon=5.0734 forget=3.4283 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=9.9250 mle=1.5349 pcon=5.0706 forget=3.3195 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=10.0593 mle=1.5914 pcon=5.0679 forget=3.4000 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=10.0928 mle=1.6660 pcon=5.0652 forget=3.3616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=10.0446 mle=1.6302 pcon=5.0629 forget=3.3515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=10.2682 mle=1.8220 pcon=5.0600 forget=3.3861 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=10.0004 mle=1.5369 pcon=5.0576 forget=3.4059 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=10.0941 mle=1.6748 pcon=5.0551 forget=3.3642 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=10.0272 mle=1.6306 pcon=5.0524 forget=3.3441 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=10.1254 mle=1.7429 pcon=5.0500 forget=3.3324 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=10.0212 mle=1.6256 pcon=5.0475 forget=3.3481 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=9.9428 mle=1.5802 pcon=5.0450 forget=3.3175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=10.0680 mle=1.6520 pcon=5.0430 forget=3.3730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=10.2315 mle=1.7959 pcon=5.0408 forget=3.3948 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=10.1684 mle=1.7416 pcon=5.0383 forget=3.3884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=10.0334 mle=1.6434 pcon=5.0362 forget=3.3538 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 45%|████▌     | 9/20 [03:13<03:54, 21.35s/it] 50%|█████     | 10/20 [03:32<03:26, 20.65s/it] 55%|█████▌    | 11/20 [03:57<03:18, 22.06s/it] 60%|██████    | 12/20 [04:19<02:57, 22.17s/it] 65%|██████▌   | 13/20 [04:38<02:27, 21.09s/it] 70%|███████   | 14/20 [04:59<02:06, 21.02s/it] 75%|███████▌  | 15/20 [05:26<01:54, 22.91s/it] 80%|████████  | 16/20 [05:45<01:27, 21.86s/it] 85%|████████▌ | 17/20 [06:05<01:03, 21.07s/it][loss] ep 8 it 30 total=9.9582 mle=1.5377 pcon=5.0342 forget=3.3862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=9.9789 mle=1.5880 pcon=5.0322 forget=3.3587 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=9.9091 mle=1.5057 pcon=5.0299 forget=3.3734 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=10.0665 mle=1.7032 pcon=5.0279 forget=3.3354 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=9.8866 mle=1.5304 pcon=5.0263 forget=3.3299 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=10.0077 mle=1.6771 pcon=5.0241 forget=3.3064 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=10.1696 mle=1.7566 pcon=5.0219 forget=3.3911 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=9.8134 mle=1.4844 pcon=5.0199 forget=3.3091 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=9.9166 mle=1.6330 pcon=5.0177 forget=3.2659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=10.0010 mle=1.6689 pcon=5.0154 forget=3.3167 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=10.1939 mle=1.8332 pcon=5.0137 forget=3.3470 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=9.8869 mle=1.5768 pcon=5.0118 forget=3.2983 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=9.8921 mle=1.5642 pcon=5.0100 forget=3.3180 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=9.8886 mle=1.5570 pcon=5.0078 forget=3.3238 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=9.8657 mle=1.5597 pcon=5.0060 forget=3.3000 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=9.9284 mle=1.6075 pcon=5.0039 forget=3.3169 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=9.9335 mle=1.5884 pcon=5.0018 forget=3.3433 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=10.0480 mle=1.7528 pcon=4.9998 forget=3.2954 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=10.1245 mle=1.8491 pcon=4.9980 forget=3.2774 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=10.1440 mle=1.8538 pcon=4.9961 forget=3.2941 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=9.8841 mle=1.5799 pcon=4.9944 forget=3.3098 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=9.8319 mle=1.5617 pcon=4.9927 forget=3.2775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=10.0005 mle=1.6523 pcon=4.9909 forget=3.3573 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=9.9005 mle=1.5715 pcon=4.9891 forget=3.3399 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=9.8990 mle=1.6205 pcon=4.9873 forget=3.2912 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=9.8377 mle=1.5659 pcon=4.9857 forget=3.2862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=9.9591 mle=1.6596 pcon=4.9836 forget=3.3159 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=10.0837 mle=1.7885 pcon=4.9821 forget=3.3132 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=9.9276 mle=1.6059 pcon=4.9809 forget=3.3408 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=10.3069 mle=1.9952 pcon=4.9792 forget=3.3325 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=9.9481 mle=1.6626 pcon=4.9777 forget=3.3078 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 20 total=9.9477 mle=1.6230 pcon=4.9760 forget=3.3487 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=10.0339 mle=1.7100 pcon=4.9746 forget=3.3493 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=9.9250 mle=1.5537 pcon=4.9732 forget=3.3981 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=10.0603 mle=1.7439 pcon=4.9719 forget=3.3445 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=9.9226 mle=1.5853 pcon=4.9705 forget=3.3668 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=9.9043 mle=1.5328 pcon=4.9696 forget=3.4019 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=9.8549 mle=1.5048 pcon=4.9684 forget=3.3816 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=10.2518 mle=1.8604 pcon=4.9672 forget=3.4242 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 30 total=9.8006 mle=1.4302 pcon=4.9663 forget=3.4041 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=9.9438 mle=1.4869 pcon=4.9652 forget=3.4918 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=9.9600 mle=1.5637 pcon=4.9640 forget=3.4323 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=9.9160 mle=1.5474 pcon=4.9632 forget=3.4054 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=10.0545 mle=1.6605 pcon=4.9624 forget=3.4316 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=9.9496 mle=1.5782 pcon=4.9614 forget=3.4100 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=10.0001 mle=1.5806 pcon=4.9605 forget=3.4589 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=9.9756 mle=1.5711 pcon=4.9597 forget=3.4448 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 40 total=10.1640 mle=1.7708 pcon=4.9586 forget=3.4345 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=10.2017 mle=1.7787 pcon=4.9581 forget=3.4649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=10.0231 mle=1.5996 pcon=4.9578 forget=3.4657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=10.2986 mle=1.8738 pcon=4.9569 forget=3.4679 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=10.0486 mle=1.6228 pcon=4.9564 forget=3.4693 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=10.0644 mle=1.6590 pcon=4.9558 forget=3.4497 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=10.1008 mle=1.6792 pcon=4.9551 forget=3.4665 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 0 total=10.1537 mle=1.6867 pcon=4.9546 forget=3.5124 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=9.9124 mle=1.4783 pcon=4.9538 forget=3.4803 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=9.9935 mle=1.5506 pcon=4.9533 forget=3.4896 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=10.2044 mle=1.7156 pcon=4.9527 forget=3.5361 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=10.0331 mle=1.6008 pcon=4.9521 forget=3.4802 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=10.3469 mle=1.8838 pcon=4.9515 forget=3.5115 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=10.0515 mle=1.5786 pcon=4.9511 forget=3.5218 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=10.0076 mle=1.5404 pcon=4.9506 forget=3.5167 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 10 total=10.0608 mle=1.5858 pcon=4.9500 forget=3.5250 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=9.9441 mle=1.4966 pcon=4.9495 forget=3.4980 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=9.9672 mle=1.5208 pcon=4.9491 forget=3.4973 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=10.1535 mle=1.7027 pcon=4.9484 forget=3.5024 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=10.0040 mle=1.5715 pcon=4.9477 forget=3.4848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=10.3660 mle=1.9362 pcon=4.9470 forget=3.4828 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=10.2317 mle=1.7725 pcon=4.9464 forget=3.5128 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=10.0559 mle=1.5909 pcon=4.9458 forget=3.5191 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 20 total=10.2261 mle=1.7566 pcon=4.9450 forget=3.5245 nr=64 nf=64 protos=540 fproto_sim=NA
 90%|█████████ | 18/20 [06:32<00:45, 22.88s/it] 95%|█████████▌| 19/20 [06:53<00:22, 22.44s/it]100%|██████████| 20/20 [07:13<00:00, 21.71s/it]100%|██████████| 20/20 [07:13<00:00, 21.68s/it]
[loss] ep 17 it 70 total=10.2578 mle=1.8156 pcon=4.9448 forget=3.4975 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=10.0403 mle=1.5770 pcon=4.9440 forget=3.5192 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=10.0287 mle=1.5826 pcon=4.9431 forget=3.5030 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=10.1442 mle=1.6621 pcon=4.9424 forget=3.5397 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=9.9188 mle=1.4549 pcon=4.9424 forget=3.5215 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=10.2563 mle=1.8067 pcon=4.9418 forget=3.5079 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=10.1728 mle=1.7467 pcon=4.9410 forget=3.4850 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=10.0236 mle=1.6004 pcon=4.9410 forget=3.4823 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=10.1664 mle=1.6923 pcon=4.9405 forget=3.5336 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=10.1200 mle=1.6572 pcon=4.9400 forget=3.5229 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=10.0183 mle=1.5572 pcon=4.9395 forget=3.5215 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=10.0350 mle=1.5959 pcon=4.9391 forget=3.5001 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=10.1405 mle=1.6764 pcon=4.9387 forget=3.5254 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=10.2351 mle=1.8017 pcon=4.9383 forget=3.4951 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=10.1600 mle=1.6916 pcon=4.9381 forget=3.5303 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 40 total=9.9474 mle=1.4635 pcon=4.9380 forget=3.5459 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=10.1869 mle=1.6970 pcon=4.9380 forget=3.5519 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=10.0469 mle=1.5916 pcon=4.9376 forget=3.5177 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=10.0752 mle=1.6373 pcon=4.9371 forget=3.5009 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=10.3065 mle=1.8644 pcon=4.9368 forget=3.5053 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=10.0211 mle=1.5366 pcon=4.9367 forget=3.5478 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=10.1308 mle=1.6164 pcon=4.9366 forget=3.5777 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:36,  2.49it/s]  3%|▎         | 10/391 [00:00<00:15, 24.96it/s]  5%|▍         | 19/391 [00:00<00:08, 42.39it/s]  7%|▋         | 28/391 [00:00<00:06, 55.51it/s]  9%|▉         | 37/391 [00:00<00:05, 65.14it/s] 12%|█▏        | 46/391 [00:00<00:04, 71.87it/s] 14%|█▍        | 55/391 [00:01<00:04, 67.41it/s] 16%|█▌        | 63/391 [00:01<00:05, 62.93it/s] 18%|█▊        | 70/391 [00:01<00:05, 60.75it/s] 20%|██        | 79/391 [00:01<00:04, 67.81it/s] 23%|██▎       | 88/391 [00:01<00:04, 73.37it/s] 25%|██▍       | 97/391 [00:01<00:03, 77.64it/s] 27%|██▋       | 106/391 [00:01<00:03, 80.78it/s] 29%|██▉       | 115/391 [00:01<00:03, 74.18it/s] 31%|███▏      | 123/391 [00:02<00:04, 65.68it/s] 33%|███▎      | 130/391 [00:02<00:04, 62.71it/s] 36%|███▌      | 139/391 [00:02<00:03, 68.42it/s] 38%|███▊      | 148/391 [00:02<00:03, 73.81it/s] 40%|████      | 157/391 [00:02<00:03, 77.20it/s] 42%|████▏     | 166/391 [00:02<00:02, 80.36it/s] 45%|████▍     | 175/391 [00:02<00:03, 68.73it/s] 47%|████▋     | 183/391 [00:02<00:03, 63.76it/s] 49%|████▊     | 190/391 [00:03<00:03, 65.21it/s] 51%|█████     | 199/391 [00:03<00:02, 71.29it/s] 53%|█████▎    | 208/391 [00:03<00:02, 76.01it/s] 55%|█████▌    | 217/391 [00:03<00:02, 79.49it/s] 58%|█████▊    | 226/391 [00:03<00:02, 82.05it/s] 60%|██████    | 235/391 [00:03<00:02, 76.51it/s] 62%|██████▏   | 243/391 [00:03<00:02, 66.47it/s] 64%|██████▍   | 250/391 [00:03<00:02, 63.11it/s] 66%|██████▌   | 259/391 [00:03<00:01, 69.51it/s] 69%|██████▊   | 268/391 [00:04<00:01, 74.72it/s] 71%|███████   | 277/391 [00:04<00:01, 78.08it/s] 73%|███████▎  | 286/391 [00:04<00:01, 80.97it/s] 75%|███████▌  | 295/391 [00:04<00:01, 83.20it/s] 78%|███████▊  | 304/391 [00:04<00:01, 70.86it/s] 80%|███████▉  | 312/391 [00:04<00:01, 66.75it/s] 82%|████████▏ | 319/391 [00:04<00:01, 65.08it/s] 84%|████████▍ | 328/391 [00:04<00:00, 71.27it/s] 86%|████████▌ | 337/391 [00:04<00:00, 75.90it/s] 88%|████████▊ | 346/391 [00:05<00:00, 79.62it/s] 91%|█████████ | 355/391 [00:05<00:00, 81.01it/s] 93%|█████████▎| 364/391 [00:05<00:00, 75.02it/s] 95%|█████████▌| 372/391 [00:05<00:00, 66.23it/s] 97%|█████████▋| 379/391 [00:05<00:00, 62.82it/s] 99%|█████████▉| 389/391 [00:05<00:00, 70.21it/s]100%|██████████| 391/391 [00:05<00:00, 68.06it/s]
50000 images processed, 5.841761112213135 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:29,  2.66it/s] 11%|█▏        | 9/79 [00:00<00:02, 23.85it/s] 23%|██▎       | 18/79 [00:00<00:01, 42.23it/s] 34%|███▍      | 27/79 [00:00<00:00, 55.70it/s] 46%|████▌     | 36/79 [00:00<00:00, 64.71it/s] 57%|█████▋    | 45/79 [00:00<00:00, 70.90it/s] 68%|██████▊   | 54/79 [00:01<00:00, 60.20it/s] 77%|███████▋  | 61/79 [00:01<00:00, 59.21it/s] 87%|████████▋ | 69/79 [00:01<00:00, 63.45it/s] 99%|█████████▊| 78/79 [00:01<00:00, 70.14it/s]100%|██████████| 79/79 [00:01<00:00, 55.06it/s]
10000 images processed, 1.459467887878418 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:03,  3.18it/s]  3%|▎         | 6/204 [00:00<00:11, 16.85it/s]  5%|▌         | 11/204 [00:00<00:07, 26.69it/s] 10%|▉         | 20/204 [00:00<00:04, 44.12it/s] 14%|█▍        | 29/204 [00:00<00:03, 56.40it/s] 19%|█▊        | 38/204 [00:00<00:02, 65.00it/s] 23%|██▎       | 47/204 [00:00<00:02, 70.91it/s] 27%|██▋       | 55/204 [00:01<00:02, 72.49it/s] 31%|███       | 63/204 [00:01<00:02, 59.91it/s] 36%|███▌      | 73/204 [00:01<00:01, 66.96it/s] 40%|███▉      | 81/204 [00:01<00:01, 64.78it/s] 44%|████▍     | 90/204 [00:01<00:01, 70.25it/s] 49%|████▊     | 99/204 [00:01<00:01, 74.67it/s] 53%|█████▎    | 108/204 [00:01<00:01, 78.00it/s] 57%|█████▋    | 117/204 [00:01<00:01, 80.90it/s] 62%|██████▏   | 126/204 [00:02<00:01, 76.12it/s] 66%|██████▌   | 134/204 [00:02<00:01, 65.83it/s] 69%|██████▉   | 141/204 [00:02<00:01, 62.36it/s] 74%|███████▎  | 150/204 [00:02<00:00, 68.90it/s] 78%|███████▊  | 159/204 [00:02<00:00, 73.72it/s] 82%|████████▏ | 168/204 [00:02<00:00, 77.28it/s] 87%|████████▋ | 177/204 [00:02<00:00, 79.93it/s] 91%|█████████ | 186/204 [00:02<00:00, 81.97it/s] 96%|█████████▌| 195/204 [00:03<00:00, 64.56it/s]100%|█████████▉| 203/204 [00:03<00:00, 62.34it/s]100%|██████████| 204/204 [00:03<00:00, 63.27it/s]
26032 images processed, 3.2674906253814697 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:43,  1.78it/s]  8%|▊         | 6/79 [00:00<00:06, 11.13it/s] 14%|█▍        | 11/79 [00:00<00:03, 19.17it/s] 22%|██▏       | 17/79 [00:00<00:02, 28.58it/s] 29%|██▉       | 23/79 [00:00<00:01, 36.30it/s] 35%|███▌      | 28/79 [00:01<00:01, 33.57it/s] 43%|████▎     | 34/79 [00:01<00:01, 37.89it/s] 53%|█████▎    | 42/79 [00:01<00:00, 41.32it/s] 59%|█████▉    | 47/79 [00:01<00:00, 41.49it/s] 67%|██████▋   | 53/79 [00:01<00:00, 45.03it/s] 73%|███████▎  | 58/79 [00:01<00:00, 43.13it/s] 84%|████████▎ | 66/79 [00:02<00:00, 41.45it/s] 91%|█████████ | 72/79 [00:02<00:00, 44.27it/s]100%|██████████| 79/79 [00:02<00:00, 35.58it/s]
10000 images processed, 2.263016700744629 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:28,  2.79it/s]  6%|▋         | 5/79 [00:00<00:05, 13.21it/s] 13%|█▎        | 10/79 [00:00<00:02, 23.30it/s] 20%|██        | 16/79 [00:00<00:01, 32.58it/s] 32%|███▏      | 25/79 [00:00<00:01, 47.57it/s] 43%|████▎     | 34/79 [00:00<00:00, 58.72it/s] 54%|█████▍    | 43/79 [00:01<00:00, 66.52it/s] 66%|██████▌   | 52/79 [00:01<00:00, 72.49it/s] 76%|███████▌  | 60/79 [00:01<00:00, 59.01it/s] 85%|████████▍ | 67/79 [00:01<00:00, 57.37it/s] 96%|█████████▌| 76/79 [00:01<00:00, 64.83it/s]100%|██████████| 79/79 [00:01<00:00, 50.64it/s]
10000 images processed, 1.587742805480957 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:24,  2.79it/s] 14%|█▍        | 10/70 [00:00<00:02, 27.28it/s] 27%|██▋       | 19/70 [00:00<00:01, 43.79it/s] 37%|███▋      | 26/70 [00:00<00:01, 43.06it/s] 46%|████▌     | 32/70 [00:00<00:00, 45.97it/s] 57%|█████▋    | 40/70 [00:00<00:00, 53.75it/s] 70%|███████   | 49/70 [00:01<00:00, 62.80it/s] 83%|████████▎ | 58/70 [00:01<00:00, 68.91it/s] 96%|█████████▌| 67/70 [00:01<00:00, 74.47it/s]100%|██████████| 70/70 [00:01<00:00, 53.14it/s]
8925 images processed, 1.3505761623382568 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:42,  1.05it/s]  9%|▉         | 4/45 [00:01<00:08,  4.68it/s] 20%|██        | 9/45 [00:01<00:04,  8.93it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.94it/s] 36%|███▌      | 16/45 [00:01<00:01, 14.94it/s] 42%|████▏     | 19/45 [00:02<00:02, 12.06it/s] 49%|████▉     | 22/45 [00:02<00:01, 13.01it/s] 56%|█████▌    | 25/45 [00:02<00:01, 13.47it/s] 60%|██████    | 27/45 [00:02<00:01, 13.28it/s] 67%|██████▋   | 30/45 [00:02<00:01, 13.99it/s] 73%|███████▎  | 33/45 [00:03<00:01, 10.73it/s] 78%|███████▊  | 35/45 [00:03<00:00, 11.38it/s] 91%|█████████ | 41/45 [00:03<00:00, 14.45it/s] 96%|█████████▌| 43/45 [00:04<00:00, 10.10it/s]100%|██████████| 45/45 [00:04<00:00, 10.84it/s]
5640 images processed, 4.175778865814209 seconds used

21.575581073760986
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.51  99.36
places365     67.74  81.19
LSUN          17.31  96.12
iSUN          72.21  81.74
dtd           37.84  91.38
forget        79.50  86.89
AVG           46.19  89.45
Retain-Acc: 0.7407
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.89 AUIN: 98.36
19.10247564315796
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.5 lr=0.0005 epochs=50
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.5, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:26<21:19, 26.10s/it]  4%|▍         | 2/50 [00:49<19:44, 24.68s/it]  6%|▌         | 3/50 [01:07<16:50, 21.50s/it]  8%|▊         | 4/50 [01:25<15:24, 20.10s/it] 10%|█         | 5/50 [01:48<15:53, 21.18s/it] 12%|█▏        | 6/50 [02:11<16:00, 21.82s/it] 14%|█▍        | 7/50 [02:29<14:46, 20.62s/it] 16%|█▌        | 8/50 [02:48<13:55, 19.90s/it][loss] ep 0 it 0 total=10.3048 mle=1.5710 pcon=5.2950 forget=3.4387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=10.3337 mle=1.5424 pcon=5.2879 forget=3.5033 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=10.4164 mle=1.7005 pcon=5.2809 forget=3.4350 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=10.5977 mle=1.8998 pcon=5.2738 forget=3.4241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=10.4367 mle=1.7132 pcon=5.2670 forget=3.4565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=10.2084 mle=1.5019 pcon=5.2603 forget=3.4461 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=10.2686 mle=1.5597 pcon=5.2540 forget=3.4548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=10.4027 mle=1.6809 pcon=5.2476 forget=3.4742 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=10.4375 mle=1.6715 pcon=5.2409 forget=3.5251 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=10.2892 mle=1.6371 pcon=5.2346 forget=3.4175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=10.2186 mle=1.5166 pcon=5.2285 forget=3.4736 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=10.4761 mle=1.7802 pcon=5.2225 forget=3.4734 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=10.4446 mle=1.7647 pcon=5.2167 forget=3.4632 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=10.2373 mle=1.6054 pcon=5.2112 forget=3.4206 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=10.3748 mle=1.7238 pcon=5.2056 forget=3.4453 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=10.4598 mle=1.7942 pcon=5.2003 forget=3.4653 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=10.1898 mle=1.5529 pcon=5.1950 forget=3.4419 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=10.4719 mle=1.8881 pcon=5.1899 forget=3.3939 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=10.3659 mle=1.7418 pcon=5.1847 forget=3.4395 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=10.1181 mle=1.5264 pcon=5.1797 forget=3.4121 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=10.2247 mle=1.5855 pcon=5.1746 forget=3.4646 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=10.4800 mle=1.8837 pcon=5.1699 forget=3.4263 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=10.1512 mle=1.5923 pcon=5.1652 forget=3.3937 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=10.3832 mle=1.8111 pcon=5.1605 forget=3.4116 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=10.2882 mle=1.7058 pcon=5.1560 forget=3.4265 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=10.2705 mle=1.6708 pcon=5.1519 forget=3.4478 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=10.3286 mle=1.7669 pcon=5.1475 forget=3.4141 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=10.4672 mle=1.9185 pcon=5.1436 forget=3.4051 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=10.1088 mle=1.5700 pcon=5.1395 forget=3.3993 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=10.3616 mle=1.8362 pcon=5.1354 forget=3.3900 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=10.2721 mle=1.7030 pcon=5.1311 forget=3.4379 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=10.2680 mle=1.6957 pcon=5.1275 forget=3.4448 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=10.1322 mle=1.6538 pcon=5.1237 forget=3.3547 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=10.2391 mle=1.7295 pcon=5.1197 forget=3.3900 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=10.1814 mle=1.6509 pcon=5.1160 forget=3.4144 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=9.9796 mle=1.4536 pcon=5.1126 forget=3.4135 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=10.2652 mle=1.7666 pcon=5.1087 forget=3.3899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=9.9789 mle=1.4912 pcon=5.1052 forget=3.3825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=10.3849 mle=1.8719 pcon=5.1017 forget=3.4113 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=10.3577 mle=1.8672 pcon=5.0984 forget=3.3922 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=10.2583 mle=1.7998 pcon=5.0947 forget=3.3638 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=10.0750 mle=1.5844 pcon=5.0916 forget=3.3991 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=10.0788 mle=1.5950 pcon=5.0881 forget=3.3957 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=10.1395 mle=1.6915 pcon=5.0852 forget=3.3628 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=10.2021 mle=1.7740 pcon=5.0822 forget=3.3458 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=10.0508 mle=1.5965 pcon=5.0791 forget=3.3751 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=10.1929 mle=1.7462 pcon=5.0762 forget=3.3704 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=9.9023 mle=1.4007 pcon=5.0734 forget=3.4282 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=9.9250 mle=1.5349 pcon=5.0706 forget=3.3195 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=10.0594 mle=1.5914 pcon=5.0679 forget=3.4000 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=10.0927 mle=1.6660 pcon=5.0652 forget=3.3616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=10.0446 mle=1.6303 pcon=5.0629 forget=3.3514 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=10.2682 mle=1.8220 pcon=5.0600 forget=3.3862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=10.0004 mle=1.5369 pcon=5.0576 forget=3.4059 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=10.0940 mle=1.6746 pcon=5.0551 forget=3.3642 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=10.0270 mle=1.6305 pcon=5.0524 forget=3.3441 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=10.1254 mle=1.7429 pcon=5.0500 forget=3.3324 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=10.0212 mle=1.6256 pcon=5.0474 forget=3.3481 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=9.9426 mle=1.5801 pcon=5.0450 forget=3.3175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=10.0680 mle=1.6520 pcon=5.0430 forget=3.3730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=10.2304 mle=1.7948 pcon=5.0408 forget=3.3948 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=10.1683 mle=1.7416 pcon=5.0383 forget=3.3884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=10.0333 mle=1.6433 pcon=5.0362 forget=3.3538 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 18%|█▊        | 9/50 [03:14<15:02, 22.00s/it] 20%|██        | 10/50 [03:34<14:16, 21.40s/it] 22%|██▏       | 11/50 [03:53<13:19, 20.49s/it] 24%|██▍       | 12/50 [04:13<12:59, 20.52s/it] 26%|██▌       | 13/50 [04:40<13:46, 22.34s/it] 28%|██▊       | 14/50 [04:58<12:37, 21.05s/it] 30%|███       | 15/50 [05:16<11:43, 20.09s/it] 32%|███▏      | 16/50 [05:39<11:56, 21.08s/it] 34%|███▍      | 17/50 [06:03<12:00, 21.83s/it][loss] ep 8 it 30 total=9.9581 mle=1.5377 pcon=5.0342 forget=3.3862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=9.9789 mle=1.5880 pcon=5.0322 forget=3.3587 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=9.9091 mle=1.5057 pcon=5.0299 forget=3.3735 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=10.0665 mle=1.7032 pcon=5.0279 forget=3.3354 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=9.8867 mle=1.5305 pcon=5.0263 forget=3.3299 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=10.0077 mle=1.6771 pcon=5.0241 forget=3.3064 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=10.1695 mle=1.7564 pcon=5.0219 forget=3.3911 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=9.8134 mle=1.4844 pcon=5.0199 forget=3.3092 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=9.9167 mle=1.6331 pcon=5.0177 forget=3.2659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=10.0009 mle=1.6688 pcon=5.0154 forget=3.3167 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=10.1937 mle=1.8330 pcon=5.0137 forget=3.3471 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=9.8869 mle=1.5767 pcon=5.0118 forget=3.2983 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=9.8922 mle=1.5642 pcon=5.0100 forget=3.3180 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=9.8886 mle=1.5570 pcon=5.0078 forget=3.3238 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=9.8655 mle=1.5595 pcon=5.0060 forget=3.3000 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=9.9283 mle=1.6074 pcon=5.0039 forget=3.3169 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=9.9336 mle=1.5885 pcon=5.0018 forget=3.3434 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=10.0483 mle=1.7530 pcon=4.9998 forget=3.2955 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=10.1246 mle=1.8491 pcon=4.9981 forget=3.2774 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=10.1441 mle=1.8539 pcon=4.9961 forget=3.2941 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=9.8842 mle=1.5800 pcon=4.9944 forget=3.3098 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=9.8318 mle=1.5616 pcon=4.9927 forget=3.2774 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=10.0006 mle=1.6524 pcon=4.9909 forget=3.3573 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=9.9006 mle=1.5717 pcon=4.9891 forget=3.3397 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=9.8990 mle=1.6206 pcon=4.9873 forget=3.2911 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=9.8375 mle=1.5659 pcon=4.9857 forget=3.2860 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=9.9591 mle=1.6597 pcon=4.9836 forget=3.3158 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=10.0835 mle=1.7885 pcon=4.9821 forget=3.3129 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=9.9277 mle=1.6061 pcon=4.9809 forget=3.3407 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=10.3071 mle=1.9954 pcon=4.9792 forget=3.3325 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=9.9480 mle=1.6625 pcon=4.9777 forget=3.3078 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 20 total=9.9478 mle=1.6230 pcon=4.9760 forget=3.3488 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=10.0349 mle=1.7106 pcon=4.9746 forget=3.3496 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=9.9259 mle=1.5544 pcon=4.9732 forget=3.3982 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=10.0611 mle=1.7442 pcon=4.9719 forget=3.3450 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=9.9247 mle=1.5863 pcon=4.9705 forget=3.3678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=9.9030 mle=1.5303 pcon=4.9697 forget=3.4029 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=9.8568 mle=1.5049 pcon=4.9685 forget=3.3834 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=10.2529 mle=1.8595 pcon=4.9674 forget=3.4260 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 30 total=9.8035 mle=1.4302 pcon=4.9664 forget=3.4069 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=9.9459 mle=1.4869 pcon=4.9653 forget=3.4937 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=9.9645 mle=1.5647 pcon=4.9642 forget=3.4356 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=9.9207 mle=1.5476 pcon=4.9634 forget=3.4097 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=10.0602 mle=1.6614 pcon=4.9626 forget=3.4362 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=9.9551 mle=1.5782 pcon=4.9617 forget=3.4153 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=10.0049 mle=1.5803 pcon=4.9608 forget=3.4638 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=9.9815 mle=1.5709 pcon=4.9601 forget=3.4505 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 40 total=10.1703 mle=1.7705 pcon=4.9590 forget=3.4407 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=10.1999 mle=1.7709 pcon=4.9585 forget=3.4705 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=10.0287 mle=1.5985 pcon=4.9583 forget=3.4719 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=10.3043 mle=1.8731 pcon=4.9574 forget=3.4738 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=10.0551 mle=1.6232 pcon=4.9570 forget=3.4750 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=10.0687 mle=1.6584 pcon=4.9563 forget=3.4541 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=10.1039 mle=1.6789 pcon=4.9556 forget=3.4694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 0 total=10.1554 mle=1.6863 pcon=4.9551 forget=3.5141 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=9.9126 mle=1.4776 pcon=4.9543 forget=3.4807 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=9.9931 mle=1.5505 pcon=4.9538 forget=3.4889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=10.1989 mle=1.7139 pcon=4.9531 forget=3.5318 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=10.0262 mle=1.6007 pcon=4.9525 forget=3.4730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=10.3369 mle=1.8830 pcon=4.9518 forget=3.5020 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=10.0394 mle=1.5782 pcon=4.9513 forget=3.5098 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=9.9937 mle=1.5391 pcon=4.9507 forget=3.5039 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 10 total=10.0373 mle=1.5849 pcon=4.9499 forget=3.5025 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=9.9258 mle=1.5004 pcon=4.9493 forget=3.4762 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=9.9351 mle=1.5187 pcon=4.9488 forget=3.4677 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=10.1214 mle=1.7019 pcon=4.9479 forget=3.4717 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=9.9681 mle=1.5750 pcon=4.9470 forget=3.4461 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=10.3248 mle=1.9348 pcon=4.9460 forget=3.4439 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=10.1828 mle=1.7711 pcon=4.9453 forget=3.4664 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=10.0103 mle=1.5953 pcon=4.9444 forget=3.4705 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 20 total=10.1813 mle=1.7620 pcon=4.9433 forget=3.4759 nr=64 nf=64 protos=540 fproto_sim=NA
 36%|███▌      | 18/50 [06:21<11:04, 20.78s/it] 38%|███▊      | 19/50 [06:40<10:25, 20.17s/it] 40%|████      | 20/50 [07:07<11:10, 22.35s/it] 42%|████▏     | 21/50 [07:27<10:22, 21.46s/it] 44%|████▍     | 22/50 [07:44<09:30, 20.37s/it] 46%|████▌     | 23/50 [08:06<09:17, 20.63s/it] 48%|████▊     | 24/50 [08:31<09:31, 21.96s/it] 50%|█████     | 25/50 [08:49<08:43, 20.92s/it][loss] ep 17 it 70 total=10.1970 mle=1.8157 pcon=4.9429 forget=3.4385 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=9.9792 mle=1.5765 pcon=4.9418 forget=3.4609 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=9.9690 mle=1.5873 pcon=4.9406 forget=3.4411 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=10.0795 mle=1.6640 pcon=4.9397 forget=3.4758 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=9.8380 mle=1.4504 pcon=4.9393 forget=3.4483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=10.1710 mle=1.8034 pcon=4.9385 forget=3.4291 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=10.1016 mle=1.7477 pcon=4.9373 forget=3.4165 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=9.9366 mle=1.5979 pcon=4.9370 forget=3.4017 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=10.0822 mle=1.6958 pcon=4.9361 forget=3.4502 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=10.0363 mle=1.6600 pcon=4.9352 forget=3.4410 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=9.9170 mle=1.5570 pcon=4.9345 forget=3.4255 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=9.9453 mle=1.6002 pcon=4.9337 forget=3.4114 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=10.0294 mle=1.6683 pcon=4.9329 forget=3.4282 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=10.1388 mle=1.8119 pcon=4.9321 forget=3.3948 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=10.0454 mle=1.6841 pcon=4.9316 forget=3.4296 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 40 total=9.8359 mle=1.4624 pcon=4.9311 forget=3.4423 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=10.0795 mle=1.6984 pcon=4.9308 forget=3.4504 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=9.9162 mle=1.5831 pcon=4.9299 forget=3.4032 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=9.9561 mle=1.6370 pcon=4.9290 forget=3.3901 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=10.1841 mle=1.8679 pcon=4.9283 forget=3.3879 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=9.8885 mle=1.5382 pcon=4.9278 forget=3.4225 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=9.9965 mle=1.6107 pcon=4.9273 forget=3.4585 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 0 total=9.9779 mle=1.6606 pcon=4.9265 forget=3.3908 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=9.9915 mle=1.6853 pcon=4.9258 forget=3.3805 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=10.0599 mle=1.7263 pcon=4.9253 forget=3.4084 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=10.1299 mle=1.8326 pcon=4.9244 forget=3.3729 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=9.9255 mle=1.6466 pcon=4.9234 forget=3.3555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=10.2855 mle=1.9957 pcon=4.9224 forget=3.3673 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=9.9961 mle=1.6774 pcon=4.9214 forget=3.3973 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=9.7062 mle=1.4079 pcon=4.9206 forget=3.3777 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 10 total=9.7032 mle=1.4003 pcon=4.9197 forget=3.3831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=10.0005 mle=1.6846 pcon=4.9190 forget=3.3968 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=9.8430 mle=1.5208 pcon=4.9183 forget=3.4039 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=9.8251 mle=1.5605 pcon=4.9178 forget=3.3468 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=10.0770 mle=1.7663 pcon=4.9171 forget=3.3936 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=9.9594 mle=1.6681 pcon=4.9163 forget=3.3750 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=9.9190 mle=1.6328 pcon=4.9153 forget=3.3708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=9.9524 mle=1.6253 pcon=4.9145 forget=3.4126 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 22 it 20 total=9.9381 mle=1.6567 pcon=4.9136 forget=3.3678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=10.0562 mle=1.8007 pcon=4.9126 forget=3.3429 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=9.8302 mle=1.5231 pcon=4.9116 forget=3.3956 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=9.9257 mle=1.6408 pcon=4.9108 forget=3.3741 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=9.9433 mle=1.6540 pcon=4.9101 forget=3.3792 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=9.8103 mle=1.5116 pcon=4.9093 forget=3.3895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=9.9004 mle=1.6461 pcon=4.9086 forget=3.3458 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=10.3356 mle=2.0520 pcon=4.9075 forget=3.3761 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 23 it 30 total=9.8017 mle=1.5327 pcon=4.9072 forget=3.3618 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=9.9545 mle=1.6953 pcon=4.9063 forget=3.3529 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=9.8428 mle=1.5528 pcon=4.9054 forget=3.3846 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=9.9927 mle=1.7562 pcon=4.9045 forget=3.3320 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=9.9329 mle=1.6668 pcon=4.9038 forget=3.3623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=9.8321 mle=1.5505 pcon=4.9028 forget=3.3787 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=9.8991 mle=1.6522 pcon=4.9020 forget=3.3449 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=9.8065 mle=1.5291 pcon=4.9010 forget=3.3764 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 24 it 40 total=9.7378 mle=1.5163 pcon=4.9003 forget=3.3212 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=9.8616 mle=1.6098 pcon=4.8995 forget=3.3523 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=9.7222 mle=1.4430 pcon=4.8986 forget=3.3806 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=9.8644 mle=1.6318 pcon=4.8979 forget=3.3346 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 240 total=9.8234 mle=1.5726 pcon=4.8972 forget=3.3536 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=10.0836 mle=1.8056 pcon=4.8963 forget=3.3817 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=9.9263 mle=1.6975 pcon=4.8953 forget=3.3334 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 25 it 0 total=9.9112 mle=1.6591 pcon=4.8942 forget=3.3579 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=10.0895 mle=1.8124 pcon=4.8936 forget=3.3836 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=9.8131 mle=1.6108 pcon=4.8926 forget=3.3097 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 150 total=9.9919 mle=1.7813 pcon=4.8921 forget=3.3186 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 200 total=10.0871 mle=1.8316 pcon=4.8911 forget=3.3644 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=10.0067 mle=1.7481 pcon=4.8901 forget=3.3686 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=9.8152 mle=1.5909 pcon=4.8891 forget=3.3352 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=9.8176 mle=1.5781 pcon=4.8885 forget=3.3510 nr=64 nf=64 protos=540 fproto_sim=NA
 52%|█████▏    | 26/50 [09:08<08:05, 20.25s/it] 54%|█████▍    | 27/50 [09:33<08:22, 21.83s/it] 56%|█████▌    | 28/50 [09:55<07:57, 21.69s/it] 58%|█████▊    | 29/50 [10:13<07:14, 20.69s/it] 60%|██████    | 30/50 [10:33<06:47, 20.37s/it] 62%|██████▏   | 31/50 [10:59<06:57, 22.00s/it] 64%|██████▍   | 32/50 [11:17<06:17, 20.96s/it] 66%|██████▌   | 33/50 [11:36<05:43, 20.24s/it][peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 26 it 10 total=9.7849 mle=1.5486 pcon=4.8877 forget=3.3487 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=9.7871 mle=1.5299 pcon=4.8867 forget=3.3705 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=10.0120 mle=1.8049 pcon=4.8858 forget=3.3213 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=9.6857 mle=1.4742 pcon=4.8847 forget=3.3267 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=9.8897 mle=1.6560 pcon=4.8835 forget=3.3501 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=9.7585 mle=1.5162 pcon=4.8826 forget=3.3596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 310 total=10.1009 mle=1.8823 pcon=4.8817 forget=3.3369 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=9.7586 mle=1.5556 pcon=4.8803 forget=3.3227 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 27 it 20 total=9.8723 mle=1.6675 pcon=4.8792 forget=3.3256 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=9.7864 mle=1.5591 pcon=4.8779 forget=3.3494 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=9.7340 mle=1.5268 pcon=4.8767 forget=3.3305 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=9.7596 mle=1.5385 pcon=4.8754 forget=3.3457 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=9.7798 mle=1.5645 pcon=4.8741 forget=3.3412 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=9.8233 mle=1.6236 pcon=4.8727 forget=3.3270 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=9.9471 mle=1.7274 pcon=4.8716 forget=3.3482 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=9.9933 mle=1.7436 pcon=4.8703 forget=3.3794 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 28 it 30 total=9.8938 mle=1.6836 pcon=4.8691 forget=3.3410 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=9.9427 mle=1.7282 pcon=4.8681 forget=3.3464 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=9.7371 mle=1.5259 pcon=4.8670 forget=3.3442 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=9.6790 mle=1.5050 pcon=4.8658 forget=3.3082 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=9.5759 mle=1.4034 pcon=4.8642 forget=3.3083 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=9.7431 mle=1.5279 pcon=4.8627 forget=3.3526 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=9.9561 mle=1.7559 pcon=4.8610 forget=3.3392 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=9.9274 mle=1.7319 pcon=4.8594 forget=3.3360 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 29 it 40 total=9.9423 mle=1.7546 pcon=4.8578 forget=3.3298 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=9.6675 mle=1.5167 pcon=4.8562 forget=3.2946 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=9.8089 mle=1.6317 pcon=4.8548 forget=3.3224 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=9.7767 mle=1.6017 pcon=4.8534 forget=3.3215 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=9.6780 mle=1.4993 pcon=4.8519 forget=3.3268 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=9.9966 mle=1.8534 pcon=4.8506 forget=3.2925 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=9.7801 mle=1.6079 pcon=4.8493 forget=3.3229 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 30 it 0 total=9.7945 mle=1.6431 pcon=4.8479 forget=3.3036 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=9.8329 mle=1.6520 pcon=4.8466 forget=3.3344 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=9.6861 mle=1.5061 pcon=4.8451 forget=3.3349 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=9.9399 mle=1.8121 pcon=4.8435 forget=3.2843 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=9.6624 mle=1.4539 pcon=4.8420 forget=3.3665 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=9.9112 mle=1.7445 pcon=4.8405 forget=3.3262 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=9.6666 mle=1.5050 pcon=4.8391 forget=3.3225 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=10.0704 mle=1.9045 pcon=4.8377 forget=3.3283 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 31 it 10 total=9.7332 mle=1.6150 pcon=4.8363 forget=3.2819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=9.6740 mle=1.5309 pcon=4.8349 forget=3.3082 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=9.7452 mle=1.5948 pcon=4.8336 forget=3.3168 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=9.6512 mle=1.5114 pcon=4.8321 forget=3.3077 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=9.6649 mle=1.5512 pcon=4.8306 forget=3.2830 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 260 total=9.5971 mle=1.4800 pcon=4.8291 forget=3.2881 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=9.7572 mle=1.6276 pcon=4.8275 forget=3.3021 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=9.6453 mle=1.5394 pcon=4.8259 forget=3.2800 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 32 it 20 total=9.8412 mle=1.7451 pcon=4.8244 forget=3.2717 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=9.6734 mle=1.5908 pcon=4.8226 forget=3.2599 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=9.6698 mle=1.5273 pcon=4.8211 forget=3.3214 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 170 total=9.6520 mle=1.5425 pcon=4.8198 forget=3.2897 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=9.8176 mle=1.7208 pcon=4.8184 forget=3.2784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=9.6499 mle=1.5509 pcon=4.8170 forget=3.2820 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=9.5512 mle=1.4697 pcon=4.8155 forget=3.2660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=9.5004 mle=1.4296 pcon=4.8143 forget=3.2565 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 33 it 30 total=9.7342 mle=1.6413 pcon=4.8129 forget=3.2799 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=10.0763 mle=2.0083 pcon=4.8115 forget=3.2565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=9.5344 mle=1.4575 pcon=4.8101 forget=3.2668 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=9.9308 mle=1.8259 pcon=4.8086 forget=3.2962 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=9.7031 mle=1.5777 pcon=4.8072 forget=3.3183 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=9.7243 mle=1.6243 pcon=4.8059 forget=3.2942 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=9.6855 mle=1.6144 pcon=4.8047 forget=3.2664 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 380 total=9.6496 mle=1.5531 pcon=4.8033 forget=3.2933 nr=64 nf=64 protos=540 fproto_sim=NA
 68%|██████▊   | 34/50 [11:59<05:39, 21.24s/it] 70%|███████   | 35/50 [12:22<05:24, 21.61s/it] 72%|███████▏  | 36/50 [12:39<04:44, 20.32s/it] 74%|███████▍  | 37/50 [12:58<04:17, 19.82s/it] 76%|███████▌  | 38/50 [13:22<04:13, 21.16s/it] 78%|███████▊  | 39/50 [13:42<03:50, 20.91s/it] 80%|████████  | 40/50 [14:00<03:20, 20.05s/it] 82%|████████▏ | 41/50 [14:21<03:00, 20.08s/it] 84%|████████▍ | 42/50 [14:47<02:56, 22.06s/it] 86%|████████▌ | 43/50 [15:05<02:26, 20.87s/it][peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 34 it 40 total=9.7601 mle=1.6749 pcon=4.8016 forget=3.2836 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=9.5027 mle=1.4212 pcon=4.8000 forget=3.2815 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 140 total=9.6421 mle=1.5918 pcon=4.7987 forget=3.2516 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=9.7337 mle=1.6621 pcon=4.7973 forget=3.2743 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 240 total=9.8294 mle=1.7627 pcon=4.7958 forget=3.2708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=9.6643 mle=1.5868 pcon=4.7942 forget=3.2832 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=9.6837 mle=1.5869 pcon=4.7929 forget=3.3039 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 35 it 0 total=9.7402 mle=1.6548 pcon=4.7915 forget=3.2939 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 50 total=9.6031 mle=1.5273 pcon=4.7902 forget=3.2856 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 100 total=9.9422 mle=1.8311 pcon=4.7886 forget=3.3225 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 150 total=9.6827 mle=1.6127 pcon=4.7873 forget=3.2827 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 200 total=9.6658 mle=1.5627 pcon=4.7861 forget=3.3171 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 250 total=9.8106 mle=1.7193 pcon=4.7846 forget=3.3068 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 300 total=9.5220 mle=1.4864 pcon=4.7833 forget=3.2523 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 350 total=9.7745 mle=1.7265 pcon=4.7820 forget=3.2659 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 36 it 10 total=9.8695 mle=1.7686 pcon=4.7808 forget=3.3201 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 60 total=9.6242 mle=1.5558 pcon=4.7797 forget=3.2886 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 110 total=9.6972 mle=1.6138 pcon=4.7784 forget=3.3050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 160 total=9.7203 mle=1.6255 pcon=4.7771 forget=3.3177 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 210 total=9.5268 mle=1.4626 pcon=4.7760 forget=3.2882 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 260 total=9.6871 mle=1.5591 pcon=4.7750 forget=3.3529 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 310 total=9.7966 mle=1.6753 pcon=4.7737 forget=3.3475 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 360 total=9.6885 mle=1.5853 pcon=4.7725 forget=3.3307 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 20 total=9.5709 mle=1.4690 pcon=4.7711 forget=3.3307 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 70 total=9.8257 mle=1.6919 pcon=4.7701 forget=3.3637 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 120 total=9.6229 mle=1.5140 pcon=4.7688 forget=3.3401 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 170 total=9.6443 mle=1.5713 pcon=4.7675 forget=3.3054 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 220 total=9.8176 mle=1.7108 pcon=4.7662 forget=3.3405 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 270 total=9.7040 mle=1.6219 pcon=4.7651 forget=3.3171 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 320 total=9.6913 mle=1.5961 pcon=4.7639 forget=3.3313 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 370 total=9.7322 mle=1.5775 pcon=4.7630 forget=3.3917 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 30 total=9.7831 mle=1.6444 pcon=4.7621 forget=3.3767 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 80 total=9.5758 mle=1.4679 pcon=4.7612 forget=3.3467 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 130 total=9.7602 mle=1.6307 pcon=4.7601 forget=3.3694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 180 total=9.7024 mle=1.5917 pcon=4.7591 forget=3.3516 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 230 total=9.6567 mle=1.5333 pcon=4.7581 forget=3.3653 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 280 total=9.7961 mle=1.6975 pcon=4.7572 forget=3.3414 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 330 total=9.7006 mle=1.5749 pcon=4.7560 forget=3.3698 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 380 total=9.5764 mle=1.4547 pcon=4.7552 forget=3.3665 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 40 total=9.8907 mle=1.7821 pcon=4.7546 forget=3.3541 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 90 total=9.8290 mle=1.6652 pcon=4.7536 forget=3.4102 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 140 total=9.7177 mle=1.6243 pcon=4.7528 forget=3.3407 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 190 total=9.6028 mle=1.4818 pcon=4.7520 forget=3.3691 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 240 total=9.8115 mle=1.6658 pcon=4.7512 forget=3.3945 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 290 total=9.5881 mle=1.4481 pcon=4.7504 forget=3.3896 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 340 total=9.7364 mle=1.5974 pcon=4.7496 forget=3.3894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 0 total=9.7112 mle=1.5531 pcon=4.7486 forget=3.4095 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 50 total=9.7704 mle=1.6100 pcon=4.7477 forget=3.4127 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 100 total=9.7201 mle=1.5841 pcon=4.7467 forget=3.3893 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 150 total=9.5753 mle=1.4727 pcon=4.7460 forget=3.3566 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 200 total=9.8816 mle=1.7578 pcon=4.7451 forget=3.3787 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 250 total=9.9677 mle=1.7932 pcon=4.7443 forget=3.4302 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 300 total=9.6050 mle=1.4905 pcon=4.7437 forget=3.3709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 350 total=9.5568 mle=1.4090 pcon=4.7430 forget=3.4048 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 10 total=9.5994 mle=1.4885 pcon=4.7423 forget=3.3685 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 60 total=10.0470 mle=1.8776 pcon=4.7418 forget=3.4276 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 110 total=9.7386 mle=1.5660 pcon=4.7409 forget=3.4317 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 160 total=9.6818 mle=1.5302 pcon=4.7404 forget=3.4112 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 210 total=9.7697 mle=1.6054 pcon=4.7397 forget=3.4246 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 260 total=9.6165 mle=1.4924 pcon=4.7394 forget=3.3848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 310 total=9.6287 mle=1.4884 pcon=4.7390 forget=3.4013 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 360 total=9.8098 mle=1.6499 pcon=4.7385 forget=3.4214 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 20 total=9.8142 mle=1.6539 pcon=4.7379 forget=3.4223 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 70 total=9.7756 mle=1.5989 pcon=4.7371 forget=3.4396 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 120 total=10.1171 mle=1.9295 pcon=4.7364 forget=3.4512 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 170 total=9.7252 mle=1.5777 pcon=4.7360 forget=3.4116 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 220 total=9.7721 mle=1.6200 pcon=4.7355 forget=3.4166 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 270 total=9.8292 mle=1.7110 pcon=4.7349 forget=3.3832 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 320 total=9.8551 mle=1.6892 pcon=4.7345 forget=3.4314 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 370 total=9.7503 mle=1.6085 pcon=4.7341 forget=3.4077 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 30 total=9.8114 mle=1.5881 pcon=4.7335 forget=3.4898 nr=64 nf=64 protos=540 fproto_sim=NA
 88%|████████▊ | 44/50 [15:23<01:59, 19.87s/it] 90%|█████████ | 45/50 [15:46<01:44, 20.95s/it] 92%|█████████▏| 46/50 [16:10<01:26, 21.65s/it] 94%|█████████▍| 47/50 [16:28<01:02, 20.68s/it] 96%|█████████▌| 48/50 [16:47<00:40, 20.25s/it] 98%|█████████▊| 49/50 [17:13<00:21, 21.85s/it]100%|██████████| 50/50 [17:32<00:00, 21.06s/it]100%|██████████| 50/50 [17:32<00:00, 21.05s/it]
[loss] ep 43 it 80 total=9.7210 mle=1.5420 pcon=4.7330 forget=3.4460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 130 total=9.8589 mle=1.6609 pcon=4.7325 forget=3.4656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 180 total=9.7793 mle=1.5221 pcon=4.7321 forget=3.5251 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 230 total=9.6682 mle=1.5515 pcon=4.7314 forget=3.3852 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 280 total=9.7103 mle=1.5457 pcon=4.7311 forget=3.4335 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 330 total=9.7001 mle=1.5136 pcon=4.7304 forget=3.4560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 380 total=9.5895 mle=1.4794 pcon=4.7299 forget=3.3802 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 40 total=9.8736 mle=1.6764 pcon=4.7294 forget=3.4678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 90 total=9.7035 mle=1.5297 pcon=4.7289 forget=3.4448 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 140 total=9.8375 mle=1.6230 pcon=4.7285 forget=3.4860 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 190 total=9.6549 mle=1.4548 pcon=4.7282 forget=3.4719 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 240 total=9.7328 mle=1.5154 pcon=4.7278 forget=3.4897 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 290 total=9.9234 mle=1.7524 pcon=4.7273 forget=3.4436 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 340 total=10.0464 mle=1.8438 pcon=4.7270 forget=3.4756 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 0 total=9.7545 mle=1.5267 pcon=4.7265 forget=3.5013 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 50 total=9.7838 mle=1.5986 pcon=4.7261 forget=3.4590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 100 total=9.7373 mle=1.5352 pcon=4.7257 forget=3.4764 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 150 total=9.6875 mle=1.5143 pcon=4.7252 forget=3.4480 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 200 total=9.7864 mle=1.5936 pcon=4.7249 forget=3.4679 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 250 total=9.7123 mle=1.5052 pcon=4.7245 forget=3.4827 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 300 total=9.7813 mle=1.6300 pcon=4.7239 forget=3.4274 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 350 total=9.6247 mle=1.5006 pcon=4.7236 forget=3.4006 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 10 total=9.6449 mle=1.4750 pcon=4.7233 forget=3.4466 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 60 total=9.7277 mle=1.4875 pcon=4.7229 forget=3.5173 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 110 total=9.8596 mle=1.6477 pcon=4.7226 forget=3.4894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 160 total=9.6119 mle=1.4657 pcon=4.7222 forget=3.4241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 210 total=9.8723 mle=1.7286 pcon=4.7217 forget=3.4220 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 260 total=9.7834 mle=1.6562 pcon=4.7213 forget=3.4059 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 310 total=9.7175 mle=1.5185 pcon=4.7211 forget=3.4779 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 360 total=9.8937 mle=1.6994 pcon=4.7208 forget=3.4735 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 20 total=9.6890 mle=1.5202 pcon=4.7203 forget=3.4485 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 70 total=9.6643 mle=1.5448 pcon=4.7199 forget=3.3995 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 120 total=9.8672 mle=1.6664 pcon=4.7195 forget=3.4813 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 170 total=9.7482 mle=1.5175 pcon=4.7190 forget=3.5118 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 220 total=9.5632 mle=1.4344 pcon=4.7187 forget=3.4101 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 270 total=9.8407 mle=1.5920 pcon=4.7185 forget=3.5303 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 320 total=9.8046 mle=1.5470 pcon=4.7181 forget=3.5395 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 370 total=9.8255 mle=1.6182 pcon=4.7178 forget=3.4894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 30 total=9.8771 mle=1.6192 pcon=4.7177 forget=3.5402 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 80 total=9.8080 mle=1.5779 pcon=4.7173 forget=3.5127 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 130 total=9.7710 mle=1.6294 pcon=4.7171 forget=3.4245 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 180 total=9.9199 mle=1.6263 pcon=4.7169 forget=3.5767 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 230 total=9.7837 mle=1.6086 pcon=4.7168 forget=3.4583 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 280 total=9.6690 mle=1.5650 pcon=4.7165 forget=3.3875 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 330 total=9.6705 mle=1.5526 pcon=4.7161 forget=3.4017 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 380 total=9.7701 mle=1.5432 pcon=4.7158 forget=3.5112 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 40 total=9.8072 mle=1.5562 pcon=4.7156 forget=3.5353 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 90 total=9.8062 mle=1.5893 pcon=4.7153 forget=3.5016 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 140 total=9.8044 mle=1.5996 pcon=4.7151 forget=3.4897 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 190 total=9.9269 mle=1.7242 pcon=4.7147 forget=3.4881 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 240 total=9.7936 mle=1.6535 pcon=4.7143 forget=3.4258 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 290 total=9.7712 mle=1.5932 pcon=4.7140 forget=3.4641 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 340 total=9.6669 mle=1.4833 pcon=4.7140 forget=3.4696 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:59,  3.27it/s]  3%|▎         | 10/391 [00:00<00:12, 30.43it/s]  5%|▍         | 19/391 [00:00<00:07, 48.38it/s]  7%|▋         | 28/391 [00:00<00:06, 60.16it/s]  9%|▉         | 37/391 [00:00<00:05, 68.42it/s] 12%|█▏        | 46/391 [00:00<00:04, 74.41it/s] 14%|█▍        | 55/391 [00:00<00:04, 77.62it/s] 16%|█▋        | 64/391 [00:01<00:04, 79.70it/s] 19%|█▊        | 73/391 [00:01<00:03, 82.35it/s] 21%|██        | 82/391 [00:01<00:03, 83.33it/s] 23%|██▎       | 91/391 [00:01<00:03, 84.98it/s] 26%|██▌       | 100/391 [00:01<00:03, 85.76it/s] 28%|██▊       | 109/391 [00:01<00:03, 86.14it/s] 30%|███       | 118/391 [00:01<00:03, 86.75it/s] 32%|███▏      | 127/391 [00:01<00:03, 87.26it/s] 35%|███▍      | 136/391 [00:01<00:02, 87.85it/s] 37%|███▋      | 145/391 [00:01<00:02, 87.48it/s] 39%|███▉      | 154/391 [00:02<00:02, 87.98it/s] 42%|████▏     | 163/391 [00:02<00:02, 88.24it/s] 44%|████▍     | 172/391 [00:02<00:02, 86.55it/s] 46%|████▋     | 181/391 [00:02<00:02, 87.33it/s] 49%|████▊     | 190/391 [00:02<00:02, 87.90it/s] 51%|█████     | 199/391 [00:02<00:02, 88.28it/s] 53%|█████▎    | 208/391 [00:02<00:02, 88.64it/s] 55%|█████▌    | 217/391 [00:02<00:01, 88.58it/s] 58%|█████▊    | 226/391 [00:02<00:01, 88.48it/s] 60%|██████    | 235/391 [00:02<00:01, 88.55it/s] 62%|██████▏   | 244/391 [00:03<00:01, 88.28it/s] 65%|██████▍   | 253/391 [00:03<00:01, 88.40it/s] 67%|██████▋   | 262/391 [00:03<00:01, 87.16it/s] 69%|██████▉   | 271/391 [00:03<00:01, 87.04it/s] 72%|███████▏  | 280/391 [00:03<00:01, 86.65it/s] 74%|███████▍  | 289/391 [00:03<00:01, 86.94it/s] 76%|███████▋  | 299/391 [00:03<00:01, 87.79it/s] 79%|███████▉  | 308/391 [00:03<00:00, 87.66it/s] 81%|████████  | 317/391 [00:03<00:00, 87.46it/s] 83%|████████▎ | 326/391 [00:04<00:00, 83.43it/s] 86%|████████▌ | 335/391 [00:04<00:00, 84.88it/s] 88%|████████▊ | 344/391 [00:04<00:00, 85.91it/s] 90%|█████████ | 353/391 [00:04<00:00, 85.78it/s] 93%|█████████▎| 362/391 [00:04<00:00, 86.67it/s] 95%|█████████▍| 371/391 [00:04<00:00, 87.02it/s] 97%|█████████▋| 380/391 [00:04<00:00, 87.03it/s]100%|█████████▉| 390/391 [00:04<00:00, 88.04it/s]100%|██████████| 391/391 [00:04<00:00, 81.67it/s]
50000 images processed, 4.879593133926392 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:21,  3.60it/s] 13%|█▎        | 10/79 [00:00<00:02, 32.03it/s] 24%|██▍       | 19/79 [00:00<00:01, 49.83it/s] 35%|███▌      | 28/79 [00:00<00:00, 61.51it/s] 47%|████▋     | 37/79 [00:00<00:00, 69.32it/s] 58%|█████▊    | 46/79 [00:00<00:00, 74.37it/s] 70%|██████▉   | 55/79 [00:00<00:00, 76.32it/s] 81%|████████  | 64/79 [00:01<00:00, 79.51it/s] 92%|█████████▏| 73/79 [00:01<00:00, 82.35it/s]100%|██████████| 79/79 [00:01<00:00, 65.41it/s]
10000 images processed, 1.2319142818450928 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<00:56,  3.58it/s]  5%|▍         | 10/204 [00:00<00:06, 31.87it/s]  9%|▉         | 19/204 [00:00<00:03, 49.76it/s] 14%|█▎        | 28/204 [00:00<00:02, 60.90it/s] 18%|█▊        | 37/204 [00:00<00:02, 68.38it/s] 23%|██▎       | 46/204 [00:00<00:02, 73.94it/s] 27%|██▋       | 55/204 [00:00<00:01, 76.75it/s] 31%|███▏      | 64/204 [00:01<00:01, 79.77it/s] 36%|███▌      | 73/204 [00:01<00:01, 81.90it/s] 40%|████      | 82/204 [00:01<00:01, 83.54it/s] 45%|████▍     | 91/204 [00:01<00:01, 84.75it/s] 49%|████▉     | 100/204 [00:01<00:01, 85.58it/s] 53%|█████▎    | 109/204 [00:01<00:01, 85.56it/s] 58%|█████▊    | 118/204 [00:01<00:01, 85.96it/s] 62%|██████▏   | 127/204 [00:01<00:00, 86.36it/s] 67%|██████▋   | 136/204 [00:01<00:00, 86.47it/s] 71%|███████   | 145/204 [00:01<00:00, 86.50it/s] 75%|███████▌  | 154/204 [00:02<00:00, 86.30it/s] 80%|███████▉  | 163/204 [00:02<00:00, 86.09it/s] 84%|████████▍ | 172/204 [00:02<00:00, 85.87it/s] 89%|████████▊ | 181/204 [00:02<00:00, 85.87it/s] 93%|█████████▎| 190/204 [00:02<00:00, 86.01it/s] 98%|█████████▊| 199/204 [00:02<00:00, 86.41it/s]100%|██████████| 204/204 [00:02<00:00, 77.00it/s]
26032 images processed, 2.699471950531006 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.93it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.37it/s] 22%|██▏       | 17/79 [00:00<00:02, 24.49it/s] 32%|███▏      | 25/79 [00:01<00:01, 30.63it/s] 42%|████▏     | 33/79 [00:01<00:01, 34.43it/s] 52%|█████▏    | 41/79 [00:01<00:00, 38.10it/s] 62%|██████▏   | 49/79 [00:01<00:00, 40.97it/s] 72%|███████▏  | 57/79 [00:01<00:00, 42.63it/s] 82%|████████▏ | 65/79 [00:01<00:00, 43.02it/s] 92%|█████████▏| 73/79 [00:02<00:00, 43.71it/s]100%|██████████| 79/79 [00:02<00:00, 36.01it/s]
10000 images processed, 2.223877191543579 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:24,  3.13it/s] 13%|█▎        | 10/79 [00:00<00:02, 29.04it/s] 24%|██▍       | 19/79 [00:00<00:01, 46.79it/s] 35%|███▌      | 28/79 [00:00<00:00, 59.15it/s] 47%|████▋     | 37/79 [00:00<00:00, 67.82it/s] 58%|█████▊    | 46/79 [00:00<00:00, 73.87it/s] 70%|██████▉   | 55/79 [00:00<00:00, 78.02it/s] 81%|████████  | 64/79 [00:01<00:00, 80.94it/s] 92%|█████████▏| 73/79 [00:01<00:00, 83.33it/s]100%|██████████| 79/79 [00:01<00:00, 65.41it/s]
10000 images processed, 1.230149745941162 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:21,  3.17it/s] 13%|█▎        | 9/70 [00:00<00:02, 26.55it/s] 26%|██▌       | 18/70 [00:00<00:01, 45.13it/s] 39%|███▊      | 27/70 [00:00<00:00, 57.77it/s] 51%|█████▏    | 36/70 [00:00<00:00, 66.62it/s] 64%|██████▍   | 45/70 [00:00<00:00, 72.81it/s] 77%|███████▋  | 54/70 [00:00<00:00, 76.40it/s] 90%|█████████ | 63/70 [00:01<00:00, 79.31it/s]100%|██████████| 70/70 [00:01<00:00, 61.84it/s]
8925 images processed, 1.1603736877441406 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:34,  1.29it/s]  4%|▍         | 2/45 [00:00<00:18,  2.29it/s] 20%|██        | 9/45 [00:01<00:03, 11.65it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.74it/s] 38%|███▊      | 17/45 [00:01<00:01, 15.37it/s] 44%|████▍     | 20/45 [00:02<00:02, 12.14it/s] 51%|█████     | 23/45 [00:02<00:01, 13.35it/s] 58%|█████▊    | 26/45 [00:02<00:01, 10.06it/s] 67%|██████▋   | 30/45 [00:02<00:01, 11.33it/s] 76%|███████▌  | 34/45 [00:03<00:01,  9.92it/s] 93%|█████████▎| 42/45 [00:04<00:00, 10.53it/s]100%|██████████| 45/45 [00:04<00:00, 10.63it/s]
5640 images processed, 4.252629041671753 seconds used

19.287224531173706
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.64  98.99
places365     77.26  78.42
LSUN          30.02  94.45
iSUN          78.88  79.42
dtd           45.66  89.68
forget        51.10  89.83
AVG           47.93  88.46
Retain-Acc: 0.7398
Forget-as-OOD (retain known vs forget novel):
  FPR: 51.10 AUROC: 89.83 AUIN: 98.70
34.66571617126465
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.5 lr=0.0001 epochs=5
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.5, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:29<01:56, 29.04s/it] 40%|████      | 2/5 [00:48<01:10, 23.45s/it] 60%|██████    | 3/5 [01:09<00:44, 22.39s/it] 80%|████████  | 4/5 [01:36<00:24, 24.06s/it]100%|██████████| 5/5 [01:55<00:00, 22.16s/it]100%|██████████| 5/5 [01:55<00:00, 23.02s/it]
[loss] ep 0 it 0 total=10.3047 mle=1.5709 pcon=5.2950 forget=3.4387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=10.3336 mle=1.5424 pcon=5.2879 forget=3.5033 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=10.4163 mle=1.7004 pcon=5.2809 forget=3.4350 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=10.5977 mle=1.8998 pcon=5.2738 forget=3.4241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=10.4368 mle=1.7132 pcon=5.2670 forget=3.4566 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=10.2084 mle=1.5020 pcon=5.2603 forget=3.4461 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=10.2687 mle=1.5598 pcon=5.2541 forget=3.4548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=10.4027 mle=1.6808 pcon=5.2476 forget=3.4742 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=10.4376 mle=1.6715 pcon=5.2409 forget=3.5252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=10.2893 mle=1.6372 pcon=5.2346 forget=3.4175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=10.2189 mle=1.5167 pcon=5.2285 forget=3.4737 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=10.4763 mle=1.7803 pcon=5.2225 forget=3.4736 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=10.4446 mle=1.7646 pcon=5.2167 forget=3.4633 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=10.2372 mle=1.6052 pcon=5.2112 forget=3.4208 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=10.3753 mle=1.7241 pcon=5.2056 forget=3.4456 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=10.4600 mle=1.7943 pcon=5.2003 forget=3.4654 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=10.1901 mle=1.5529 pcon=5.1951 forget=3.4421 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=10.4726 mle=1.8884 pcon=5.1899 forget=3.3943 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=10.3660 mle=1.7416 pcon=5.1847 forget=3.4397 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=10.1186 mle=1.5264 pcon=5.1797 forget=3.4125 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=10.2253 mle=1.5858 pcon=5.1746 forget=3.4649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=10.4808 mle=1.8840 pcon=5.1699 forget=3.4269 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=10.1523 mle=1.5928 pcon=5.1652 forget=3.3943 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=10.3841 mle=1.8113 pcon=5.1605 forget=3.4122 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=10.2892 mle=1.7061 pcon=5.1560 forget=3.4271 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=10.2711 mle=1.6708 pcon=5.1519 forget=3.4484 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=10.3292 mle=1.7668 pcon=5.1476 forget=3.4148 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=10.4679 mle=1.9184 pcon=5.1436 forget=3.4059 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=10.1096 mle=1.5700 pcon=5.1396 forget=3.4000 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=10.3624 mle=1.8363 pcon=5.1354 forget=3.3907 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=10.2729 mle=1.7029 pcon=5.1312 forget=3.4388 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=10.2696 mle=1.6959 pcon=5.1276 forget=3.4461 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=10.1336 mle=1.6543 pcon=5.1238 forget=3.3555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=10.2405 mle=1.7298 pcon=5.1198 forget=3.3908 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=10.1819 mle=1.6501 pcon=5.1161 forget=3.4157 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=9.9811 mle=1.4538 pcon=5.1127 forget=3.4146 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=10.2676 mle=1.7671 pcon=5.1088 forget=3.3917 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=9.9803 mle=1.4911 pcon=5.1053 forget=3.3839 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=10.3873 mle=1.8721 pcon=5.1019 forget=3.4133 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:59,  3.26it/s]  3%|▎         | 10/391 [00:00<00:12, 30.34it/s]  5%|▍         | 19/391 [00:00<00:07, 48.31it/s]  7%|▋         | 28/391 [00:00<00:05, 60.62it/s]  9%|▉         | 37/391 [00:00<00:05, 69.21it/s] 12%|█▏        | 46/391 [00:00<00:04, 73.80it/s] 14%|█▍        | 55/391 [00:00<00:04, 77.54it/s] 16%|█▋        | 64/391 [00:01<00:04, 80.78it/s] 19%|█▊        | 73/391 [00:01<00:03, 82.64it/s] 21%|██        | 82/391 [00:01<00:03, 84.47it/s] 23%|██▎       | 91/391 [00:01<00:03, 85.86it/s] 26%|██▌       | 100/391 [00:01<00:03, 86.23it/s] 28%|██▊       | 109/391 [00:01<00:03, 87.06it/s] 30%|███       | 118/391 [00:01<00:03, 87.70it/s] 32%|███▏      | 127/391 [00:01<00:02, 88.04it/s] 35%|███▍      | 136/391 [00:01<00:02, 88.26it/s] 37%|███▋      | 146/391 [00:01<00:02, 88.94it/s] 40%|███▉      | 155/391 [00:02<00:02, 88.57it/s] 42%|████▏     | 164/391 [00:02<00:02, 88.52it/s] 44%|████▍     | 173/391 [00:02<00:02, 88.27it/s] 47%|████▋     | 182/391 [00:02<00:02, 88.05it/s] 49%|████▉     | 191/391 [00:02<00:02, 86.95it/s] 51%|█████     | 200/391 [00:02<00:02, 85.26it/s] 53%|█████▎    | 209/391 [00:02<00:02, 85.95it/s] 56%|█████▌    | 218/391 [00:02<00:02, 86.39it/s] 58%|█████▊    | 227/391 [00:02<00:01, 85.13it/s] 60%|██████    | 236/391 [00:03<00:01, 84.70it/s] 63%|██████▎   | 245/391 [00:03<00:01, 85.65it/s] 65%|██████▍   | 254/391 [00:03<00:01, 85.66it/s] 67%|██████▋   | 263/391 [00:03<00:01, 86.18it/s] 70%|██████▉   | 272/391 [00:03<00:01, 85.52it/s] 72%|███████▏  | 281/391 [00:03<00:01, 86.29it/s] 74%|███████▍  | 290/391 [00:03<00:01, 87.10it/s] 76%|███████▋  | 299/391 [00:03<00:01, 87.32it/s] 79%|███████▉  | 308/391 [00:03<00:00, 87.13it/s] 81%|████████  | 317/391 [00:03<00:00, 86.59it/s] 83%|████████▎ | 326/391 [00:04<00:00, 87.26it/s] 86%|████████▌ | 335/391 [00:04<00:00, 86.01it/s] 88%|████████▊ | 344/391 [00:04<00:00, 86.43it/s] 90%|█████████ | 353/391 [00:04<00:00, 85.26it/s] 93%|█████████▎| 362/391 [00:04<00:00, 86.32it/s] 95%|█████████▍| 371/391 [00:04<00:00, 86.10it/s] 97%|█████████▋| 380/391 [00:04<00:00, 86.42it/s] 99%|█████████▉| 389/391 [00:04<00:00, 87.40it/s]100%|██████████| 391/391 [00:04<00:00, 81.40it/s]
50000 images processed, 5.145625591278076 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:20,  3.74it/s] 13%|█▎        | 10/79 [00:00<00:02, 33.29it/s] 24%|██▍       | 19/79 [00:00<00:01, 51.38it/s] 35%|███▌      | 28/79 [00:00<00:00, 62.83it/s] 47%|████▋     | 37/79 [00:00<00:00, 70.58it/s] 58%|█████▊    | 46/79 [00:00<00:00, 75.80it/s] 70%|██████▉   | 55/79 [00:00<00:00, 77.95it/s] 81%|████████  | 64/79 [00:00<00:00, 80.68it/s] 92%|█████████▏| 73/79 [00:01<00:00, 83.31it/s]100%|██████████| 79/79 [00:01<00:00, 67.35it/s]
10000 images processed, 1.210186243057251 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<00:53,  3.82it/s]  5%|▍         | 10/204 [00:00<00:05, 33.87it/s]  9%|▉         | 19/204 [00:00<00:03, 51.90it/s] 14%|█▎        | 28/204 [00:00<00:02, 62.34it/s] 18%|█▊        | 37/204 [00:00<00:02, 69.78it/s] 23%|██▎       | 46/204 [00:00<00:02, 74.87it/s] 27%|██▋       | 55/204 [00:00<00:01, 78.82it/s] 31%|███▏      | 64/204 [00:00<00:01, 81.46it/s] 36%|███▌      | 73/204 [00:01<00:01, 83.18it/s] 40%|████      | 82/204 [00:01<00:01, 84.56it/s] 45%|████▍     | 91/204 [00:01<00:01, 85.28it/s] 49%|████▉     | 100/204 [00:01<00:01, 85.62it/s] 53%|█████▎    | 109/204 [00:01<00:01, 86.14it/s] 58%|█████▊    | 118/204 [00:01<00:00, 86.54it/s] 63%|██████▎   | 128/204 [00:01<00:00, 87.73it/s] 67%|██████▋   | 137/204 [00:01<00:00, 87.50it/s] 72%|███████▏  | 146/204 [00:01<00:00, 87.04it/s] 76%|███████▌  | 155/204 [00:02<00:00, 87.02it/s] 80%|████████  | 164/204 [00:02<00:00, 86.97it/s] 85%|████████▍ | 173/204 [00:02<00:00, 86.91it/s] 89%|████████▉ | 182/204 [00:02<00:00, 86.89it/s] 94%|█████████▎| 191/204 [00:02<00:00, 87.05it/s] 98%|█████████▊| 200/204 [00:02<00:00, 87.58it/s]100%|██████████| 204/204 [00:02<00:00, 78.56it/s]
26032 images processed, 2.7932019233703613 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:33,  2.30it/s] 11%|█▏        | 9/79 [00:00<00:03, 18.52it/s] 22%|██▏       | 17/79 [00:00<00:02, 28.90it/s] 32%|███▏      | 25/79 [00:00<00:01, 35.23it/s] 42%|████▏     | 33/79 [00:01<00:01, 38.91it/s] 52%|█████▏    | 41/79 [00:01<00:00, 41.58it/s] 62%|██████▏   | 49/79 [00:01<00:00, 43.37it/s] 72%|███████▏  | 57/79 [00:01<00:00, 41.91it/s] 82%|████████▏ | 65/79 [00:01<00:00, 45.26it/s] 92%|█████████▏| 73/79 [00:01<00:00, 48.03it/s]100%|██████████| 79/79 [00:01<00:00, 39.74it/s]
10000 images processed, 2.0216963291168213 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:26,  3.00it/s] 13%|█▎        | 10/79 [00:00<00:02, 28.73it/s] 24%|██▍       | 19/79 [00:00<00:01, 46.38it/s] 35%|███▌      | 28/79 [00:00<00:00, 58.59it/s] 47%|████▋     | 37/79 [00:00<00:00, 66.83it/s] 58%|█████▊    | 46/79 [00:00<00:00, 72.09it/s] 70%|██████▉   | 55/79 [00:00<00:00, 76.56it/s] 81%|████████  | 64/79 [00:01<00:00, 78.79it/s] 92%|█████████▏| 73/79 [00:01<00:00, 81.44it/s]100%|██████████| 79/79 [00:01<00:00, 64.02it/s]
10000 images processed, 1.2568693161010742 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:26,  2.57it/s] 14%|█▍        | 10/70 [00:00<00:02, 25.80it/s] 27%|██▋       | 19/70 [00:00<00:01, 43.06it/s] 40%|████      | 28/70 [00:00<00:00, 55.60it/s] 53%|█████▎    | 37/70 [00:00<00:00, 64.71it/s] 66%|██████▌   | 46/70 [00:00<00:00, 71.22it/s] 79%|███████▊  | 55/70 [00:01<00:00, 75.69it/s] 91%|█████████▏| 64/70 [00:01<00:00, 79.55it/s]100%|██████████| 70/70 [00:01<00:00, 59.00it/s]
8925 images processed, 1.2158770561218262 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:36,  1.22it/s]  4%|▍         | 2/45 [00:00<00:18,  2.32it/s] 20%|██        | 9/45 [00:01<00:03, 10.46it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.94it/s] 33%|███▎      | 15/45 [00:01<00:02, 13.31it/s] 40%|████      | 18/45 [00:02<00:02, 10.22it/s] 49%|████▉     | 22/45 [00:02<00:01, 13.13it/s] 56%|█████▌    | 25/45 [00:02<00:01, 11.56it/s] 67%|██████▋   | 30/45 [00:02<00:01, 12.89it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.01it/s] 78%|███████▊  | 35/45 [00:03<00:00, 12.06it/s] 91%|█████████ | 41/45 [00:03<00:00, 13.95it/s] 96%|█████████▌| 43/45 [00:04<00:00, 10.32it/s]100%|██████████| 45/45 [00:04<00:00, 10.62it/s]
5640 images processed, 4.2637646198272705 seconds used

19.465543508529663
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.53  99.36
places365     67.87  81.18
LSUN          17.61  96.07
iSUN          72.34  81.68
dtd           37.87  91.39
forget        79.50  86.90
AVG           46.29  89.43
Retain-Acc: 0.7406
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
42.13377857208252
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.5 lr=0.0001 epochs=10
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.5, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:28<04:18, 28.75s/it] 20%|██        | 2/10 [00:54<03:35, 27.00s/it] 30%|███       | 3/10 [01:19<03:03, 26.22s/it] 40%|████      | 4/10 [01:44<02:33, 25.60s/it] 50%|█████     | 5/10 [02:09<02:06, 25.37s/it] 60%|██████    | 6/10 [02:34<01:41, 25.43s/it] 70%|███████   | 7/10 [02:59<01:15, 25.21s/it] 80%|████████  | 8/10 [03:25<00:50, 25.41s/it][loss] ep 0 it 0 total=10.3047 mle=1.5709 pcon=5.2950 forget=3.4387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=10.3336 mle=1.5424 pcon=5.2879 forget=3.5033 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=10.4163 mle=1.7004 pcon=5.2809 forget=3.4350 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=10.5977 mle=1.8998 pcon=5.2738 forget=3.4241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=10.4368 mle=1.7132 pcon=5.2670 forget=3.4566 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=10.2084 mle=1.5020 pcon=5.2603 forget=3.4461 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=10.2686 mle=1.5597 pcon=5.2541 forget=3.4548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=10.4026 mle=1.6808 pcon=5.2476 forget=3.4742 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=10.4376 mle=1.6716 pcon=5.2409 forget=3.5251 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=10.2892 mle=1.6371 pcon=5.2346 forget=3.4175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=10.2188 mle=1.5166 pcon=5.2285 forget=3.4737 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=10.4761 mle=1.7802 pcon=5.2224 forget=3.4735 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=10.4447 mle=1.7647 pcon=5.2167 forget=3.4633 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=10.2372 mle=1.6052 pcon=5.2112 forget=3.4207 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=10.3754 mle=1.7242 pcon=5.2056 forget=3.4456 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=10.4600 mle=1.7943 pcon=5.2003 forget=3.4654 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=10.1900 mle=1.5528 pcon=5.1951 forget=3.4421 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=10.4726 mle=1.8884 pcon=5.1899 forget=3.3942 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=10.3662 mle=1.7418 pcon=5.1847 forget=3.4397 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=10.1186 mle=1.5264 pcon=5.1797 forget=3.4125 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=10.2252 mle=1.5857 pcon=5.1746 forget=3.4649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=10.4806 mle=1.8838 pcon=5.1699 forget=3.4268 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=10.1520 mle=1.5925 pcon=5.1652 forget=3.3943 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=10.3840 mle=1.8112 pcon=5.1606 forget=3.4122 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=10.2894 mle=1.7062 pcon=5.1560 forget=3.4272 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=10.2712 mle=1.6709 pcon=5.1519 forget=3.4483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=10.3290 mle=1.7666 pcon=5.1476 forget=3.4149 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=10.4678 mle=1.9183 pcon=5.1436 forget=3.4059 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=10.1094 mle=1.5699 pcon=5.1396 forget=3.3999 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=10.3624 mle=1.8362 pcon=5.1354 forget=3.3907 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=10.2730 mle=1.7031 pcon=5.1312 forget=3.4387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=10.2696 mle=1.6958 pcon=5.1276 forget=3.4461 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=10.1336 mle=1.6543 pcon=5.1238 forget=3.3555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=10.2407 mle=1.7300 pcon=5.1198 forget=3.3908 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=10.1819 mle=1.6501 pcon=5.1161 forget=3.4157 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=9.9809 mle=1.4537 pcon=5.1127 forget=3.4146 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=10.2675 mle=1.7671 pcon=5.1088 forget=3.3916 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=9.9804 mle=1.4912 pcon=5.1053 forget=3.3839 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=10.3875 mle=1.8723 pcon=5.1019 forget=3.4133 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=10.3593 mle=1.8671 pcon=5.0985 forget=3.3936 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=10.2604 mle=1.8000 pcon=5.0949 forget=3.3655 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=10.0768 mle=1.5843 pcon=5.0918 forget=3.4007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=10.0813 mle=1.5956 pcon=5.0883 forget=3.3973 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=10.1414 mle=1.6910 pcon=5.0854 forget=3.3649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=10.2033 mle=1.7733 pcon=5.0825 forget=3.3475 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=10.0541 mle=1.5965 pcon=5.0794 forget=3.3782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=10.1965 mle=1.7470 pcon=5.0766 forget=3.3729 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=9.9056 mle=1.4012 pcon=5.0737 forget=3.4308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=9.9291 mle=1.5360 pcon=5.0710 forget=3.3221 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=10.0647 mle=1.5916 pcon=5.0683 forget=3.4048 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=10.0969 mle=1.6655 pcon=5.0656 forget=3.3658 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=10.0488 mle=1.6303 pcon=5.0634 forget=3.3551 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=10.2749 mle=1.8236 pcon=5.0605 forget=3.3908 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=10.0033 mle=1.5347 pcon=5.0581 forget=3.4105 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=10.1005 mle=1.6750 pcon=5.0557 forget=3.3699 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=10.0340 mle=1.6311 pcon=5.0530 forget=3.3499 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=10.1347 mle=1.7453 pcon=5.0507 forget=3.3388 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=10.0318 mle=1.6272 pcon=5.0482 forget=3.3564 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=9.9513 mle=1.5823 pcon=5.0459 forget=3.3231 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=10.0803 mle=1.6548 pcon=5.0439 forget=3.3816 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=10.2429 mle=1.7957 pcon=5.0418 forget=3.4054 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=10.1819 mle=1.7420 pcon=5.0393 forget=3.4006 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=10.0450 mle=1.6407 pcon=5.0373 forget=3.3670 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 90%|█████████ | 9/10 [03:51<00:25, 25.55s/it]100%|██████████| 10/10 [04:17<00:00, 25.64s/it]100%|██████████| 10/10 [04:17<00:00, 25.73s/it]
[loss] ep 8 it 30 total=9.9748 mle=1.5369 pcon=5.0355 forget=3.4025 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=9.9970 mle=1.5907 pcon=5.0335 forget=3.3728 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=9.9276 mle=1.5004 pcon=5.0314 forget=3.3957 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=10.0884 mle=1.7033 pcon=5.0295 forget=3.3556 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=9.9101 mle=1.5306 pcon=5.0280 forget=3.3514 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=10.0328 mle=1.6774 pcon=5.0261 forget=3.3293 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=10.2004 mle=1.7566 pcon=5.0240 forget=3.4198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=9.8462 mle=1.4893 pcon=5.0221 forget=3.3347 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=9.9458 mle=1.6367 pcon=5.0202 forget=3.2888 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=10.0293 mle=1.6693 pcon=5.0182 forget=3.3418 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=10.2392 mle=1.8421 pcon=5.0167 forget=3.3804 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=9.9389 mle=1.5783 pcon=5.0152 forget=3.3454 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=9.9341 mle=1.5674 pcon=5.0137 forget=3.3531 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=9.9407 mle=1.5594 pcon=5.0118 forget=3.3694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=9.9116 mle=1.5582 pcon=5.0104 forget=3.3431 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:58,  3.28it/s]  2%|▏         | 6/391 [00:00<00:21, 17.81it/s]  3%|▎         | 13/391 [00:00<00:11, 32.25it/s]  6%|▌         | 22/391 [00:00<00:07, 47.75it/s]  8%|▊         | 31/391 [00:00<00:06, 59.79it/s] 10%|█         | 40/391 [00:00<00:05, 68.09it/s] 13%|█▎        | 49/391 [00:00<00:04, 74.06it/s] 15%|█▍        | 58/391 [00:01<00:04, 78.41it/s] 17%|█▋        | 67/391 [00:01<00:04, 72.73it/s] 19%|█▉        | 75/391 [00:01<00:04, 65.35it/s] 21%|██        | 82/391 [00:01<00:04, 62.14it/s] 23%|██▎       | 91/391 [00:01<00:04, 68.84it/s] 26%|██▌       | 100/391 [00:01<00:03, 74.13it/s] 28%|██▊       | 109/391 [00:01<00:03, 78.26it/s] 30%|███       | 118/391 [00:01<00:03, 81.09it/s] 32%|███▏      | 127/391 [00:01<00:03, 83.31it/s] 35%|███▍      | 136/391 [00:02<00:03, 68.85it/s] 37%|███▋      | 144/391 [00:02<00:03, 64.54it/s] 39%|███▉      | 152/391 [00:02<00:03, 67.39it/s] 41%|████      | 161/391 [00:02<00:03, 71.38it/s] 43%|████▎     | 170/391 [00:02<00:02, 74.42it/s] 46%|████▌     | 179/391 [00:02<00:02, 78.35it/s] 48%|████▊     | 188/391 [00:02<00:02, 73.24it/s] 50%|█████     | 196/391 [00:03<00:02, 65.54it/s] 52%|█████▏    | 203/391 [00:03<00:03, 61.96it/s] 54%|█████▍    | 212/391 [00:03<00:02, 68.68it/s] 57%|█████▋    | 221/391 [00:03<00:02, 73.98it/s] 59%|█████▉    | 230/391 [00:03<00:02, 78.13it/s] 61%|██████    | 239/391 [00:03<00:01, 81.36it/s] 63%|██████▎   | 248/391 [00:03<00:01, 83.59it/s] 66%|██████▌   | 257/391 [00:03<00:01, 70.68it/s] 68%|██████▊   | 265/391 [00:03<00:01, 67.01it/s] 70%|██████▉   | 273/391 [00:04<00:01, 66.11it/s] 72%|███████▏  | 282/391 [00:04<00:01, 71.74it/s] 74%|███████▍  | 291/391 [00:04<00:01, 76.32it/s] 77%|███████▋  | 300/391 [00:04<00:01, 79.86it/s] 79%|███████▉  | 309/391 [00:04<00:00, 82.37it/s] 81%|████████▏ | 318/391 [00:04<00:00, 80.36it/s] 84%|████████▎ | 327/391 [00:04<00:00, 66.61it/s] 86%|████████▌ | 335/391 [00:04<00:00, 63.77it/s] 88%|████████▊ | 344/391 [00:05<00:00, 69.87it/s] 90%|█████████ | 353/391 [00:05<00:00, 74.69it/s] 93%|█████████▎| 362/391 [00:05<00:00, 78.62it/s] 95%|█████████▍| 371/391 [00:05<00:00, 81.28it/s] 97%|█████████▋| 380/391 [00:05<00:00, 79.70it/s] 99%|█████████▉| 389/391 [00:05<00:00, 66.01it/s]100%|██████████| 391/391 [00:05<00:00, 68.64it/s]
50000 images processed, 5.815694570541382 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:23,  3.33it/s]  9%|▉         | 7/79 [00:00<00:03, 21.20it/s] 16%|█▋        | 13/79 [00:00<00:02, 32.22it/s] 28%|██▊       | 22/79 [00:00<00:01, 48.64it/s] 39%|███▉      | 31/79 [00:00<00:00, 59.90it/s] 51%|█████     | 40/79 [00:00<00:00, 67.65it/s] 61%|██████    | 48/79 [00:00<00:00, 69.08it/s] 71%|███████   | 56/79 [00:01<00:00, 59.84it/s] 80%|███████▉  | 63/79 [00:01<00:00, 57.73it/s] 91%|█████████ | 72/79 [00:01<00:00, 65.39it/s]100%|██████████| 79/79 [00:01<00:00, 54.70it/s]
10000 images processed, 1.4853153228759766 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:03,  3.21it/s]  3%|▎         | 6/204 [00:00<00:11, 17.32it/s]  7%|▋         | 15/204 [00:00<00:04, 38.79it/s] 12%|█▏        | 24/204 [00:00<00:03, 53.40it/s] 16%|█▌        | 33/204 [00:00<00:02, 63.31it/s] 20%|██        | 41/204 [00:00<00:02, 67.70it/s] 24%|██▍       | 49/204 [00:01<00:02, 56.49it/s] 27%|██▋       | 56/204 [00:01<00:02, 55.81it/s] 32%|███▏      | 65/204 [00:01<00:02, 63.36it/s] 36%|███▋      | 74/204 [00:01<00:01, 68.78it/s] 41%|████      | 83/204 [00:01<00:01, 72.59it/s] 45%|████▍     | 91/204 [00:01<00:01, 64.95it/s] 48%|████▊     | 98/204 [00:01<00:01, 59.64it/s] 51%|█████▏    | 105/204 [00:01<00:01, 59.45it/s] 56%|█████▌    | 114/204 [00:02<00:01, 66.05it/s] 60%|██████    | 123/204 [00:02<00:01, 70.47it/s] 65%|██████▍   | 132/204 [00:02<00:00, 74.29it/s] 69%|██████▊   | 140/204 [00:02<00:01, 60.91it/s] 72%|███████▏  | 147/204 [00:02<00:00, 58.27it/s] 76%|███████▋  | 156/204 [00:02<00:00, 64.79it/s] 81%|████████  | 165/204 [00:02<00:00, 70.23it/s] 85%|████████▌ | 174/204 [00:02<00:00, 74.39it/s] 90%|████████▉ | 183/204 [00:02<00:00, 77.33it/s] 94%|█████████▎| 191/204 [00:03<00:00, 62.93it/s] 97%|█████████▋| 198/204 [00:03<00:00, 59.32it/s]100%|██████████| 204/204 [00:03<00:00, 60.54it/s]
26032 images processed, 3.7544007301330566 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:41,  1.88it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.86it/s] 19%|█▉        | 15/79 [00:00<00:02, 24.18it/s] 27%|██▋       | 21/79 [00:00<00:01, 31.94it/s] 37%|███▋      | 29/79 [00:01<00:01, 42.61it/s] 44%|████▍     | 35/79 [00:01<00:01, 37.46it/s] 52%|█████▏    | 41/79 [00:01<00:00, 41.34it/s] 62%|██████▏   | 49/79 [00:01<00:00, 46.01it/s] 72%|███████▏  | 57/79 [00:01<00:00, 43.93it/s] 80%|███████▉  | 63/79 [00:01<00:00, 46.55it/s] 87%|████████▋ | 69/79 [00:01<00:00, 49.44it/s] 95%|█████████▍| 75/79 [00:02<00:00, 42.11it/s]100%|██████████| 79/79 [00:02<00:00, 37.02it/s]
10000 images processed, 2.1706342697143555 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:26,  2.94it/s] 10%|█         | 8/79 [00:00<00:03, 22.10it/s] 22%|██▏       | 17/79 [00:00<00:01, 41.20it/s] 33%|███▎      | 26/79 [00:00<00:00, 54.79it/s] 44%|████▍     | 35/79 [00:00<00:00, 64.45it/s] 56%|█████▌    | 44/79 [00:00<00:00, 71.31it/s] 67%|██████▋   | 53/79 [00:01<00:00, 63.36it/s] 77%|███████▋  | 61/79 [00:01<00:00, 59.35it/s] 86%|████████▌ | 68/79 [00:01<00:00, 61.90it/s] 97%|█████████▋| 77/79 [00:01<00:00, 68.72it/s]100%|██████████| 79/79 [00:01<00:00, 56.01it/s]
10000 images processed, 1.6022014617919922 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:27,  2.52it/s] 14%|█▍        | 10/70 [00:00<00:02, 25.16it/s] 27%|██▋       | 19/70 [00:00<00:01, 42.08it/s] 40%|████      | 28/70 [00:00<00:00, 54.57it/s] 53%|█████▎    | 37/70 [00:00<00:00, 63.12it/s] 64%|██████▍   | 45/70 [00:00<00:00, 55.90it/s] 74%|███████▍  | 52/70 [00:01<00:00, 55.84it/s] 84%|████████▍ | 59/70 [00:01<00:00, 58.78it/s] 97%|█████████▋| 68/70 [00:01<00:00, 66.54it/s]100%|██████████| 70/70 [00:01<00:00, 51.46it/s]
8925 images processed, 1.3934764862060547 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:42,  1.03it/s] 13%|█▎        | 6/45 [00:01<00:05,  7.27it/s] 20%|██        | 9/45 [00:01<00:04,  7.79it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.55it/s] 31%|███       | 14/45 [00:01<00:02, 11.49it/s] 38%|███▊      | 17/45 [00:01<00:02, 11.18it/s] 49%|████▉     | 22/45 [00:02<00:01, 13.95it/s] 56%|█████▌    | 25/45 [00:02<00:01, 13.84it/s] 60%|██████    | 27/45 [00:02<00:01, 14.40it/s] 67%|██████▋   | 30/45 [00:02<00:01, 13.80it/s] 73%|███████▎  | 33/45 [00:03<00:00, 12.01it/s] 78%|███████▊  | 35/45 [00:03<00:00, 12.85it/s] 89%|████████▉ | 40/45 [00:03<00:00, 18.64it/s] 96%|█████████▌| 43/45 [00:03<00:00, 11.19it/s]100%|██████████| 45/45 [00:03<00:00, 11.30it/s]
5640 images processed, 4.004876613616943 seconds used

21.987538814544678
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.90  81.18
LSUN          17.59  96.07
iSUN          72.38  81.69
dtd           37.93  91.39
forget        79.50  86.90
AVG           46.30  89.43
Retain-Acc: 0.7406
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
32.91441869735718
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.5 lr=0.0001 epochs=20
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.5, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:29<09:25, 29.75s/it] 10%|█         | 2/20 [00:54<08:05, 26.98s/it] 15%|█▌        | 3/20 [01:19<07:24, 26.17s/it] 20%|██        | 4/20 [01:45<06:56, 26.05s/it] 25%|██▌       | 5/20 [02:11<06:25, 25.72s/it] 30%|███       | 6/20 [02:35<05:55, 25.42s/it] 35%|███▌      | 7/20 [03:01<05:32, 25.56s/it] 40%|████      | 8/20 [03:27<05:06, 25.51s/it][loss] ep 0 it 0 total=10.3043 mle=1.5706 pcon=5.2950 forget=3.4387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=10.3335 mle=1.5423 pcon=5.2879 forget=3.5032 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=10.4165 mle=1.7007 pcon=5.2809 forget=3.4349 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=10.5974 mle=1.8995 pcon=5.2738 forget=3.4241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=10.4365 mle=1.7130 pcon=5.2670 forget=3.4565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=10.2085 mle=1.5020 pcon=5.2603 forget=3.4461 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=10.2690 mle=1.5600 pcon=5.2541 forget=3.4549 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=10.4029 mle=1.6811 pcon=5.2476 forget=3.4742 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=10.4377 mle=1.6716 pcon=5.2409 forget=3.5251 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=10.2891 mle=1.6370 pcon=5.2346 forget=3.4175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=10.2188 mle=1.5167 pcon=5.2285 forget=3.4737 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=10.4764 mle=1.7804 pcon=5.2224 forget=3.4736 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=10.4444 mle=1.7644 pcon=5.2167 forget=3.4634 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=10.2375 mle=1.6054 pcon=5.2112 forget=3.4208 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=10.3750 mle=1.7237 pcon=5.2056 forget=3.4456 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=10.4602 mle=1.7945 pcon=5.2003 forget=3.4654 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=10.1902 mle=1.5530 pcon=5.1951 forget=3.4422 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=10.4726 mle=1.8885 pcon=5.1899 forget=3.3942 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=10.3660 mle=1.7416 pcon=5.1847 forget=3.4397 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=10.1186 mle=1.5264 pcon=5.1797 forget=3.4125 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=10.2253 mle=1.5858 pcon=5.1746 forget=3.4649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=10.4805 mle=1.8837 pcon=5.1699 forget=3.4268 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=10.1522 mle=1.5927 pcon=5.1652 forget=3.3943 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=10.3844 mle=1.8117 pcon=5.1606 forget=3.4122 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=10.2892 mle=1.7060 pcon=5.1560 forget=3.4272 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=10.2712 mle=1.6710 pcon=5.1519 forget=3.4483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=10.3291 mle=1.7668 pcon=5.1476 forget=3.4148 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=10.4679 mle=1.9183 pcon=5.1436 forget=3.4059 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=10.1096 mle=1.5700 pcon=5.1396 forget=3.4000 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=10.3624 mle=1.8363 pcon=5.1354 forget=3.3907 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=10.2733 mle=1.7033 pcon=5.1312 forget=3.4388 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=10.2697 mle=1.6959 pcon=5.1276 forget=3.4461 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=10.1336 mle=1.6543 pcon=5.1238 forget=3.3555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=10.2407 mle=1.7300 pcon=5.1198 forget=3.3909 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=10.1819 mle=1.6501 pcon=5.1161 forget=3.4157 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=9.9809 mle=1.4536 pcon=5.1127 forget=3.4146 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=10.2680 mle=1.7675 pcon=5.1088 forget=3.3917 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=9.9806 mle=1.4913 pcon=5.1053 forget=3.3839 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=10.3873 mle=1.8721 pcon=5.1019 forget=3.4133 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=10.3594 mle=1.8672 pcon=5.0986 forget=3.3936 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=10.2601 mle=1.7996 pcon=5.0950 forget=3.3655 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=10.0767 mle=1.5842 pcon=5.0918 forget=3.4007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=10.0814 mle=1.5958 pcon=5.0883 forget=3.3973 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=10.1415 mle=1.6911 pcon=5.0855 forget=3.3649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=10.2034 mle=1.7734 pcon=5.0825 forget=3.3475 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=10.0543 mle=1.5966 pcon=5.0794 forget=3.3783 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=10.1962 mle=1.7467 pcon=5.0766 forget=3.3729 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=9.9056 mle=1.4010 pcon=5.0737 forget=3.4308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=9.9292 mle=1.5361 pcon=5.0710 forget=3.3222 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=10.0646 mle=1.5915 pcon=5.0683 forget=3.4048 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=10.0973 mle=1.6657 pcon=5.0656 forget=3.3659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=10.0486 mle=1.6302 pcon=5.0634 forget=3.3551 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=10.2749 mle=1.8235 pcon=5.0605 forget=3.3909 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=10.0035 mle=1.5348 pcon=5.0581 forget=3.4105 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=10.1003 mle=1.6748 pcon=5.0557 forget=3.3699 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=10.0337 mle=1.6308 pcon=5.0530 forget=3.3499 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=10.1349 mle=1.7454 pcon=5.0507 forget=3.3387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=10.0317 mle=1.6271 pcon=5.0482 forget=3.3564 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=9.9514 mle=1.5824 pcon=5.0459 forget=3.3231 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=10.0800 mle=1.6544 pcon=5.0439 forget=3.3816 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=10.2429 mle=1.7957 pcon=5.0418 forget=3.4054 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=10.1818 mle=1.7418 pcon=5.0394 forget=3.4006 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=10.0450 mle=1.6406 pcon=5.0373 forget=3.3670 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 45%|████▌     | 9/20 [03:51<04:38, 25.29s/it] 50%|█████     | 10/20 [04:16<04:11, 25.16s/it] 55%|█████▌    | 11/20 [04:42<03:47, 25.30s/it] 60%|██████    | 12/20 [05:08<03:23, 25.50s/it] 65%|██████▌   | 13/20 [05:33<02:58, 25.51s/it] 70%|███████   | 14/20 [05:59<02:32, 25.45s/it] 75%|███████▌  | 15/20 [06:24<02:07, 25.43s/it] 80%|████████  | 16/20 [06:49<01:41, 25.37s/it][loss] ep 8 it 30 total=9.9749 mle=1.5369 pcon=5.0355 forget=3.4025 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=9.9968 mle=1.5905 pcon=5.0336 forget=3.3728 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=9.9275 mle=1.5003 pcon=5.0314 forget=3.3958 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=10.0886 mle=1.7034 pcon=5.0295 forget=3.3557 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=9.9101 mle=1.5307 pcon=5.0280 forget=3.3514 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=10.0328 mle=1.6774 pcon=5.0261 forget=3.3293 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=10.2005 mle=1.7567 pcon=5.0240 forget=3.4198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=9.8463 mle=1.4894 pcon=5.0222 forget=3.3347 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=9.9456 mle=1.6365 pcon=5.0203 forget=3.2888 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=10.0295 mle=1.6695 pcon=5.0182 forget=3.3418 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=10.2395 mle=1.8423 pcon=5.0167 forget=3.3804 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=9.9389 mle=1.5783 pcon=5.0152 forget=3.3454 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=9.9343 mle=1.5675 pcon=5.0137 forget=3.3531 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=9.9399 mle=1.5587 pcon=5.0118 forget=3.3694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=9.9116 mle=1.5581 pcon=5.0104 forget=3.3431 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=9.9846 mle=1.6070 pcon=5.0086 forget=3.3689 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=9.9903 mle=1.5900 pcon=5.0070 forget=3.3934 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=10.1058 mle=1.7543 pcon=5.0055 forget=3.3460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=10.1824 mle=1.8513 pcon=5.0041 forget=3.3269 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=10.1924 mle=1.8599 pcon=5.0027 forget=3.3298 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=9.9321 mle=1.5826 pcon=5.0014 forget=3.3481 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=9.8756 mle=1.5625 pcon=5.0002 forget=3.3129 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=10.0655 mle=1.6677 pcon=4.9989 forget=3.3989 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=9.9510 mle=1.5763 pcon=4.9975 forget=3.3772 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=9.9490 mle=1.6218 pcon=4.9961 forget=3.3311 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=9.8785 mle=1.5693 pcon=4.9949 forget=3.3143 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=10.0037 mle=1.6657 pcon=4.9933 forget=3.3447 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=10.1391 mle=1.7817 pcon=4.9922 forget=3.3652 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=9.9541 mle=1.6037 pcon=4.9914 forget=3.3591 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=10.3116 mle=1.9672 pcon=4.9900 forget=3.3544 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=9.9574 mle=1.6706 pcon=4.9888 forget=3.2980 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 12 it 20 total=9.9491 mle=1.6202 pcon=4.9874 forget=3.3415 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=10.0350 mle=1.6950 pcon=4.9863 forget=3.3537 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=9.9134 mle=1.5483 pcon=4.9851 forget=3.3799 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=10.0471 mle=1.7468 pcon=4.9840 forget=3.3163 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=9.9080 mle=1.5996 pcon=4.9828 forget=3.3256 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=9.8893 mle=1.5350 pcon=4.9821 forget=3.3722 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=9.8500 mle=1.5203 pcon=4.9810 forget=3.3487 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=10.2210 mle=1.8677 pcon=4.9800 forget=3.3734 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=9.7585 mle=1.4304 pcon=4.9791 forget=3.3491 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=9.9047 mle=1.4955 pcon=4.9780 forget=3.4312 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=9.8960 mle=1.5713 pcon=4.9768 forget=3.3479 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=9.8616 mle=1.5529 pcon=4.9760 forget=3.3326 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=9.9799 mle=1.6445 pcon=4.9751 forget=3.3602 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=9.8763 mle=1.5837 pcon=4.9742 forget=3.3184 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=9.9155 mle=1.5894 pcon=4.9732 forget=3.3529 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=9.8818 mle=1.5783 pcon=4.9723 forget=3.3312 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=10.0463 mle=1.7689 pcon=4.9712 forget=3.3063 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=10.0908 mle=1.7691 pcon=4.9706 forget=3.3512 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=9.9185 mle=1.6155 pcon=4.9702 forget=3.3329 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=10.1962 mle=1.8836 pcon=4.9691 forget=3.3435 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=9.9169 mle=1.6166 pcon=4.9685 forget=3.3318 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=9.9436 mle=1.6702 pcon=4.9677 forget=3.3057 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=9.9574 mle=1.6808 pcon=4.9669 forget=3.3097 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=10.0239 mle=1.6788 pcon=4.9662 forget=3.3789 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=9.7694 mle=1.4930 pcon=4.9653 forget=3.3111 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=9.8723 mle=1.5573 pcon=4.9647 forget=3.3504 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=10.0745 mle=1.7259 pcon=4.9639 forget=3.3847 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=9.8917 mle=1.6170 pcon=4.9633 forget=3.3114 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=10.1829 mle=1.8742 pcon=4.9626 forget=3.3461 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=9.9102 mle=1.5938 pcon=4.9621 forget=3.3543 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=9.8621 mle=1.5446 pcon=4.9614 forget=3.3561 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 16 it 10 total=9.9021 mle=1.5862 pcon=4.9607 forget=3.3552 nr=64 nf=64 protos=540 fproto_sim=NA
 85%|████████▌ | 17/20 [07:14<01:15, 25.07s/it] 90%|█████████ | 18/20 [07:38<00:49, 24.71s/it] 95%|█████████▌| 19/20 [08:02<00:24, 24.58s/it]100%|██████████| 20/20 [08:28<00:00, 25.02s/it]100%|██████████| 20/20 [08:28<00:00, 25.42s/it]
[loss] ep 16 it 60 total=9.7739 mle=1.4940 pcon=4.9601 forget=3.3198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=9.7994 mle=1.5134 pcon=4.9597 forget=3.3264 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=9.9582 mle=1.7002 pcon=4.9589 forget=3.2991 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=9.8226 mle=1.5736 pcon=4.9581 forget=3.2909 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=10.2061 mle=1.9476 pcon=4.9573 forget=3.3012 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=10.0751 mle=1.7938 pcon=4.9567 forget=3.3246 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=9.8757 mle=1.6136 pcon=4.9561 forget=3.3060 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 17 it 20 total=10.0747 mle=1.7776 pcon=4.9552 forget=3.3419 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=10.0751 mle=1.8260 pcon=4.9549 forget=3.2942 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=9.8563 mle=1.5746 pcon=4.9542 forget=3.3274 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=9.8635 mle=1.5824 pcon=4.9533 forget=3.3278 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=9.9631 mle=1.6654 pcon=4.9526 forget=3.3451 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=9.7513 mle=1.4681 pcon=4.9525 forget=3.3307 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=10.0677 mle=1.8041 pcon=4.9520 forget=3.3116 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=9.9939 mle=1.7472 pcon=4.9512 forget=3.2955 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=9.8299 mle=1.5868 pcon=4.9511 forget=3.2920 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=9.9594 mle=1.6886 pcon=4.9507 forget=3.3201 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=9.9423 mle=1.6649 pcon=4.9501 forget=3.3273 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=9.8278 mle=1.5481 pcon=4.9497 forget=3.3300 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=9.8301 mle=1.5911 pcon=4.9492 forget=3.2899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=9.9466 mle=1.6877 pcon=4.9488 forget=3.3101 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=10.0573 mle=1.8073 pcon=4.9483 forget=3.3016 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=9.9793 mle=1.7011 pcon=4.9482 forget=3.3301 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 19 it 40 total=9.7465 mle=1.4657 pcon=4.9480 forget=3.3327 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=10.0020 mle=1.7007 pcon=4.9480 forget=3.3533 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=9.8667 mle=1.6119 pcon=4.9476 forget=3.3072 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=9.8675 mle=1.6441 pcon=4.9471 forget=3.2763 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=10.1065 mle=1.8489 pcon=4.9468 forget=3.3108 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=9.8484 mle=1.5579 pcon=4.9467 forget=3.3438 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=9.9365 mle=1.5979 pcon=4.9466 forget=3.3920 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:54,  3.41it/s]  2%|▏         | 8/391 [00:00<00:15, 24.59it/s]  3%|▎         | 13/391 [00:00<00:11, 31.77it/s]  5%|▍         | 19/391 [00:00<00:09, 39.50it/s]  6%|▋         | 25/391 [00:00<00:08, 45.33it/s]  9%|▊         | 34/391 [00:00<00:06, 57.81it/s] 11%|█         | 43/391 [00:00<00:05, 66.76it/s] 13%|█▎        | 52/391 [00:01<00:04, 73.03it/s] 16%|█▌        | 61/391 [00:01<00:04, 77.70it/s] 18%|█▊        | 70/391 [00:01<00:04, 74.29it/s] 20%|█▉        | 78/391 [00:01<00:04, 64.45it/s] 22%|██▏       | 85/391 [00:01<00:05, 60.51it/s] 24%|██▍       | 94/391 [00:01<00:04, 67.50it/s] 26%|██▋       | 103/391 [00:01<00:03, 72.88it/s] 29%|██▊       | 112/391 [00:01<00:03, 77.17it/s] 31%|███       | 121/391 [00:01<00:03, 80.24it/s] 33%|███▎      | 130/391 [00:02<00:03, 80.89it/s] 36%|███▌      | 139/391 [00:02<00:03, 65.98it/s] 38%|███▊      | 147/391 [00:02<00:03, 62.74it/s] 40%|███▉      | 156/391 [00:02<00:03, 68.94it/s] 42%|████▏     | 165/391 [00:02<00:03, 73.92it/s] 45%|████▍     | 174/391 [00:02<00:02, 77.76it/s] 47%|████▋     | 183/391 [00:02<00:02, 80.75it/s] 49%|████▉     | 192/391 [00:02<00:02, 82.86it/s] 51%|█████▏    | 201/391 [00:03<00:02, 70.73it/s] 53%|█████▎    | 209/391 [00:03<00:02, 64.68it/s] 55%|█████▌    | 216/391 [00:03<00:02, 65.49it/s] 58%|█████▊    | 225/391 [00:03<00:02, 71.49it/s] 60%|█████▉    | 234/391 [00:03<00:02, 76.04it/s] 62%|██████▏   | 243/391 [00:03<00:01, 79.45it/s] 64%|██████▍   | 252/391 [00:03<00:01, 82.05it/s] 67%|██████▋   | 261/391 [00:03<00:01, 77.89it/s] 69%|██████▉   | 269/391 [00:04<00:01, 66.35it/s] 71%|███████   | 277/391 [00:04<00:01, 63.73it/s] 73%|███████▎  | 286/391 [00:04<00:01, 69.56it/s] 75%|███████▌  | 295/391 [00:04<00:01, 74.25it/s] 78%|███████▊  | 304/391 [00:04<00:01, 78.02it/s] 80%|████████  | 313/391 [00:04<00:00, 80.19it/s] 82%|████████▏ | 322/391 [00:04<00:00, 76.96it/s] 84%|████████▍ | 330/391 [00:04<00:00, 65.57it/s] 86%|████████▌ | 337/391 [00:05<00:00, 62.69it/s] 88%|████████▊ | 346/391 [00:05<00:00, 69.01it/s] 91%|█████████ | 355/391 [00:05<00:00, 74.06it/s] 93%|█████████▎| 364/391 [00:05<00:00, 77.88it/s] 95%|█████████▌| 373/391 [00:05<00:00, 80.63it/s] 98%|█████████▊| 382/391 [00:05<00:00, 82.56it/s]100%|██████████| 391/391 [00:05<00:00, 68.55it/s]100%|██████████| 391/391 [00:05<00:00, 68.33it/s]
50000 images processed, 5.820166826248169 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:19,  4.02it/s] 10%|█         | 8/79 [00:00<00:02, 27.31it/s] 22%|██▏       | 17/79 [00:00<00:01, 47.71it/s] 33%|███▎      | 26/79 [00:00<00:00, 60.34it/s] 44%|████▍     | 35/79 [00:00<00:00, 68.76it/s] 56%|█████▌    | 44/79 [00:00<00:00, 74.16it/s] 66%|██████▌   | 52/79 [00:00<00:00, 70.77it/s] 76%|███████▌  | 60/79 [00:01<00:00, 62.25it/s] 85%|████████▍ | 67/79 [00:01<00:00, 59.98it/s] 97%|█████████▋| 77/79 [00:01<00:00, 68.29it/s]100%|██████████| 79/79 [00:01<00:00, 59.52it/s]
10000 images processed, 1.347832441329956 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<00:56,  3.57it/s]  2%|▏         | 5/204 [00:00<00:12, 15.58it/s]  5%|▍         | 10/204 [00:00<00:07, 26.30it/s]  7%|▋         | 15/204 [00:00<00:05, 33.05it/s] 12%|█▏        | 24/204 [00:00<00:03, 49.78it/s] 16%|█▌        | 33/204 [00:00<00:02, 61.13it/s] 21%|██        | 42/204 [00:00<00:02, 68.97it/s] 25%|██▌       | 51/204 [00:01<00:02, 74.30it/s] 29%|██▉       | 60/204 [00:01<00:01, 78.10it/s] 34%|███▍      | 69/204 [00:01<00:01, 71.23it/s] 38%|███▊      | 77/204 [00:01<00:01, 64.69it/s] 41%|████      | 84/204 [00:01<00:01, 61.07it/s] 46%|████▌     | 93/204 [00:01<00:01, 67.75it/s] 50%|█████     | 102/204 [00:01<00:01, 72.62it/s] 54%|█████▍    | 111/204 [00:01<00:01, 76.49it/s] 59%|█████▉    | 120/204 [00:01<00:01, 79.18it/s] 63%|██████▎   | 129/204 [00:02<00:01, 70.35it/s] 67%|██████▋   | 137/204 [00:02<00:01, 63.21it/s] 71%|███████   | 144/204 [00:02<00:00, 63.67it/s] 75%|███████▌  | 153/204 [00:02<00:00, 69.56it/s] 79%|███████▉  | 162/204 [00:02<00:00, 73.52it/s] 84%|████████▍ | 171/204 [00:02<00:00, 77.08it/s] 88%|████████▊ | 179/204 [00:02<00:00, 77.61it/s] 92%|█████████▏| 187/204 [00:02<00:00, 62.91it/s] 95%|█████████▌| 194/204 [00:03<00:00, 60.20it/s]100%|█████████▉| 203/204 [00:03<00:00, 67.18it/s]100%|██████████| 204/204 [00:03<00:00, 62.85it/s]
26032 images processed, 3.293286085128784 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:52,  1.48it/s] 11%|█▏        | 9/79 [00:00<00:05, 12.61it/s] 18%|█▊        | 14/79 [00:01<00:03, 18.63it/s] 23%|██▎       | 18/79 [00:01<00:02, 22.24it/s] 32%|███▏      | 25/79 [00:01<00:01, 27.06it/s] 38%|███▊      | 30/79 [00:01<00:01, 31.50it/s] 43%|████▎     | 34/79 [00:01<00:01, 30.80it/s] 52%|█████▏    | 41/79 [00:01<00:01, 33.04it/s] 58%|█████▊    | 46/79 [00:01<00:00, 36.11it/s] 63%|██████▎   | 50/79 [00:01<00:00, 33.26it/s] 72%|███████▏  | 57/79 [00:02<00:00, 35.14it/s] 77%|███████▋  | 61/79 [00:02<00:00, 34.50it/s] 82%|████████▏ | 65/79 [00:02<00:00, 35.58it/s] 90%|████████▉ | 71/79 [00:02<00:00, 40.39it/s] 96%|█████████▌| 76/79 [00:02<00:00, 34.29it/s]100%|██████████| 79/79 [00:02<00:00, 28.80it/s]
10000 images processed, 2.7984116077423096 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:27,  2.85it/s] 10%|█         | 8/79 [00:00<00:03, 21.04it/s] 16%|█▋        | 13/79 [00:00<00:02, 27.67it/s] 24%|██▍       | 19/79 [00:00<00:01, 35.83it/s] 34%|███▍      | 27/79 [00:00<00:01, 47.13it/s] 46%|████▌     | 36/79 [00:00<00:00, 58.42it/s] 57%|█████▋    | 45/79 [00:01<00:00, 66.12it/s] 68%|██████▊   | 54/79 [00:01<00:00, 72.21it/s] 78%|███████▊  | 62/79 [00:01<00:00, 69.61it/s] 89%|████████▊ | 70/79 [00:01<00:00, 61.77it/s] 97%|█████████▋| 77/79 [00:01<00:00, 59.40it/s]100%|██████████| 79/79 [00:01<00:00, 50.87it/s]
10000 images processed, 1.5795910358428955 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:36,  1.91it/s] 10%|█         | 7/70 [00:00<00:04, 14.15it/s] 20%|██        | 14/70 [00:00<00:02, 26.88it/s] 33%|███▎      | 23/70 [00:00<00:01, 41.79it/s] 46%|████▌     | 32/70 [00:00<00:00, 52.81it/s] 59%|█████▊    | 41/70 [00:01<00:00, 61.00it/s] 70%|███████   | 49/70 [00:01<00:00, 53.50it/s] 80%|████████  | 56/70 [00:01<00:00, 52.08it/s] 93%|█████████▎| 65/70 [00:01<00:00, 60.16it/s]100%|██████████| 70/70 [00:01<00:00, 44.69it/s]
8925 images processed, 1.6048307418823242 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:51,  1.16s/it]  4%|▍         | 2/45 [00:01<00:23,  1.83it/s] 18%|█▊        | 8/45 [00:01<00:03,  9.36it/s] 24%|██▍       | 11/45 [00:02<00:04,  6.82it/s] 33%|███▎      | 15/45 [00:02<00:02, 10.28it/s] 40%|████      | 18/45 [00:02<00:03,  8.31it/s] 49%|████▉     | 22/45 [00:02<00:02, 10.46it/s] 56%|█████▌    | 25/45 [00:03<00:02,  9.38it/s] 60%|██████    | 27/45 [00:03<00:01,  9.96it/s] 67%|██████▋   | 30/45 [00:03<00:01, 10.60it/s] 73%|███████▎  | 33/45 [00:04<00:01,  8.56it/s] 78%|███████▊  | 35/45 [00:04<00:01,  9.24it/s] 89%|████████▉ | 40/45 [00:04<00:00, 14.54it/s] 96%|█████████▌| 43/45 [00:05<00:00,  8.36it/s]100%|██████████| 45/45 [00:05<00:00,  8.62it/s]
5640 images processed, 5.2466206550598145 seconds used

24.365204095840454
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.91  81.18
LSUN          17.54  96.08
iSUN          72.38  81.69
dtd           37.91  91.39
forget        79.50  86.90
AVG           46.29  89.43
Retain-Acc: 0.7406
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
14.31026816368103
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.5 lr=0.0001 epochs=50
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.5, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:29<24:29, 30.00s/it]  4%|▍         | 2/50 [11:10<5:11:19, 389.16s/it]  6%|▌         | 3/50 [11:35<2:54:25, 222.66s/it]  8%|▊         | 4/50 [11:58<1:50:18, 143.89s/it] 10%|█         | 5/50 [12:20<1:15:03, 100.07s/it] 12%|█▏        | 6/50 [12:44<54:23, 74.18s/it]    14%|█▍        | 7/50 [13:07<41:09, 57.42s/it] 16%|█▌        | 8/50 [13:30<32:27, 46.38s/it][loss] ep 0 it 0 total=10.3050 mle=1.5712 pcon=5.2950 forget=3.4388 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=10.3338 mle=1.5426 pcon=5.2879 forget=3.5033 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=10.4164 mle=1.7005 pcon=5.2809 forget=3.4350 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=10.5977 mle=1.8997 pcon=5.2738 forget=3.4242 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=10.4363 mle=1.7128 pcon=5.2670 forget=3.4566 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=10.2084 mle=1.5020 pcon=5.2603 forget=3.4461 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=10.2684 mle=1.5595 pcon=5.2540 forget=3.4548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=10.4027 mle=1.6809 pcon=5.2476 forget=3.4742 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=10.4377 mle=1.6716 pcon=5.2409 forget=3.5253 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=10.2893 mle=1.6372 pcon=5.2346 forget=3.4175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=10.2186 mle=1.5164 pcon=5.2284 forget=3.4738 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=10.4762 mle=1.7803 pcon=5.2224 forget=3.4734 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=10.4445 mle=1.7644 pcon=5.2167 forget=3.4634 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=10.2379 mle=1.6059 pcon=5.2113 forget=3.4207 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=10.3755 mle=1.7243 pcon=5.2056 forget=3.4456 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=10.4602 mle=1.7944 pcon=5.2003 forget=3.4655 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=10.1903 mle=1.5531 pcon=5.1951 forget=3.4422 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=10.4728 mle=1.8886 pcon=5.1899 forget=3.3942 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=10.3663 mle=1.7419 pcon=5.1847 forget=3.4397 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=10.1187 mle=1.5265 pcon=5.1797 forget=3.4126 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=10.2253 mle=1.5857 pcon=5.1746 forget=3.4650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=10.4805 mle=1.8837 pcon=5.1699 forget=3.4269 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=10.1519 mle=1.5925 pcon=5.1652 forget=3.3942 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=10.3841 mle=1.8113 pcon=5.1606 forget=3.4123 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=10.2892 mle=1.7060 pcon=5.1560 forget=3.4272 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=10.2714 mle=1.6711 pcon=5.1519 forget=3.4484 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=10.3291 mle=1.7667 pcon=5.1476 forget=3.4149 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=10.4678 mle=1.9183 pcon=5.1436 forget=3.4059 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=10.1097 mle=1.5701 pcon=5.1396 forget=3.4000 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=10.3626 mle=1.8364 pcon=5.1354 forget=3.3908 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=10.2732 mle=1.7032 pcon=5.1312 forget=3.4388 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=10.2695 mle=1.6957 pcon=5.1276 forget=3.4461 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=10.1336 mle=1.6543 pcon=5.1238 forget=3.3555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=10.2406 mle=1.7299 pcon=5.1198 forget=3.3909 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=10.1818 mle=1.6501 pcon=5.1161 forget=3.4156 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=9.9809 mle=1.4536 pcon=5.1127 forget=3.4146 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=10.2680 mle=1.7676 pcon=5.1088 forget=3.3916 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=9.9805 mle=1.4913 pcon=5.1053 forget=3.3839 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=10.3875 mle=1.8723 pcon=5.1019 forget=3.4133 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=10.3593 mle=1.8671 pcon=5.0985 forget=3.3936 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=10.2604 mle=1.7999 pcon=5.0949 forget=3.3655 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=10.0764 mle=1.5839 pcon=5.0918 forget=3.4007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=10.0817 mle=1.5960 pcon=5.0883 forget=3.3974 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=10.1417 mle=1.6913 pcon=5.0854 forget=3.3649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=10.2032 mle=1.7732 pcon=5.0825 forget=3.3475 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=10.0540 mle=1.5963 pcon=5.0794 forget=3.3783 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=10.1960 mle=1.7466 pcon=5.0766 forget=3.3729 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=9.9057 mle=1.4011 pcon=5.0737 forget=3.4308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=9.9292 mle=1.5361 pcon=5.0710 forget=3.3221 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=10.0649 mle=1.5917 pcon=5.0683 forget=3.4049 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=10.0969 mle=1.6654 pcon=5.0656 forget=3.3659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=10.0491 mle=1.6306 pcon=5.0634 forget=3.3551 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=10.2749 mle=1.8236 pcon=5.0605 forget=3.3908 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=10.0038 mle=1.5352 pcon=5.0581 forget=3.4105 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=10.1005 mle=1.6749 pcon=5.0557 forget=3.3699 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=10.0335 mle=1.6307 pcon=5.0530 forget=3.3498 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=10.1347 mle=1.7453 pcon=5.0507 forget=3.3387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=10.0317 mle=1.6271 pcon=5.0482 forget=3.3564 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=9.9518 mle=1.5828 pcon=5.0459 forget=3.3232 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=10.0802 mle=1.6546 pcon=5.0439 forget=3.3816 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=10.2427 mle=1.7956 pcon=5.0418 forget=3.4054 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=10.1821 mle=1.7420 pcon=5.0394 forget=3.4007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=10.0448 mle=1.6405 pcon=5.0373 forget=3.3670 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 18%|█▊        | 9/50 [13:54<26:53, 39.35s/it] 20%|██        | 10/50 [14:17<22:51, 34.30s/it] 22%|██▏       | 11/50 [14:40<20:04, 30.89s/it] 24%|██▍       | 12/50 [15:03<18:10, 28.70s/it] 26%|██▌       | 13/50 [15:27<16:46, 27.20s/it] 28%|██▊       | 14/50 [15:51<15:38, 26.08s/it] 30%|███       | 15/50 [16:14<14:42, 25.23s/it] 32%|███▏      | 16/50 [16:38<14:04, 24.83s/it][loss] ep 8 it 30 total=9.9751 mle=1.5371 pcon=5.0355 forget=3.4025 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=9.9966 mle=1.5903 pcon=5.0336 forget=3.3727 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=9.9275 mle=1.5004 pcon=5.0314 forget=3.3957 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=10.0887 mle=1.7035 pcon=5.0295 forget=3.3557 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=9.9100 mle=1.5305 pcon=5.0280 forget=3.3515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=10.0329 mle=1.6775 pcon=5.0261 forget=3.3293 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=10.2003 mle=1.7564 pcon=5.0240 forget=3.4199 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=9.8464 mle=1.4895 pcon=5.0222 forget=3.3347 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=9.9455 mle=1.6364 pcon=5.0203 forget=3.2889 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=10.0296 mle=1.6696 pcon=5.0182 forget=3.3418 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=10.2391 mle=1.8420 pcon=5.0167 forget=3.3804 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=9.9391 mle=1.5785 pcon=5.0152 forget=3.3455 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=9.9346 mle=1.5679 pcon=5.0137 forget=3.3530 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=9.9400 mle=1.5587 pcon=5.0118 forget=3.3694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=9.9115 mle=1.5580 pcon=5.0104 forget=3.3431 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=9.9848 mle=1.6072 pcon=5.0086 forget=3.3690 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=9.9903 mle=1.5901 pcon=5.0070 forget=3.3933 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=10.1057 mle=1.7542 pcon=5.0055 forget=3.3460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=10.1824 mle=1.8513 pcon=5.0041 forget=3.3269 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=10.1924 mle=1.8598 pcon=5.0027 forget=3.3299 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=9.9318 mle=1.5824 pcon=5.0014 forget=3.3480 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=9.8758 mle=1.5627 pcon=5.0002 forget=3.3129 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=10.0654 mle=1.6675 pcon=4.9989 forget=3.3990 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=9.9511 mle=1.5764 pcon=4.9975 forget=3.3772 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=9.9489 mle=1.6217 pcon=4.9961 forget=3.3311 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=9.8780 mle=1.5688 pcon=4.9949 forget=3.3143 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=10.0037 mle=1.6656 pcon=4.9933 forget=3.3448 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=10.1390 mle=1.7815 pcon=4.9922 forget=3.3652 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=9.9544 mle=1.6040 pcon=4.9914 forget=3.3590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=10.3112 mle=1.9667 pcon=4.9900 forget=3.3545 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=9.9572 mle=1.6704 pcon=4.9888 forget=3.2980 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 12 it 20 total=9.9491 mle=1.6202 pcon=4.9875 forget=3.3415 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=10.0352 mle=1.6951 pcon=4.9863 forget=3.3538 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=9.9133 mle=1.5482 pcon=4.9851 forget=3.3800 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=10.0489 mle=1.7486 pcon=4.9841 forget=3.3163 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=9.9080 mle=1.5997 pcon=4.9828 forget=3.3255 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=9.8893 mle=1.5351 pcon=4.9821 forget=3.3721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=9.8500 mle=1.5204 pcon=4.9810 forget=3.3486 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=10.2208 mle=1.8676 pcon=4.9800 forget=3.3732 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=9.7584 mle=1.4304 pcon=4.9791 forget=3.3489 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=9.9042 mle=1.4952 pcon=4.9780 forget=3.4310 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=9.8958 mle=1.5713 pcon=4.9768 forget=3.3477 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=9.8611 mle=1.5528 pcon=4.9760 forget=3.3323 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=9.9798 mle=1.6448 pcon=4.9751 forget=3.3599 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=9.8757 mle=1.5836 pcon=4.9742 forget=3.3180 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=9.9154 mle=1.5896 pcon=4.9732 forget=3.3526 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=9.8819 mle=1.5788 pcon=4.9723 forget=3.3308 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=10.0460 mle=1.7690 pcon=4.9711 forget=3.3058 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=10.0900 mle=1.7687 pcon=4.9706 forget=3.3508 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=9.9173 mle=1.6149 pcon=4.9701 forget=3.3323 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=10.1956 mle=1.8838 pcon=4.9691 forget=3.3427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=9.9162 mle=1.6166 pcon=4.9685 forget=3.3311 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=9.9422 mle=1.6698 pcon=4.9677 forget=3.3048 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=9.9565 mle=1.6809 pcon=4.9668 forget=3.3088 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=10.0229 mle=1.6787 pcon=4.9662 forget=3.3780 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=9.7681 mle=1.4926 pcon=4.9653 forget=3.3103 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=9.8707 mle=1.5572 pcon=4.9646 forget=3.3489 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=10.0729 mle=1.7258 pcon=4.9639 forget=3.3832 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=9.8898 mle=1.6167 pcon=4.9632 forget=3.3099 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=10.1809 mle=1.8738 pcon=4.9625 forget=3.3446 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=9.9083 mle=1.5938 pcon=4.9620 forget=3.3526 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=9.8598 mle=1.5443 pcon=4.9613 forget=3.3542 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 16 it 10 total=9.8996 mle=1.5863 pcon=4.9605 forget=3.3528 nr=64 nf=64 protos=540 fproto_sim=NA
 34%|███▍      | 17/50 [17:01<13:23, 24.36s/it] 36%|███▌      | 18/50 [17:24<12:44, 23.89s/it] 38%|███▊      | 19/50 [17:48<12:23, 23.98s/it] 40%|████      | 20/50 [18:12<11:56, 23.88s/it] 42%|████▏     | 21/50 [18:34<11:19, 23.41s/it] 44%|████▍     | 22/50 [18:57<10:48, 23.16s/it] 46%|████▌     | 23/50 [19:20<10:30, 23.36s/it] 48%|████▊     | 24/50 [19:43<10:01, 23.12s/it][loss] ep 16 it 60 total=9.7717 mle=1.4940 pcon=4.9599 forget=3.3177 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=9.7967 mle=1.5132 pcon=4.9595 forget=3.3240 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=9.9589 mle=1.7030 pcon=4.9587 forget=3.2972 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=9.8205 mle=1.5742 pcon=4.9579 forget=3.2884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=10.2026 mle=1.9473 pcon=4.9571 forget=3.2982 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=10.0679 mle=1.7897 pcon=4.9565 forget=3.3217 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=9.8725 mle=1.6129 pcon=4.9559 forget=3.3038 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 17 it 20 total=10.0711 mle=1.7773 pcon=4.9549 forget=3.3388 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=10.0704 mle=1.8255 pcon=4.9547 forget=3.2903 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=9.8524 mle=1.5747 pcon=4.9539 forget=3.3237 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=9.8589 mle=1.5826 pcon=4.9529 forget=3.3234 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=9.9585 mle=1.6654 pcon=4.9522 forget=3.3408 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=9.7459 mle=1.4675 pcon=4.9521 forget=3.3262 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=10.0630 mle=1.8043 pcon=4.9516 forget=3.3071 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=9.9889 mle=1.7470 pcon=4.9507 forget=3.2912 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=9.8227 mle=1.5864 pcon=4.9506 forget=3.2856 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=9.9624 mle=1.6962 pcon=4.9501 forget=3.3161 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=9.9360 mle=1.6640 pcon=4.9495 forget=3.3225 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=9.8206 mle=1.5481 pcon=4.9490 forget=3.3235 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=9.8248 mle=1.5913 pcon=4.9486 forget=3.2850 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=9.9396 mle=1.6868 pcon=4.9481 forget=3.3047 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=10.0511 mle=1.8083 pcon=4.9476 forget=3.2951 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=9.9714 mle=1.7009 pcon=4.9474 forget=3.3231 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 19 it 40 total=9.7372 mle=1.4650 pcon=4.9472 forget=3.3250 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=9.9943 mle=1.7006 pcon=4.9472 forget=3.3465 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=9.8581 mle=1.6105 pcon=4.9467 forget=3.3009 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=9.8602 mle=1.6441 pcon=4.9461 forget=3.2699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=10.0953 mle=1.8493 pcon=4.9458 forget=3.3002 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=9.8368 mle=1.5571 pcon=4.9457 forget=3.3340 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=9.9279 mle=1.5988 pcon=4.9455 forget=3.3835 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 20 it 0 total=9.8818 mle=1.6453 pcon=4.9451 forget=3.2914 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=9.9098 mle=1.6833 pcon=4.9447 forget=3.2818 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=9.9727 mle=1.7314 pcon=4.9446 forget=3.2967 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=10.0480 mle=1.8371 pcon=4.9440 forget=3.2669 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=9.8664 mle=1.6657 pcon=4.9435 forget=3.2573 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=10.2624 mle=2.0321 pcon=4.9429 forget=3.2875 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=9.9042 mle=1.6581 pcon=4.9422 forget=3.3039 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=9.6220 mle=1.3906 pcon=4.9418 forget=3.2896 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 10 total=9.6512 mle=1.4110 pcon=4.9413 forget=3.2989 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=9.9581 mle=1.6813 pcon=4.9410 forget=3.3358 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=9.7839 mle=1.5043 pcon=4.9407 forget=3.3388 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=9.7625 mle=1.5587 pcon=4.9407 forget=3.2632 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=10.0354 mle=1.7838 pcon=4.9403 forget=3.3113 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=9.9139 mle=1.6841 pcon=4.9400 forget=3.2897 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=9.8687 mle=1.6167 pcon=4.9394 forget=3.3126 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=9.9184 mle=1.6523 pcon=4.9390 forget=3.3270 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 22 it 20 total=9.8970 mle=1.6632 pcon=4.9385 forget=3.2954 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=10.0344 mle=1.8220 pcon=4.9380 forget=3.2744 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=9.7805 mle=1.5216 pcon=4.9373 forget=3.3216 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=9.8898 mle=1.6533 pcon=4.9371 forget=3.2994 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=9.9440 mle=1.6879 pcon=4.9368 forget=3.3193 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=9.7710 mle=1.5061 pcon=4.9364 forget=3.3285 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=9.8371 mle=1.6272 pcon=4.9362 forget=3.2737 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=10.2970 mle=2.0749 pcon=4.9356 forget=3.2865 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 23 it 30 total=9.7942 mle=1.5544 pcon=4.9357 forget=3.3041 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=9.9208 mle=1.6979 pcon=4.9353 forget=3.2876 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=9.8350 mle=1.5631 pcon=4.9349 forget=3.3371 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=9.9770 mle=1.7608 pcon=4.9345 forget=3.2817 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=9.8990 mle=1.6698 pcon=4.9343 forget=3.2948 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=9.8042 mle=1.5567 pcon=4.9339 forget=3.3136 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=9.8738 mle=1.6664 pcon=4.9336 forget=3.2738 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=9.7629 mle=1.5314 pcon=4.9331 forget=3.2984 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 24 it 40 total=9.7159 mle=1.5270 pcon=4.9329 forget=3.2560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=9.8344 mle=1.6152 pcon=4.9327 forget=3.2865 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=9.6729 mle=1.4176 pcon=4.9323 forget=3.3230 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=9.8754 mle=1.6648 pcon=4.9322 forget=3.2784 nr=64 nf=64 protos=540 fproto_sim=NA
 50%|█████     | 25/50 [20:05<09:30, 22.82s/it] 52%|█████▏    | 26/50 [20:28<09:10, 22.95s/it] 54%|█████▍    | 27/50 [20:51<08:47, 22.95s/it] 56%|█████▌    | 28/50 [21:14<08:22, 22.83s/it] 58%|█████▊    | 29/50 [21:37<08:04, 23.06s/it] 60%|██████    | 30/50 [22:00<07:36, 22.81s/it] 62%|██████▏   | 31/50 [22:23<07:13, 22.82s/it] 64%|██████▍   | 32/50 [22:47<06:58, 23.24s/it] 66%|██████▌   | 33/50 [23:09<06:31, 23.04s/it][loss] ep 24 it 240 total=9.8335 mle=1.5978 pcon=4.9320 forget=3.3037 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=10.0515 mle=1.7946 pcon=4.9317 forget=3.3252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=9.9398 mle=1.7333 pcon=4.9313 forget=3.2752 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 25 it 0 total=9.8992 mle=1.6827 pcon=4.9309 forget=3.2855 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=10.0794 mle=1.8128 pcon=4.9308 forget=3.3357 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=9.8202 mle=1.6430 pcon=4.9305 forget=3.2467 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 150 total=10.0196 mle=1.8172 pcon=4.9306 forget=3.2718 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 200 total=10.0574 mle=1.8350 pcon=4.9303 forget=3.2922 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=10.0227 mle=1.7962 pcon=4.9299 forget=3.2966 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=9.8257 mle=1.6027 pcon=4.9296 forget=3.2934 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=9.8365 mle=1.6074 pcon=4.9297 forget=3.2994 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 26 it 10 total=9.7433 mle=1.5065 pcon=4.9296 forget=3.3073 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=9.8219 mle=1.5633 pcon=4.9294 forget=3.3291 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=9.9681 mle=1.7765 pcon=4.9292 forget=3.2623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=9.6557 mle=1.4627 pcon=4.9290 forget=3.2640 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=9.8874 mle=1.6587 pcon=4.9285 forget=3.3002 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=9.7725 mle=1.5293 pcon=4.9285 forget=3.3147 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 310 total=10.1220 mle=1.9210 pcon=4.9284 forget=3.2726 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=9.8202 mle=1.6160 pcon=4.9278 forget=3.2764 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 20 total=9.9145 mle=1.6832 pcon=4.9276 forget=3.3037 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=9.7872 mle=1.5711 pcon=4.9271 forget=3.2891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=9.7627 mle=1.5331 pcon=4.9267 forget=3.3028 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=9.8232 mle=1.5823 pcon=4.9263 forget=3.3146 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=9.8018 mle=1.5820 pcon=4.9260 forget=3.2939 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=9.8539 mle=1.6475 pcon=4.9255 forget=3.2809 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=9.9521 mle=1.7278 pcon=4.9252 forget=3.2991 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=10.0184 mle=1.7697 pcon=4.9251 forget=3.3237 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 30 total=9.9541 mle=1.7181 pcon=4.9249 forget=3.3111 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=9.9835 mle=1.7514 pcon=4.9249 forget=3.3071 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=9.7935 mle=1.5506 pcon=4.9248 forget=3.3181 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=9.6598 mle=1.4652 pcon=4.9246 forget=3.2700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=9.5874 mle=1.3872 pcon=4.9241 forget=3.2761 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=9.7887 mle=1.5326 pcon=4.9237 forget=3.3324 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=10.0094 mle=1.7843 pcon=4.9230 forget=3.3021 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=10.0098 mle=1.7538 pcon=4.9226 forget=3.3333 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 40 total=9.9874 mle=1.7602 pcon=4.9222 forget=3.3050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=9.7936 mle=1.6010 pcon=4.9217 forget=3.2709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=9.8866 mle=1.6661 pcon=4.9216 forget=3.2990 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=9.7794 mle=1.5648 pcon=4.9213 forget=3.2934 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=9.7003 mle=1.4720 pcon=4.9211 forget=3.3072 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=10.0606 mle=1.8611 pcon=4.9210 forget=3.2785 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=9.8870 mle=1.6620 pcon=4.9209 forget=3.3041 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 0 total=9.8168 mle=1.6097 pcon=4.9208 forget=3.2863 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=9.8592 mle=1.6189 pcon=4.9208 forget=3.3195 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=9.6807 mle=1.4628 pcon=4.9205 forget=3.2975 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=10.1395 mle=1.9163 pcon=4.9202 forget=3.3031 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=9.7879 mle=1.4863 pcon=4.9200 forget=3.3816 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=10.0201 mle=1.7752 pcon=4.9197 forget=3.3253 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=9.7338 mle=1.4863 pcon=4.9195 forget=3.3280 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=10.2380 mle=1.9902 pcon=4.9194 forget=3.3284 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 10 total=9.8254 mle=1.6035 pcon=4.9193 forget=3.3026 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=9.8294 mle=1.5940 pcon=4.9192 forget=3.3162 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=9.8661 mle=1.6107 pcon=4.9192 forget=3.3363 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=9.8151 mle=1.5787 pcon=4.9191 forget=3.3173 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=9.7761 mle=1.5562 pcon=4.9189 forget=3.3009 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 260 total=9.6880 mle=1.4817 pcon=4.9186 forget=3.2877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=9.8454 mle=1.5899 pcon=4.9185 forget=3.3371 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=9.7955 mle=1.5880 pcon=4.9182 forget=3.2893 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 20 total=9.9501 mle=1.7443 pcon=4.9180 forget=3.2878 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=9.8938 mle=1.6928 pcon=4.9174 forget=3.2836 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=9.7857 mle=1.5109 pcon=4.9172 forget=3.3575 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 170 total=9.8499 mle=1.6097 pcon=4.9174 forget=3.3229 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=10.0611 mle=1.8545 pcon=4.9174 forget=3.2893 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=9.8108 mle=1.5880 pcon=4.9173 forget=3.3055 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=9.7109 mle=1.4944 pcon=4.9171 forget=3.2995 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=9.6595 mle=1.4325 pcon=4.9173 forget=3.3096 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 30 total=9.9376 mle=1.6817 pcon=4.9173 forget=3.3386 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=10.2367 mle=2.0407 pcon=4.9171 forget=3.2789 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=9.7611 mle=1.5233 pcon=4.9169 forget=3.3209 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=10.0963 mle=1.8355 pcon=4.9169 forget=3.3439 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=9.7803 mle=1.5392 pcon=4.9168 forget=3.3242 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=9.8315 mle=1.5835 pcon=4.9167 forget=3.3313 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=9.8601 mle=1.6391 pcon=4.9169 forget=3.3041 nr=64 nf=64 protos=540 fproto_sim=NA
 68%|██████▊   | 34/50 [23:32<06:08, 23.06s/it] 70%|███████   | 35/50 [23:56<05:46, 23.07s/it] 72%|███████▏  | 36/50 [24:20<05:29, 23.56s/it] 74%|███████▍  | 37/50 [24:44<05:06, 23.56s/it] 76%|███████▌  | 38/50 [25:07<04:41, 23.48s/it] 78%|███████▊  | 39/50 [25:31<04:18, 23.50s/it] 80%|████████  | 40/50 [25:53<03:51, 23.15s/it] 82%|████████▏ | 41/50 [26:16<03:27, 23.03s/it] 84%|████████▍ | 42/50 [26:40<03:06, 23.29s/it] 86%|████████▌ | 43/50 [27:03<02:42, 23.22s/it][loss] ep 33 it 380 total=9.8080 mle=1.5378 pcon=4.9168 forget=3.3534 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 40 total=10.0254 mle=1.7766 pcon=4.9164 forget=3.3324 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=9.6451 mle=1.4026 pcon=4.9161 forget=3.3264 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 140 total=9.7921 mle=1.5981 pcon=4.9161 forget=3.2780 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=9.9470 mle=1.7306 pcon=4.9160 forget=3.3004 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 240 total=10.1267 mle=1.8968 pcon=4.9157 forget=3.3143 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=9.8453 mle=1.5952 pcon=4.9152 forget=3.3349 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=9.8500 mle=1.5838 pcon=4.9151 forget=3.3511 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 0 total=9.9556 mle=1.7291 pcon=4.9150 forget=3.3115 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 50 total=9.7315 mle=1.5051 pcon=4.9150 forget=3.3114 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 100 total=10.0563 mle=1.8098 pcon=4.9145 forget=3.3321 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 150 total=9.9067 mle=1.6995 pcon=4.9144 forget=3.2929 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 200 total=9.9138 mle=1.6309 pcon=4.9143 forget=3.3685 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 250 total=9.9684 mle=1.7074 pcon=4.9140 forget=3.3470 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 300 total=9.6789 mle=1.4680 pcon=4.9139 forget=3.2970 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 350 total=9.9990 mle=1.7585 pcon=4.9137 forget=3.3267 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 10 total=10.1361 mle=1.8895 pcon=4.9136 forget=3.3329 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 60 total=9.7385 mle=1.5176 pcon=4.9137 forget=3.3072 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 110 total=9.9092 mle=1.6711 pcon=4.9135 forget=3.3246 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 160 total=9.9168 mle=1.6869 pcon=4.9133 forget=3.3166 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 210 total=9.6952 mle=1.4731 pcon=4.9134 forget=3.3087 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 260 total=9.8597 mle=1.5762 pcon=4.9136 forget=3.3699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 310 total=10.0612 mle=1.7697 pcon=4.9133 forget=3.3782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 360 total=9.7683 mle=1.5115 pcon=4.9132 forget=3.3436 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 20 total=9.7165 mle=1.4786 pcon=4.9127 forget=3.3251 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 70 total=10.0535 mle=1.7761 pcon=4.9128 forget=3.3646 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 120 total=9.8726 mle=1.6110 pcon=4.9125 forget=3.3491 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 170 total=9.9086 mle=1.6796 pcon=4.9122 forget=3.3167 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 220 total=10.0558 mle=1.7869 pcon=4.9118 forget=3.3571 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 270 total=9.8885 mle=1.6747 pcon=4.9118 forget=3.3021 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 320 total=9.8729 mle=1.6310 pcon=4.9115 forget=3.3304 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 370 total=9.9110 mle=1.5985 pcon=4.9115 forget=3.4010 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 30 total=10.0797 mle=1.7987 pcon=4.9116 forget=3.3695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 80 total=9.7591 mle=1.5143 pcon=4.9116 forget=3.3332 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 130 total=9.9411 mle=1.6891 pcon=4.9115 forget=3.3406 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 180 total=9.8698 mle=1.6122 pcon=4.9114 forget=3.3462 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 230 total=9.8392 mle=1.5734 pcon=4.9112 forget=3.3545 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 280 total=9.8990 mle=1.6653 pcon=4.9112 forget=3.3224 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 330 total=9.9085 mle=1.6583 pcon=4.9108 forget=3.3394 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 380 total=9.8432 mle=1.5657 pcon=4.9109 forget=3.3666 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 40 total=10.1046 mle=1.8513 pcon=4.9111 forget=3.3423 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 90 total=10.0100 mle=1.7444 pcon=4.9111 forget=3.3546 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 140 total=9.9115 mle=1.6627 pcon=4.9111 forget=3.3377 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 190 total=9.7884 mle=1.5416 pcon=4.9111 forget=3.3357 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 240 total=10.0561 mle=1.7992 pcon=4.9110 forget=3.3460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 290 total=9.7679 mle=1.4963 pcon=4.9109 forget=3.3607 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 340 total=9.9273 mle=1.6675 pcon=4.9109 forget=3.3489 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 0 total=9.9925 mle=1.6808 pcon=4.9107 forget=3.4010 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 50 total=9.9508 mle=1.6681 pcon=4.9105 forget=3.3722 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 100 total=9.8476 mle=1.6128 pcon=4.9102 forget=3.3246 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 150 total=9.7028 mle=1.4545 pcon=4.9103 forget=3.3381 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 200 total=10.0246 mle=1.7417 pcon=4.9101 forget=3.3728 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 250 total=10.2073 mle=1.9338 pcon=4.9101 forget=3.3634 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 300 total=9.7708 mle=1.4923 pcon=4.9102 forget=3.3683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 350 total=9.6499 mle=1.4070 pcon=4.9103 forget=3.3325 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 10 total=9.7370 mle=1.5131 pcon=4.9104 forget=3.3135 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 60 total=10.2909 mle=1.9938 pcon=4.9105 forget=3.3867 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 110 total=9.9210 mle=1.6413 pcon=4.9102 forget=3.3695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 160 total=9.7642 mle=1.4949 pcon=4.9105 forget=3.3588 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 210 total=9.8215 mle=1.5459 pcon=4.9104 forget=3.3651 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 260 total=9.7305 mle=1.4916 pcon=4.9107 forget=3.3282 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 310 total=9.7869 mle=1.5346 pcon=4.9110 forget=3.3413 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 360 total=9.9314 mle=1.6834 pcon=4.9111 forget=3.3368 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 20 total=9.9000 mle=1.6340 pcon=4.9112 forget=3.3549 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 70 total=9.9357 mle=1.6593 pcon=4.9111 forget=3.3653 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 120 total=10.2842 mle=1.9897 pcon=4.9109 forget=3.3837 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 170 total=9.8410 mle=1.5617 pcon=4.9110 forget=3.3684 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 220 total=9.9077 mle=1.6380 pcon=4.9111 forget=3.3586 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 270 total=10.0175 mle=1.7637 pcon=4.9111 forget=3.3426 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 320 total=9.9434 mle=1.6886 pcon=4.9113 forget=3.3435 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 370 total=10.0072 mle=1.7494 pcon=4.9114 forget=3.3464 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 30 total=9.9010 mle=1.5880 pcon=4.9114 forget=3.4016 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 80 total=9.8569 mle=1.5595 pcon=4.9115 forget=3.3860 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 130 total=10.0053 mle=1.7040 pcon=4.9116 forget=3.3897 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 180 total=9.7898 mle=1.4771 pcon=4.9117 forget=3.4010 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 230 total=9.9143 mle=1.6621 pcon=4.9116 forget=3.3406 nr=64 nf=64 protos=540 fproto_sim=NA
 88%|████████▊ | 44/50 [27:25<02:18, 23.07s/it] 90%|█████████ | 45/50 [27:48<01:54, 22.89s/it] 92%|█████████▏| 46/50 [28:12<01:32, 23.16s/it] 94%|█████████▍| 47/50 [28:34<01:08, 22.94s/it] 96%|█████████▌| 48/50 [28:57<00:45, 22.90s/it] 98%|█████████▊| 49/50 [29:21<00:23, 23.25s/it]100%|██████████| 50/50 [29:44<00:00, 23.29s/it]100%|██████████| 50/50 [29:44<00:00, 35.70s/it]
[loss] ep 43 it 280 total=9.7980 mle=1.5410 pcon=4.9117 forget=3.3453 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 330 total=9.8122 mle=1.5433 pcon=4.9116 forget=3.3573 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 380 total=9.7543 mle=1.5087 pcon=4.9116 forget=3.3340 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 40 total=10.1058 mle=1.8045 pcon=4.9116 forget=3.3898 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 90 total=9.8522 mle=1.6028 pcon=4.9115 forget=3.3379 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 140 total=10.0827 mle=1.7802 pcon=4.9117 forget=3.3909 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 190 total=9.6529 mle=1.3829 pcon=4.9115 forget=3.3585 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 240 total=9.9352 mle=1.6019 pcon=4.9114 forget=3.4218 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 290 total=10.0951 mle=1.8107 pcon=4.9114 forget=3.3730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 340 total=10.2428 mle=1.9676 pcon=4.9114 forget=3.3638 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 0 total=9.8487 mle=1.5581 pcon=4.9113 forget=3.3793 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 50 total=9.8527 mle=1.6065 pcon=4.9112 forget=3.3349 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 100 total=9.8759 mle=1.6020 pcon=4.9112 forget=3.3628 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 150 total=9.8620 mle=1.5696 pcon=4.9110 forget=3.3814 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 200 total=9.8972 mle=1.5864 pcon=4.9111 forget=3.3997 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 250 total=9.7439 mle=1.4689 pcon=4.9112 forget=3.3639 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 300 total=9.9591 mle=1.6888 pcon=4.9109 forget=3.3593 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 350 total=9.7665 mle=1.4934 pcon=4.9111 forget=3.3620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 10 total=9.7809 mle=1.4875 pcon=4.9111 forget=3.3823 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 60 total=9.7117 mle=1.4408 pcon=4.9113 forget=3.3596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 110 total=9.9823 mle=1.6955 pcon=4.9113 forget=3.3754 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 160 total=9.7457 mle=1.4850 pcon=4.9112 forget=3.3495 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 210 total=9.9894 mle=1.7308 pcon=4.9111 forget=3.3475 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 260 total=9.9301 mle=1.6842 pcon=4.9108 forget=3.3351 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 310 total=9.7806 mle=1.4986 pcon=4.9109 forget=3.3710 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 360 total=9.9551 mle=1.6716 pcon=4.9108 forget=3.3726 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 20 total=9.9107 mle=1.6146 pcon=4.9107 forget=3.3854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 70 total=9.8590 mle=1.5731 pcon=4.9105 forget=3.3754 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 120 total=9.9243 mle=1.6258 pcon=4.9103 forget=3.3881 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 170 total=9.8422 mle=1.5103 pcon=4.9101 forget=3.4218 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 220 total=9.7410 mle=1.4396 pcon=4.9101 forget=3.3913 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 270 total=9.8750 mle=1.5784 pcon=4.9101 forget=3.3865 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 320 total=9.9106 mle=1.6131 pcon=4.9099 forget=3.3876 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 370 total=9.9695 mle=1.6951 pcon=4.9098 forget=3.3646 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 30 total=10.0714 mle=1.7272 pcon=4.9099 forget=3.4342 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 80 total=9.9330 mle=1.6150 pcon=4.9099 forget=3.4080 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 130 total=9.9958 mle=1.7051 pcon=4.9101 forget=3.3806 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 180 total=10.0037 mle=1.7013 pcon=4.9101 forget=3.3923 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 230 total=10.0241 mle=1.7695 pcon=4.9103 forget=3.3443 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 280 total=9.7579 mle=1.5009 pcon=4.9101 forget=3.3469 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 330 total=9.7532 mle=1.5215 pcon=4.9099 forget=3.3218 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 380 total=9.8983 mle=1.6183 pcon=4.9096 forget=3.3704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 40 total=9.8201 mle=1.5427 pcon=4.9098 forget=3.3677 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 90 total=9.8869 mle=1.6196 pcon=4.9096 forget=3.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 140 total=9.9125 mle=1.6174 pcon=4.9095 forget=3.3856 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 190 total=10.0121 mle=1.7416 pcon=4.9092 forget=3.3613 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 240 total=9.9323 mle=1.6800 pcon=4.9089 forget=3.3434 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 290 total=9.9228 mle=1.6706 pcon=4.9088 forget=3.3434 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 340 total=9.7371 mle=1.4697 pcon=4.9090 forget=3.3584 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:44,  2.37it/s]  3%|▎         | 10/391 [00:00<00:15, 24.34it/s]  5%|▍         | 19/391 [00:00<00:08, 41.47it/s]  7%|▋         | 28/391 [00:00<00:06, 54.47it/s]  9%|▉         | 37/391 [00:00<00:05, 63.95it/s] 12%|█▏        | 46/391 [00:00<00:05, 66.90it/s] 14%|█▍        | 54/391 [00:01<00:05, 58.83it/s] 16%|█▌        | 61/391 [00:01<00:05, 57.97it/s] 18%|█▊        | 70/391 [00:01<00:04, 65.16it/s] 20%|██        | 79/391 [00:01<00:04, 71.35it/s] 23%|██▎       | 88/391 [00:01<00:03, 76.08it/s] 25%|██▍       | 97/391 [00:01<00:03, 79.51it/s] 27%|██▋       | 106/391 [00:01<00:03, 82.03it/s] 29%|██▉       | 115/391 [00:01<00:04, 66.83it/s] 31%|███▏      | 123/391 [00:02<00:04, 61.64it/s] 34%|███▍      | 132/391 [00:02<00:03, 67.94it/s] 36%|███▌      | 141/391 [00:02<00:03, 73.18it/s] 38%|███▊      | 150/391 [00:02<00:03, 77.26it/s] 41%|████      | 159/391 [00:02<00:02, 80.35it/s] 43%|████▎     | 168/391 [00:02<00:02, 82.73it/s] 45%|████▌     | 177/391 [00:02<00:03, 70.56it/s] 47%|████▋     | 185/391 [00:02<00:03, 64.08it/s] 49%|████▉     | 192/391 [00:03<00:03, 65.40it/s] 51%|█████▏    | 201/391 [00:03<00:02, 71.36it/s] 54%|█████▎    | 210/391 [00:03<00:02, 76.11it/s] 56%|█████▌    | 219/391 [00:03<00:02, 79.71it/s] 58%|█████▊    | 228/391 [00:03<00:01, 82.25it/s] 61%|██████    | 237/391 [00:03<00:02, 73.94it/s] 63%|██████▎   | 245/391 [00:03<00:02, 67.53it/s] 65%|██████▍   | 253/391 [00:03<00:02, 64.08it/s] 67%|██████▋   | 262/391 [00:03<00:01, 69.86it/s] 69%|██████▉   | 271/391 [00:04<00:01, 74.84it/s] 72%|███████▏  | 280/391 [00:04<00:01, 78.64it/s] 74%|███████▍  | 289/391 [00:04<00:01, 81.36it/s] 76%|███████▌  | 298/391 [00:04<00:01, 77.41it/s] 78%|███████▊  | 306/391 [00:04<00:01, 66.22it/s] 80%|████████  | 313/391 [00:04<00:01, 62.94it/s] 82%|████████▏ | 322/391 [00:04<00:00, 69.43it/s] 85%|████████▍ | 331/391 [00:04<00:00, 74.61it/s] 87%|████████▋ | 340/391 [00:05<00:00, 78.62it/s] 89%|████████▉ | 349/391 [00:05<00:00, 81.51it/s] 92%|█████████▏| 358/391 [00:05<00:00, 79.81it/s] 94%|█████████▍| 367/391 [00:05<00:00, 65.98it/s] 96%|█████████▌| 375/391 [00:05<00:00, 63.25it/s] 98%|█████████▊| 385/391 [00:05<00:00, 70.19it/s]100%|██████████| 391/391 [00:05<00:00, 67.70it/s]
50000 images processed, 5.9019670486450195 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:22,  3.41it/s] 13%|█▎        | 10/79 [00:00<00:02, 31.19it/s] 24%|██▍       | 19/79 [00:00<00:01, 49.28it/s] 35%|███▌      | 28/79 [00:00<00:00, 61.42it/s] 46%|████▌     | 36/79 [00:00<00:00, 65.16it/s] 56%|█████▌    | 44/79 [00:00<00:00, 57.30it/s] 65%|██████▍   | 51/79 [00:01<00:00, 55.92it/s] 76%|███████▌  | 60/79 [00:01<00:00, 64.36it/s] 87%|████████▋ | 69/79 [00:01<00:00, 70.86it/s] 99%|█████████▊| 78/79 [00:01<00:00, 75.94it/s]100%|██████████| 79/79 [00:01<00:00, 58.65it/s]
10000 images processed, 1.3716435432434082 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:15,  2.69it/s]  3%|▎         | 6/204 [00:00<00:12, 15.44it/s]  5%|▌         | 11/204 [00:00<00:07, 24.88it/s] 10%|▉         | 20/204 [00:00<00:04, 42.22it/s] 14%|█▍        | 29/204 [00:00<00:03, 54.90it/s] 19%|█▊        | 38/204 [00:00<00:02, 64.12it/s] 23%|██▎       | 47/204 [00:01<00:02, 68.86it/s] 27%|██▋       | 55/204 [00:01<00:02, 57.83it/s] 30%|███       | 62/204 [00:01<00:02, 56.87it/s] 35%|███▍      | 71/204 [00:01<00:02, 64.62it/s] 39%|███▉      | 80/204 [00:01<00:01, 70.64it/s] 44%|████▎     | 89/204 [00:01<00:01, 74.94it/s] 48%|████▊     | 98/204 [00:01<00:01, 78.23it/s] 52%|█████▏    | 107/204 [00:01<00:01, 63.80it/s] 56%|█████▋    | 115/204 [00:02<00:01, 59.80it/s] 61%|██████    | 124/204 [00:02<00:01, 65.99it/s] 65%|██████▌   | 133/204 [00:02<00:00, 71.00it/s] 70%|██████▉   | 142/204 [00:02<00:00, 74.97it/s] 74%|███████▍  | 151/204 [00:02<00:00, 77.71it/s] 78%|███████▊  | 160/204 [00:02<00:00, 62.72it/s] 82%|████████▏ | 167/204 [00:02<00:00, 60.11it/s] 86%|████████▋ | 176/204 [00:02<00:00, 66.44it/s] 91%|█████████ | 185/204 [00:03<00:00, 71.30it/s] 95%|█████████▌| 194/204 [00:03<00:00, 75.51it/s]100%|█████████▉| 203/204 [00:03<00:00, 79.10it/s]100%|██████████| 204/204 [00:03<00:00, 62.03it/s]
26032 images processed, 3.3302149772644043 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:55,  1.41it/s] 11%|█▏        | 9/79 [00:01<00:06, 10.00it/s] 18%|█▊        | 14/79 [00:01<00:04, 15.31it/s] 22%|██▏       | 17/79 [00:01<00:03, 16.47it/s] 32%|███▏      | 25/79 [00:01<00:02, 22.40it/s] 38%|███▊      | 30/79 [00:01<00:01, 26.81it/s] 43%|████▎     | 34/79 [00:01<00:01, 27.47it/s] 52%|█████▏    | 41/79 [00:02<00:01, 29.24it/s] 59%|█████▉    | 47/79 [00:02<00:00, 34.24it/s] 65%|██████▍   | 51/79 [00:02<00:00, 31.04it/s] 71%|███████   | 56/79 [00:02<00:00, 33.64it/s] 76%|███████▌  | 60/79 [00:02<00:00, 32.34it/s] 82%|████████▏ | 65/79 [00:02<00:00, 31.95it/s] 89%|████████▊ | 70/79 [00:02<00:00, 34.50it/s] 94%|█████████▎| 74/79 [00:02<00:00, 33.52it/s]100%|██████████| 79/79 [00:03<00:00, 26.01it/s]
10000 images processed, 3.077482223510742 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:37,  2.10it/s]  8%|▊         | 6/79 [00:00<00:05, 13.08it/s] 14%|█▍        | 11/79 [00:00<00:03, 22.06it/s] 25%|██▌       | 20/79 [00:00<00:01, 39.30it/s] 37%|███▋      | 29/79 [00:00<00:00, 52.44it/s] 48%|████▊     | 38/79 [00:00<00:00, 61.81it/s] 59%|█████▉    | 47/79 [00:01<00:00, 69.00it/s] 70%|██████▉   | 55/79 [00:01<00:00, 59.38it/s] 78%|███████▊  | 62/79 [00:01<00:00, 55.19it/s] 90%|████████▉ | 71/79 [00:01<00:00, 62.96it/s]100%|██████████| 79/79 [00:01<00:00, 49.01it/s]
10000 images processed, 1.635002851486206 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:28,  2.45it/s] 14%|█▍        | 10/70 [00:00<00:02, 24.94it/s] 27%|██▋       | 19/70 [00:00<00:01, 41.96it/s] 37%|███▋      | 26/70 [00:00<00:00, 44.96it/s] 47%|████▋     | 33/70 [00:00<00:00, 46.76it/s] 56%|█████▌    | 39/70 [00:00<00:00, 49.45it/s] 69%|██████▊   | 48/70 [00:01<00:00, 59.64it/s] 81%|████████▏ | 57/70 [00:01<00:00, 67.41it/s] 94%|█████████▍| 66/70 [00:01<00:00, 73.19it/s]100%|██████████| 70/70 [00:01<00:00, 51.39it/s]
8925 images processed, 1.4003376960754395 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:45,  1.03s/it]  4%|▍         | 2/45 [00:01<00:21,  2.02it/s] 16%|█▌        | 7/45 [00:01<00:04,  8.74it/s] 22%|██▏       | 10/45 [00:01<00:04,  7.21it/s] 31%|███       | 14/45 [00:01<00:02, 10.48it/s] 38%|███▊      | 17/45 [00:02<00:02,  9.35it/s] 42%|████▏     | 19/45 [00:02<00:02, 10.55it/s] 49%|████▉     | 22/45 [00:02<00:02, 10.48it/s] 56%|█████▌    | 25/45 [00:03<00:01, 10.83it/s] 60%|██████    | 27/45 [00:03<00:01, 11.79it/s] 67%|██████▋   | 30/45 [00:03<00:01, 10.33it/s] 73%|███████▎  | 33/45 [00:03<00:01,  9.12it/s] 78%|███████▊  | 35/45 [00:04<00:00, 10.05it/s] 89%|████████▉ | 40/45 [00:04<00:00, 14.56it/s] 93%|█████████▎| 42/45 [00:04<00:00,  8.33it/s]100%|██████████| 45/45 [00:04<00:00,  9.19it/s]
5640 images processed, 4.9195356369018555 seconds used

24.174206018447876
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.51  99.36
places365     67.89  81.18
LSUN          17.48  96.10
iSUN          72.35  81.70
dtd           37.89  91.38
forget        79.50  86.89
AVG           46.27  89.44
Retain-Acc: 0.7408
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.89 AUIN: 98.36
31.35566258430481
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.5-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.2 lr=0.001 epochs=5
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.2, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:38<02:35, 38.82s/it] 40%|████      | 2/5 [01:12<01:47, 35.68s/it] 60%|██████    | 3/5 [01:45<01:08, 34.47s/it] 80%|████████  | 4/5 [02:18<00:34, 34.08s/it]100%|██████████| 5/5 [02:53<00:00, 34.28s/it]100%|██████████| 5/5 [02:53<00:00, 34.69s/it]
[loss] ep 0 it 0 total=8.2411 mle=1.5707 pcon=5.2950 forget=1.3754 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=8.2318 mle=1.5425 pcon=5.2879 forget=1.4013 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=8.3553 mle=1.7004 pcon=5.2809 forget=1.3740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8.5431 mle=1.8996 pcon=5.2738 forget=1.3696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=8.3628 mle=1.7132 pcon=5.2670 forget=1.3826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=8.1407 mle=1.5019 pcon=5.2603 forget=1.3784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=8.1959 mle=1.5599 pcon=5.2541 forget=1.3819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=8.3182 mle=1.6809 pcon=5.2476 forget=1.3896 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=8.3225 mle=1.6716 pcon=5.2409 forget=1.4100 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=8.2387 mle=1.6371 pcon=5.2346 forget=1.3670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=8.1346 mle=1.5167 pcon=5.2285 forget=1.3895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=8.3921 mle=1.7803 pcon=5.2224 forget=1.3894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=8.3668 mle=1.7648 pcon=5.2167 forget=1.3853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=8.1854 mle=1.6059 pcon=5.2112 forget=1.3683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=8.3077 mle=1.7239 pcon=5.2056 forget=1.3782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=8.3808 mle=1.7943 pcon=5.2003 forget=1.3861 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=8.1247 mle=1.5528 pcon=5.1950 forget=1.3768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=8.4356 mle=1.8881 pcon=5.1899 forget=1.3576 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=8.3023 mle=1.7418 pcon=5.1847 forget=1.3758 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=8.0707 mle=1.5262 pcon=5.1797 forget=1.3648 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=8.1461 mle=1.5857 pcon=5.1746 forget=1.3858 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=8.4243 mle=1.8839 pcon=5.1699 forget=1.3705 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=8.1152 mle=1.5924 pcon=5.1652 forget=1.3575 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=8.3364 mle=1.8112 pcon=5.1605 forget=1.3647 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=8.2322 mle=1.7056 pcon=5.1560 forget=1.3706 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=8.2018 mle=1.6708 pcon=5.1519 forget=1.3791 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=8.2803 mle=1.7671 pcon=5.1475 forget=1.3657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=8.4238 mle=1.9181 pcon=5.1435 forget=1.3621 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=8.0690 mle=1.5698 pcon=5.1395 forget=1.3598 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=8.3273 mle=1.8359 pcon=5.1353 forget=1.3561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=8.2092 mle=1.7029 pcon=5.1311 forget=1.3752 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=8.2007 mle=1.6952 pcon=5.1275 forget=1.3780 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=8.1192 mle=1.6536 pcon=5.1237 forget=1.3419 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=8.2051 mle=1.7295 pcon=5.1196 forget=1.3560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8.1323 mle=1.6505 pcon=5.1159 forget=1.3659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.9312 mle=1.4532 pcon=5.1125 forget=1.3655 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=8.2311 mle=1.7664 pcon=5.1086 forget=1.3561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.9498 mle=1.4917 pcon=5.1051 forget=1.3531 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=8.3378 mle=1.8716 pcon=5.1016 forget=1.3647 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:15,  2.00it/s]  2%|▏         | 9/391 [00:00<00:20, 18.94it/s]  4%|▍         | 15/391 [00:00<00:13, 27.87it/s]  5%|▌         | 20/391 [00:00<00:11, 32.04it/s]  7%|▋         | 28/391 [00:00<00:08, 43.38it/s]  9%|▊         | 34/391 [00:01<00:07, 47.16it/s] 11%|█         | 43/391 [00:01<00:06, 56.35it/s] 13%|█▎        | 52/391 [00:01<00:05, 64.03it/s] 15%|█▌        | 60/391 [00:01<00:04, 68.23it/s] 17%|█▋        | 68/391 [00:01<00:05, 64.59it/s] 19%|█▉        | 75/391 [00:01<00:05, 60.02it/s] 21%|██        | 82/391 [00:01<00:05, 58.23it/s] 23%|██▎       | 90/391 [00:01<00:04, 62.85it/s] 25%|██▌       | 98/391 [00:01<00:04, 67.33it/s] 27%|██▋       | 107/391 [00:02<00:03, 71.02it/s] 29%|██▉       | 115/391 [00:02<00:04, 65.28it/s] 31%|███       | 122/391 [00:02<00:04, 61.10it/s] 33%|███▎      | 129/391 [00:02<00:04, 57.99it/s] 35%|███▌      | 137/391 [00:02<00:04, 62.63it/s] 37%|███▋      | 145/391 [00:02<00:03, 65.96it/s] 39%|███▉      | 152/391 [00:02<00:03, 66.18it/s] 41%|████      | 159/391 [00:02<00:04, 57.55it/s] 42%|████▏     | 166/391 [00:03<00:03, 57.21it/s] 45%|████▍     | 174/391 [00:03<00:03, 62.64it/s] 47%|████▋     | 183/391 [00:03<00:03, 67.97it/s] 49%|████▉     | 192/391 [00:03<00:02, 72.48it/s] 51%|█████     | 200/391 [00:03<00:02, 68.42it/s] 53%|█████▎    | 207/391 [00:03<00:02, 61.44it/s] 55%|█████▍    | 214/391 [00:03<00:03, 58.89it/s] 57%|█████▋    | 223/391 [00:03<00:02, 65.56it/s] 59%|█████▉    | 231/391 [00:04<00:02, 66.87it/s] 61%|██████    | 239/391 [00:04<00:02, 69.77it/s] 63%|██████▎   | 247/391 [00:04<00:02, 59.21it/s] 65%|██████▍   | 254/391 [00:04<00:02, 58.20it/s] 67%|██████▋   | 261/391 [00:04<00:02, 60.86it/s] 69%|██████▉   | 270/391 [00:04<00:01, 65.78it/s] 71%|███████   | 277/391 [00:04<00:01, 66.59it/s] 73%|███████▎  | 284/391 [00:04<00:01, 57.56it/s] 74%|███████▍  | 291/391 [00:05<00:01, 57.84it/s] 76%|███████▌  | 298/391 [00:05<00:01, 60.79it/s] 78%|███████▊  | 306/391 [00:05<00:01, 64.55it/s] 81%|████████  | 315/391 [00:05<00:01, 69.82it/s] 83%|████████▎ | 323/391 [00:05<00:01, 62.26it/s] 84%|████████▍ | 330/391 [00:05<00:01, 57.51it/s] 86%|████████▌ | 337/391 [00:05<00:00, 59.11it/s] 88%|████████▊ | 346/391 [00:05<00:00, 65.00it/s] 91%|█████████ | 354/391 [00:06<00:00, 67.64it/s] 92%|█████████▏| 361/391 [00:06<00:00, 60.60it/s] 94%|█████████▍| 368/391 [00:06<00:00, 59.64it/s] 96%|█████████▌| 375/391 [00:06<00:00, 58.13it/s] 98%|█████████▊| 385/391 [00:06<00:00, 66.93it/s]100%|██████████| 391/391 [00:06<00:00, 59.05it/s]
50000 images processed, 6.758365869522095 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:32,  2.43it/s]  8%|▊         | 6/79 [00:00<00:05, 14.26it/s] 14%|█▍        | 11/79 [00:00<00:02, 22.98it/s] 22%|██▏       | 17/79 [00:00<00:01, 31.44it/s] 32%|███▏      | 25/79 [00:00<00:01, 43.66it/s] 42%|████▏     | 33/79 [00:00<00:00, 52.36it/s] 51%|█████     | 40/79 [00:01<00:00, 53.31it/s] 58%|█████▊    | 46/79 [00:01<00:00, 51.00it/s] 67%|██████▋   | 53/79 [00:01<00:00, 53.78it/s] 76%|███████▌  | 60/79 [00:01<00:00, 56.56it/s] 87%|████████▋ | 69/79 [00:01<00:00, 63.89it/s] 99%|█████████▊| 78/79 [00:01<00:00, 70.96it/s]100%|██████████| 79/79 [00:01<00:00, 47.04it/s]
10000 images processed, 1.720266580581665 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:12,  2.81it/s]  4%|▍         | 9/204 [00:00<00:07, 24.84it/s]  8%|▊         | 17/204 [00:00<00:04, 40.04it/s] 12%|█▏        | 24/204 [00:00<00:03, 45.93it/s] 15%|█▍        | 30/204 [00:00<00:03, 44.89it/s] 18%|█▊        | 36/204 [00:00<00:03, 46.59it/s] 22%|██▏       | 44/204 [00:01<00:02, 54.99it/s] 25%|██▌       | 52/204 [00:01<00:02, 60.26it/s] 29%|██▉       | 59/204 [00:01<00:02, 58.59it/s] 32%|███▏      | 66/204 [00:01<00:02, 54.13it/s] 35%|███▌      | 72/204 [00:01<00:02, 52.33it/s] 39%|███▉      | 80/204 [00:01<00:02, 57.33it/s] 44%|████▎     | 89/204 [00:01<00:01, 63.71it/s] 47%|████▋     | 96/204 [00:01<00:01, 54.47it/s] 50%|█████     | 102/204 [00:02<00:01, 53.65it/s] 53%|█████▎    | 109/204 [00:02<00:01, 56.82it/s] 58%|█████▊    | 118/204 [00:02<00:01, 63.56it/s] 62%|██████▏   | 126/204 [00:02<00:01, 67.02it/s] 65%|██████▌   | 133/204 [00:02<00:01, 58.04it/s] 69%|██████▊   | 140/204 [00:02<00:01, 55.67it/s] 72%|███████▏  | 146/204 [00:02<00:01, 56.12it/s] 75%|███████▌  | 154/204 [00:02<00:00, 61.81it/s] 80%|███████▉  | 163/204 [00:03<00:00, 66.01it/s] 83%|████████▎ | 170/204 [00:03<00:00, 56.05it/s] 87%|████████▋ | 177/204 [00:03<00:00, 56.20it/s] 91%|█████████ | 185/204 [00:03<00:00, 61.67it/s] 95%|█████████▌| 194/204 [00:03<00:00, 67.72it/s]100%|█████████▉| 203/204 [00:03<00:00, 73.38it/s]100%|██████████| 204/204 [00:03<00:00, 55.83it/s]
26032 images processed, 3.70133900642395 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:42,  1.84it/s]  9%|▉         | 7/79 [00:00<00:05, 13.56it/s] 15%|█▌        | 12/79 [00:00<00:03, 21.40it/s] 22%|██▏       | 17/79 [00:00<00:02, 28.07it/s] 28%|██▊       | 22/79 [00:01<00:01, 29.57it/s] 35%|███▌      | 28/79 [00:01<00:01, 36.07it/s] 43%|████▎     | 34/79 [00:01<00:01, 39.83it/s] 52%|█████▏    | 41/79 [00:01<00:00, 40.89it/s] 58%|█████▊    | 46/79 [00:01<00:00, 41.85it/s] 65%|██████▍   | 51/79 [00:01<00:00, 43.06it/s] 73%|███████▎  | 58/79 [00:01<00:00, 45.60it/s] 80%|███████▉  | 63/79 [00:01<00:00, 43.93it/s] 87%|████████▋ | 69/79 [00:02<00:00, 47.79it/s] 94%|█████████▎| 74/79 [00:02<00:00, 47.80it/s]100%|██████████| 79/79 [00:02<00:00, 36.62it/s]
10000 images processed, 2.1950368881225586 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:36,  2.16it/s]  8%|▊         | 6/79 [00:00<00:05, 13.26it/s] 15%|█▌        | 12/79 [00:00<00:02, 23.93it/s] 25%|██▌       | 20/79 [00:00<00:01, 37.23it/s] 35%|███▌      | 28/79 [00:00<00:01, 47.81it/s] 47%|████▋     | 37/79 [00:01<00:00, 58.47it/s] 56%|█████▌    | 44/79 [00:01<00:00, 51.42it/s] 65%|██████▍   | 51/79 [00:01<00:00, 51.86it/s] 75%|███████▍  | 59/79 [00:01<00:00, 57.45it/s] 85%|████████▍ | 67/79 [00:01<00:00, 62.87it/s] 96%|█████████▌| 76/79 [00:01<00:00, 69.76it/s]100%|██████████| 79/79 [00:01<00:00, 47.88it/s]
10000 images processed, 1.6783874034881592 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:30,  2.29it/s]  7%|▋         | 5/70 [00:00<00:05, 11.30it/s] 14%|█▍        | 10/70 [00:00<00:02, 21.05it/s] 26%|██▌       | 18/70 [00:00<00:01, 35.63it/s] 37%|███▋      | 26/70 [00:00<00:00, 46.88it/s] 47%|████▋     | 33/70 [00:00<00:00, 50.05it/s] 56%|█████▌    | 39/70 [00:01<00:00, 46.94it/s] 64%|██████▍   | 45/70 [00:01<00:00, 49.92it/s] 74%|███████▍  | 52/70 [00:01<00:00, 54.17it/s] 87%|████████▋ | 61/70 [00:01<00:00, 62.48it/s]100%|██████████| 70/70 [00:01<00:00, 68.08it/s]100%|██████████| 70/70 [00:01<00:00, 44.71it/s]
8925 images processed, 1.6039941310882568 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:48,  1.11s/it]  4%|▍         | 2/45 [00:01<00:24,  1.75it/s] 16%|█▌        | 7/45 [00:01<00:04,  7.87it/s] 22%|██▏       | 10/45 [00:01<00:05,  6.60it/s] 38%|███▊      | 17/45 [00:02<00:02, 10.82it/s] 42%|████▏     | 19/45 [00:02<00:02, 10.85it/s] 56%|█████▌    | 25/45 [00:02<00:01, 12.75it/s] 60%|██████    | 27/45 [00:03<00:01, 12.51it/s] 71%|███████   | 32/45 [00:03<00:00, 17.59it/s] 78%|███████▊  | 35/45 [00:03<00:00, 10.21it/s] 91%|█████████ | 41/45 [00:04<00:00, 12.01it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.94it/s]100%|██████████| 45/45 [00:04<00:00,  9.77it/s]
5640 images processed, 4.627369165420532 seconds used

24.156229496002197
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.90  81.18
LSUN          17.55  96.08
iSUN          72.31  81.69
dtd           37.89  91.39
forget        79.50  86.89
AVG           46.28  89.43
Retain-Acc: 0.7407
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.89 AUIN: 98.36
8.68441891670227
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.2 lr=0.001 epochs=10
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.2, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:18<02:48, 18.77s/it] 20%|██        | 2/10 [00:33<02:11, 16.38s/it] 30%|███       | 3/10 [00:48<01:49, 15.67s/it] 40%|████      | 4/10 [01:03<01:31, 15.30s/it] 50%|█████     | 5/10 [01:17<01:15, 15.14s/it] 60%|██████    | 6/10 [01:33<01:00, 15.16s/it] 70%|███████   | 7/10 [01:48<00:45, 15.17s/it] 80%|████████  | 8/10 [02:03<00:30, 15.10s/it][loss] ep 0 it 0 total=8.2415 mle=1.5710 pcon=5.2950 forget=1.3755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=8.2316 mle=1.5424 pcon=5.2879 forget=1.4013 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=8.3553 mle=1.7004 pcon=5.2809 forget=1.3740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8.5432 mle=1.8997 pcon=5.2738 forget=1.3696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=8.3626 mle=1.7130 pcon=5.2670 forget=1.3826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=8.1408 mle=1.5020 pcon=5.2603 forget=1.3784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=8.1957 mle=1.5597 pcon=5.2540 forget=1.3819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=8.3182 mle=1.6810 pcon=5.2476 forget=1.3897 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=8.3225 mle=1.6716 pcon=5.2409 forget=1.4100 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=8.2385 mle=1.6369 pcon=5.2346 forget=1.3670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=8.1345 mle=1.5166 pcon=5.2284 forget=1.3895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=8.3919 mle=1.7801 pcon=5.2224 forget=1.3894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=8.3666 mle=1.7646 pcon=5.2167 forget=1.3853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=8.1850 mle=1.6055 pcon=5.2112 forget=1.3683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=8.3076 mle=1.7238 pcon=5.2056 forget=1.3782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=8.3806 mle=1.7942 pcon=5.2003 forget=1.3861 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=8.1246 mle=1.5528 pcon=5.1950 forget=1.3768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=8.4353 mle=1.8878 pcon=5.1899 forget=1.3576 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=8.3023 mle=1.7418 pcon=5.1847 forget=1.3758 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=8.0708 mle=1.5263 pcon=5.1796 forget=1.3648 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=8.1461 mle=1.5857 pcon=5.1745 forget=1.3859 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=8.4244 mle=1.8839 pcon=5.1699 forget=1.3706 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=8.1149 mle=1.5922 pcon=5.1652 forget=1.3575 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=8.3364 mle=1.8112 pcon=5.1605 forget=1.3647 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=8.2324 mle=1.7058 pcon=5.1559 forget=1.3706 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=8.2022 mle=1.6712 pcon=5.1519 forget=1.3791 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=8.2801 mle=1.7670 pcon=5.1475 forget=1.3657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=8.4237 mle=1.9181 pcon=5.1435 forget=1.3621 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=8.0691 mle=1.5699 pcon=5.1395 forget=1.3597 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=8.3271 mle=1.8357 pcon=5.1353 forget=1.3560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=8.2090 mle=1.7027 pcon=5.1311 forget=1.3752 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=8.2004 mle=1.6950 pcon=5.1275 forget=1.3780 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=8.1192 mle=1.6536 pcon=5.1236 forget=1.3419 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=8.2050 mle=1.7293 pcon=5.1196 forget=1.3560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8.1324 mle=1.6506 pcon=5.1159 forget=1.3659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.9315 mle=1.4535 pcon=5.1125 forget=1.3655 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=8.2314 mle=1.7667 pcon=5.1086 forget=1.3561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.9495 mle=1.4914 pcon=5.1050 forget=1.3530 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=8.3377 mle=1.8715 pcon=5.1016 forget=1.3647 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=8.3222 mle=1.8670 pcon=5.0982 forget=1.3571 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=8.2402 mle=1.8000 pcon=5.0946 forget=1.3456 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=8.0353 mle=1.5841 pcon=5.0914 forget=1.3598 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=8.0412 mle=1.5949 pcon=5.0879 forget=1.3584 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=8.1209 mle=1.6907 pcon=5.0850 forget=1.3453 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=8.1942 mle=1.7737 pcon=5.0820 forget=1.3384 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=8.0257 mle=1.5965 pcon=5.0789 forget=1.3503 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=8.1711 mle=1.7467 pcon=5.0760 forget=1.3483 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.8449 mle=1.4003 pcon=5.0731 forget=1.3715 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.9327 mle=1.5344 pcon=5.0703 forget=1.3280 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=8.0191 mle=1.5910 pcon=5.0676 forget=1.3605 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=8.0752 mle=1.6655 pcon=5.0649 forget=1.3448 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=8.0329 mle=1.6295 pcon=5.0626 forget=1.3408 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=8.2357 mle=1.8211 pcon=5.0597 forget=1.3549 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.9566 mle=1.5367 pcon=5.0573 forget=1.3627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=8.0751 mle=1.6740 pcon=5.0547 forget=1.3463 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=8.0213 mle=1.6311 pcon=5.0521 forget=1.3381 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=8.1262 mle=1.7429 pcon=5.0496 forget=1.3336 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=8.0118 mle=1.6249 pcon=5.0470 forget=1.3399 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.9520 mle=1.5799 pcon=5.0446 forget=1.3275 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=8.0428 mle=1.6502 pcon=5.0426 forget=1.3500 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=8.1940 mle=1.7947 pcon=5.0404 forget=1.3590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=8.1351 mle=1.7408 pcon=5.0378 forget=1.3565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=8.0216 mle=1.6429 pcon=5.0357 forget=1.3430 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 8 it 30 total=7.9264 mle=1.5364 pcon=5.0337 forget=1.3562 nr=64 nf=64 protos=540 fproto_sim=NA
 90%|█████████ | 9/10 [02:17<00:14, 14.97s/it]100%|██████████| 10/10 [02:33<00:00, 15.02s/it]100%|██████████| 10/10 [02:33<00:00, 15.30s/it]
[loss] ep 8 it 80 total=7.9649 mle=1.5880 pcon=5.0317 forget=1.3452 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.8852 mle=1.5044 pcon=5.0294 forget=1.3514 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=8.0656 mle=1.7021 pcon=5.0274 forget=1.3362 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.8903 mle=1.5303 pcon=5.0257 forget=1.3344 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=8.0252 mle=1.6769 pcon=5.0236 forget=1.3247 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=8.1358 mle=1.7546 pcon=5.0213 forget=1.3598 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.8295 mle=1.4832 pcon=5.0193 forget=1.3270 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.9579 mle=1.6317 pcon=5.0171 forget=1.3091 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=8.0125 mle=1.6675 pcon=5.0148 forget=1.3302 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=8.1872 mle=1.8310 pcon=5.0131 forget=1.3431 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.9054 mle=1.5695 pcon=5.0112 forget=1.3246 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.9098 mle=1.5694 pcon=5.0094 forget=1.3310 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.9043 mle=1.5627 pcon=5.0072 forget=1.3344 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.8947 mle=1.5645 pcon=5.0054 forget=1.3248 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:51,  3.51it/s]  3%|▎         | 11/391 [00:00<00:10, 34.99it/s]  5%|▌         | 21/391 [00:00<00:06, 54.77it/s]  8%|▊         | 31/391 [00:00<00:05, 67.83it/s] 10%|█         | 41/391 [00:00<00:04, 76.67it/s] 13%|█▎        | 51/391 [00:00<00:04, 82.39it/s] 16%|█▌        | 61/391 [00:00<00:03, 85.99it/s] 18%|█▊        | 71/391 [00:01<00:03, 88.53it/s] 21%|██        | 81/391 [00:01<00:03, 90.45it/s] 23%|██▎       | 91/391 [00:01<00:03, 91.55it/s] 26%|██▌       | 101/391 [00:01<00:03, 92.42it/s] 28%|██▊       | 111/391 [00:01<00:03, 93.12it/s] 31%|███       | 121/391 [00:01<00:02, 93.44it/s] 34%|███▎      | 131/391 [00:01<00:02, 93.72it/s] 36%|███▌      | 141/391 [00:01<00:02, 93.96it/s] 39%|███▊      | 151/391 [00:01<00:02, 93.99it/s] 41%|████      | 161/391 [00:01<00:02, 94.07it/s] 44%|████▎     | 171/391 [00:02<00:02, 94.24it/s] 46%|████▋     | 181/391 [00:02<00:02, 94.27it/s] 49%|████▉     | 191/391 [00:02<00:02, 94.57it/s] 51%|█████▏    | 201/391 [00:02<00:02, 94.97it/s] 54%|█████▍    | 211/391 [00:02<00:01, 93.91it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.04it/s] 59%|█████▉    | 231/391 [00:02<00:01, 94.13it/s] 62%|██████▏   | 241/391 [00:02<00:01, 94.22it/s] 64%|██████▍   | 251/391 [00:02<00:01, 94.25it/s] 67%|██████▋   | 261/391 [00:03<00:01, 94.41it/s] 69%|██████▉   | 271/391 [00:03<00:01, 94.47it/s] 72%|███████▏  | 281/391 [00:03<00:01, 94.55it/s] 74%|███████▍  | 291/391 [00:03<00:01, 94.73it/s] 77%|███████▋  | 301/391 [00:03<00:00, 94.66it/s] 80%|███████▉  | 311/391 [00:03<00:00, 94.51it/s] 82%|████████▏ | 321/391 [00:03<00:00, 94.31it/s] 85%|████████▍ | 331/391 [00:03<00:00, 94.45it/s] 87%|████████▋ | 341/391 [00:03<00:00, 94.78it/s] 90%|████████▉ | 351/391 [00:03<00:00, 94.87it/s] 92%|█████████▏| 361/391 [00:04<00:00, 95.13it/s] 95%|█████████▍| 371/391 [00:04<00:00, 95.39it/s] 97%|█████████▋| 381/391 [00:04<00:00, 95.34it/s]100%|██████████| 391/391 [00:04<00:00, 92.38it/s]100%|██████████| 391/391 [00:04<00:00, 88.54it/s]
50000 images processed, 4.509803295135498 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:29,  2.61it/s] 14%|█▍        | 11/79 [00:00<00:02, 28.69it/s] 27%|██▋       | 21/79 [00:00<00:01, 47.47it/s] 39%|███▉      | 31/79 [00:00<00:00, 60.93it/s] 52%|█████▏    | 41/79 [00:00<00:00, 70.57it/s] 65%|██████▍   | 51/79 [00:00<00:00, 77.15it/s] 77%|███████▋  | 61/79 [00:01<00:00, 81.91it/s] 90%|████████▉ | 71/79 [00:01<00:00, 85.92it/s]100%|██████████| 79/79 [00:01<00:00, 64.69it/s]
10000 images processed, 1.2450041770935059 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:27,  2.32it/s]  5%|▌         | 11/204 [00:00<00:07, 26.26it/s] 10%|█         | 21/204 [00:00<00:04, 44.64it/s] 15%|█▌        | 31/204 [00:00<00:02, 58.30it/s] 20%|██        | 41/204 [00:00<00:02, 68.38it/s] 25%|██▌       | 51/204 [00:00<00:02, 75.37it/s] 30%|██▉       | 61/204 [00:01<00:01, 80.77it/s] 35%|███▍      | 71/204 [00:01<00:01, 84.58it/s] 40%|███▉      | 81/204 [00:01<00:01, 87.00it/s] 45%|████▍     | 91/204 [00:01<00:01, 89.15it/s] 50%|████▉     | 101/204 [00:01<00:01, 90.48it/s] 54%|█████▍    | 111/204 [00:01<00:01, 91.02it/s] 59%|█████▉    | 121/204 [00:01<00:00, 91.51it/s] 64%|██████▍   | 131/204 [00:01<00:00, 91.83it/s] 69%|██████▉   | 141/204 [00:01<00:00, 92.41it/s] 74%|███████▍  | 151/204 [00:02<00:00, 92.47it/s] 79%|███████▉  | 161/204 [00:02<00:00, 92.86it/s] 84%|████████▍ | 171/204 [00:02<00:00, 93.02it/s] 89%|████████▊ | 181/204 [00:02<00:00, 92.99it/s] 94%|█████████▎| 191/204 [00:02<00:00, 93.32it/s] 99%|█████████▊| 201/204 [00:02<00:00, 93.95it/s]100%|██████████| 204/204 [00:02<00:00, 78.17it/s]
26032 images processed, 2.6543829441070557 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:37,  2.07it/s] 11%|█▏        | 9/79 [00:00<00:03, 17.53it/s] 22%|██▏       | 17/79 [00:00<00:02, 27.77it/s] 32%|███▏      | 25/79 [00:00<00:01, 34.68it/s] 41%|████      | 32/79 [00:01<00:01, 42.07it/s] 48%|████▊     | 38/79 [00:01<00:00, 41.54it/s] 54%|█████▍    | 43/79 [00:01<00:00, 40.79it/s] 63%|██████▎   | 50/79 [00:01<00:00, 42.61it/s] 71%|███████   | 56/79 [00:01<00:00, 46.58it/s] 78%|███████▊  | 62/79 [00:01<00:00, 48.93it/s] 86%|████████▌ | 68/79 [00:01<00:00, 46.69it/s] 92%|█████████▏| 73/79 [00:01<00:00, 45.09it/s]100%|██████████| 79/79 [00:02<00:00, 38.93it/s]
10000 images processed, 2.0643935203552246 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:30,  2.55it/s] 14%|█▍        | 11/79 [00:00<00:02, 28.11it/s] 27%|██▋       | 21/79 [00:00<00:01, 46.44it/s] 39%|███▉      | 31/79 [00:00<00:00, 60.22it/s] 52%|█████▏    | 41/79 [00:00<00:00, 70.31it/s] 65%|██████▍   | 51/79 [00:00<00:00, 77.57it/s] 77%|███████▋  | 61/79 [00:01<00:00, 82.61it/s] 90%|████████▉ | 71/79 [00:01<00:00, 86.45it/s]100%|██████████| 79/79 [00:01<00:00, 65.21it/s]
10000 images processed, 1.2317798137664795 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:25,  2.72it/s] 16%|█▌        | 11/70 [00:00<00:02, 29.45it/s] 30%|███       | 21/70 [00:00<00:01, 48.58it/s] 44%|████▍     | 31/70 [00:00<00:00, 62.20it/s] 59%|█████▊    | 41/70 [00:00<00:00, 71.88it/s] 73%|███████▎  | 51/70 [00:00<00:00, 78.60it/s] 87%|████████▋ | 61/70 [00:01<00:00, 83.67it/s]100%|██████████| 70/70 [00:01<00:00, 63.61it/s]
8925 images processed, 1.1308009624481201 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:42,  1.05it/s]  4%|▍         | 2/45 [00:01<00:20,  2.12it/s] 20%|██        | 9/45 [00:01<00:03,  9.17it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.92it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.32it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.69it/s] 49%|████▉     | 22/45 [00:02<00:01, 13.99it/s] 56%|█████▌    | 25/45 [00:02<00:01, 14.04it/s] 60%|██████    | 27/45 [00:02<00:01, 11.65it/s] 67%|██████▋   | 30/45 [00:02<00:01, 13.66it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.86it/s] 78%|███████▊  | 35/45 [00:03<00:00, 10.31it/s] 91%|█████████ | 41/45 [00:03<00:00, 15.33it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.25it/s]100%|██████████| 45/45 [00:04<00:00, 10.37it/s]
5640 images processed, 4.360426664352417 seconds used

18.948089361190796
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     68.02  81.18
LSUN          17.30  96.13
iSUN          72.29  81.68
dtd           37.94  91.36
forget        79.40  86.87
AVG           46.25  89.43
Retain-Acc: 0.7411
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.40 AUROC: 86.87 AUIN: 98.36
9.836559295654297
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.2 lr=0.001 epochs=20
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.2, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:17<05:37, 17.78s/it] 10%|█         | 2/20 [00:31<04:37, 15.44s/it] 15%|█▌        | 3/20 [00:45<04:11, 14.77s/it] 20%|██        | 4/20 [00:59<03:51, 14.46s/it] 25%|██▌       | 5/20 [01:13<03:36, 14.43s/it] 30%|███       | 6/20 [01:28<03:23, 14.51s/it] 35%|███▌      | 7/20 [01:43<03:08, 14.49s/it] 40%|████      | 8/20 [01:57<02:52, 14.40s/it][loss] ep 0 it 0 total=8.2415 mle=1.5710 pcon=5.2950 forget=1.3755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=8.2316 mle=1.5424 pcon=5.2879 forget=1.4013 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=8.3553 mle=1.7004 pcon=5.2809 forget=1.3740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8.5432 mle=1.8997 pcon=5.2738 forget=1.3696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=8.3626 mle=1.7130 pcon=5.2670 forget=1.3826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=8.1408 mle=1.5020 pcon=5.2603 forget=1.3784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=8.1957 mle=1.5597 pcon=5.2540 forget=1.3819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=8.3182 mle=1.6810 pcon=5.2476 forget=1.3897 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=8.3225 mle=1.6716 pcon=5.2409 forget=1.4100 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=8.2385 mle=1.6369 pcon=5.2346 forget=1.3670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=8.1345 mle=1.5166 pcon=5.2284 forget=1.3895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=8.3919 mle=1.7801 pcon=5.2224 forget=1.3894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=8.3666 mle=1.7646 pcon=5.2167 forget=1.3853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=8.1850 mle=1.6055 pcon=5.2112 forget=1.3683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=8.3076 mle=1.7238 pcon=5.2056 forget=1.3782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=8.3806 mle=1.7942 pcon=5.2003 forget=1.3861 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=8.1246 mle=1.5528 pcon=5.1950 forget=1.3768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=8.4353 mle=1.8878 pcon=5.1899 forget=1.3576 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=8.3023 mle=1.7418 pcon=5.1847 forget=1.3758 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=8.0708 mle=1.5263 pcon=5.1796 forget=1.3648 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=8.1461 mle=1.5857 pcon=5.1745 forget=1.3859 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=8.4244 mle=1.8839 pcon=5.1699 forget=1.3706 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=8.1149 mle=1.5922 pcon=5.1652 forget=1.3575 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=8.3364 mle=1.8112 pcon=5.1605 forget=1.3647 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=8.2324 mle=1.7058 pcon=5.1559 forget=1.3706 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=8.2022 mle=1.6712 pcon=5.1519 forget=1.3791 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=8.2801 mle=1.7670 pcon=5.1475 forget=1.3657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=8.4237 mle=1.9181 pcon=5.1435 forget=1.3621 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=8.0691 mle=1.5699 pcon=5.1395 forget=1.3597 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=8.3271 mle=1.8357 pcon=5.1353 forget=1.3560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=8.2090 mle=1.7027 pcon=5.1311 forget=1.3752 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=8.2004 mle=1.6950 pcon=5.1275 forget=1.3780 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=8.1192 mle=1.6536 pcon=5.1236 forget=1.3419 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=8.2050 mle=1.7293 pcon=5.1196 forget=1.3560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8.1324 mle=1.6506 pcon=5.1159 forget=1.3659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.9315 mle=1.4535 pcon=5.1125 forget=1.3655 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=8.2314 mle=1.7667 pcon=5.1086 forget=1.3561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.9495 mle=1.4914 pcon=5.1050 forget=1.3530 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=8.3377 mle=1.8715 pcon=5.1016 forget=1.3647 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=8.3222 mle=1.8670 pcon=5.0982 forget=1.3571 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=8.2402 mle=1.8000 pcon=5.0946 forget=1.3456 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=8.0353 mle=1.5841 pcon=5.0914 forget=1.3598 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=8.0412 mle=1.5949 pcon=5.0879 forget=1.3584 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=8.1209 mle=1.6907 pcon=5.0850 forget=1.3453 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=8.1942 mle=1.7737 pcon=5.0820 forget=1.3384 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=8.0257 mle=1.5965 pcon=5.0789 forget=1.3503 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=8.1711 mle=1.7467 pcon=5.0760 forget=1.3483 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.8449 mle=1.4003 pcon=5.0731 forget=1.3715 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.9327 mle=1.5344 pcon=5.0703 forget=1.3280 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=8.0191 mle=1.5910 pcon=5.0676 forget=1.3605 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=8.0752 mle=1.6655 pcon=5.0649 forget=1.3448 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=8.0329 mle=1.6295 pcon=5.0626 forget=1.3408 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=8.2357 mle=1.8211 pcon=5.0597 forget=1.3549 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.9566 mle=1.5367 pcon=5.0573 forget=1.3627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=8.0751 mle=1.6740 pcon=5.0547 forget=1.3463 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=8.0213 mle=1.6311 pcon=5.0521 forget=1.3381 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=8.1262 mle=1.7429 pcon=5.0496 forget=1.3336 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=8.0118 mle=1.6249 pcon=5.0470 forget=1.3399 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.9520 mle=1.5799 pcon=5.0446 forget=1.3275 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=8.0428 mle=1.6502 pcon=5.0426 forget=1.3500 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=8.1940 mle=1.7947 pcon=5.0404 forget=1.3590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=8.1351 mle=1.7408 pcon=5.0378 forget=1.3565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=8.0216 mle=1.6429 pcon=5.0357 forget=1.3430 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 8 it 30 total=7.9264 mle=1.5364 pcon=5.0337 forget=1.3562 nr=64 nf=64 protos=540 fproto_sim=NA
 45%|████▌     | 9/20 [02:11<02:38, 14.40s/it] 50%|█████     | 10/20 [02:25<02:22, 14.27s/it] 55%|█████▌    | 11/20 [02:39<02:08, 14.24s/it] 60%|██████    | 12/20 [02:53<01:53, 14.17s/it] 65%|██████▌   | 13/20 [03:07<01:38, 14.10s/it] 70%|███████   | 14/20 [03:21<01:24, 14.11s/it] 75%|███████▌  | 15/20 [03:35<01:10, 14.09s/it] 80%|████████  | 16/20 [03:49<00:56, 14.06s/it][loss] ep 8 it 80 total=7.9649 mle=1.5880 pcon=5.0317 forget=1.3452 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.8852 mle=1.5044 pcon=5.0294 forget=1.3514 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=8.0656 mle=1.7021 pcon=5.0274 forget=1.3362 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.8903 mle=1.5303 pcon=5.0257 forget=1.3344 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=8.0252 mle=1.6769 pcon=5.0236 forget=1.3247 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=8.1358 mle=1.7546 pcon=5.0213 forget=1.3598 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.8295 mle=1.4832 pcon=5.0193 forget=1.3270 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.9579 mle=1.6317 pcon=5.0171 forget=1.3091 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=8.0125 mle=1.6675 pcon=5.0148 forget=1.3302 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=8.1872 mle=1.8310 pcon=5.0131 forget=1.3431 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.9054 mle=1.5695 pcon=5.0112 forget=1.3246 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.9098 mle=1.5694 pcon=5.0094 forget=1.3310 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.9043 mle=1.5627 pcon=5.0072 forget=1.3344 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.8947 mle=1.5645 pcon=5.0054 forget=1.3248 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=7.9423 mle=1.6075 pcon=5.0033 forget=1.3315 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=7.9315 mle=1.5876 pcon=5.0011 forget=1.3427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=8.0730 mle=1.7524 pcon=4.9992 forget=1.3214 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=8.1515 mle=1.8397 pcon=4.9974 forget=1.3144 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=8.1649 mle=1.8498 pcon=4.9954 forget=1.3197 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=7.9049 mle=1.5857 pcon=4.9936 forget=1.3256 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=7.8617 mle=1.5590 pcon=4.9918 forget=1.3108 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=7.9839 mle=1.6510 pcon=4.9899 forget=1.3431 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=7.8877 mle=1.5620 pcon=4.9879 forget=1.3377 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=7.9175 mle=1.6155 pcon=4.9860 forget=1.3160 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=7.8697 mle=1.5731 pcon=4.9841 forget=1.3124 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=7.9592 mle=1.6539 pcon=4.9819 forget=1.3234 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=8.0869 mle=1.7848 pcon=4.9801 forget=1.3219 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=7.9111 mle=1.6011 pcon=4.9787 forget=1.3313 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=8.2940 mle=1.9901 pcon=4.9767 forget=1.3272 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=7.9542 mle=1.6636 pcon=4.9748 forget=1.3158 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 12 it 20 total=7.9248 mle=1.6202 pcon=4.9728 forget=1.3318 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=8.0000 mle=1.6989 pcon=4.9710 forget=1.3301 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=7.8721 mle=1.5520 pcon=4.9692 forget=1.3509 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=8.0354 mle=1.7408 pcon=4.9675 forget=1.3271 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=7.8753 mle=1.5747 pcon=4.9656 forget=1.3349 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=7.8431 mle=1.5290 pcon=4.9643 forget=1.3497 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=7.7916 mle=1.4911 pcon=4.9626 forget=1.3379 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=8.1682 mle=1.8520 pcon=4.9610 forget=1.3553 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=7.7327 mle=1.4258 pcon=4.9595 forget=1.3474 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=7.8144 mle=1.4729 pcon=4.9579 forget=1.3836 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=7.8591 mle=1.5468 pcon=4.9561 forget=1.3561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=7.8440 mle=1.5453 pcon=4.9548 forget=1.3440 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=7.9711 mle=1.6635 pcon=4.9533 forget=1.3543 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=7.8553 mle=1.5578 pcon=4.9518 forget=1.3457 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=7.8901 mle=1.5760 pcon=4.9503 forget=1.3638 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=7.8771 mle=1.5707 pcon=4.9488 forget=1.3576 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=8.0689 mle=1.7684 pcon=4.9472 forget=1.3534 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=8.0887 mle=1.7749 pcon=4.9460 forget=1.3678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=7.8886 mle=1.5771 pcon=4.9451 forget=1.3665 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=8.1662 mle=1.8574 pcon=4.9435 forget=1.3652 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=7.9383 mle=1.6281 pcon=4.9424 forget=1.3678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=7.9316 mle=1.6318 pcon=4.9410 forget=1.3587 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=7.9836 mle=1.6769 pcon=4.9397 forget=1.3669 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 0 total=8.0177 mle=1.6927 pcon=4.9385 forget=1.3865 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=7.7719 mle=1.4625 pcon=4.9370 forget=1.3724 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=7.8681 mle=1.5562 pcon=4.9358 forget=1.3762 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=8.0315 mle=1.7001 pcon=4.9344 forget=1.3970 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=7.9183 mle=1.6098 pcon=4.9332 forget=1.3753 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=8.2212 mle=1.8986 pcon=4.9318 forget=1.3907 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=7.8978 mle=1.5756 pcon=4.9307 forget=1.3915 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=7.8338 mle=1.5118 pcon=4.9295 forget=1.3926 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 10 total=7.8923 mle=1.5687 pcon=4.9281 forget=1.3954 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=7.8079 mle=1.4963 pcon=4.9269 forget=1.3847 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=7.8291 mle=1.5166 pcon=4.9259 forget=1.3867 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=7.9701 mle=1.6554 pcon=4.9244 forget=1.3903 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=7.8850 mle=1.5800 pcon=4.9229 forget=1.3821 nr=64 nf=64 protos=540 fproto_sim=NA
 85%|████████▌ | 17/20 [04:04<00:42, 14.16s/it] 90%|█████████ | 18/20 [04:18<00:28, 14.08s/it] 95%|█████████▌| 19/20 [04:32<00:14, 14.14s/it]100%|██████████| 20/20 [04:46<00:00, 14.22s/it]100%|██████████| 20/20 [04:46<00:00, 14.34s/it]
[loss] ep 16 it 260 total=8.2018 mle=1.8970 pcon=4.9215 forget=1.3834 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=8.0747 mle=1.7585 pcon=4.9201 forget=1.3961 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=7.9142 mle=1.5984 pcon=4.9188 forget=1.3969 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 20 total=8.0675 mle=1.7503 pcon=4.9172 forget=1.3999 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=8.1060 mle=1.7986 pcon=4.9163 forget=1.3912 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=7.8706 mle=1.5524 pcon=4.9148 forget=1.4034 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=7.8864 mle=1.5771 pcon=4.9132 forget=1.3961 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=7.9656 mle=1.6416 pcon=4.9118 forget=1.4122 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=7.7657 mle=1.4530 pcon=4.9110 forget=1.4017 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=8.0832 mle=1.7748 pcon=4.9097 forget=1.3987 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=8.0446 mle=1.7429 pcon=4.9082 forget=1.3935 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=7.8528 mle=1.5553 pcon=4.9074 forget=1.3901 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=7.9784 mle=1.6585 pcon=4.9063 forget=1.4136 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=7.9330 mle=1.6176 pcon=4.9051 forget=1.4102 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=7.8609 mle=1.5489 pcon=4.9040 forget=1.4080 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=7.9160 mle=1.6096 pcon=4.9029 forget=1.4036 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=7.9478 mle=1.6374 pcon=4.9019 forget=1.4085 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=8.0672 mle=1.7687 pcon=4.9009 forget=1.3977 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=7.9901 mle=1.6749 pcon=4.9001 forget=1.4151 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 40 total=7.7870 mle=1.4633 pcon=4.8994 forget=1.4243 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=8.0092 mle=1.6845 pcon=4.8987 forget=1.4260 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=7.8587 mle=1.5509 pcon=4.8977 forget=1.4102 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=7.9410 mle=1.6391 pcon=4.8966 forget=1.4053 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=8.1680 mle=1.8647 pcon=4.8958 forget=1.4075 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=7.8738 mle=1.5545 pcon=4.8952 forget=1.4241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=7.9040 mle=1.5760 pcon=4.8946 forget=1.4334 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:51,  3.50it/s]  3%|▎         | 11/391 [00:00<00:10, 35.06it/s]  5%|▌         | 21/391 [00:00<00:06, 54.87it/s]  8%|▊         | 31/391 [00:00<00:05, 66.44it/s] 10%|█         | 41/391 [00:00<00:04, 74.64it/s] 13%|█▎        | 51/391 [00:00<00:04, 80.61it/s] 16%|█▌        | 61/391 [00:00<00:03, 84.81it/s] 18%|█▊        | 71/391 [00:01<00:03, 87.91it/s] 21%|██        | 81/391 [00:01<00:03, 89.92it/s] 23%|██▎       | 91/391 [00:01<00:03, 91.37it/s] 26%|██▌       | 101/391 [00:01<00:03, 92.25it/s] 28%|██▊       | 111/391 [00:01<00:03, 92.90it/s] 31%|███       | 121/391 [00:01<00:02, 93.33it/s] 34%|███▎      | 131/391 [00:01<00:02, 93.90it/s] 36%|███▌      | 141/391 [00:01<00:02, 93.70it/s] 39%|███▊      | 151/391 [00:01<00:02, 94.05it/s] 41%|████      | 161/391 [00:01<00:02, 94.44it/s] 44%|████▎     | 171/391 [00:02<00:02, 94.56it/s] 46%|████▋     | 181/391 [00:02<00:02, 94.56it/s] 49%|████▉     | 191/391 [00:02<00:02, 94.79it/s] 51%|█████▏    | 201/391 [00:02<00:02, 94.64it/s] 54%|█████▍    | 211/391 [00:02<00:01, 94.62it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.73it/s] 59%|█████▉    | 231/391 [00:02<00:01, 94.73it/s] 62%|██████▏   | 241/391 [00:02<00:01, 94.94it/s] 64%|██████▍   | 251/391 [00:02<00:01, 94.83it/s] 67%|██████▋   | 261/391 [00:03<00:01, 95.00it/s] 69%|██████▉   | 271/391 [00:03<00:01, 94.97it/s] 72%|███████▏  | 281/391 [00:03<00:01, 94.77it/s] 74%|███████▍  | 291/391 [00:03<00:01, 94.54it/s] 77%|███████▋  | 301/391 [00:03<00:00, 93.38it/s] 80%|███████▉  | 311/391 [00:03<00:00, 93.79it/s] 82%|████████▏ | 321/391 [00:03<00:00, 94.25it/s] 85%|████████▍ | 331/391 [00:03<00:00, 94.67it/s] 87%|████████▋ | 341/391 [00:03<00:00, 94.79it/s] 90%|████████▉ | 351/391 [00:03<00:00, 94.81it/s] 92%|█████████▏| 361/391 [00:04<00:00, 94.82it/s] 95%|█████████▍| 371/391 [00:04<00:00, 94.82it/s] 97%|█████████▋| 381/391 [00:04<00:00, 94.59it/s]100%|██████████| 391/391 [00:04<00:00, 93.38it/s]100%|██████████| 391/391 [00:04<00:00, 88.45it/s]
50000 images processed, 4.4915406703948975 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:21,  3.67it/s] 14%|█▍        | 11/79 [00:00<00:01, 35.88it/s] 27%|██▋       | 21/79 [00:00<00:01, 55.67it/s] 39%|███▉      | 31/79 [00:00<00:00, 68.44it/s] 52%|█████▏    | 41/79 [00:00<00:00, 76.73it/s] 65%|██████▍   | 51/79 [00:00<00:00, 82.15it/s] 77%|███████▋  | 61/79 [00:00<00:00, 85.99it/s] 90%|████████▉ | 71/79 [00:01<00:00, 88.15it/s]100%|██████████| 79/79 [00:01<00:00, 71.30it/s]
10000 images processed, 1.1315035820007324 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:08,  2.96it/s]  5%|▍         | 10/204 [00:00<00:06, 28.30it/s] 10%|▉         | 20/204 [00:00<00:03, 48.52it/s] 15%|█▍        | 30/204 [00:00<00:02, 62.28it/s] 20%|█▉        | 40/204 [00:00<00:02, 71.64it/s] 25%|██▍       | 50/204 [00:00<00:01, 78.38it/s] 29%|██▉       | 60/204 [00:00<00:01, 82.78it/s] 34%|███▍      | 70/204 [00:01<00:01, 85.83it/s] 39%|███▉      | 80/204 [00:01<00:01, 87.90it/s] 44%|████▍     | 90/204 [00:01<00:01, 89.56it/s] 49%|████▉     | 100/204 [00:01<00:01, 90.22it/s] 54%|█████▍    | 110/204 [00:01<00:01, 91.04it/s] 59%|█████▉    | 120/204 [00:01<00:00, 90.83it/s] 64%|██████▎   | 130/204 [00:01<00:00, 91.53it/s] 69%|██████▊   | 140/204 [00:01<00:00, 92.27it/s] 74%|███████▎  | 150/204 [00:01<00:00, 92.53it/s] 78%|███████▊  | 160/204 [00:02<00:00, 92.44it/s] 83%|████████▎ | 170/204 [00:02<00:00, 92.46it/s] 88%|████████▊ | 180/204 [00:02<00:00, 92.43it/s] 93%|█████████▎| 190/204 [00:02<00:00, 92.58it/s] 98%|█████████▊| 200/204 [00:02<00:00, 93.56it/s]100%|██████████| 204/204 [00:02<00:00, 80.50it/s]
26032 images processed, 2.5804824829101562 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:37,  2.09it/s] 11%|█▏        | 9/79 [00:00<00:03, 17.71it/s] 22%|██▏       | 17/79 [00:00<00:02, 27.96it/s] 30%|███       | 24/79 [00:00<00:01, 36.53it/s] 37%|███▋      | 29/79 [00:01<00:01, 35.49it/s] 43%|████▎     | 34/79 [00:01<00:01, 34.93it/s] 53%|█████▎    | 42/79 [00:01<00:00, 38.58it/s] 63%|██████▎   | 50/79 [00:01<00:00, 42.28it/s] 73%|███████▎  | 58/79 [00:01<00:00, 45.32it/s] 84%|████████▎ | 66/79 [00:01<00:00, 46.01it/s] 94%|█████████▎| 74/79 [00:02<00:00, 48.15it/s]100%|██████████| 79/79 [00:02<00:00, 38.50it/s]
10000 images processed, 2.0903501510620117 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:29,  2.65it/s] 14%|█▍        | 11/79 [00:00<00:02, 28.95it/s] 27%|██▋       | 21/79 [00:00<00:01, 47.97it/s] 39%|███▉      | 31/79 [00:00<00:00, 61.55it/s] 52%|█████▏    | 41/79 [00:00<00:00, 71.34it/s] 65%|██████▍   | 51/79 [00:00<00:00, 78.26it/s] 77%|███████▋  | 61/79 [00:01<00:00, 82.87it/s] 90%|████████▉ | 71/79 [00:01<00:00, 86.64it/s]100%|██████████| 79/79 [00:01<00:00, 66.15it/s]
10000 images processed, 1.216961145401001 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:23,  2.93it/s] 16%|█▌        | 11/70 [00:00<00:01, 30.92it/s] 30%|███       | 21/70 [00:00<00:00, 50.26it/s] 44%|████▍     | 31/70 [00:00<00:00, 63.72it/s] 59%|█████▊    | 41/70 [00:00<00:00, 73.04it/s] 73%|███████▎  | 51/70 [00:00<00:00, 79.57it/s] 87%|████████▋ | 61/70 [00:00<00:00, 84.35it/s]100%|██████████| 70/70 [00:01<00:00, 65.10it/s]
8925 images processed, 1.1081383228302002 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:42,  1.04it/s]  7%|▋         | 3/45 [00:01<00:12,  3.46it/s] 20%|██        | 9/45 [00:01<00:04,  8.78it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.07it/s] 31%|███       | 14/45 [00:01<00:02, 11.03it/s] 38%|███▊      | 17/45 [00:02<00:02, 11.92it/s] 42%|████▏     | 19/45 [00:02<00:02, 12.37it/s] 47%|████▋     | 21/45 [00:02<00:02, 11.68it/s] 51%|█████     | 23/45 [00:02<00:01, 12.24it/s] 58%|█████▊    | 26/45 [00:02<00:01, 12.95it/s] 64%|██████▍   | 29/45 [00:02<00:01, 11.71it/s] 69%|██████▉   | 31/45 [00:03<00:01, 11.10it/s] 73%|███████▎  | 33/45 [00:03<00:00, 12.43it/s] 78%|███████▊  | 35/45 [00:03<00:00, 12.90it/s] 87%|████████▋ | 39/45 [00:03<00:00, 13.20it/s] 93%|█████████▎| 42/45 [00:04<00:00,  9.71it/s]100%|██████████| 45/45 [00:04<00:00, 10.54it/s]
5640 images processed, 4.300341844558716 seconds used

18.689441919326782
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.16  99.28
places365     70.23  80.52
LSUN          21.09  95.66
iSUN          74.49  80.71
dtd           41.05  90.71
forget        75.00  87.32
AVG           47.50  89.03
Retain-Acc: 0.7401
Forget-as-OOD (retain known vs forget novel):
  FPR: 75.00 AUROC: 87.32 AUIN: 98.40
9.138234615325928
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.2 lr=0.001 epochs=50
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.2, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:16<13:52, 16.98s/it]  4%|▍         | 2/50 [00:30<12:07, 15.16s/it]  6%|▌         | 3/50 [00:44<11:23, 14.54s/it]  8%|▊         | 4/50 [00:58<11:01, 14.38s/it] 10%|█         | 5/50 [01:12<10:38, 14.19s/it] 12%|█▏        | 6/50 [01:26<10:25, 14.22s/it] 14%|█▍        | 7/50 [01:41<10:13, 14.26s/it] 16%|█▌        | 8/50 [01:55<10:00, 14.30s/it][loss] ep 0 it 0 total=8.2415 mle=1.5709 pcon=5.2950 forget=1.3755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=8.2318 mle=1.5425 pcon=5.2879 forget=1.4013 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=8.3552 mle=1.7003 pcon=5.2809 forget=1.3740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8.5434 mle=1.8999 pcon=5.2738 forget=1.3696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=8.3627 mle=1.7131 pcon=5.2670 forget=1.3826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=8.1408 mle=1.5021 pcon=5.2603 forget=1.3784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=8.1956 mle=1.5596 pcon=5.2541 forget=1.3819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=8.3183 mle=1.6810 pcon=5.2476 forget=1.3896 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=8.3225 mle=1.6716 pcon=5.2409 forget=1.4100 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=8.2386 mle=1.6371 pcon=5.2346 forget=1.3670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=8.1346 mle=1.5166 pcon=5.2284 forget=1.3895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=8.3920 mle=1.7802 pcon=5.2224 forget=1.3894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=8.3665 mle=1.7645 pcon=5.2167 forget=1.3853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=8.1848 mle=1.6053 pcon=5.2112 forget=1.3683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=8.3075 mle=1.7237 pcon=5.2056 forget=1.3782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=8.3806 mle=1.7942 pcon=5.2003 forget=1.3861 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=8.1247 mle=1.5529 pcon=5.1950 forget=1.3768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=8.4355 mle=1.8880 pcon=5.1899 forget=1.3576 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=8.3024 mle=1.7419 pcon=5.1847 forget=1.3758 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=8.0707 mle=1.5262 pcon=5.1796 forget=1.3648 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=8.1461 mle=1.5857 pcon=5.1745 forget=1.3859 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=8.4244 mle=1.8839 pcon=5.1699 forget=1.3706 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=8.1149 mle=1.5922 pcon=5.1652 forget=1.3575 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=8.3363 mle=1.8111 pcon=5.1605 forget=1.3647 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=8.2321 mle=1.7056 pcon=5.1559 forget=1.3706 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=8.2019 mle=1.6709 pcon=5.1519 forget=1.3791 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=8.2801 mle=1.7669 pcon=5.1475 forget=1.3657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=8.4239 mle=1.9183 pcon=5.1435 forget=1.3621 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=8.0691 mle=1.5699 pcon=5.1395 forget=1.3597 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=8.3269 mle=1.8356 pcon=5.1353 forget=1.3560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=8.2091 mle=1.7029 pcon=5.1311 forget=1.3752 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=8.2006 mle=1.6951 pcon=5.1275 forget=1.3780 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=8.1192 mle=1.6536 pcon=5.1236 forget=1.3419 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=8.2049 mle=1.7293 pcon=5.1196 forget=1.3560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8.1322 mle=1.6504 pcon=5.1159 forget=1.3659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.9315 mle=1.4535 pcon=5.1125 forget=1.3655 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=8.2313 mle=1.7666 pcon=5.1086 forget=1.3561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.9497 mle=1.4916 pcon=5.1050 forget=1.3530 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=8.3376 mle=1.8713 pcon=5.1016 forget=1.3647 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=8.3223 mle=1.8670 pcon=5.0982 forget=1.3570 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=8.2403 mle=1.8001 pcon=5.0946 forget=1.3456 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=8.0353 mle=1.5841 pcon=5.0914 forget=1.3598 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=8.0413 mle=1.5950 pcon=5.0879 forget=1.3584 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=8.1210 mle=1.6908 pcon=5.0850 forget=1.3453 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=8.1942 mle=1.7738 pcon=5.0820 forget=1.3384 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=8.0257 mle=1.5965 pcon=5.0789 forget=1.3503 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=8.1711 mle=1.7468 pcon=5.0760 forget=1.3483 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.8448 mle=1.4002 pcon=5.0731 forget=1.3715 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.9328 mle=1.5345 pcon=5.0703 forget=1.3280 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=8.0194 mle=1.5913 pcon=5.0677 forget=1.3605 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=8.0753 mle=1.6656 pcon=5.0649 forget=1.3448 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=8.0329 mle=1.6295 pcon=5.0626 forget=1.3408 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=8.2357 mle=1.8212 pcon=5.0597 forget=1.3549 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.9567 mle=1.5367 pcon=5.0573 forget=1.3627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=8.0747 mle=1.6737 pcon=5.0547 forget=1.3463 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=8.0212 mle=1.6310 pcon=5.0521 forget=1.3381 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=8.1263 mle=1.7429 pcon=5.0497 forget=1.3337 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=8.0119 mle=1.6249 pcon=5.0470 forget=1.3399 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.9522 mle=1.5801 pcon=5.0447 forget=1.3275 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=8.0429 mle=1.6503 pcon=5.0426 forget=1.3500 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=8.1940 mle=1.7947 pcon=5.0404 forget=1.3590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=8.1350 mle=1.7407 pcon=5.0378 forget=1.3565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=8.0217 mle=1.6430 pcon=5.0357 forget=1.3430 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 8 it 30 total=7.9263 mle=1.5364 pcon=5.0337 forget=1.3562 nr=64 nf=64 protos=540 fproto_sim=NA
 18%|█▊        | 9/50 [02:10<09:49, 14.37s/it] 20%|██        | 10/50 [02:24<09:35, 14.38s/it] 22%|██▏       | 11/50 [02:39<09:22, 14.43s/it] 24%|██▍       | 12/50 [02:53<09:06, 14.39s/it] 26%|██▌       | 13/50 [03:07<08:52, 14.38s/it] 28%|██▊       | 14/50 [03:22<08:36, 14.33s/it] 30%|███       | 15/50 [03:36<08:23, 14.39s/it] 32%|███▏      | 16/50 [03:51<08:10, 14.44s/it][loss] ep 8 it 80 total=7.9650 mle=1.5881 pcon=5.0317 forget=1.3453 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.8851 mle=1.5043 pcon=5.0294 forget=1.3515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=8.0656 mle=1.7021 pcon=5.0274 forget=1.3362 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.8904 mle=1.5304 pcon=5.0257 forget=1.3344 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=8.0252 mle=1.6769 pcon=5.0236 forget=1.3247 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=8.1360 mle=1.7549 pcon=5.0213 forget=1.3598 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.8296 mle=1.4833 pcon=5.0193 forget=1.3270 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.9581 mle=1.6319 pcon=5.0171 forget=1.3091 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=8.0126 mle=1.6675 pcon=5.0148 forget=1.3302 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=8.1875 mle=1.8313 pcon=5.0131 forget=1.3431 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.9054 mle=1.5696 pcon=5.0112 forget=1.3246 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.9098 mle=1.5694 pcon=5.0094 forget=1.3310 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.9043 mle=1.5627 pcon=5.0072 forget=1.3344 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.8947 mle=1.5645 pcon=5.0054 forget=1.3248 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=7.9421 mle=1.6074 pcon=5.0033 forget=1.3315 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=7.9313 mle=1.5874 pcon=5.0011 forget=1.3428 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=8.0733 mle=1.7527 pcon=4.9992 forget=1.3214 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=8.1518 mle=1.8400 pcon=4.9974 forget=1.3144 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=8.1651 mle=1.8500 pcon=4.9954 forget=1.3197 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=7.9048 mle=1.5856 pcon=4.9936 forget=1.3257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=7.8614 mle=1.5588 pcon=4.9918 forget=1.3108 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=7.9838 mle=1.6509 pcon=4.9899 forget=1.3430 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=7.8873 mle=1.5618 pcon=4.9879 forget=1.3377 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=7.9171 mle=1.6152 pcon=4.9859 forget=1.3160 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=7.8694 mle=1.5729 pcon=4.9841 forget=1.3124 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=7.9655 mle=1.6603 pcon=4.9818 forget=1.3234 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=8.0868 mle=1.7849 pcon=4.9801 forget=1.3218 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=7.9106 mle=1.6007 pcon=4.9786 forget=1.3312 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=8.2939 mle=1.9901 pcon=4.9766 forget=1.3272 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=7.9557 mle=1.6652 pcon=4.9748 forget=1.3158 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 12 it 20 total=7.9247 mle=1.6202 pcon=4.9727 forget=1.3318 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=8.0008 mle=1.6998 pcon=4.9709 forget=1.3301 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=7.8728 mle=1.5528 pcon=4.9691 forget=1.3509 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=8.0426 mle=1.7482 pcon=4.9674 forget=1.3271 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=7.8748 mle=1.5741 pcon=4.9655 forget=1.3351 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=7.8426 mle=1.5285 pcon=4.9642 forget=1.3500 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=7.7907 mle=1.4899 pcon=4.9625 forget=1.3383 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=8.1667 mle=1.8502 pcon=4.9608 forget=1.3558 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=7.7332 mle=1.4257 pcon=4.9592 forget=1.3483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=7.8125 mle=1.4707 pcon=4.9576 forget=1.3842 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=7.8581 mle=1.5450 pcon=4.9558 forget=1.3572 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=7.8378 mle=1.5380 pcon=4.9543 forget=1.3454 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=7.9730 mle=1.6643 pcon=4.9528 forget=1.3559 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=7.8525 mle=1.5535 pcon=4.9512 forget=1.3478 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=7.8903 mle=1.5751 pcon=4.9496 forget=1.3657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=7.8877 mle=1.5796 pcon=4.9480 forget=1.3601 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=8.0766 mle=1.7738 pcon=4.9463 forget=1.3565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=8.0916 mle=1.7757 pcon=4.9449 forget=1.3710 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=7.8874 mle=1.5731 pcon=4.9438 forget=1.3705 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=8.1469 mle=1.8355 pcon=4.9420 forget=1.3694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=7.9404 mle=1.6276 pcon=4.9406 forget=1.3721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=7.9243 mle=1.6218 pcon=4.9389 forget=1.3636 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=7.9944 mle=1.6859 pcon=4.9372 forget=1.3714 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 0 total=8.0253 mle=1.6988 pcon=4.9356 forget=1.3909 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=7.7741 mle=1.4638 pcon=4.9336 forget=1.3767 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=7.8754 mle=1.5616 pcon=4.9317 forget=1.3821 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=8.0265 mle=1.6947 pcon=4.9297 forget=1.4021 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=7.9156 mle=1.6067 pcon=4.9277 forget=1.3812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=8.2510 mle=1.9287 pcon=4.9256 forget=1.3968 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=7.8864 mle=1.5684 pcon=4.9235 forget=1.3945 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=7.8281 mle=1.5095 pcon=4.9213 forget=1.3973 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 16 it 10 total=7.8894 mle=1.5714 pcon=4.9188 forget=1.3992 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=7.8042 mle=1.5007 pcon=4.9164 forget=1.3871 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=7.8295 mle=1.5255 pcon=4.9141 forget=1.3899 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=7.9176 mle=1.6160 pcon=4.9111 forget=1.3904 nr=64 nf=64 protos=540 fproto_sim=NA
 34%|███▍      | 17/50 [04:05<07:58, 14.50s/it] 36%|███▌      | 18/50 [04:20<07:45, 14.56s/it] 38%|███▊      | 19/50 [04:34<07:29, 14.51s/it] 40%|████      | 20/50 [04:49<07:13, 14.44s/it] 42%|████▏     | 21/50 [05:03<06:59, 14.47s/it] 44%|████▍     | 22/50 [05:17<06:43, 14.41s/it] 46%|████▌     | 23/50 [05:32<06:30, 14.45s/it] 48%|████▊     | 24/50 [05:47<06:16, 14.50s/it][loss] ep 16 it 210 total=7.8593 mle=1.5698 pcon=4.9081 forget=1.3814 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=8.1410 mle=1.8514 pcon=4.9050 forget=1.3845 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=8.0337 mle=1.7391 pcon=4.9021 forget=1.3926 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=7.8777 mle=1.5903 pcon=4.8991 forget=1.3883 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 17 it 20 total=8.0553 mle=1.7678 pcon=4.8957 forget=1.3917 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=8.0249 mle=1.7467 pcon=4.8928 forget=1.3855 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=7.8323 mle=1.5465 pcon=4.8895 forget=1.3962 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=7.8141 mle=1.5431 pcon=4.8859 forget=1.3851 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=7.8693 mle=1.5841 pcon=4.8825 forget=1.4027 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=7.7236 mle=1.4618 pcon=4.8796 forget=1.3822 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=8.0169 mle=1.7632 pcon=4.8764 forget=1.3774 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=7.9753 mle=1.7270 pcon=4.8728 forget=1.3754 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 18 it 30 total=7.7416 mle=1.5052 pcon=4.8699 forget=1.3664 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=7.8474 mle=1.5964 pcon=4.8668 forget=1.3842 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=7.8589 mle=1.6092 pcon=4.8636 forget=1.3861 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=7.7680 mle=1.5370 pcon=4.8605 forget=1.3705 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=7.8990 mle=1.6712 pcon=4.8573 forget=1.3705 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=7.8288 mle=1.6077 pcon=4.8543 forget=1.3669 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=7.9157 mle=1.7137 pcon=4.8513 forget=1.3507 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=7.9006 mle=1.6753 pcon=4.8485 forget=1.3767 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 19 it 40 total=7.7752 mle=1.5487 pcon=4.8458 forget=1.3807 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=7.8063 mle=1.5900 pcon=4.8431 forget=1.3732 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=7.7618 mle=1.5747 pcon=4.8402 forget=1.3469 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=7.8099 mle=1.6308 pcon=4.8374 forget=1.3417 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=7.9690 mle=1.8009 pcon=4.8348 forget=1.3333 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=7.8009 mle=1.6336 pcon=4.8325 forget=1.3348 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=7.7057 mle=1.5335 pcon=4.8300 forget=1.3422 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 20 it 0 total=7.7771 mle=1.6318 pcon=4.8274 forget=1.3178 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=7.7708 mle=1.6472 pcon=4.8249 forget=1.2987 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=7.8295 mle=1.6969 pcon=4.8226 forget=1.3100 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=7.8734 mle=1.7670 pcon=4.8200 forget=1.2864 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=7.6317 mle=1.5387 pcon=4.8174 forget=1.2756 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=7.9663 mle=1.8836 pcon=4.8149 forget=1.2678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=7.8267 mle=1.7395 pcon=4.8123 forget=1.2749 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=7.5411 mle=1.4721 pcon=4.8098 forget=1.2591 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 21 it 10 total=7.5027 mle=1.4585 pcon=4.8074 forget=1.2368 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=7.6453 mle=1.6122 pcon=4.8049 forget=1.2282 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=7.6015 mle=1.5624 pcon=4.8026 forget=1.2365 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=7.5898 mle=1.5695 pcon=4.8003 forget=1.2200 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=7.7168 mle=1.7126 pcon=4.7979 forget=1.2063 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=7.6300 mle=1.6377 pcon=4.7955 forget=1.1968 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=7.5440 mle=1.5473 pcon=4.7928 forget=1.2039 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=7.5300 mle=1.5401 pcon=4.7902 forget=1.1997 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 22 it 20 total=7.5736 mle=1.5904 pcon=4.7875 forget=1.1956 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=7.6374 mle=1.6682 pcon=4.7849 forget=1.1843 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=7.5281 mle=1.5461 pcon=4.7821 forget=1.1999 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=7.5640 mle=1.5847 pcon=4.7795 forget=1.1997 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=7.5489 mle=1.5606 pcon=4.7771 forget=1.2112 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=7.4525 mle=1.4697 pcon=4.7744 forget=1.2084 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=7.6679 mle=1.6941 pcon=4.7720 forget=1.2017 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=7.8466 mle=1.8570 pcon=4.7693 forget=1.2203 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 23 it 30 total=7.4884 mle=1.5019 pcon=4.7672 forget=1.2194 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=7.5894 mle=1.5979 pcon=4.7646 forget=1.2268 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=7.5364 mle=1.5417 pcon=4.7621 forget=1.2326 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=7.6727 mle=1.6809 pcon=4.7596 forget=1.2323 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=7.5754 mle=1.5759 pcon=4.7574 forget=1.2421 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=7.5840 mle=1.5715 pcon=4.7550 forget=1.2576 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=7.6270 mle=1.6193 pcon=4.7526 forget=1.2550 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=7.5533 mle=1.5374 pcon=4.7503 forget=1.2656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 40 total=7.5517 mle=1.5352 pcon=4.7482 forget=1.2684 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=7.6108 mle=1.5787 pcon=4.7461 forget=1.2861 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=7.5950 mle=1.5512 pcon=4.7439 forget=1.2999 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=7.6485 mle=1.6067 pcon=4.7420 forget=1.2998 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 240 total=7.5246 mle=1.4773 pcon=4.7401 forget=1.3071 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=7.7264 mle=1.6748 pcon=4.7383 forget=1.3133 nr=64 nf=64 protos=540 fproto_sim=NA
 50%|█████     | 25/50 [06:01<06:01, 14.46s/it] 52%|█████▏    | 26/50 [06:15<05:46, 14.44s/it] 54%|█████▍    | 27/50 [06:30<05:33, 14.49s/it] 56%|█████▌    | 28/50 [06:44<05:17, 14.44s/it] 58%|█████▊    | 29/50 [06:59<05:02, 14.41s/it] 60%|██████    | 30/50 [07:13<04:47, 14.38s/it] 62%|██████▏   | 31/50 [07:27<04:32, 14.36s/it] 64%|██████▍   | 32/50 [07:41<04:17, 14.33s/it] 66%|██████▌   | 33/50 [07:56<04:03, 14.32s/it] 68%|██████▊   | 34/50 [08:10<03:48, 14.27s/it][loss] ep 24 it 340 total=7.6277 mle=1.5744 pcon=4.7365 forget=1.3168 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 0 total=7.6472 mle=1.5873 pcon=4.7347 forget=1.3251 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=7.7830 mle=1.7086 pcon=4.7332 forget=1.3413 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=7.6712 mle=1.6109 pcon=4.7315 forget=1.3287 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 150 total=7.7847 mle=1.7169 pcon=4.7301 forget=1.3377 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 200 total=7.8484 mle=1.7749 pcon=4.7284 forget=1.3451 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=7.7794 mle=1.6964 pcon=4.7269 forget=1.3561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=7.6705 mle=1.5920 pcon=4.7254 forget=1.3530 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=7.5683 mle=1.4867 pcon=4.7243 forget=1.3573 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 10 total=7.6891 mle=1.6021 pcon=4.7231 forget=1.3640 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=7.6245 mle=1.5365 pcon=4.7218 forget=1.3662 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=7.7752 mle=1.6907 pcon=4.7207 forget=1.3639 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=7.5894 mle=1.4983 pcon=4.7195 forget=1.3717 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=7.7004 mle=1.6130 pcon=4.7181 forget=1.3693 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=7.5776 mle=1.4824 pcon=4.7171 forget=1.3781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 310 total=7.9242 mle=1.8382 pcon=4.7160 forget=1.3700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=7.6270 mle=1.5452 pcon=4.7148 forget=1.3669 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 20 total=7.7187 mle=1.6335 pcon=4.7138 forget=1.3714 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=7.6253 mle=1.5416 pcon=4.7126 forget=1.3711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=7.6304 mle=1.5442 pcon=4.7115 forget=1.3746 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=7.6134 mle=1.5268 pcon=4.7104 forget=1.3762 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=7.5533 mle=1.4687 pcon=4.7093 forget=1.3753 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=7.6463 mle=1.5663 pcon=4.7082 forget=1.3718 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=7.7670 mle=1.6862 pcon=4.7073 forget=1.3736 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=7.7719 mle=1.6795 pcon=4.7064 forget=1.3860 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 30 total=7.7145 mle=1.6489 pcon=4.7056 forget=1.3600 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=7.7277 mle=1.6484 pcon=4.7048 forget=1.3745 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=7.6128 mle=1.5336 pcon=4.7041 forget=1.3751 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=7.5936 mle=1.5283 pcon=4.7032 forget=1.3620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=7.5009 mle=1.4345 pcon=4.7022 forget=1.3642 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=7.6152 mle=1.5471 pcon=4.7013 forget=1.3669 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=7.8231 mle=1.7543 pcon=4.7002 forget=1.3685 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=7.7268 mle=1.6632 pcon=4.6993 forget=1.3643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 40 total=7.7230 mle=1.6725 pcon=4.6983 forget=1.3522 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=7.5776 mle=1.5248 pcon=4.6973 forget=1.3555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=7.7102 mle=1.6583 pcon=4.6964 forget=1.3554 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=7.6396 mle=1.5923 pcon=4.6956 forget=1.3517 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=7.6129 mle=1.5638 pcon=4.6948 forget=1.3543 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=7.7110 mle=1.6676 pcon=4.6941 forget=1.3493 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=7.5869 mle=1.5433 pcon=4.6934 forget=1.3502 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 0 total=7.6274 mle=1.5934 pcon=4.6926 forget=1.3414 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=7.6343 mle=1.5917 pcon=4.6920 forget=1.3506 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=7.5911 mle=1.5532 pcon=4.6913 forget=1.3466 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=7.7580 mle=1.7284 pcon=4.6905 forget=1.3391 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=7.5299 mle=1.4821 pcon=4.6897 forget=1.3581 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=7.7757 mle=1.7390 pcon=4.6890 forget=1.3478 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=7.4999 mle=1.4631 pcon=4.6883 forget=1.3486 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=7.7364 mle=1.7071 pcon=4.6876 forget=1.3416 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 10 total=7.6430 mle=1.6258 pcon=4.6870 forget=1.3302 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=7.5534 mle=1.5307 pcon=4.6864 forget=1.3363 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=7.5632 mle=1.5365 pcon=4.6858 forget=1.3409 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=7.5539 mle=1.5335 pcon=4.6851 forget=1.3353 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=7.5392 mle=1.5253 pcon=4.6844 forget=1.3295 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 260 total=7.5117 mle=1.4979 pcon=4.6837 forget=1.3301 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=7.5963 mle=1.5769 pcon=4.6830 forget=1.3364 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=7.5539 mle=1.5493 pcon=4.6823 forget=1.3223 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 20 total=7.6919 mle=1.6874 pcon=4.6816 forget=1.3229 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=7.5642 mle=1.5646 pcon=4.6808 forget=1.3188 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=7.5720 mle=1.5507 pcon=4.6803 forget=1.3410 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 170 total=7.5703 mle=1.5590 pcon=4.6798 forget=1.3316 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=7.6232 mle=1.6213 pcon=4.6792 forget=1.3226 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=7.5733 mle=1.5631 pcon=4.6787 forget=1.3314 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=7.4632 mle=1.4666 pcon=4.6781 forget=1.3185 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=7.4505 mle=1.4447 pcon=4.6777 forget=1.3281 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 30 total=7.6207 mle=1.6123 pcon=4.6773 forget=1.3312 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=7.8451 mle=1.8546 pcon=4.6768 forget=1.3137 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=7.4787 mle=1.4801 pcon=4.6762 forget=1.3224 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=7.6558 mle=1.6470 pcon=4.6757 forget=1.3331 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=7.6041 mle=1.6052 pcon=4.6752 forget=1.3237 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=7.6742 mle=1.6697 pcon=4.6747 forget=1.3297 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=7.6690 mle=1.6765 pcon=4.6743 forget=1.3182 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 380 total=7.5849 mle=1.5702 pcon=4.6738 forget=1.3409 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 40 total=7.6890 mle=1.6944 pcon=4.6731 forget=1.3214 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=7.5048 mle=1.4975 pcon=4.6726 forget=1.3347 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 140 total=7.5669 mle=1.5830 pcon=4.6721 forget=1.3118 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=7.6097 mle=1.6183 pcon=4.6717 forget=1.3198 nr=64 nf=64 protos=540 fproto_sim=NA
 70%|███████   | 35/50 [08:24<03:34, 14.28s/it] 72%|███████▏  | 36/50 [08:39<03:19, 14.27s/it] 74%|███████▍  | 37/50 [08:53<03:05, 14.27s/it] 76%|███████▌  | 38/50 [09:07<02:51, 14.29s/it] 78%|███████▊  | 39/50 [09:22<02:37, 14.32s/it] 80%|████████  | 40/50 [09:36<02:22, 14.30s/it] 82%|████████▏ | 41/50 [09:50<02:08, 14.30s/it] 84%|████████▍ | 42/50 [10:04<01:54, 14.34s/it] 86%|████████▌ | 43/50 [10:19<01:40, 14.29s/it] 88%|████████▊ | 44/50 [10:33<01:25, 14.30s/it][loss] ep 34 it 240 total=7.6635 mle=1.6697 pcon=4.6711 forget=1.3227 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=7.5902 mle=1.5968 pcon=4.6705 forget=1.3229 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=7.5299 mle=1.5259 pcon=4.6701 forget=1.3340 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 0 total=7.5700 mle=1.5771 pcon=4.6695 forget=1.3234 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 50 total=7.5603 mle=1.5676 pcon=4.6690 forget=1.3236 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 100 total=7.7936 mle=1.7942 pcon=4.6684 forget=1.3310 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 150 total=7.6079 mle=1.6190 pcon=4.6680 forget=1.3209 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 200 total=7.5394 mle=1.5389 pcon=4.6676 forget=1.3329 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 250 total=7.6850 mle=1.6870 pcon=4.6670 forget=1.3310 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 300 total=7.5266 mle=1.5373 pcon=4.6666 forget=1.3228 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 350 total=7.6826 mle=1.6934 pcon=4.6661 forget=1.3231 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 36 it 10 total=7.7490 mle=1.7557 pcon=4.6658 forget=1.3275 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 60 total=7.5112 mle=1.5247 pcon=4.6655 forget=1.3210 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 110 total=7.5539 mle=1.5581 pcon=4.6651 forget=1.3307 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 160 total=7.6180 mle=1.6263 pcon=4.6647 forget=1.3271 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 210 total=7.4889 mle=1.5015 pcon=4.6644 forget=1.3230 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 260 total=7.5995 mle=1.5913 pcon=4.6641 forget=1.3441 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 310 total=7.6389 mle=1.6279 pcon=4.6637 forget=1.3473 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 360 total=7.6226 mle=1.6256 pcon=4.6633 forget=1.3337 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 20 total=7.4794 mle=1.4857 pcon=4.6628 forget=1.3309 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 70 total=7.6187 mle=1.6175 pcon=4.6625 forget=1.3387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 120 total=7.6177 mle=1.6129 pcon=4.6621 forget=1.3427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 170 total=7.5719 mle=1.5778 pcon=4.6617 forget=1.3323 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 220 total=7.7191 mle=1.7126 pcon=4.6613 forget=1.3452 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 270 total=7.5839 mle=1.5930 pcon=4.6609 forget=1.3299 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 320 total=7.6314 mle=1.6312 pcon=4.6606 forget=1.3397 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 370 total=7.6224 mle=1.5994 pcon=4.6603 forget=1.3627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 30 total=7.6665 mle=1.6537 pcon=4.6602 forget=1.3527 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 80 total=7.4874 mle=1.4825 pcon=4.6600 forget=1.3449 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 130 total=7.6050 mle=1.5996 pcon=4.6597 forget=1.3458 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 180 total=7.5608 mle=1.5605 pcon=4.6594 forget=1.3409 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 230 total=7.5173 mle=1.5118 pcon=4.6591 forget=1.3463 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 280 total=7.6596 mle=1.6518 pcon=4.6589 forget=1.3488 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 330 total=7.5888 mle=1.5874 pcon=4.6585 forget=1.3429 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 380 total=7.4966 mle=1.4952 pcon=4.6584 forget=1.3430 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 40 total=7.6992 mle=1.6957 pcon=4.6583 forget=1.3452 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 90 total=7.6143 mle=1.5976 pcon=4.6581 forget=1.3585 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 140 total=7.5960 mle=1.5924 pcon=4.6580 forget=1.3456 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 190 total=7.4662 mle=1.4593 pcon=4.6578 forget=1.3492 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 240 total=7.6473 mle=1.6350 pcon=4.6576 forget=1.3547 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 290 total=7.4926 mle=1.4817 pcon=4.6575 forget=1.3534 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 340 total=7.5785 mle=1.5683 pcon=4.6573 forget=1.3529 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 0 total=7.5600 mle=1.5456 pcon=4.6570 forget=1.3573 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 50 total=7.6175 mle=1.6035 pcon=4.6569 forget=1.3571 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 100 total=7.6234 mle=1.6244 pcon=4.6565 forget=1.3425 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 150 total=7.4349 mle=1.4332 pcon=4.6564 forget=1.3453 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 200 total=7.7456 mle=1.7336 pcon=4.6562 forget=1.3559 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 250 total=7.7922 mle=1.7709 pcon=4.6560 forget=1.3654 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 300 total=7.5277 mle=1.5243 pcon=4.6558 forget=1.3475 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 350 total=7.4486 mle=1.4427 pcon=4.6557 forget=1.3501 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 10 total=7.4996 mle=1.5043 pcon=4.6556 forget=1.3397 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 60 total=7.8303 mle=1.8083 pcon=4.6556 forget=1.3663 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 110 total=7.5856 mle=1.5751 pcon=4.6554 forget=1.3551 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 160 total=7.5642 mle=1.5554 pcon=4.6554 forget=1.3534 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 210 total=7.6174 mle=1.6095 pcon=4.6553 forget=1.3525 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 260 total=7.5259 mle=1.5274 pcon=4.6554 forget=1.3431 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 310 total=7.5176 mle=1.5102 pcon=4.6555 forget=1.3519 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 360 total=7.6739 mle=1.6690 pcon=4.6554 forget=1.3495 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 20 total=7.6536 mle=1.6349 pcon=4.6554 forget=1.3633 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 70 total=7.5812 mle=1.5663 pcon=4.6551 forget=1.3597 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 120 total=7.8338 mle=1.8147 pcon=4.6550 forget=1.3641 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 170 total=7.6062 mle=1.5962 pcon=4.6549 forget=1.3551 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 220 total=7.6432 mle=1.6279 pcon=4.6548 forget=1.3605 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 270 total=7.6885 mle=1.6874 pcon=4.6547 forget=1.3464 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 320 total=7.6617 mle=1.6564 pcon=4.6547 forget=1.3506 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 370 total=7.5996 mle=1.5959 pcon=4.6547 forget=1.3489 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 30 total=7.6321 mle=1.6108 pcon=4.6545 forget=1.3667 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 80 total=7.5848 mle=1.5720 pcon=4.6544 forget=1.3584 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 130 total=7.6596 mle=1.6426 pcon=4.6543 forget=1.3627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 180 total=7.5034 mle=1.4766 pcon=4.6543 forget=1.3725 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 230 total=7.4991 mle=1.5055 pcon=4.6541 forget=1.3395 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 280 total=7.5177 mle=1.5145 pcon=4.6540 forget=1.3492 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 330 total=7.5389 mle=1.5250 pcon=4.6538 forget=1.3601 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 380 total=7.4838 mle=1.4855 pcon=4.6536 forget=1.3447 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 40 total=7.6752 mle=1.6557 pcon=4.6535 forget=1.3660 nr=64 nf=64 protos=540 fproto_sim=NA
 90%|█████████ | 45/50 [10:47<01:11, 14.22s/it] 92%|█████████▏| 46/50 [11:01<00:56, 14.09s/it] 94%|█████████▍| 47/50 [11:15<00:42, 14.14s/it] 96%|█████████▌| 48/50 [11:29<00:28, 14.03s/it] 98%|█████████▊| 49/50 [11:43<00:14, 14.04s/it]100%|██████████| 50/50 [11:57<00:00, 14.02s/it]100%|██████████| 50/50 [11:57<00:00, 14.35s/it]
[loss] ep 44 it 90 total=7.5392 mle=1.5344 pcon=4.6534 forget=1.3514 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 140 total=7.6543 mle=1.6369 pcon=4.6533 forget=1.3640 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 190 total=7.5082 mle=1.5020 pcon=4.6532 forget=1.3530 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 240 total=7.5487 mle=1.5222 pcon=4.6532 forget=1.3733 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 290 total=7.7601 mle=1.7574 pcon=4.6531 forget=1.3495 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 340 total=7.7853 mle=1.7716 pcon=4.6530 forget=1.3607 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 0 total=7.5672 mle=1.5471 pcon=4.6529 forget=1.3672 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 50 total=7.5215 mle=1.5199 pcon=4.6529 forget=1.3488 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 100 total=7.5551 mle=1.5436 pcon=4.6528 forget=1.3588 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 150 total=7.5331 mle=1.5274 pcon=4.6526 forget=1.3530 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 200 total=7.5893 mle=1.5727 pcon=4.6526 forget=1.3640 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 250 total=7.5370 mle=1.5290 pcon=4.6524 forget=1.3555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 300 total=7.6031 mle=1.6092 pcon=4.6522 forget=1.3416 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 350 total=7.5190 mle=1.5248 pcon=4.6522 forget=1.3420 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 10 total=7.4816 mle=1.4725 pcon=4.6521 forget=1.3570 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 60 total=7.5117 mle=1.4997 pcon=4.6520 forget=1.3600 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 110 total=7.6404 mle=1.6303 pcon=4.6519 forget=1.3582 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 160 total=7.4579 mle=1.4588 pcon=4.6519 forget=1.3473 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 210 total=7.7121 mle=1.7153 pcon=4.6517 forget=1.3451 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 260 total=7.6416 mle=1.6540 pcon=4.6516 forget=1.3360 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 310 total=7.5257 mle=1.5154 pcon=4.6516 forget=1.3587 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 360 total=7.6975 mle=1.6922 pcon=4.6515 forget=1.3537 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 20 total=7.5284 mle=1.5223 pcon=4.6513 forget=1.3548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 70 total=7.5189 mle=1.5247 pcon=4.6512 forget=1.3430 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 120 total=7.6666 mle=1.6555 pcon=4.6510 forget=1.3600 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 170 total=7.5586 mle=1.5369 pcon=4.6508 forget=1.3709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 220 total=7.4580 mle=1.4516 pcon=4.6508 forget=1.3556 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 270 total=7.5474 mle=1.5324 pcon=4.6508 forget=1.3642 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 320 total=7.5586 mle=1.5436 pcon=4.6507 forget=1.3643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 370 total=7.6259 mle=1.6237 pcon=4.6506 forget=1.3517 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 30 total=7.5737 mle=1.5491 pcon=4.6506 forget=1.3741 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 80 total=7.5828 mle=1.5635 pcon=4.6505 forget=1.3687 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 130 total=7.6229 mle=1.6318 pcon=4.6504 forget=1.3407 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 180 total=7.6507 mle=1.6334 pcon=4.6504 forget=1.3668 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 230 total=7.6419 mle=1.6519 pcon=4.6504 forget=1.3395 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 280 total=7.5359 mle=1.5470 pcon=4.6503 forget=1.3387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 330 total=7.5637 mle=1.5851 pcon=4.6502 forget=1.3284 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 380 total=7.5271 mle=1.5233 pcon=4.6500 forget=1.3538 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 40 total=7.5489 mle=1.5376 pcon=4.6500 forget=1.3614 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 90 total=7.6047 mle=1.5995 pcon=4.6500 forget=1.3553 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 140 total=7.6146 mle=1.6127 pcon=4.6500 forget=1.3519 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 190 total=7.7107 mle=1.7105 pcon=4.6497 forget=1.3506 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 240 total=7.6466 mle=1.6528 pcon=4.6496 forget=1.3442 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 290 total=7.5740 mle=1.5848 pcon=4.6494 forget=1.3398 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 340 total=7.5185 mle=1.5176 pcon=4.6495 forget=1.3514 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:43,  3.78it/s]  3%|▎         | 11/391 [00:00<00:10, 36.83it/s]  5%|▌         | 21/391 [00:00<00:06, 56.73it/s]  8%|▊         | 31/391 [00:00<00:05, 69.31it/s] 10%|█         | 41/391 [00:00<00:04, 77.60it/s] 13%|█▎        | 51/391 [00:00<00:04, 83.15it/s] 16%|█▌        | 61/391 [00:00<00:03, 87.09it/s] 18%|█▊        | 71/391 [00:00<00:03, 89.80it/s] 21%|██        | 81/391 [00:01<00:03, 91.56it/s] 23%|██▎       | 91/391 [00:01<00:03, 92.93it/s] 26%|██▌       | 101/391 [00:01<00:03, 93.77it/s] 28%|██▊       | 111/391 [00:01<00:02, 94.37it/s] 31%|███       | 121/391 [00:01<00:02, 94.74it/s] 34%|███▎      | 131/391 [00:01<00:02, 94.98it/s] 36%|███▌      | 141/391 [00:01<00:02, 95.16it/s] 39%|███▊      | 151/391 [00:01<00:02, 95.24it/s] 41%|████      | 161/391 [00:01<00:02, 95.32it/s] 44%|████▎     | 171/391 [00:02<00:02, 95.45it/s] 46%|████▋     | 181/391 [00:02<00:02, 95.61it/s] 49%|████▉     | 191/391 [00:02<00:02, 95.63it/s] 51%|█████▏    | 201/391 [00:02<00:01, 95.67it/s] 54%|█████▍    | 211/391 [00:02<00:01, 95.63it/s] 57%|█████▋    | 221/391 [00:02<00:01, 95.52it/s] 59%|█████▉    | 231/391 [00:02<00:01, 95.67it/s] 62%|██████▏   | 241/391 [00:02<00:01, 95.68it/s] 64%|██████▍   | 251/391 [00:02<00:01, 95.56it/s] 67%|██████▋   | 261/391 [00:02<00:01, 95.48it/s] 69%|██████▉   | 271/391 [00:03<00:01, 94.65it/s] 72%|███████▏  | 281/391 [00:03<00:01, 94.72it/s] 74%|███████▍  | 291/391 [00:03<00:01, 95.01it/s] 77%|███████▋  | 301/391 [00:03<00:00, 95.13it/s] 80%|███████▉  | 311/391 [00:03<00:00, 95.23it/s] 82%|████████▏ | 321/391 [00:03<00:00, 95.15it/s] 85%|████████▍ | 331/391 [00:03<00:00, 95.34it/s] 87%|████████▋ | 341/391 [00:03<00:00, 95.31it/s] 90%|████████▉ | 351/391 [00:03<00:00, 95.38it/s] 92%|█████████▏| 361/391 [00:04<00:00, 95.33it/s] 95%|█████████▍| 371/391 [00:04<00:00, 95.26it/s] 97%|█████████▋| 381/391 [00:04<00:00, 95.02it/s]100%|██████████| 391/391 [00:04<00:00, 91.84it/s]100%|██████████| 391/391 [00:04<00:00, 89.57it/s]
50000 images processed, 4.478513479232788 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:21,  3.59it/s] 14%|█▍        | 11/79 [00:00<00:01, 35.74it/s] 27%|██▋       | 21/79 [00:00<00:01, 55.38it/s] 39%|███▉      | 31/79 [00:00<00:00, 67.76it/s] 52%|█████▏    | 41/79 [00:00<00:00, 75.85it/s] 65%|██████▍   | 51/79 [00:00<00:00, 81.26it/s] 77%|███████▋  | 61/79 [00:00<00:00, 84.99it/s] 90%|████████▉ | 71/79 [00:01<00:00, 88.08it/s]100%|██████████| 79/79 [00:01<00:00, 70.72it/s]
10000 images processed, 1.140993356704712 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:18,  2.58it/s]  5%|▌         | 11/204 [00:00<00:06, 28.23it/s] 10%|█         | 21/204 [00:00<00:03, 46.49it/s] 15%|█▌        | 31/204 [00:00<00:02, 59.64it/s] 20%|██        | 41/204 [00:00<00:02, 68.96it/s] 25%|██▌       | 51/204 [00:00<00:02, 75.70it/s] 30%|██▉       | 61/204 [00:01<00:01, 80.59it/s] 35%|███▍      | 71/204 [00:01<00:01, 84.06it/s] 40%|███▉      | 81/204 [00:01<00:01, 86.46it/s] 45%|████▍     | 91/204 [00:01<00:01, 88.22it/s] 50%|████▉     | 101/204 [00:01<00:01, 89.26it/s] 54%|█████▍    | 111/204 [00:01<00:01, 90.17it/s] 59%|█████▉    | 121/204 [00:01<00:00, 90.91it/s] 64%|██████▍   | 131/204 [00:01<00:00, 91.60it/s] 69%|██████▉   | 141/204 [00:01<00:00, 92.35it/s] 74%|███████▍  | 151/204 [00:02<00:00, 93.09it/s] 79%|███████▉  | 161/204 [00:02<00:00, 93.74it/s] 84%|████████▍ | 171/204 [00:02<00:00, 94.28it/s] 89%|████████▊ | 181/204 [00:02<00:00, 93.83it/s] 94%|█████████▎| 191/204 [00:02<00:00, 93.79it/s] 99%|█████████▊| 201/204 [00:02<00:00, 94.18it/s]100%|██████████| 204/204 [00:02<00:00, 79.16it/s]
26032 images processed, 2.616091012954712 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:42,  1.82it/s] 11%|█▏        | 9/79 [00:00<00:04, 16.39it/s] 22%|██▏       | 17/79 [00:00<00:02, 26.29it/s] 32%|███▏      | 25/79 [00:01<00:01, 33.17it/s] 42%|████▏     | 33/79 [00:01<00:01, 37.88it/s] 52%|█████▏    | 41/79 [00:01<00:00, 41.47it/s] 62%|██████▏   | 49/79 [00:01<00:00, 43.81it/s] 72%|███████▏  | 57/79 [00:01<00:00, 44.48it/s] 82%|████████▏ | 65/79 [00:01<00:00, 46.01it/s] 91%|█████████ | 72/79 [00:01<00:00, 50.89it/s] 99%|█████████▊| 78/79 [00:02<00:00, 51.37it/s]100%|██████████| 79/79 [00:02<00:00, 38.27it/s]
10000 images processed, 2.0998246669769287 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:28,  2.70it/s] 14%|█▍        | 11/79 [00:00<00:02, 29.19it/s] 27%|██▋       | 21/79 [00:00<00:01, 48.03it/s] 39%|███▉      | 31/79 [00:00<00:00, 61.35it/s] 52%|█████▏    | 41/79 [00:00<00:00, 70.98it/s] 65%|██████▍   | 51/79 [00:00<00:00, 77.78it/s] 77%|███████▋  | 61/79 [00:01<00:00, 82.63it/s] 90%|████████▉ | 71/79 [00:01<00:00, 86.39it/s]100%|██████████| 79/79 [00:01<00:00, 66.17it/s]
10000 images processed, 1.2151787281036377 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:28,  2.43it/s] 16%|█▌        | 11/70 [00:00<00:02, 27.14it/s] 30%|███       | 21/70 [00:00<00:01, 45.65it/s] 44%|████▍     | 31/70 [00:00<00:00, 59.56it/s] 59%|█████▊    | 41/70 [00:00<00:00, 69.77it/s] 73%|███████▎  | 51/70 [00:00<00:00, 77.06it/s] 87%|████████▋ | 61/70 [00:01<00:00, 82.42it/s]100%|██████████| 70/70 [00:01<00:00, 61.07it/s]
8925 images processed, 1.178741693496704 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:41,  1.06it/s]  4%|▍         | 2/45 [00:01<00:19,  2.17it/s] 20%|██        | 9/45 [00:01<00:03,  9.39it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.31it/s] 31%|███       | 14/45 [00:01<00:02, 12.07it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.06it/s] 42%|████▏     | 19/45 [00:02<00:02, 12.10it/s] 47%|████▋     | 21/45 [00:02<00:01, 13.42it/s] 51%|█████     | 23/45 [00:02<00:01, 14.59it/s] 56%|█████▌    | 25/45 [00:02<00:01, 13.23it/s] 60%|██████    | 27/45 [00:02<00:01, 12.59it/s] 67%|██████▋   | 30/45 [00:02<00:01, 12.41it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.48it/s] 78%|███████▊  | 35/45 [00:03<00:00, 11.71it/s] 91%|█████████ | 41/45 [00:03<00:00, 13.62it/s] 96%|█████████▌| 43/45 [00:04<00:00, 10.00it/s]100%|██████████| 45/45 [00:04<00:00, 10.63it/s]
5640 images processed, 4.25513219833374 seconds used

18.57901430130005
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           5.17  98.81
places365     73.57  79.67
LSUN          23.36  95.44
iSUN          75.35  79.92
dtd           42.66  90.16
forget        39.50  91.83
AVG           43.27  89.31
Retain-Acc: 0.7330
Forget-as-OOD (retain known vs forget novel):
  FPR: 39.50 AUROC: 91.83 AUIN: 98.92
8.85008716583252
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.2 lr=0.0005 epochs=5
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.2, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:19<01:16, 19.21s/it] 40%|████      | 2/5 [00:33<00:49, 16.54s/it] 60%|██████    | 3/5 [00:48<00:31, 15.71s/it] 80%|████████  | 4/5 [01:03<00:15, 15.27s/it]100%|██████████| 5/5 [01:17<00:00, 14.94s/it]100%|██████████| 5/5 [01:17<00:00, 15.51s/it]
[loss] ep 0 it 0 total=8.2415 mle=1.5710 pcon=5.2950 forget=1.3755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=8.2316 mle=1.5424 pcon=5.2879 forget=1.4013 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=8.3554 mle=1.7005 pcon=5.2809 forget=1.3740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8.5432 mle=1.8998 pcon=5.2738 forget=1.3696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=8.3629 mle=1.7133 pcon=5.2670 forget=1.3826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=8.1407 mle=1.5020 pcon=5.2603 forget=1.3784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=8.1955 mle=1.5595 pcon=5.2541 forget=1.3819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=8.3182 mle=1.6809 pcon=5.2476 forget=1.3897 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=8.3226 mle=1.6716 pcon=5.2409 forget=1.4101 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=8.2388 mle=1.6372 pcon=5.2346 forget=1.3670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=8.1345 mle=1.5165 pcon=5.2285 forget=1.3895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=8.3921 mle=1.7802 pcon=5.2225 forget=1.3894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=8.3667 mle=1.7647 pcon=5.2167 forget=1.3853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=8.1848 mle=1.6053 pcon=5.2112 forget=1.3683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=8.3080 mle=1.7241 pcon=5.2056 forget=1.3782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=8.3807 mle=1.7943 pcon=5.2003 forget=1.3862 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=8.1248 mle=1.5530 pcon=5.1950 forget=1.3768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=8.4359 mle=1.8883 pcon=5.1899 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=8.3021 mle=1.7416 pcon=5.1847 forget=1.3759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=8.0711 mle=1.5265 pcon=5.1797 forget=1.3649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=8.1461 mle=1.5856 pcon=5.1746 forget=1.3859 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=8.4243 mle=1.8837 pcon=5.1699 forget=1.3707 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=8.1152 mle=1.5923 pcon=5.1652 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=8.3366 mle=1.8112 pcon=5.1605 forget=1.3648 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=8.2327 mle=1.7059 pcon=5.1560 forget=1.3708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=8.2020 mle=1.6709 pcon=5.1519 forget=1.3793 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=8.2802 mle=1.7667 pcon=5.1475 forget=1.3659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=8.4242 mle=1.9183 pcon=5.1436 forget=1.3623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=8.0694 mle=1.5700 pcon=5.1395 forget=1.3599 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=8.3277 mle=1.8362 pcon=5.1354 forget=1.3562 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=8.2095 mle=1.7029 pcon=5.1312 forget=1.3754 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=8.2015 mle=1.6956 pcon=5.1276 forget=1.3783 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=8.1198 mle=1.6539 pcon=5.1237 forget=1.3421 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=8.2054 mle=1.7295 pcon=5.1197 forget=1.3562 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8.1324 mle=1.6502 pcon=5.1161 forget=1.3662 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.9320 mle=1.4537 pcon=5.1126 forget=1.3657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=8.2319 mle=1.7667 pcon=5.1088 forget=1.3565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.9500 mle=1.4914 pcon=5.1052 forget=1.3534 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=8.3387 mle=1.8718 pcon=5.1017 forget=1.3651 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:46,  3.66it/s]  3%|▎         | 11/391 [00:00<00:10, 35.93it/s]  5%|▌         | 21/391 [00:00<00:06, 55.63it/s]  8%|▊         | 31/391 [00:00<00:05, 68.10it/s] 10%|█         | 41/391 [00:00<00:04, 76.47it/s] 13%|█▎        | 51/391 [00:00<00:04, 82.00it/s] 16%|█▌        | 61/391 [00:00<00:03, 85.79it/s] 18%|█▊        | 71/391 [00:01<00:03, 88.33it/s] 21%|██        | 81/391 [00:01<00:03, 90.20it/s] 23%|██▎       | 91/391 [00:01<00:03, 91.45it/s] 26%|██▌       | 101/391 [00:01<00:03, 91.66it/s] 28%|██▊       | 111/391 [00:01<00:03, 92.53it/s] 31%|███       | 121/391 [00:01<00:02, 93.01it/s] 34%|███▎      | 131/391 [00:01<00:02, 92.78it/s] 36%|███▌      | 141/391 [00:01<00:02, 93.29it/s] 39%|███▊      | 151/391 [00:01<00:02, 93.41it/s] 41%|████      | 161/391 [00:01<00:02, 93.55it/s] 44%|████▎     | 171/391 [00:02<00:02, 93.67it/s] 46%|████▋     | 181/391 [00:02<00:02, 93.68it/s] 49%|████▉     | 191/391 [00:02<00:02, 93.84it/s] 51%|█████▏    | 201/391 [00:02<00:02, 94.22it/s] 54%|█████▍    | 211/391 [00:02<00:01, 94.49it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.38it/s] 59%|█████▉    | 231/391 [00:02<00:01, 94.29it/s] 62%|██████▏   | 241/391 [00:02<00:01, 94.56it/s] 64%|██████▍   | 251/391 [00:02<00:01, 94.69it/s] 67%|██████▋   | 261/391 [00:03<00:01, 94.86it/s] 69%|██████▉   | 271/391 [00:03<00:01, 95.01it/s] 72%|███████▏  | 281/391 [00:03<00:01, 94.98it/s] 74%|███████▍  | 291/391 [00:03<00:01, 94.83it/s] 77%|███████▋  | 301/391 [00:03<00:00, 94.53it/s] 80%|███████▉  | 311/391 [00:03<00:00, 94.66it/s] 82%|████████▏ | 321/391 [00:03<00:00, 94.56it/s] 85%|████████▍ | 331/391 [00:03<00:00, 94.57it/s] 87%|████████▋ | 341/391 [00:03<00:00, 94.78it/s] 90%|████████▉ | 351/391 [00:03<00:00, 93.33it/s] 92%|█████████▏| 361/391 [00:04<00:00, 93.73it/s] 95%|█████████▍| 371/391 [00:04<00:00, 93.97it/s] 97%|█████████▋| 381/391 [00:04<00:00, 94.17it/s]100%|██████████| 391/391 [00:04<00:00, 92.86it/s]100%|██████████| 391/391 [00:04<00:00, 88.50it/s]
50000 images processed, 4.493046522140503 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:20,  3.75it/s] 14%|█▍        | 11/79 [00:00<00:01, 36.56it/s] 25%|██▌       | 20/79 [00:00<00:01, 54.07it/s] 38%|███▊      | 30/79 [00:00<00:00, 67.43it/s] 51%|█████     | 40/79 [00:00<00:00, 75.71it/s] 63%|██████▎   | 50/79 [00:00<00:00, 81.05it/s] 76%|███████▌  | 60/79 [00:00<00:00, 85.03it/s] 89%|████████▊ | 70/79 [00:01<00:00, 87.80it/s]100%|██████████| 79/79 [00:01<00:00, 70.93it/s]
10000 images processed, 1.1357405185699463 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:14,  2.72it/s]  5%|▌         | 11/204 [00:00<00:06, 29.44it/s] 10%|█         | 21/204 [00:00<00:03, 48.22it/s] 15%|█▌        | 31/204 [00:00<00:02, 61.34it/s] 20%|██        | 41/204 [00:00<00:02, 70.67it/s] 25%|██▌       | 51/204 [00:00<00:01, 77.33it/s] 30%|██▉       | 61/204 [00:01<00:01, 81.94it/s] 35%|███▍      | 71/204 [00:01<00:01, 85.17it/s] 40%|███▉      | 81/204 [00:01<00:01, 87.31it/s] 45%|████▍     | 91/204 [00:01<00:01, 88.91it/s] 50%|████▉     | 101/204 [00:01<00:01, 89.87it/s] 54%|█████▍    | 111/204 [00:01<00:01, 90.80it/s] 59%|█████▉    | 121/204 [00:01<00:00, 91.33it/s] 64%|██████▍   | 131/204 [00:01<00:00, 91.89it/s] 69%|██████▉   | 141/204 [00:01<00:00, 91.09it/s] 74%|███████▍  | 151/204 [00:01<00:00, 91.56it/s] 79%|███████▉  | 161/204 [00:02<00:00, 91.82it/s] 84%|████████▍ | 171/204 [00:02<00:00, 92.09it/s] 89%|████████▊ | 181/204 [00:02<00:00, 92.11it/s] 94%|█████████▎| 191/204 [00:02<00:00, 92.57it/s] 99%|█████████▊| 201/204 [00:02<00:00, 93.61it/s]100%|██████████| 204/204 [00:02<00:00, 79.64it/s]
26032 images processed, 2.6056060791015625 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:39,  1.99it/s] 11%|█▏        | 9/79 [00:00<00:04, 17.36it/s] 22%|██▏       | 17/79 [00:00<00:02, 27.94it/s] 32%|███▏      | 25/79 [00:00<00:01, 35.37it/s] 42%|████▏     | 33/79 [00:01<00:01, 40.26it/s] 51%|█████     | 40/79 [00:01<00:00, 46.23it/s] 58%|█████▊    | 46/79 [00:01<00:00, 45.04it/s] 66%|██████▌   | 52/79 [00:01<00:00, 44.20it/s] 73%|███████▎  | 58/79 [00:01<00:00, 43.14it/s] 84%|████████▎ | 66/79 [00:01<00:00, 45.52it/s] 94%|█████████▎| 74/79 [00:01<00:00, 46.49it/s]100%|██████████| 79/79 [00:02<00:00, 39.13it/s]
10000 images processed, 2.0542356967926025 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:32,  2.41it/s] 14%|█▍        | 11/79 [00:00<00:02, 26.80it/s] 27%|██▋       | 21/79 [00:00<00:01, 45.42it/s] 39%|███▉      | 31/79 [00:00<00:00, 59.28it/s] 52%|█████▏    | 41/79 [00:00<00:00, 69.39it/s] 65%|██████▍   | 51/79 [00:00<00:00, 76.77it/s] 77%|███████▋  | 61/79 [00:01<00:00, 82.10it/s] 90%|████████▉ | 71/79 [00:01<00:00, 86.07it/s]100%|██████████| 79/79 [00:01<00:00, 64.09it/s]
10000 images processed, 1.2534306049346924 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:19,  3.61it/s] 16%|█▌        | 11/70 [00:00<00:01, 35.52it/s] 30%|███       | 21/70 [00:00<00:00, 55.29it/s] 44%|████▍     | 31/70 [00:00<00:00, 68.06it/s] 59%|█████▊    | 41/70 [00:00<00:00, 76.48it/s] 73%|███████▎  | 51/70 [00:00<00:00, 82.14it/s] 87%|████████▋ | 61/70 [00:00<00:00, 86.31it/s]100%|██████████| 70/70 [00:01<00:00, 69.23it/s]
8925 images processed, 1.0444602966308594 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:37,  1.17it/s]  4%|▍         | 2/45 [00:01<00:19,  2.17it/s] 20%|██        | 9/45 [00:01<00:03, 10.51it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.71it/s] 38%|███▊      | 17/45 [00:01<00:01, 14.05it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.37it/s] 51%|█████     | 23/45 [00:02<00:01, 13.45it/s] 58%|█████▊    | 26/45 [00:02<00:01, 11.63it/s] 69%|██████▉   | 31/45 [00:02<00:00, 15.62it/s] 73%|███████▎  | 33/45 [00:02<00:00, 14.93it/s] 78%|███████▊  | 35/45 [00:03<00:00, 10.06it/s] 93%|█████████▎| 42/45 [00:04<00:00,  9.58it/s]100%|██████████| 45/45 [00:04<00:00, 10.65it/s]
5640 images processed, 4.24501371383667 seconds used

18.54245901107788
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.87  81.18
LSUN          17.57  96.07
iSUN          72.35  81.68
dtd           37.85  91.39
forget        79.50  86.90
AVG           46.28  89.43
Retain-Acc: 0.7407
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
12.897369146347046
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.2 lr=0.0005 epochs=10
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.2, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:18<02:47, 18.63s/it] 20%|██        | 2/10 [00:34<02:14, 16.79s/it] 30%|███       | 3/10 [00:50<01:56, 16.60s/it] 40%|████      | 4/10 [01:06<01:38, 16.39s/it] 50%|█████     | 5/10 [01:22<01:20, 16.12s/it] 60%|██████    | 6/10 [01:38<01:04, 16.16s/it] 70%|███████   | 7/10 [01:54<00:48, 16.13s/it] 80%|████████  | 8/10 [02:09<00:31, 15.70s/it][loss] ep 0 it 0 total=8.2415 mle=1.5710 pcon=5.2950 forget=1.3755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=8.2316 mle=1.5424 pcon=5.2879 forget=1.4013 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=8.3554 mle=1.7005 pcon=5.2809 forget=1.3740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8.5432 mle=1.8998 pcon=5.2738 forget=1.3696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=8.3629 mle=1.7133 pcon=5.2670 forget=1.3826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=8.1407 mle=1.5020 pcon=5.2603 forget=1.3784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=8.1955 mle=1.5595 pcon=5.2541 forget=1.3819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=8.3182 mle=1.6809 pcon=5.2476 forget=1.3897 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=8.3226 mle=1.6716 pcon=5.2409 forget=1.4101 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=8.2388 mle=1.6372 pcon=5.2346 forget=1.3670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=8.1345 mle=1.5165 pcon=5.2285 forget=1.3895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=8.3921 mle=1.7802 pcon=5.2225 forget=1.3894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=8.3667 mle=1.7647 pcon=5.2167 forget=1.3853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=8.1848 mle=1.6053 pcon=5.2112 forget=1.3683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=8.3080 mle=1.7241 pcon=5.2056 forget=1.3782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=8.3807 mle=1.7943 pcon=5.2003 forget=1.3862 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=8.1248 mle=1.5530 pcon=5.1950 forget=1.3768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=8.4359 mle=1.8883 pcon=5.1899 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=8.3021 mle=1.7416 pcon=5.1847 forget=1.3759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=8.0711 mle=1.5265 pcon=5.1797 forget=1.3649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=8.1461 mle=1.5856 pcon=5.1746 forget=1.3859 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=8.4243 mle=1.8837 pcon=5.1699 forget=1.3707 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=8.1152 mle=1.5923 pcon=5.1652 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=8.3366 mle=1.8112 pcon=5.1605 forget=1.3648 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=8.2327 mle=1.7059 pcon=5.1560 forget=1.3708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=8.2020 mle=1.6709 pcon=5.1519 forget=1.3793 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=8.2802 mle=1.7667 pcon=5.1475 forget=1.3659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=8.4242 mle=1.9183 pcon=5.1436 forget=1.3623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=8.0694 mle=1.5700 pcon=5.1395 forget=1.3599 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=8.3277 mle=1.8362 pcon=5.1354 forget=1.3562 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=8.2095 mle=1.7029 pcon=5.1312 forget=1.3754 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=8.2015 mle=1.6956 pcon=5.1276 forget=1.3783 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=8.1198 mle=1.6539 pcon=5.1237 forget=1.3421 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=8.2054 mle=1.7295 pcon=5.1197 forget=1.3562 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8.1324 mle=1.6502 pcon=5.1161 forget=1.3662 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.9320 mle=1.4537 pcon=5.1126 forget=1.3657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=8.2320 mle=1.7668 pcon=5.1088 forget=1.3565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.9499 mle=1.4913 pcon=5.1052 forget=1.3534 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=8.3387 mle=1.8718 pcon=5.1017 forget=1.3651 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=8.3228 mle=1.8671 pcon=5.0984 forget=1.3573 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=8.2406 mle=1.7998 pcon=5.0948 forget=1.3460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=8.0359 mle=1.5841 pcon=5.0917 forget=1.3602 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=8.0423 mle=1.5953 pcon=5.0882 forget=1.3588 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=8.1222 mle=1.6912 pcon=5.0853 forget=1.3458 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=8.1948 mle=1.7736 pcon=5.0823 forget=1.3389 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=8.0266 mle=1.5964 pcon=5.0792 forget=1.3510 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=8.1721 mle=1.7468 pcon=5.0764 forget=1.3489 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.8466 mle=1.4010 pcon=5.0735 forget=1.3721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.9352 mle=1.5358 pcon=5.0707 forget=1.3286 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=8.0211 mle=1.5915 pcon=5.0681 forget=1.3616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=8.0767 mle=1.6654 pcon=5.0653 forget=1.3460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=8.0349 mle=1.6301 pcon=5.0631 forget=1.3417 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=8.2388 mle=1.8226 pcon=5.0602 forget=1.3560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.9567 mle=1.5350 pcon=5.0578 forget=1.3638 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=8.0775 mle=1.6746 pcon=5.0553 forget=1.3476 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=8.0229 mle=1.6306 pcon=5.0527 forget=1.3396 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=8.1302 mle=1.7447 pcon=5.0503 forget=1.3351 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=8.0165 mle=1.6266 pcon=5.0478 forget=1.3420 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.9562 mle=1.5819 pcon=5.0455 forget=1.3289 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=8.0488 mle=1.6532 pcon=5.0435 forget=1.3522 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=8.1985 mle=1.7956 pcon=5.0413 forget=1.3616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=8.1397 mle=1.7413 pcon=5.0389 forget=1.3596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=8.0230 mle=1.6399 pcon=5.0368 forget=1.3462 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 90%|█████████ | 9/10 [02:23<00:15, 15.16s/it]100%|██████████| 10/10 [02:37<00:00, 14.81s/it]100%|██████████| 10/10 [02:37<00:00, 15.73s/it]
[loss] ep 8 it 30 total=7.9318 mle=1.5367 pcon=5.0349 forget=1.3602 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=7.9714 mle=1.5899 pcon=5.0330 forget=1.3485 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.8948 mle=1.5067 pcon=5.0308 forget=1.3572 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=8.0734 mle=1.7032 pcon=5.0289 forget=1.3414 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.8978 mle=1.5307 pcon=5.0274 forget=1.3397 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=8.0336 mle=1.6774 pcon=5.0254 forget=1.3308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=8.1460 mle=1.7558 pcon=5.0233 forget=1.3669 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.8424 mle=1.4881 pcon=5.0214 forget=1.3330 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.9699 mle=1.6357 pcon=5.0195 forget=1.3147 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=8.0219 mle=1.6687 pcon=5.0173 forget=1.3359 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=8.2075 mle=1.8405 pcon=5.0159 forget=1.3511 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.9287 mle=1.5777 pcon=5.0143 forget=1.3368 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.9191 mle=1.5664 pcon=5.0127 forget=1.3401 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.9165 mle=1.5594 pcon=5.0108 forget=1.3463 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.9024 mle=1.5572 pcon=5.0093 forget=1.3359 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:20,  2.78it/s]  3%|▎         | 11/391 [00:00<00:12, 29.87it/s]  5%|▌         | 21/391 [00:00<00:07, 49.08it/s]  8%|▊         | 31/391 [00:00<00:05, 62.74it/s] 10%|█         | 41/391 [00:00<00:04, 72.27it/s] 13%|█▎        | 51/391 [00:00<00:04, 78.99it/s] 16%|█▌        | 61/391 [00:00<00:03, 83.87it/s] 18%|█▊        | 71/391 [00:01<00:03, 87.21it/s] 21%|██        | 81/391 [00:01<00:03, 89.60it/s] 23%|██▎       | 91/391 [00:01<00:03, 91.31it/s] 26%|██▌       | 101/391 [00:01<00:03, 92.43it/s] 28%|██▊       | 111/391 [00:01<00:03, 93.23it/s] 31%|███       | 121/391 [00:01<00:02, 93.62it/s] 34%|███▎      | 131/391 [00:01<00:02, 94.13it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.37it/s] 39%|███▊      | 151/391 [00:01<00:02, 94.66it/s] 41%|████      | 161/391 [00:02<00:02, 94.78it/s] 44%|████▎     | 171/391 [00:02<00:02, 95.00it/s] 46%|████▋     | 181/391 [00:02<00:02, 94.93it/s] 49%|████▉     | 191/391 [00:02<00:02, 95.06it/s] 51%|█████▏    | 201/391 [00:02<00:01, 95.04it/s] 54%|█████▍    | 211/391 [00:02<00:01, 95.01it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.94it/s] 59%|█████▉    | 231/391 [00:02<00:01, 95.16it/s] 62%|██████▏   | 241/391 [00:02<00:01, 95.13it/s] 64%|██████▍   | 251/391 [00:02<00:01, 94.89it/s] 67%|██████▋   | 261/391 [00:03<00:01, 95.00it/s] 69%|██████▉   | 271/391 [00:03<00:01, 95.15it/s] 72%|███████▏  | 281/391 [00:03<00:01, 94.96it/s] 74%|███████▍  | 291/391 [00:03<00:01, 95.06it/s] 77%|███████▋  | 301/391 [00:03<00:00, 95.19it/s] 80%|███████▉  | 311/391 [00:03<00:00, 95.14it/s] 82%|████████▏ | 321/391 [00:03<00:00, 95.14it/s] 85%|████████▍ | 331/391 [00:03<00:00, 94.80it/s] 87%|████████▋ | 341/391 [00:03<00:00, 94.97it/s] 90%|████████▉ | 351/391 [00:04<00:00, 95.13it/s] 92%|█████████▏| 361/391 [00:04<00:00, 95.02it/s] 95%|█████████▍| 371/391 [00:04<00:00, 94.89it/s] 97%|█████████▋| 381/391 [00:04<00:00, 94.88it/s]100%|██████████| 391/391 [00:04<00:00, 92.38it/s]100%|██████████| 391/391 [00:04<00:00, 87.38it/s]
50000 images processed, 4.56134557723999 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:22,  3.50it/s] 13%|█▎        | 10/79 [00:00<00:02, 32.34it/s] 25%|██▌       | 20/79 [00:00<00:01, 52.81it/s] 38%|███▊      | 30/79 [00:00<00:00, 65.99it/s] 51%|█████     | 40/79 [00:00<00:00, 74.54it/s] 63%|██████▎   | 50/79 [00:00<00:00, 80.13it/s] 76%|███████▌  | 60/79 [00:00<00:00, 84.10it/s] 89%|████████▊ | 70/79 [00:01<00:00, 87.57it/s]100%|██████████| 79/79 [00:01<00:00, 69.89it/s]
10000 images processed, 1.1529216766357422 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:02,  3.23it/s]  5%|▍         | 10/204 [00:00<00:06, 30.15it/s] 10%|▉         | 20/204 [00:00<00:03, 50.39it/s] 15%|█▍        | 30/204 [00:00<00:02, 63.88it/s] 20%|█▉        | 40/204 [00:00<00:02, 73.08it/s] 25%|██▍       | 50/204 [00:00<00:01, 79.38it/s] 29%|██▉       | 60/204 [00:00<00:01, 83.48it/s] 34%|███▍      | 70/204 [00:01<00:01, 86.60it/s] 39%|███▉      | 80/204 [00:01<00:01, 88.69it/s] 44%|████▍     | 90/204 [00:01<00:01, 90.13it/s] 49%|████▉     | 100/204 [00:01<00:01, 91.00it/s] 54%|█████▍    | 110/204 [00:01<00:01, 91.54it/s] 59%|█████▉    | 120/204 [00:01<00:00, 92.09it/s] 64%|██████▎   | 130/204 [00:01<00:00, 92.51it/s] 69%|██████▊   | 140/204 [00:01<00:00, 92.96it/s] 74%|███████▎  | 150/204 [00:01<00:00, 93.13it/s] 78%|███████▊  | 160/204 [00:02<00:00, 93.10it/s] 83%|████████▎ | 170/204 [00:02<00:00, 93.26it/s] 88%|████████▊ | 180/204 [00:02<00:00, 93.38it/s] 93%|█████████▎| 190/204 [00:02<00:00, 93.57it/s] 98%|█████████▊| 200/204 [00:02<00:00, 94.26it/s]100%|██████████| 204/204 [00:02<00:00, 81.78it/s]
26032 images processed, 2.5417022705078125 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:39,  1.97it/s] 11%|█▏        | 9/79 [00:00<00:04, 17.07it/s] 22%|██▏       | 17/79 [00:00<00:02, 27.18it/s] 32%|███▏      | 25/79 [00:00<00:01, 34.62it/s] 42%|████▏     | 33/79 [00:01<00:01, 39.03it/s] 52%|█████▏    | 41/79 [00:01<00:00, 42.30it/s] 62%|██████▏   | 49/79 [00:01<00:00, 44.32it/s] 72%|███████▏  | 57/79 [00:01<00:00, 45.54it/s] 82%|████████▏ | 65/79 [00:01<00:00, 46.70it/s] 92%|█████████▏| 73/79 [00:01<00:00, 47.65it/s]100%|██████████| 79/79 [00:02<00:00, 39.31it/s]
10000 images processed, 2.0465331077575684 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:28,  2.75it/s] 14%|█▍        | 11/79 [00:00<00:02, 29.69it/s] 27%|██▋       | 21/79 [00:00<00:01, 48.86it/s] 39%|███▉      | 31/79 [00:00<00:00, 62.39it/s] 52%|█████▏    | 41/79 [00:00<00:00, 71.99it/s] 65%|██████▍   | 51/79 [00:00<00:00, 78.82it/s] 77%|███████▋  | 61/79 [00:00<00:00, 83.55it/s] 90%|████████▉ | 71/79 [00:01<00:00, 87.13it/s]100%|██████████| 79/79 [00:01<00:00, 67.01it/s]
10000 images processed, 1.2011232376098633 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:23,  3.00it/s] 16%|█▌        | 11/70 [00:00<00:01, 31.48it/s] 30%|███       | 21/70 [00:00<00:00, 50.85it/s] 44%|████▍     | 31/70 [00:00<00:00, 64.20it/s] 59%|█████▊    | 41/70 [00:00<00:00, 73.47it/s] 73%|███████▎  | 51/70 [00:00<00:00, 79.96it/s] 87%|████████▋ | 61/70 [00:00<00:00, 84.65it/s]100%|██████████| 70/70 [00:01<00:00, 65.57it/s]
8925 images processed, 1.0999042987823486 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:39,  1.11it/s]  4%|▍         | 2/45 [00:01<00:19,  2.24it/s] 20%|██        | 9/45 [00:01<00:03,  9.41it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.17it/s] 36%|███▌      | 16/45 [00:01<00:01, 14.98it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.35it/s] 49%|████▉     | 22/45 [00:02<00:01, 13.03it/s] 56%|█████▌    | 25/45 [00:02<00:01, 13.22it/s] 60%|██████    | 27/45 [00:02<00:01, 12.53it/s] 67%|██████▋   | 30/45 [00:02<00:01, 12.91it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.31it/s] 78%|███████▊  | 35/45 [00:03<00:00, 11.27it/s] 91%|█████████ | 41/45 [00:03<00:00, 13.60it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.94it/s]100%|██████████| 45/45 [00:04<00:00, 10.65it/s]
5640 images processed, 4.247833967208862 seconds used

18.573903560638428
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.91  81.18
LSUN          17.49  96.10
iSUN          72.35  81.68
dtd           37.87  91.38
forget        79.50  86.88
AVG           46.27  89.43
Retain-Acc: 0.7408
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.88 AUIN: 98.36
9.000999689102173
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.2 lr=0.0005 epochs=20
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.2, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:17<05:36, 17.72s/it] 10%|█         | 2/20 [00:32<04:44, 15.81s/it] 15%|█▌        | 3/20 [00:46<04:19, 15.25s/it] 20%|██        | 4/20 [01:01<04:01, 15.09s/it] 25%|██▌       | 5/20 [01:16<03:45, 15.04s/it] 30%|███       | 6/20 [01:31<03:30, 15.02s/it] 35%|███▌      | 7/20 [01:46<03:15, 15.00s/it] 40%|████      | 8/20 [02:01<02:58, 14.86s/it][loss] ep 0 it 0 total=8.2415 mle=1.5709 pcon=5.2950 forget=1.3755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=8.2316 mle=1.5423 pcon=5.2879 forget=1.4013 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=8.3553 mle=1.7004 pcon=5.2809 forget=1.3740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8.5432 mle=1.8998 pcon=5.2738 forget=1.3696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=8.3628 mle=1.7132 pcon=5.2670 forget=1.3826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=8.1407 mle=1.5019 pcon=5.2603 forget=1.3784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=8.1955 mle=1.5595 pcon=5.2540 forget=1.3819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=8.3183 mle=1.6810 pcon=5.2476 forget=1.3897 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=8.3225 mle=1.6716 pcon=5.2409 forget=1.4101 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=8.2387 mle=1.6371 pcon=5.2346 forget=1.3670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=8.1345 mle=1.5166 pcon=5.2284 forget=1.3895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=8.3922 mle=1.7804 pcon=5.2224 forget=1.3894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=8.3666 mle=1.7646 pcon=5.2167 forget=1.3853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=8.1849 mle=1.6054 pcon=5.2112 forget=1.3683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=8.3080 mle=1.7241 pcon=5.2056 forget=1.3782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=8.3807 mle=1.7942 pcon=5.2003 forget=1.3862 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=8.1247 mle=1.5529 pcon=5.1950 forget=1.3768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=8.4358 mle=1.8883 pcon=5.1899 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=8.3020 mle=1.7414 pcon=5.1847 forget=1.3759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=8.0711 mle=1.5265 pcon=5.1797 forget=1.3649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=8.1462 mle=1.5857 pcon=5.1746 forget=1.3859 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=8.4241 mle=1.8835 pcon=5.1699 forget=1.3707 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=8.1152 mle=1.5924 pcon=5.1652 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=8.3367 mle=1.8113 pcon=5.1605 forget=1.3648 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=8.2325 mle=1.7058 pcon=5.1560 forget=1.3708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=8.2021 mle=1.6709 pcon=5.1519 forget=1.3793 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=8.2802 mle=1.7668 pcon=5.1475 forget=1.3659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=8.4244 mle=1.9185 pcon=5.1436 forget=1.3623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=8.0693 mle=1.5699 pcon=5.1395 forget=1.3599 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=8.3276 mle=1.8360 pcon=5.1354 forget=1.3562 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=8.2098 mle=1.7032 pcon=5.1312 forget=1.3754 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=8.2014 mle=1.6955 pcon=5.1276 forget=1.3783 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=8.1198 mle=1.6539 pcon=5.1237 forget=1.3421 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=8.2055 mle=1.7296 pcon=5.1197 forget=1.3563 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8.1325 mle=1.6502 pcon=5.1161 forget=1.3662 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.9319 mle=1.4535 pcon=5.1126 forget=1.3657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=8.2319 mle=1.7666 pcon=5.1088 forget=1.3565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.9500 mle=1.4914 pcon=5.1052 forget=1.3534 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=8.3389 mle=1.8720 pcon=5.1017 forget=1.3651 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=8.3228 mle=1.8671 pcon=5.0984 forget=1.3573 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=8.2406 mle=1.7997 pcon=5.0948 forget=1.3460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=8.0359 mle=1.5841 pcon=5.0917 forget=1.3601 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=8.0412 mle=1.5942 pcon=5.0882 forget=1.3588 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=8.1222 mle=1.6911 pcon=5.0853 forget=1.3458 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=8.1948 mle=1.7736 pcon=5.0823 forget=1.3389 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=8.0267 mle=1.5964 pcon=5.0792 forget=1.3510 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=8.1722 mle=1.7469 pcon=5.0764 forget=1.3489 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.8467 mle=1.4011 pcon=5.0735 forget=1.3721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.9350 mle=1.5356 pcon=5.0707 forget=1.3286 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=8.0211 mle=1.5915 pcon=5.0681 forget=1.3616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=8.0768 mle=1.6655 pcon=5.0653 forget=1.3460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=8.0351 mle=1.6303 pcon=5.0631 forget=1.3418 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=8.2389 mle=1.8227 pcon=5.0602 forget=1.3560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.9568 mle=1.5351 pcon=5.0578 forget=1.3638 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=8.0774 mle=1.6745 pcon=5.0554 forget=1.3476 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=8.0231 mle=1.6308 pcon=5.0527 forget=1.3396 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=8.1301 mle=1.7446 pcon=5.0504 forget=1.3351 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=8.0164 mle=1.6266 pcon=5.0478 forget=1.3420 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.9564 mle=1.5821 pcon=5.0455 forget=1.3289 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=8.0488 mle=1.6532 pcon=5.0435 forget=1.3522 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=8.1985 mle=1.7956 pcon=5.0413 forget=1.3616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=8.1396 mle=1.7412 pcon=5.0389 forget=1.3596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=8.0230 mle=1.6399 pcon=5.0368 forget=1.3462 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 45%|████▌     | 9/20 [02:15<02:42, 14.74s/it] 50%|█████     | 10/20 [02:30<02:28, 14.82s/it] 55%|█████▌    | 11/20 [02:45<02:13, 14.85s/it] 60%|██████    | 12/20 [03:00<01:58, 14.81s/it] 65%|██████▌   | 13/20 [03:14<01:43, 14.77s/it] 70%|███████   | 14/20 [03:29<01:28, 14.79s/it] 75%|███████▌  | 15/20 [03:44<01:13, 14.70s/it] 80%|████████  | 16/20 [03:58<00:58, 14.57s/it][loss] ep 8 it 30 total=7.9318 mle=1.5367 pcon=5.0349 forget=1.3602 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=7.9716 mle=1.5900 pcon=5.0330 forget=1.3485 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.8947 mle=1.5067 pcon=5.0308 forget=1.3572 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=8.0736 mle=1.7033 pcon=5.0289 forget=1.3414 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.8978 mle=1.5307 pcon=5.0274 forget=1.3397 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=8.0336 mle=1.6774 pcon=5.0254 forget=1.3308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=8.1460 mle=1.7558 pcon=5.0233 forget=1.3669 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.8424 mle=1.4881 pcon=5.0214 forget=1.3330 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.9699 mle=1.6357 pcon=5.0195 forget=1.3147 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=8.0220 mle=1.6687 pcon=5.0174 forget=1.3359 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=8.2075 mle=1.8405 pcon=5.0159 forget=1.3511 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.9286 mle=1.5776 pcon=5.0143 forget=1.3368 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.9190 mle=1.5663 pcon=5.0127 forget=1.3401 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.9157 mle=1.5586 pcon=5.0108 forget=1.3463 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.9025 mle=1.5572 pcon=5.0093 forget=1.3359 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=7.9604 mle=1.6070 pcon=5.0075 forget=1.3458 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=7.9507 mle=1.5894 pcon=5.0058 forget=1.3555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=8.0942 mle=1.7537 pcon=5.0042 forget=1.3363 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=8.1805 mle=1.8489 pcon=5.0028 forget=1.3288 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=8.1888 mle=1.8574 pcon=5.0013 forget=1.3302 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=7.9194 mle=1.5823 pcon=5.0000 forget=1.3372 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=7.8834 mle=1.5618 pcon=4.9987 forget=1.3228 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=8.0214 mle=1.6671 pcon=4.9973 forget=1.3570 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=7.9181 mle=1.5735 pcon=4.9958 forget=1.3487 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=7.9446 mle=1.6207 pcon=4.9944 forget=1.3295 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=7.8846 mle=1.5686 pcon=4.9931 forget=1.3229 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=7.9893 mle=1.6635 pcon=4.9914 forget=1.3343 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=8.1137 mle=1.7817 pcon=4.9902 forget=1.3417 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=7.9399 mle=1.6111 pcon=4.9893 forget=1.3395 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=8.2922 mle=1.9672 pcon=4.9878 forget=1.3372 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=7.9712 mle=1.6686 pcon=4.9865 forget=1.3161 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 12 it 20 total=7.9373 mle=1.6197 pcon=4.9850 forget=1.3326 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=8.0016 mle=1.6817 pcon=4.9837 forget=1.3361 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=7.8759 mle=1.5457 pcon=4.9825 forget=1.3477 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=8.0449 mle=1.7413 pcon=4.9812 forget=1.3224 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=7.9002 mle=1.5942 pcon=4.9798 forget=1.3261 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=7.8533 mle=1.5313 pcon=4.9790 forget=1.3431 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=7.8236 mle=1.5136 pcon=4.9778 forget=1.3323 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=8.1817 mle=1.8628 pcon=4.9766 forget=1.3424 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=7.7367 mle=1.4281 pcon=4.9755 forget=1.3331 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=7.8317 mle=1.4906 pcon=4.9743 forget=1.3668 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=7.8703 mle=1.5645 pcon=4.9729 forget=1.3330 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=7.8522 mle=1.5556 pcon=4.9719 forget=1.3247 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=7.9485 mle=1.6423 pcon=4.9709 forget=1.3353 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=7.8702 mle=1.5814 pcon=4.9698 forget=1.3190 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=7.8891 mle=1.5865 pcon=4.9686 forget=1.3340 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=7.8645 mle=1.5733 pcon=4.9675 forget=1.3237 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=8.0456 mle=1.7642 pcon=4.9662 forget=1.3151 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=8.0815 mle=1.7830 pcon=4.9654 forget=1.3331 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=7.9009 mle=1.6117 pcon=4.9648 forget=1.3244 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=8.1705 mle=1.8810 pcon=4.9635 forget=1.3260 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=7.9020 mle=1.6166 pcon=4.9627 forget=1.3227 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=7.9363 mle=1.6635 pcon=4.9617 forget=1.3111 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=7.9582 mle=1.6814 pcon=4.9607 forget=1.3161 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=7.9899 mle=1.6887 pcon=4.9598 forget=1.3414 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=7.7606 mle=1.4855 pcon=4.9587 forget=1.3164 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=7.8475 mle=1.5631 pcon=4.9578 forget=1.3267 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=8.0190 mle=1.7207 pcon=4.9569 forget=1.3415 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=7.8824 mle=1.6117 pcon=4.9560 forget=1.3147 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=8.1686 mle=1.8845 pcon=4.9550 forget=1.3291 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=7.8838 mle=1.5985 pcon=4.9542 forget=1.3310 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=7.8200 mle=1.5361 pcon=4.9534 forget=1.3305 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 16 it 10 total=7.8672 mle=1.5842 pcon=4.9525 forget=1.3305 nr=64 nf=64 protos=540 fproto_sim=NA
 85%|████████▌ | 17/20 [04:12<00:43, 14.50s/it] 90%|█████████ | 18/20 [04:27<00:28, 14.49s/it] 95%|█████████▌| 19/20 [04:41<00:14, 14.43s/it]100%|██████████| 20/20 [04:55<00:00, 14.41s/it]100%|██████████| 20/20 [04:55<00:00, 14.80s/it]
[loss] ep 16 it 60 total=7.7639 mle=1.4940 pcon=4.9516 forget=1.3182 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=7.7834 mle=1.5129 pcon=4.9510 forget=1.3195 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=7.9708 mle=1.7068 pcon=4.9500 forget=1.3141 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=7.8363 mle=1.5798 pcon=4.9489 forget=1.3076 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=8.1992 mle=1.9415 pcon=4.9480 forget=1.3097 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=8.0510 mle=1.7819 pcon=4.9471 forget=1.3220 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=7.8700 mle=1.6040 pcon=4.9463 forget=1.3196 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 17 it 20 total=8.0372 mle=1.7632 pcon=4.9452 forget=1.3288 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=8.0745 mle=1.8213 pcon=4.9447 forget=1.3085 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=7.8458 mle=1.5793 pcon=4.9438 forget=1.3227 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=7.8401 mle=1.5774 pcon=4.9426 forget=1.3201 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=7.9385 mle=1.6652 pcon=4.9417 forget=1.3316 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=7.7233 mle=1.4575 pcon=4.9415 forget=1.3244 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=8.0642 mle=1.8058 pcon=4.9407 forget=1.3177 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=7.9961 mle=1.7439 pcon=4.9397 forget=1.3126 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=7.8347 mle=1.5899 pcon=4.9394 forget=1.3054 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=7.9552 mle=1.6888 pcon=4.9388 forget=1.3276 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=7.9292 mle=1.6642 pcon=4.9380 forget=1.3269 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=7.8106 mle=1.5488 pcon=4.9374 forget=1.3243 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=7.8431 mle=1.5927 pcon=4.9368 forget=1.3136 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=7.9406 mle=1.6823 pcon=4.9363 forget=1.3220 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=8.0627 mle=1.8112 pcon=4.9357 forget=1.3158 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=7.9591 mle=1.6961 pcon=4.9354 forget=1.3277 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 19 it 40 total=7.7300 mle=1.4666 pcon=4.9351 forget=1.3283 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=7.9856 mle=1.7120 pcon=4.9349 forget=1.3387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=7.8580 mle=1.6011 pcon=4.9343 forget=1.3226 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=7.8815 mle=1.6364 pcon=4.9337 forget=1.3113 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=8.1077 mle=1.8572 pcon=4.9333 forget=1.3173 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=7.8120 mle=1.5452 pcon=4.9331 forget=1.3337 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=7.8873 mle=1.6011 pcon=4.9328 forget=1.3533 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:53,  3.44it/s]  3%|▎         | 11/391 [00:00<00:10, 34.65it/s]  5%|▌         | 21/391 [00:00<00:06, 54.39it/s]  8%|▊         | 31/391 [00:00<00:05, 67.27it/s] 10%|█         | 41/391 [00:00<00:04, 75.64it/s] 13%|█▎        | 51/391 [00:00<00:04, 81.43it/s] 16%|█▌        | 61/391 [00:00<00:03, 85.34it/s] 18%|█▊        | 71/391 [00:01<00:03, 88.22it/s] 21%|██        | 81/391 [00:01<00:03, 90.35it/s] 23%|██▎       | 91/391 [00:01<00:03, 91.91it/s] 26%|██▌       | 101/391 [00:01<00:03, 92.11it/s] 28%|██▊       | 111/391 [00:01<00:03, 93.07it/s] 31%|███       | 121/391 [00:01<00:02, 93.66it/s] 34%|███▎      | 131/391 [00:01<00:02, 94.25it/s] 36%|███▌      | 141/391 [00:01<00:02, 93.81it/s] 39%|███▊      | 151/391 [00:01<00:02, 94.20it/s] 41%|████      | 161/391 [00:01<00:02, 94.61it/s] 44%|████▎     | 171/391 [00:02<00:02, 94.70it/s] 46%|████▋     | 181/391 [00:02<00:02, 94.76it/s] 49%|████▉     | 191/391 [00:02<00:02, 95.02it/s] 51%|█████▏    | 201/391 [00:02<00:01, 95.13it/s] 54%|█████▍    | 211/391 [00:02<00:01, 94.50it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.74it/s] 59%|█████▉    | 231/391 [00:02<00:01, 94.65it/s] 62%|██████▏   | 241/391 [00:02<00:01, 94.87it/s] 64%|██████▍   | 251/391 [00:02<00:01, 94.86it/s] 67%|██████▋   | 261/391 [00:03<00:01, 94.95it/s] 69%|██████▉   | 271/391 [00:03<00:01, 95.08it/s] 72%|███████▏  | 281/391 [00:03<00:01, 95.21it/s] 74%|███████▍  | 291/391 [00:03<00:01, 95.53it/s] 77%|███████▋  | 301/391 [00:03<00:00, 95.59it/s] 80%|███████▉  | 311/391 [00:03<00:00, 95.66it/s] 82%|████████▏ | 321/391 [00:03<00:00, 95.74it/s] 85%|████████▍ | 331/391 [00:03<00:00, 95.88it/s] 87%|████████▋ | 341/391 [00:03<00:00, 96.16it/s] 90%|████████▉ | 351/391 [00:03<00:00, 96.02it/s] 92%|█████████▏| 361/391 [00:04<00:00, 96.06it/s] 95%|█████████▍| 371/391 [00:04<00:00, 96.03it/s] 97%|█████████▋| 381/391 [00:04<00:00, 96.00it/s]100%|██████████| 391/391 [00:04<00:00, 94.33it/s]100%|██████████| 391/391 [00:04<00:00, 88.94it/s]
50000 images processed, 4.4676220417022705 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:18,  4.11it/s] 14%|█▍        | 11/79 [00:00<00:01, 38.81it/s] 27%|██▋       | 21/79 [00:00<00:00, 58.56it/s] 39%|███▉      | 31/79 [00:00<00:00, 70.69it/s] 52%|█████▏    | 41/79 [00:00<00:00, 78.40it/s] 65%|██████▍   | 51/79 [00:00<00:00, 83.48it/s] 77%|███████▋  | 61/79 [00:00<00:00, 87.03it/s] 90%|████████▉ | 71/79 [00:00<00:00, 89.96it/s]100%|██████████| 79/79 [00:01<00:00, 73.92it/s]
10000 images processed, 1.08929443359375 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<00:55,  3.63it/s]  5%|▌         | 11/204 [00:00<00:05, 35.42it/s] 10%|█         | 21/204 [00:00<00:03, 54.59it/s] 15%|█▌        | 31/204 [00:00<00:02, 67.26it/s] 20%|██        | 41/204 [00:00<00:02, 75.57it/s] 25%|██▌       | 51/204 [00:00<00:01, 81.12it/s] 30%|██▉       | 61/204 [00:00<00:01, 84.64it/s] 35%|███▍      | 71/204 [00:01<00:01, 87.54it/s] 40%|███▉      | 81/204 [00:01<00:01, 89.53it/s] 45%|████▍     | 91/204 [00:01<00:01, 89.99it/s] 50%|████▉     | 101/204 [00:01<00:01, 90.87it/s] 54%|█████▍    | 111/204 [00:01<00:01, 91.78it/s] 59%|█████▉    | 121/204 [00:01<00:00, 92.37it/s] 64%|██████▍   | 131/204 [00:01<00:00, 92.80it/s] 69%|██████▉   | 141/204 [00:01<00:00, 92.93it/s] 74%|███████▍  | 151/204 [00:01<00:00, 93.20it/s] 79%|███████▉  | 161/204 [00:01<00:00, 93.13it/s] 84%|████████▍ | 171/204 [00:02<00:00, 93.25it/s] 89%|████████▊ | 181/204 [00:02<00:00, 93.15it/s] 94%|█████████▎| 191/204 [00:02<00:00, 93.55it/s] 99%|█████████▊| 201/204 [00:02<00:00, 94.18it/s]100%|██████████| 204/204 [00:02<00:00, 83.07it/s]
26032 images processed, 2.5189969539642334 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:36,  2.15it/s] 11%|█▏        | 9/79 [00:00<00:03, 17.90it/s] 22%|██▏       | 17/79 [00:00<00:02, 27.91it/s] 32%|███▏      | 25/79 [00:00<00:01, 34.66it/s] 42%|████▏     | 33/79 [00:01<00:01, 39.16it/s] 52%|█████▏    | 41/79 [00:01<00:00, 42.61it/s] 62%|██████▏   | 49/79 [00:01<00:00, 44.62it/s] 72%|███████▏  | 57/79 [00:01<00:00, 46.15it/s] 82%|████████▏ | 65/79 [00:01<00:00, 47.37it/s] 92%|█████████▏| 73/79 [00:01<00:00, 48.31it/s]100%|██████████| 79/79 [00:01<00:00, 40.09it/s]
10000 images processed, 2.007056713104248 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.06it/s] 14%|█▍        | 11/79 [00:00<00:02, 31.96it/s] 27%|██▋       | 21/79 [00:00<00:01, 51.50it/s] 39%|███▉      | 31/79 [00:00<00:00, 64.79it/s] 52%|█████▏    | 41/79 [00:00<00:00, 73.62it/s] 65%|██████▍   | 51/79 [00:00<00:00, 80.08it/s] 77%|███████▋  | 61/79 [00:00<00:00, 84.60it/s] 90%|████████▉ | 71/79 [00:01<00:00, 88.10it/s]100%|██████████| 79/79 [00:01<00:00, 69.26it/s]
10000 images processed, 1.1585404872894287 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:18,  3.77it/s] 16%|█▌        | 11/70 [00:00<00:01, 36.71it/s] 30%|███       | 21/70 [00:00<00:00, 56.55it/s] 44%|████▍     | 31/70 [00:00<00:00, 69.20it/s] 59%|█████▊    | 41/70 [00:00<00:00, 77.47it/s] 73%|███████▎  | 51/70 [00:00<00:00, 83.02it/s] 87%|████████▋ | 61/70 [00:00<00:00, 87.12it/s]100%|██████████| 70/70 [00:00<00:00, 70.57it/s]
8925 images processed, 1.0231213569641113 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:31,  1.39it/s]  4%|▍         | 2/45 [00:00<00:17,  2.39it/s] 20%|██        | 9/45 [00:01<00:02, 12.26it/s] 27%|██▋       | 12/45 [00:01<00:03,  9.99it/s] 36%|███▌      | 16/45 [00:01<00:02, 14.13it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.19it/s] 49%|████▉     | 22/45 [00:02<00:01, 12.40it/s] 58%|█████▊    | 26/45 [00:02<00:01, 11.76it/s] 67%|██████▋   | 30/45 [00:02<00:01, 12.87it/s] 76%|███████▌  | 34/45 [00:03<00:01, 10.90it/s] 93%|█████████▎| 42/45 [00:04<00:00, 10.65it/s]100%|██████████| 45/45 [00:04<00:00, 10.97it/s]
5640 images processed, 4.124694108963013 seconds used

18.03189969062805
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.51  99.36
places365     67.95  81.18
LSUN          17.32  96.12
iSUN          72.34  81.66
dtd           37.87  91.36
forget        79.50  86.86
AVG           46.25  89.42
Retain-Acc: 0.7409
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.86 AUIN: 98.36
8.924608707427979
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.2 lr=0.0005 epochs=50
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.2, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:18<15:20, 18.78s/it]  4%|▍         | 2/50 [00:32<12:51, 16.07s/it]  6%|▌         | 3/50 [00:47<12:02, 15.37s/it]  8%|▊         | 4/50 [01:01<11:23, 14.85s/it] 10%|█         | 5/50 [01:15<11:00, 14.67s/it] 12%|█▏        | 6/50 [01:30<10:40, 14.56s/it] 14%|█▍        | 7/50 [01:44<10:26, 14.57s/it] 16%|█▌        | 8/50 [01:59<10:10, 14.53s/it][loss] ep 0 it 0 total=8.2415 mle=1.5710 pcon=5.2950 forget=1.3755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=8.2316 mle=1.5424 pcon=5.2879 forget=1.4013 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=8.3554 mle=1.7005 pcon=5.2809 forget=1.3740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8.5432 mle=1.8998 pcon=5.2738 forget=1.3696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=8.3628 mle=1.7131 pcon=5.2670 forget=1.3826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=8.1407 mle=1.5020 pcon=5.2603 forget=1.3784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=8.1956 mle=1.5596 pcon=5.2540 forget=1.3819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=8.3181 mle=1.6808 pcon=5.2476 forget=1.3897 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=8.3226 mle=1.6717 pcon=5.2409 forget=1.4101 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=8.2387 mle=1.6371 pcon=5.2346 forget=1.3670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=8.1345 mle=1.5166 pcon=5.2284 forget=1.3895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=8.3921 mle=1.7803 pcon=5.2224 forget=1.3894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=8.3667 mle=1.7647 pcon=5.2167 forget=1.3853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=8.1850 mle=1.6054 pcon=5.2112 forget=1.3683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=8.3079 mle=1.7241 pcon=5.2056 forget=1.3782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=8.3808 mle=1.7944 pcon=5.2003 forget=1.3862 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=8.1249 mle=1.5531 pcon=5.1950 forget=1.3768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=8.4358 mle=1.8882 pcon=5.1899 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=8.3021 mle=1.7415 pcon=5.1847 forget=1.3759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=8.0712 mle=1.5266 pcon=5.1797 forget=1.3650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=8.1462 mle=1.5857 pcon=5.1746 forget=1.3859 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=8.4242 mle=1.8836 pcon=5.1699 forget=1.3707 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=8.1153 mle=1.5924 pcon=5.1652 forget=1.3576 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=8.3366 mle=1.8113 pcon=5.1605 forget=1.3648 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=8.2326 mle=1.7058 pcon=5.1560 forget=1.3708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=8.2022 mle=1.6710 pcon=5.1519 forget=1.3793 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=8.2802 mle=1.7668 pcon=5.1475 forget=1.3659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=8.4241 mle=1.9182 pcon=5.1436 forget=1.3623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=8.0695 mle=1.5700 pcon=5.1395 forget=1.3599 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=8.3278 mle=1.8362 pcon=5.1354 forget=1.3562 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=8.2094 mle=1.7029 pcon=5.1312 forget=1.3754 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=8.2014 mle=1.6956 pcon=5.1276 forget=1.3783 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=8.1199 mle=1.6540 pcon=5.1237 forget=1.3421 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=8.2055 mle=1.7295 pcon=5.1197 forget=1.3562 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8.1326 mle=1.6504 pcon=5.1161 forget=1.3662 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.9319 mle=1.4535 pcon=5.1126 forget=1.3657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=8.2319 mle=1.7666 pcon=5.1087 forget=1.3565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.9501 mle=1.4915 pcon=5.1052 forget=1.3534 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=8.3387 mle=1.8718 pcon=5.1017 forget=1.3652 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=8.3230 mle=1.8673 pcon=5.0984 forget=1.3573 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=8.2406 mle=1.7998 pcon=5.0948 forget=1.3460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=8.0360 mle=1.5842 pcon=5.0917 forget=1.3601 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=8.0423 mle=1.5953 pcon=5.0882 forget=1.3588 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=8.1221 mle=1.6910 pcon=5.0853 forget=1.3458 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=8.1947 mle=1.7736 pcon=5.0823 forget=1.3389 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=8.0266 mle=1.5963 pcon=5.0792 forget=1.3510 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=8.1720 mle=1.7468 pcon=5.0763 forget=1.3489 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.8466 mle=1.4010 pcon=5.0735 forget=1.3721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.9348 mle=1.5354 pcon=5.0707 forget=1.3286 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=8.0213 mle=1.5917 pcon=5.0681 forget=1.3615 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=8.0768 mle=1.6655 pcon=5.0653 forget=1.3460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=8.0349 mle=1.6301 pcon=5.0631 forget=1.3417 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=8.2389 mle=1.8227 pcon=5.0602 forget=1.3560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.9567 mle=1.5351 pcon=5.0578 forget=1.3639 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=8.0775 mle=1.6746 pcon=5.0553 forget=1.3475 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=8.0230 mle=1.6307 pcon=5.0527 forget=1.3396 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=8.1302 mle=1.7447 pcon=5.0503 forget=1.3351 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=8.0164 mle=1.6266 pcon=5.0478 forget=1.3420 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.9565 mle=1.5821 pcon=5.0455 forget=1.3289 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=8.0488 mle=1.6532 pcon=5.0435 forget=1.3522 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=8.1984 mle=1.7955 pcon=5.0413 forget=1.3616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=8.1396 mle=1.7412 pcon=5.0389 forget=1.3596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=8.0230 mle=1.6399 pcon=5.0368 forget=1.3462 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 18%|█▊        | 9/50 [02:13<09:52, 14.44s/it] 20%|██        | 10/50 [02:27<09:35, 14.38s/it] 22%|██▏       | 11/50 [02:42<09:19, 14.36s/it] 24%|██▍       | 12/50 [02:56<09:03, 14.31s/it] 26%|██▌       | 13/50 [03:10<08:50, 14.33s/it] 28%|██▊       | 14/50 [03:24<08:34, 14.30s/it] 30%|███       | 15/50 [03:39<08:18, 14.24s/it] 32%|███▏      | 16/50 [03:53<08:03, 14.21s/it][loss] ep 8 it 30 total=7.9317 mle=1.5366 pcon=5.0349 forget=1.3602 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=7.9715 mle=1.5900 pcon=5.0330 forget=1.3485 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.8948 mle=1.5067 pcon=5.0308 forget=1.3573 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=8.0736 mle=1.7033 pcon=5.0289 forget=1.3414 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.8977 mle=1.5306 pcon=5.0274 forget=1.3397 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=8.0336 mle=1.6774 pcon=5.0254 forget=1.3308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=8.1461 mle=1.7558 pcon=5.0233 forget=1.3669 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.8426 mle=1.4882 pcon=5.0214 forget=1.3330 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.9700 mle=1.6358 pcon=5.0195 forget=1.3147 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=8.0220 mle=1.6688 pcon=5.0173 forget=1.3359 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=8.2073 mle=1.8404 pcon=5.0159 forget=1.3511 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.9286 mle=1.5776 pcon=5.0143 forget=1.3368 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.9191 mle=1.5663 pcon=5.0127 forget=1.3400 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.9160 mle=1.5589 pcon=5.0108 forget=1.3463 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.9026 mle=1.5573 pcon=5.0093 forget=1.3359 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=7.9603 mle=1.6070 pcon=5.0075 forget=1.3458 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=7.9506 mle=1.5893 pcon=5.0058 forget=1.3555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=8.0941 mle=1.7536 pcon=5.0042 forget=1.3363 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=8.1805 mle=1.8489 pcon=5.0028 forget=1.3288 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=8.1889 mle=1.8574 pcon=5.0013 forget=1.3302 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=7.9195 mle=1.5823 pcon=5.0000 forget=1.3372 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=7.8833 mle=1.5617 pcon=4.9987 forget=1.3228 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=8.0214 mle=1.6671 pcon=4.9973 forget=1.3570 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=7.9181 mle=1.5735 pcon=4.9958 forget=1.3487 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=7.9446 mle=1.6207 pcon=4.9944 forget=1.3295 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=7.8843 mle=1.5684 pcon=4.9931 forget=1.3228 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=7.9890 mle=1.6633 pcon=4.9914 forget=1.3342 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=8.1136 mle=1.7818 pcon=4.9902 forget=1.3417 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=7.9395 mle=1.6108 pcon=4.9893 forget=1.3394 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=8.2918 mle=1.9669 pcon=4.9878 forget=1.3370 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=7.9710 mle=1.6685 pcon=4.9865 forget=1.3160 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 12 it 20 total=7.9373 mle=1.6199 pcon=4.9850 forget=1.3324 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=8.0014 mle=1.6818 pcon=4.9837 forget=1.3359 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=7.8753 mle=1.5454 pcon=4.9824 forget=1.3474 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=8.0462 mle=1.7429 pcon=4.9812 forget=1.3221 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=7.8994 mle=1.5939 pcon=4.9798 forget=1.3257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=7.8527 mle=1.5312 pcon=4.9789 forget=1.3426 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=7.8224 mle=1.5131 pcon=4.9777 forget=1.3316 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=8.1813 mle=1.8633 pcon=4.9765 forget=1.3416 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=7.7359 mle=1.4283 pcon=4.9754 forget=1.3322 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=7.8304 mle=1.4902 pcon=4.9741 forget=1.3660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=7.8687 mle=1.5640 pcon=4.9728 forget=1.3320 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=7.8500 mle=1.5548 pcon=4.9718 forget=1.3234 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=7.9463 mle=1.6419 pcon=4.9707 forget=1.3337 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=7.8678 mle=1.5809 pcon=4.9695 forget=1.3174 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=7.8865 mle=1.5858 pcon=4.9683 forget=1.3324 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=7.8614 mle=1.5724 pcon=4.9672 forget=1.3217 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=8.0432 mle=1.7642 pcon=4.9658 forget=1.3132 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=8.0783 mle=1.7825 pcon=4.9649 forget=1.3309 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=7.8914 mle=1.6052 pcon=4.9643 forget=1.3220 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=8.1666 mle=1.8809 pcon=4.9630 forget=1.3228 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=7.8990 mle=1.6174 pcon=4.9621 forget=1.3195 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=7.9305 mle=1.6616 pcon=4.9610 forget=1.3079 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=7.9549 mle=1.6816 pcon=4.9598 forget=1.3134 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=7.9862 mle=1.6897 pcon=4.9589 forget=1.3377 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=7.7542 mle=1.4833 pcon=4.9577 forget=1.3133 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=7.8410 mle=1.5626 pcon=4.9567 forget=1.3217 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=8.0116 mle=1.7196 pcon=4.9556 forget=1.3364 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=7.8751 mle=1.6096 pcon=4.9546 forget=1.3109 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=8.1641 mle=1.8857 pcon=4.9535 forget=1.3249 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=7.8746 mle=1.5953 pcon=4.9526 forget=1.3266 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=7.8096 mle=1.5326 pcon=4.9516 forget=1.3253 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 16 it 10 total=7.8572 mle=1.5807 pcon=4.9506 forget=1.3259 nr=64 nf=64 protos=540 fproto_sim=NA
 34%|███▍      | 17/50 [04:07<07:46, 14.13s/it] 36%|███▌      | 18/50 [04:21<07:34, 14.19s/it] 38%|███▊      | 19/50 [04:35<07:20, 14.21s/it] 40%|████      | 20/50 [04:50<07:09, 14.31s/it] 42%|████▏     | 21/50 [05:04<06:54, 14.28s/it] 44%|████▍     | 22/50 [05:18<06:38, 14.24s/it] 46%|████▌     | 23/50 [05:32<06:25, 14.26s/it] 48%|████▊     | 24/50 [05:47<06:11, 14.27s/it] 50%|█████     | 25/50 [06:01<05:56, 14.28s/it][loss] ep 16 it 60 total=7.7597 mle=1.4955 pcon=4.9496 forget=1.3145 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=7.7756 mle=1.5117 pcon=4.9488 forget=1.3152 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=7.9658 mle=1.7056 pcon=4.9476 forget=1.3126 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=7.8341 mle=1.5829 pcon=4.9464 forget=1.3047 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=8.1903 mle=1.9384 pcon=4.9453 forget=1.3066 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=8.0430 mle=1.7791 pcon=4.9443 forget=1.3196 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=7.8613 mle=1.5979 pcon=4.9433 forget=1.3201 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 17 it 20 total=8.0375 mle=1.7689 pcon=4.9420 forget=1.3266 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=8.0660 mle=1.8176 pcon=4.9413 forget=1.3070 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=7.8402 mle=1.5786 pcon=4.9402 forget=1.3214 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=7.8366 mle=1.5797 pcon=4.9388 forget=1.3181 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=7.9331 mle=1.6637 pcon=4.9378 forget=1.3316 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=7.7132 mle=1.4511 pcon=4.9373 forget=1.3248 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=8.0582 mle=1.8026 pcon=4.9363 forget=1.3193 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=7.9973 mle=1.7477 pcon=4.9352 forget=1.3144 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=7.8382 mle=1.5968 pcon=4.9347 forget=1.3067 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=7.9603 mle=1.6914 pcon=4.9339 forget=1.3351 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=7.9144 mle=1.6500 pcon=4.9329 forget=1.3315 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=7.8139 mle=1.5539 pcon=4.9321 forget=1.3278 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=7.8479 mle=1.5957 pcon=4.9313 forget=1.3209 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=7.9391 mle=1.6785 pcon=4.9306 forget=1.3301 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=8.0571 mle=1.8051 pcon=4.9298 forget=1.3222 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=7.9595 mle=1.6948 pcon=4.9293 forget=1.3354 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 40 total=7.7291 mle=1.4630 pcon=4.9289 forget=1.3372 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=7.9850 mle=1.7079 pcon=4.9285 forget=1.3486 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=7.8541 mle=1.5905 pcon=4.9277 forget=1.3358 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=7.8878 mle=1.6339 pcon=4.9269 forget=1.3269 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=8.1187 mle=1.8618 pcon=4.9263 forget=1.3306 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=7.8165 mle=1.5420 pcon=4.9259 forget=1.3486 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=7.9035 mle=1.6134 pcon=4.9255 forget=1.3646 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 20 it 0 total=7.9234 mle=1.6622 pcon=4.9249 forget=1.3363 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=7.9373 mle=1.6787 pcon=4.9242 forget=1.3344 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=7.9963 mle=1.7233 pcon=4.9238 forget=1.3491 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=8.0924 mle=1.8323 pcon=4.9231 forget=1.3369 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=7.8957 mle=1.6448 pcon=4.9223 forget=1.3286 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=8.2592 mle=2.0037 pcon=4.9216 forget=1.3339 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=7.9431 mle=1.6701 pcon=4.9207 forget=1.3523 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=7.6668 mle=1.4044 pcon=4.9201 forget=1.3423 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 10 total=7.6691 mle=1.3986 pcon=4.9194 forget=1.3510 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=7.9624 mle=1.6848 pcon=4.9189 forget=1.3587 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=7.7936 mle=1.5125 pcon=4.9185 forget=1.3626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=7.8128 mle=1.5576 pcon=4.9182 forget=1.3371 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=8.0471 mle=1.7673 pcon=4.9177 forget=1.3620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=7.9411 mle=1.6671 pcon=4.9172 forget=1.3568 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=7.8973 mle=1.6254 pcon=4.9165 forget=1.3554 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=7.9250 mle=1.6392 pcon=4.9160 forget=1.3698 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 20 total=7.9249 mle=1.6537 pcon=4.9153 forget=1.3559 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=8.0596 mle=1.7984 pcon=4.9146 forget=1.3466 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=7.8082 mle=1.5253 pcon=4.9138 forget=1.3691 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=7.9198 mle=1.6430 pcon=4.9134 forget=1.3634 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=7.9457 mle=1.6638 pcon=4.9129 forget=1.3690 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=7.7908 mle=1.5061 pcon=4.9124 forget=1.3723 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=7.9155 mle=1.6466 pcon=4.9120 forget=1.3568 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=8.3481 mle=2.0673 pcon=4.9113 forget=1.3695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 30 total=7.8156 mle=1.5399 pcon=4.9113 forget=1.3645 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=7.9652 mle=1.6968 pcon=4.9107 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=7.8380 mle=1.5466 pcon=4.9101 forget=1.3813 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=8.0129 mle=1.7492 pcon=4.9095 forget=1.3542 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=7.9400 mle=1.6613 pcon=4.9092 forget=1.3696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=7.8383 mle=1.5555 pcon=4.9085 forget=1.3743 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=7.9356 mle=1.6672 pcon=4.9080 forget=1.3604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=7.8148 mle=1.5303 pcon=4.9074 forget=1.3771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 40 total=7.7839 mle=1.5199 pcon=4.9070 forget=1.3570 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=7.8738 mle=1.6012 pcon=4.9066 forget=1.3660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=7.7133 mle=1.4304 pcon=4.9060 forget=1.3769 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=7.9210 mle=1.6470 pcon=4.9057 forget=1.3682 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 240 total=7.8723 mle=1.5922 pcon=4.9053 forget=1.3749 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=8.0736 mle=1.7877 pcon=4.9048 forget=1.3812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=7.9836 mle=1.7134 pcon=4.9042 forget=1.3660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 0 total=7.9383 mle=1.6577 pcon=4.9035 forget=1.3771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=8.1015 mle=1.8092 pcon=4.9032 forget=1.3891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=7.8737 mle=1.6131 pcon=4.9026 forget=1.3580 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 150 total=8.0567 mle=1.7883 pcon=4.9025 forget=1.3659 nr=64 nf=64 protos=540 fproto_sim=NA
 52%|█████▏    | 26/50 [06:15<05:43, 14.32s/it] 54%|█████▍    | 27/50 [06:30<05:28, 14.28s/it] 56%|█████▌    | 28/50 [06:44<05:14, 14.30s/it] 58%|█████▊    | 29/50 [06:58<05:00, 14.32s/it] 60%|██████    | 30/50 [07:13<04:47, 14.37s/it] 62%|██████▏   | 31/50 [07:27<04:31, 14.30s/it] 64%|██████▍   | 32/50 [07:41<04:16, 14.24s/it] 66%|██████▌   | 33/50 [07:55<04:02, 14.25s/it][loss] ep 25 it 200 total=8.1048 mle=1.8221 pcon=4.9019 forget=1.3808 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=8.0368 mle=1.7580 pcon=4.9012 forget=1.3776 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=7.8565 mle=1.5878 pcon=4.9007 forget=1.3681 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=7.8701 mle=1.5956 pcon=4.9004 forget=1.3741 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 26 it 10 total=7.8093 mle=1.5270 pcon=4.9000 forget=1.3824 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=7.8326 mle=1.5467 pcon=4.8995 forget=1.3864 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=8.0564 mle=1.7911 pcon=4.8989 forget=1.3664 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=7.7397 mle=1.4736 pcon=4.8983 forget=1.3678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=7.9370 mle=1.6612 pcon=4.8975 forget=1.3783 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=7.8088 mle=1.5331 pcon=4.8970 forget=1.3787 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 310 total=8.1647 mle=1.8969 pcon=4.8965 forget=1.3712 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=7.8405 mle=1.5775 pcon=4.8955 forget=1.3676 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 20 total=7.9442 mle=1.6724 pcon=4.8948 forget=1.3770 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=7.8198 mle=1.5484 pcon=4.8939 forget=1.3776 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=7.8109 mle=1.5384 pcon=4.8931 forget=1.3794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=7.8303 mle=1.5550 pcon=4.8922 forget=1.3831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=7.8384 mle=1.5695 pcon=4.8914 forget=1.3775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=7.8934 mle=1.6275 pcon=4.8904 forget=1.3755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=8.0030 mle=1.7389 pcon=4.8897 forget=1.3744 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=8.0387 mle=1.7624 pcon=4.8889 forget=1.3873 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 30 total=7.9477 mle=1.6818 pcon=4.8882 forget=1.3777 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=7.9952 mle=1.7320 pcon=4.8876 forget=1.3756 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=7.7875 mle=1.5190 pcon=4.8870 forget=1.3815 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=7.7346 mle=1.4867 pcon=4.8862 forget=1.3616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=7.6382 mle=1.3888 pcon=4.8851 forget=1.3642 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=7.8010 mle=1.5385 pcon=4.8841 forget=1.3785 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=8.0044 mle=1.7443 pcon=4.8828 forget=1.3773 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=8.0141 mle=1.7464 pcon=4.8817 forget=1.3860 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 29 it 40 total=8.0059 mle=1.7484 pcon=4.8806 forget=1.3769 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=7.7899 mle=1.5506 pcon=4.8794 forget=1.3599 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=7.9074 mle=1.6553 pcon=4.8786 forget=1.3735 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=7.8338 mle=1.5832 pcon=4.8776 forget=1.3730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=7.7378 mle=1.4871 pcon=4.8766 forget=1.3741 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=8.0845 mle=1.8519 pcon=4.8758 forget=1.3568 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=7.8497 mle=1.6037 pcon=4.8749 forget=1.3710 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 30 it 0 total=7.8690 mle=1.6328 pcon=4.8740 forget=1.3622 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=7.8904 mle=1.6389 pcon=4.8732 forget=1.3783 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=7.7225 mle=1.4808 pcon=4.8722 forget=1.3695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=8.0700 mle=1.8329 pcon=4.8710 forget=1.3661 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=7.7355 mle=1.4664 pcon=4.8700 forget=1.3991 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=8.0008 mle=1.7553 pcon=4.8689 forget=1.3766 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=7.7463 mle=1.5009 pcon=4.8680 forget=1.3774 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=8.1942 mle=1.9481 pcon=4.8669 forget=1.3791 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 31 it 10 total=7.8268 mle=1.5909 pcon=4.8660 forget=1.3699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=7.7825 mle=1.5460 pcon=4.8650 forget=1.3715 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=7.8301 mle=1.5881 pcon=4.8640 forget=1.3780 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=7.7509 mle=1.5165 pcon=4.8630 forget=1.3713 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=7.7779 mle=1.5537 pcon=4.8620 forget=1.3623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 260 total=7.6889 mle=1.4718 pcon=4.8607 forget=1.3564 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=7.8311 mle=1.6009 pcon=4.8596 forget=1.3706 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=7.7611 mle=1.5460 pcon=4.8584 forget=1.3567 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 32 it 20 total=7.9686 mle=1.7522 pcon=4.8573 forget=1.3592 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=7.8134 mle=1.6074 pcon=4.8558 forget=1.3502 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=7.7680 mle=1.5307 pcon=4.8546 forget=1.3826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 170 total=7.7830 mle=1.5593 pcon=4.8537 forget=1.3701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=7.9588 mle=1.7521 pcon=4.8526 forget=1.3541 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=7.7714 mle=1.5622 pcon=4.8516 forget=1.3576 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=7.6722 mle=1.4639 pcon=4.8504 forget=1.3580 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=7.6248 mle=1.4143 pcon=4.8496 forget=1.3608 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 33 it 30 total=7.8736 mle=1.6550 pcon=4.8485 forget=1.3700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=8.2238 mle=2.0303 pcon=4.8474 forget=1.3461 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=7.6776 mle=1.4694 pcon=4.8462 forget=1.3620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=8.0244 mle=1.8043 pcon=4.8451 forget=1.3751 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=7.7643 mle=1.5536 pcon=4.8439 forget=1.3667 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=7.8276 mle=1.6254 pcon=4.8428 forget=1.3594 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=7.8217 mle=1.6267 pcon=4.8419 forget=1.3531 nr=64 nf=64 protos=540 fproto_sim=NA
 68%|██████▊   | 34/50 [08:10<03:47, 14.25s/it] 70%|███████   | 35/50 [08:24<03:33, 14.25s/it] 72%|███████▏  | 36/50 [08:38<03:19, 14.23s/it] 74%|███████▍  | 37/50 [08:52<03:05, 14.30s/it] 76%|███████▌  | 38/50 [09:07<02:52, 14.36s/it] 78%|███████▊  | 39/50 [09:21<02:36, 14.27s/it] 80%|████████  | 40/50 [09:35<02:23, 14.32s/it] 82%|████████▏ | 41/50 [09:50<02:09, 14.35s/it][loss] ep 33 it 380 total=7.7727 mle=1.5596 pcon=4.8407 forget=1.3724 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 34 it 40 total=7.9222 mle=1.7194 pcon=4.8393 forget=1.3635 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=7.6143 mle=1.4160 pcon=4.8379 forget=1.3604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 140 total=7.7746 mle=1.5976 pcon=4.8368 forget=1.3402 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=7.8557 mle=1.6684 pcon=4.8357 forget=1.3516 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 240 total=7.9758 mle=1.7879 pcon=4.8344 forget=1.3536 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=7.7729 mle=1.5793 pcon=4.8329 forget=1.3606 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=7.7457 mle=1.5434 pcon=4.8318 forget=1.3705 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 35 it 0 total=7.8631 mle=1.6803 pcon=4.8305 forget=1.3522 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 50 total=7.6999 mle=1.5189 pcon=4.8294 forget=1.3515 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 100 total=8.0181 mle=1.8280 pcon=4.8280 forget=1.3622 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 150 total=7.8004 mle=1.6282 pcon=4.8268 forget=1.3454 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 200 total=7.7844 mle=1.5887 pcon=4.8257 forget=1.3700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 250 total=7.8757 mle=1.6912 pcon=4.8244 forget=1.3601 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 300 total=7.6343 mle=1.4681 pcon=4.8232 forget=1.3431 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 350 total=7.9388 mle=1.7635 pcon=4.8220 forget=1.3534 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 36 it 10 total=7.9996 mle=1.8179 pcon=4.8208 forget=1.3609 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 60 total=7.6844 mle=1.5185 pcon=4.8198 forget=1.3462 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 110 total=7.8155 mle=1.6432 pcon=4.8186 forget=1.3538 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 160 total=7.8193 mle=1.6518 pcon=4.8174 forget=1.3500 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 210 total=7.6279 mle=1.4691 pcon=4.8163 forget=1.3425 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 260 total=7.7354 mle=1.5510 pcon=4.8155 forget=1.3689 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 310 total=7.8841 mle=1.6956 pcon=4.8142 forget=1.3743 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 360 total=7.7477 mle=1.5766 pcon=4.8130 forget=1.3581 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 37 it 20 total=7.6235 mle=1.4606 pcon=4.8116 forget=1.3514 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 70 total=7.9024 mle=1.7280 pcon=4.8106 forget=1.3639 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 120 total=7.6883 mle=1.5166 pcon=4.8093 forget=1.3623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 170 total=7.7768 mle=1.6158 pcon=4.8081 forget=1.3529 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 220 total=7.9144 mle=1.7411 pcon=4.8067 forget=1.3666 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 270 total=7.7659 mle=1.6189 pcon=4.8055 forget=1.3415 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 320 total=7.7742 mle=1.6196 pcon=4.8043 forget=1.3502 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 370 total=7.7557 mle=1.5735 pcon=4.8034 forget=1.3788 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 38 it 30 total=7.8371 mle=1.6659 pcon=4.8024 forget=1.3687 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 80 total=7.5899 mle=1.4354 pcon=4.8015 forget=1.3529 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 130 total=7.8202 mle=1.6626 pcon=4.8004 forget=1.3572 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 180 total=7.7429 mle=1.5866 pcon=4.7993 forget=1.3570 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 230 total=7.6888 mle=1.5324 pcon=4.7983 forget=1.3582 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 280 total=7.8181 mle=1.6695 pcon=4.7973 forget=1.3513 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 330 total=7.7508 mle=1.6036 pcon=4.7960 forget=1.3511 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 380 total=7.6233 mle=1.4707 pcon=4.7952 forget=1.3574 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 39 it 40 total=7.9638 mle=1.8158 pcon=4.7945 forget=1.3535 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 90 total=7.8501 mle=1.6960 pcon=4.7935 forget=1.3606 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 140 total=7.8087 mle=1.6660 pcon=4.7926 forget=1.3501 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 190 total=7.6218 mle=1.4807 pcon=4.7917 forget=1.3494 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 240 total=7.8176 mle=1.6738 pcon=4.7908 forget=1.3530 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 290 total=7.5828 mle=1.4381 pcon=4.7899 forget=1.3548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 340 total=7.7661 mle=1.6229 pcon=4.7891 forget=1.3542 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 40 it 0 total=7.7395 mle=1.5880 pcon=4.7880 forget=1.3634 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 50 total=7.7670 mle=1.6178 pcon=4.7870 forget=1.3621 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 100 total=7.7347 mle=1.6086 pcon=4.7859 forget=1.3402 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 150 total=7.5871 mle=1.4552 pcon=4.7851 forget=1.3468 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 200 total=7.9225 mle=1.7735 pcon=4.7841 forget=1.3648 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 250 total=7.9468 mle=1.8073 pcon=4.7832 forget=1.3563 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 300 total=7.6243 mle=1.4888 pcon=4.7825 forget=1.3530 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 350 total=7.5350 mle=1.4070 pcon=4.7817 forget=1.3463 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 41 it 10 total=7.6434 mle=1.5231 pcon=4.7809 forget=1.3395 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 60 total=8.0730 mle=1.9244 pcon=4.7802 forget=1.3684 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 110 total=7.6913 mle=1.5570 pcon=4.7793 forget=1.3550 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 160 total=7.6470 mle=1.5183 pcon=4.7787 forget=1.3500 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 210 total=7.7105 mle=1.5803 pcon=4.7779 forget=1.3523 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 260 total=7.5802 mle=1.4670 pcon=4.7774 forget=1.3358 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 310 total=7.6155 mle=1.4957 pcon=4.7770 forget=1.3428 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 360 total=7.7369 mle=1.6172 pcon=4.7764 forget=1.3433 nr=64 nf=64 protos=540 fproto_sim=NA
 84%|████████▍ | 42/50 [10:04<01:55, 14.39s/it] 86%|████████▌ | 43/50 [10:19<01:40, 14.40s/it] 88%|████████▊ | 44/50 [10:33<01:26, 14.36s/it] 90%|█████████ | 45/50 [10:47<01:11, 14.38s/it] 92%|█████████▏| 46/50 [11:02<00:57, 14.33s/it] 94%|█████████▍| 47/50 [11:16<00:43, 14.35s/it] 96%|█████████▌| 48/50 [11:31<00:28, 14.39s/it] 98%|█████████▊| 49/50 [11:45<00:14, 14.36s/it]100%|██████████| 50/50 [11:59<00:00, 14.25s/it]100%|██████████| 50/50 [11:59<00:00, 14.39s/it]
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 42 it 20 total=7.7858 mle=1.6553 pcon=4.7758 forget=1.3547 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 70 total=7.7444 mle=1.6208 pcon=4.7749 forget=1.3486 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 120 total=8.0775 mle=1.9422 pcon=4.7741 forget=1.3612 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 170 total=7.7121 mle=1.5797 pcon=4.7735 forget=1.3589 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 220 total=7.7489 mle=1.6203 pcon=4.7729 forget=1.3557 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 270 total=7.8700 mle=1.7495 pcon=4.7723 forget=1.3483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 320 total=7.7939 mle=1.6767 pcon=4.7718 forget=1.3453 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 370 total=7.7512 mle=1.6324 pcon=4.7713 forget=1.3475 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 43 it 30 total=7.7370 mle=1.6055 pcon=4.7706 forget=1.3608 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 80 total=7.6938 mle=1.5640 pcon=4.7700 forget=1.3598 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 130 total=7.8014 mle=1.6706 pcon=4.7694 forget=1.3613 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 180 total=7.6337 mle=1.4982 pcon=4.7689 forget=1.3666 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 230 total=7.6598 mle=1.5488 pcon=4.7682 forget=1.3428 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 280 total=7.6464 mle=1.5346 pcon=4.7677 forget=1.3441 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 330 total=7.6466 mle=1.5294 pcon=4.7669 forget=1.3502 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 380 total=7.6058 mle=1.4980 pcon=4.7663 forget=1.3415 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 44 it 40 total=7.8526 mle=1.7270 pcon=4.7657 forget=1.3599 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 90 total=7.6467 mle=1.5422 pcon=4.7651 forget=1.3394 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 140 total=7.7554 mle=1.6270 pcon=4.7647 forget=1.3637 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 190 total=7.5643 mle=1.4517 pcon=4.7641 forget=1.3485 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 240 total=7.6554 mle=1.5193 pcon=4.7636 forget=1.3724 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 290 total=7.8797 mle=1.7649 pcon=4.7631 forget=1.3517 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 340 total=7.9733 mle=1.8607 pcon=4.7626 forget=1.3500 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 45 it 0 total=7.6402 mle=1.5231 pcon=4.7621 forget=1.3550 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 50 total=7.7079 mle=1.6097 pcon=4.7616 forget=1.3367 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 100 total=7.6514 mle=1.5426 pcon=4.7611 forget=1.3477 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 150 total=7.6388 mle=1.5261 pcon=4.7605 forget=1.3522 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 200 total=7.7019 mle=1.5849 pcon=4.7601 forget=1.3569 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 250 total=7.6321 mle=1.5258 pcon=4.7596 forget=1.3467 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 300 total=7.7595 mle=1.6591 pcon=4.7590 forget=1.3414 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 350 total=7.6201 mle=1.5128 pcon=4.7586 forget=1.3487 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 10 total=7.5731 mle=1.4590 pcon=4.7582 forget=1.3559 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 60 total=7.5863 mle=1.4813 pcon=4.7577 forget=1.3473 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 110 total=7.7641 mle=1.6507 pcon=4.7573 forget=1.3560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 160 total=7.5829 mle=1.4792 pcon=4.7568 forget=1.3468 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 210 total=7.8285 mle=1.7313 pcon=4.7563 forget=1.3410 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 260 total=7.7621 mle=1.6682 pcon=4.7558 forget=1.3381 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 310 total=7.6358 mle=1.5302 pcon=4.7555 forget=1.3501 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 360 total=7.8021 mle=1.6946 pcon=4.7550 forget=1.3525 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 47 it 20 total=7.6408 mle=1.5289 pcon=4.7545 forget=1.3574 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 70 total=7.6654 mle=1.5588 pcon=4.7540 forget=1.3525 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 120 total=7.8015 mle=1.6886 pcon=4.7535 forget=1.3594 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 170 total=7.6342 mle=1.5160 pcon=4.7528 forget=1.3654 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 220 total=7.5584 mle=1.4439 pcon=4.7525 forget=1.3620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 270 total=7.7030 mle=1.5966 pcon=4.7521 forget=1.3543 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 320 total=7.6842 mle=1.5823 pcon=4.7517 forget=1.3501 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 370 total=7.7447 mle=1.6413 pcon=4.7513 forget=1.3521 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 30 total=7.7499 mle=1.6230 pcon=4.7511 forget=1.3758 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 80 total=7.6948 mle=1.5803 pcon=4.7507 forget=1.3638 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 130 total=7.7416 mle=1.6413 pcon=4.7504 forget=1.3499 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 180 total=7.7277 mle=1.6215 pcon=4.7502 forget=1.3560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 230 total=7.7456 mle=1.6615 pcon=4.7500 forget=1.3341 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 280 total=7.6226 mle=1.5331 pcon=4.7496 forget=1.3399 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 330 total=7.6327 mle=1.5534 pcon=4.7492 forget=1.3301 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 380 total=7.6435 mle=1.5516 pcon=4.7487 forget=1.3432 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 49 it 40 total=7.6754 mle=1.5807 pcon=4.7485 forget=1.3462 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 90 total=7.7163 mle=1.6239 pcon=4.7482 forget=1.3442 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 140 total=7.6978 mle=1.5975 pcon=4.7479 forget=1.3524 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 190 total=7.8067 mle=1.7152 pcon=4.7474 forget=1.3441 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 240 total=7.7560 mle=1.6683 pcon=4.7470 forget=1.3408 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 290 total=7.7003 mle=1.6108 pcon=4.7466 forget=1.3429 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 340 total=7.5723 mle=1.4817 pcon=4.7465 forget=1.3440 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:18,  2.81it/s]  3%|▎         | 11/391 [00:00<00:12, 30.11it/s]  5%|▌         | 21/391 [00:00<00:07, 49.14it/s]  8%|▊         | 31/391 [00:00<00:05, 62.42it/s] 10%|█         | 41/391 [00:00<00:04, 71.67it/s] 13%|█▎        | 51/391 [00:00<00:04, 78.14it/s] 15%|█▌        | 60/391 [00:00<00:04, 80.85it/s] 18%|█▊        | 70/391 [00:01<00:03, 83.70it/s] 20%|██        | 80/391 [00:01<00:03, 86.18it/s] 23%|██▎       | 90/391 [00:01<00:03, 88.65it/s] 26%|██▌       | 100/391 [00:01<00:03, 90.37it/s] 28%|██▊       | 110/391 [00:01<00:03, 90.65it/s] 31%|███       | 120/391 [00:01<00:02, 90.75it/s] 33%|███▎      | 130/391 [00:01<00:02, 91.51it/s] 36%|███▌      | 140/391 [00:01<00:02, 91.34it/s] 38%|███▊      | 150/391 [00:01<00:02, 92.10it/s] 41%|████      | 160/391 [00:02<00:02, 92.77it/s] 43%|████▎     | 170/391 [00:02<00:02, 92.98it/s] 46%|████▌     | 180/391 [00:02<00:02, 93.40it/s] 49%|████▊     | 190/391 [00:02<00:02, 93.54it/s] 51%|█████     | 200/391 [00:02<00:02, 93.77it/s] 54%|█████▎    | 210/391 [00:02<00:01, 93.81it/s] 56%|█████▋    | 220/391 [00:02<00:01, 93.91it/s] 59%|█████▉    | 230/391 [00:02<00:01, 93.68it/s] 61%|██████▏   | 240/391 [00:02<00:01, 93.85it/s] 64%|██████▍   | 250/391 [00:03<00:01, 93.96it/s] 66%|██████▋   | 260/391 [00:03<00:01, 94.09it/s] 69%|██████▉   | 270/391 [00:03<00:01, 94.25it/s] 72%|███████▏  | 280/391 [00:03<00:01, 93.75it/s] 74%|███████▍  | 290/391 [00:03<00:01, 93.71it/s] 77%|███████▋  | 300/391 [00:03<00:00, 93.79it/s] 79%|███████▉  | 310/391 [00:03<00:00, 93.40it/s] 82%|████████▏ | 320/391 [00:03<00:00, 93.41it/s] 84%|████████▍ | 330/391 [00:03<00:00, 93.55it/s] 87%|████████▋ | 340/391 [00:03<00:00, 93.72it/s] 90%|████████▉ | 350/391 [00:04<00:00, 93.89it/s] 92%|█████████▏| 360/391 [00:04<00:00, 94.05it/s] 95%|█████████▍| 370/391 [00:04<00:00, 94.22it/s] 97%|█████████▋| 380/391 [00:04<00:00, 94.15it/s]100%|█████████▉| 390/391 [00:04<00:00, 95.05it/s]100%|██████████| 391/391 [00:04<00:00, 86.08it/s]
50000 images processed, 4.651934862136841 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:28,  2.71it/s] 14%|█▍        | 11/79 [00:00<00:02, 29.31it/s] 25%|██▌       | 20/79 [00:00<00:01, 46.34it/s] 38%|███▊      | 30/79 [00:00<00:00, 60.42it/s] 51%|█████     | 40/79 [00:00<00:00, 70.07it/s] 63%|██████▎   | 50/79 [00:00<00:00, 76.79it/s] 76%|███████▌  | 60/79 [00:01<00:00, 81.58it/s] 89%|████████▊ | 70/79 [00:01<00:00, 85.61it/s]100%|██████████| 79/79 [00:01<00:00, 64.97it/s]
10000 images processed, 1.2392661571502686 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:19,  2.56it/s]  5%|▌         | 11/204 [00:00<00:06, 28.22it/s] 10%|█         | 21/204 [00:00<00:03, 46.86it/s] 15%|█▌        | 31/204 [00:00<00:02, 60.08it/s] 20%|██        | 41/204 [00:00<00:02, 69.75it/s] 25%|██▌       | 51/204 [00:00<00:01, 76.55it/s] 30%|██▉       | 61/204 [00:01<00:01, 81.35it/s] 35%|███▍      | 71/204 [00:01<00:01, 84.57it/s] 40%|███▉      | 81/204 [00:01<00:01, 86.62it/s] 45%|████▍     | 91/204 [00:01<00:01, 88.22it/s] 50%|████▉     | 101/204 [00:01<00:01, 89.54it/s] 54%|█████▍    | 111/204 [00:01<00:01, 90.76it/s] 59%|█████▉    | 121/204 [00:01<00:00, 91.05it/s] 64%|██████▍   | 131/204 [00:01<00:00, 91.74it/s] 69%|██████▉   | 141/204 [00:01<00:00, 92.37it/s] 74%|███████▍  | 151/204 [00:02<00:00, 92.47it/s] 79%|███████▉  | 161/204 [00:02<00:00, 92.11it/s] 84%|████████▍ | 171/204 [00:02<00:00, 92.36it/s] 89%|████████▊ | 181/204 [00:02<00:00, 92.29it/s] 94%|█████████▎| 191/204 [00:02<00:00, 92.68it/s] 99%|█████████▊| 201/204 [00:02<00:00, 93.61it/s]100%|██████████| 204/204 [00:02<00:00, 78.98it/s]
26032 images processed, 2.629035711288452 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:43,  1.81it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.90it/s] 22%|██▏       | 17/79 [00:00<00:02, 25.62it/s] 30%|███       | 24/79 [00:00<00:01, 34.13it/s] 37%|███▋      | 29/79 [00:01<00:01, 33.46it/s] 43%|████▎     | 34/79 [00:01<00:01, 33.37it/s] 53%|█████▎    | 42/79 [00:01<00:01, 36.26it/s] 63%|██████▎   | 50/79 [00:01<00:00, 38.10it/s] 73%|███████▎  | 58/79 [00:01<00:00, 39.47it/s] 84%|████████▎ | 66/79 [00:02<00:00, 40.10it/s] 94%|█████████▎| 74/79 [00:02<00:00, 41.31it/s]100%|██████████| 79/79 [00:02<00:00, 34.51it/s]
10000 images processed, 2.323913097381592 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:29,  2.67it/s] 14%|█▍        | 11/79 [00:00<00:02, 29.07it/s] 27%|██▋       | 21/79 [00:00<00:01, 48.12it/s] 39%|███▉      | 31/79 [00:00<00:00, 61.62it/s] 52%|█████▏    | 41/79 [00:00<00:00, 71.39it/s] 65%|██████▍   | 51/79 [00:00<00:00, 78.29it/s] 77%|███████▋  | 61/79 [00:01<00:00, 82.98it/s] 90%|████████▉ | 71/79 [00:01<00:00, 86.78it/s]100%|██████████| 79/79 [00:01<00:00, 66.31it/s]
10000 images processed, 1.211174488067627 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:24,  2.79it/s] 16%|█▌        | 11/70 [00:00<00:01, 29.96it/s] 30%|███       | 21/70 [00:00<00:00, 49.18it/s] 44%|████▍     | 31/70 [00:00<00:00, 62.55it/s] 59%|█████▊    | 41/70 [00:00<00:00, 72.00it/s] 73%|███████▎  | 51/70 [00:00<00:00, 78.64it/s] 87%|████████▋ | 61/70 [00:00<00:00, 83.33it/s]100%|██████████| 70/70 [00:01<00:00, 63.59it/s]
8925 images processed, 1.1344034671783447 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:38,  1.13it/s]  4%|▍         | 2/45 [00:00<00:18,  2.32it/s] 20%|██        | 9/45 [00:01<00:03,  9.75it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.33it/s] 31%|███       | 14/45 [00:01<00:02, 11.76it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.38it/s] 42%|████▏     | 19/45 [00:02<00:02, 12.27it/s] 49%|████▉     | 22/45 [00:02<00:01, 12.42it/s] 56%|█████▌    | 25/45 [00:02<00:01, 13.77it/s] 60%|██████    | 27/45 [00:02<00:01, 13.08it/s] 67%|██████▋   | 30/45 [00:02<00:01, 12.37it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.65it/s] 78%|███████▊  | 35/45 [00:03<00:00, 11.34it/s] 91%|█████████ | 41/45 [00:03<00:00, 13.62it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.64it/s]100%|██████████| 45/45 [00:04<00:00, 10.57it/s]
5640 images processed, 4.2798850536346436 seconds used

19.297298669815063
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.74  98.97
places365     77.69  78.33
LSUN          30.41  94.35
iSUN          80.65  78.59
dtd           46.83  89.54
forget        54.10  89.18
AVG           49.07  88.16
Retain-Acc: 0.7397
Forget-as-OOD (retain known vs forget novel):
  FPR: 54.10 AUROC: 89.18 AUIN: 98.62
9.190717220306396
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.2 lr=0.0001 epochs=5
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.2, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:17<01:08, 17.06s/it] 40%|████      | 2/5 [00:31<00:46, 15.37s/it] 60%|██████    | 3/5 [00:45<00:29, 14.88s/it] 80%|████████  | 4/5 [00:59<00:14, 14.66s/it]100%|██████████| 5/5 [01:13<00:00, 14.44s/it]100%|██████████| 5/5 [01:13<00:00, 14.78s/it]
[loss] ep 0 it 0 total=8.2415 mle=1.5710 pcon=5.2950 forget=1.3755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=8.2317 mle=1.5424 pcon=5.2879 forget=1.4013 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=8.3553 mle=1.7004 pcon=5.2809 forget=1.3740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8.5431 mle=1.8996 pcon=5.2738 forget=1.3696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=8.3627 mle=1.7130 pcon=5.2670 forget=1.3826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=8.1407 mle=1.5019 pcon=5.2603 forget=1.3784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=8.1956 mle=1.5596 pcon=5.2540 forget=1.3819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=8.3182 mle=1.6809 pcon=5.2476 forget=1.3897 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=8.3225 mle=1.6716 pcon=5.2409 forget=1.4101 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=8.2387 mle=1.6371 pcon=5.2346 forget=1.3670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=8.1345 mle=1.5165 pcon=5.2284 forget=1.3895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=8.3920 mle=1.7802 pcon=5.2224 forget=1.3894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=8.3665 mle=1.7645 pcon=5.2167 forget=1.3853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=8.1851 mle=1.6055 pcon=5.2112 forget=1.3683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=8.3080 mle=1.7242 pcon=5.2056 forget=1.3782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=8.3809 mle=1.7944 pcon=5.2003 forget=1.3862 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=8.1250 mle=1.5530 pcon=5.1950 forget=1.3769 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=8.4361 mle=1.8885 pcon=5.1899 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=8.3022 mle=1.7416 pcon=5.1847 forget=1.3759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=8.0709 mle=1.5263 pcon=5.1797 forget=1.3650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=8.1464 mle=1.5858 pcon=5.1746 forget=1.3860 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=8.4246 mle=1.8839 pcon=5.1700 forget=1.3708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=8.1157 mle=1.5927 pcon=5.1652 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=8.3367 mle=1.8113 pcon=5.1605 forget=1.3649 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=8.2329 mle=1.7060 pcon=5.1560 forget=1.3709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=8.2022 mle=1.6709 pcon=5.1519 forget=1.3794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=8.2802 mle=1.7667 pcon=5.1476 forget=1.3660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=8.4244 mle=1.9184 pcon=5.1436 forget=1.3624 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=8.0696 mle=1.5700 pcon=5.1396 forget=1.3600 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=8.3279 mle=1.8362 pcon=5.1354 forget=1.3563 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=8.2099 mle=1.7031 pcon=5.1312 forget=1.3756 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=8.2021 mle=1.6959 pcon=5.1276 forget=1.3785 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=8.1203 mle=1.6542 pcon=5.1238 forget=1.3422 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=8.2061 mle=1.7299 pcon=5.1198 forget=1.3564 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8.1324 mle=1.6499 pcon=5.1162 forget=1.3663 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.9324 mle=1.4538 pcon=5.1127 forget=1.3659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=8.2325 mle=1.7669 pcon=5.1089 forget=1.3567 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.9504 mle=1.4914 pcon=5.1053 forget=1.3536 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=8.3396 mle=1.8723 pcon=5.1019 forget=1.3654 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:58,  3.28it/s]  3%|▎         | 11/391 [00:00<00:11, 33.47it/s]  5%|▌         | 21/391 [00:00<00:06, 53.00it/s]  8%|▊         | 31/391 [00:00<00:05, 66.22it/s] 10%|█         | 41/391 [00:00<00:04, 75.04it/s] 13%|█▎        | 51/391 [00:00<00:04, 80.33it/s] 16%|█▌        | 61/391 [00:00<00:03, 84.48it/s] 18%|█▊        | 71/391 [00:01<00:03, 87.53it/s] 21%|██        | 81/391 [00:01<00:03, 89.48it/s] 23%|██▎       | 91/391 [00:01<00:03, 91.01it/s] 26%|██▌       | 101/391 [00:01<00:03, 92.11it/s] 28%|██▊       | 111/391 [00:01<00:03, 92.89it/s] 31%|███       | 121/391 [00:01<00:02, 93.28it/s] 34%|███▎      | 131/391 [00:01<00:02, 93.44it/s] 36%|███▌      | 141/391 [00:01<00:02, 93.74it/s] 39%|███▊      | 151/391 [00:01<00:02, 93.94it/s] 41%|████      | 161/391 [00:02<00:02, 93.92it/s] 44%|████▎     | 171/391 [00:02<00:02, 94.09it/s] 46%|████▋     | 181/391 [00:02<00:02, 94.17it/s] 49%|████▉     | 191/391 [00:02<00:02, 94.24it/s] 51%|█████▏    | 201/391 [00:02<00:02, 94.34it/s] 54%|█████▍    | 211/391 [00:02<00:01, 93.67it/s] 57%|█████▋    | 221/391 [00:02<00:01, 92.51it/s] 59%|█████▉    | 231/391 [00:02<00:01, 92.34it/s] 62%|██████▏   | 241/391 [00:02<00:01, 91.89it/s] 64%|██████▍   | 251/391 [00:02<00:01, 92.45it/s] 67%|██████▋   | 261/391 [00:03<00:01, 91.89it/s] 69%|██████▉   | 271/391 [00:03<00:01, 91.83it/s] 72%|███████▏  | 281/391 [00:03<00:01, 90.33it/s] 74%|███████▍  | 291/391 [00:03<00:01, 91.21it/s] 77%|███████▋  | 301/391 [00:03<00:00, 91.23it/s] 80%|███████▉  | 311/391 [00:03<00:00, 91.16it/s] 82%|████████▏ | 321/391 [00:03<00:00, 91.34it/s] 85%|████████▍ | 331/391 [00:03<00:00, 91.83it/s] 87%|████████▋ | 341/391 [00:03<00:00, 92.35it/s] 90%|████████▉ | 351/391 [00:04<00:00, 92.63it/s] 92%|█████████▏| 361/391 [00:04<00:00, 93.19it/s] 95%|█████████▍| 371/391 [00:04<00:00, 93.42it/s] 97%|█████████▋| 381/391 [00:04<00:00, 93.78it/s]100%|██████████| 391/391 [00:04<00:00, 91.70it/s]100%|██████████| 391/391 [00:04<00:00, 86.96it/s]
50000 images processed, 4.588235855102539 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.10it/s] 14%|█▍        | 11/79 [00:00<00:02, 32.26it/s] 27%|██▋       | 21/79 [00:00<00:01, 51.41it/s] 39%|███▉      | 31/79 [00:00<00:00, 64.48it/s] 52%|█████▏    | 41/79 [00:00<00:00, 73.44it/s] 65%|██████▍   | 51/79 [00:00<00:00, 79.38it/s] 77%|███████▋  | 61/79 [00:00<00:00, 83.50it/s] 90%|████████▉ | 71/79 [00:01<00:00, 87.09it/s]100%|██████████| 79/79 [00:01<00:00, 67.99it/s]
10000 images processed, 1.1899335384368896 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:05,  3.10it/s]  5%|▌         | 11/204 [00:00<00:05, 32.19it/s] 10%|█         | 21/204 [00:00<00:03, 51.46it/s] 15%|█▌        | 31/204 [00:00<00:02, 64.39it/s] 20%|██        | 41/204 [00:00<00:02, 73.28it/s] 25%|██▌       | 51/204 [00:00<00:01, 79.55it/s] 30%|██▉       | 61/204 [00:00<00:01, 83.50it/s] 35%|███▍      | 71/204 [00:01<00:01, 86.27it/s] 40%|███▉      | 81/204 [00:01<00:01, 88.36it/s] 45%|████▍     | 91/204 [00:01<00:01, 88.55it/s] 50%|████▉     | 101/204 [00:01<00:01, 89.66it/s] 54%|█████▍    | 111/204 [00:01<00:01, 90.80it/s] 59%|█████▉    | 121/204 [00:01<00:00, 90.56it/s] 64%|██████▍   | 131/204 [00:01<00:00, 90.87it/s] 69%|██████▉   | 141/204 [00:01<00:00, 91.81it/s] 74%|███████▍  | 151/204 [00:01<00:00, 92.42it/s] 79%|███████▉  | 161/204 [00:02<00:00, 92.28it/s] 84%|████████▍ | 171/204 [00:02<00:00, 92.69it/s] 89%|████████▊ | 181/204 [00:02<00:00, 92.60it/s] 94%|█████████▎| 191/204 [00:02<00:00, 92.85it/s] 99%|█████████▊| 201/204 [00:02<00:00, 93.75it/s]100%|██████████| 204/204 [00:02<00:00, 81.13it/s]
26032 images processed, 2.574277877807617 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:41,  1.86it/s] 11%|█▏        | 9/79 [00:00<00:04, 16.32it/s] 22%|██▏       | 17/79 [00:00<00:02, 26.35it/s] 32%|███▏      | 25/79 [00:01<00:01, 33.24it/s] 42%|████▏     | 33/79 [00:01<00:01, 37.86it/s] 52%|█████▏    | 41/79 [00:01<00:00, 41.15it/s] 62%|██████▏   | 49/79 [00:01<00:00, 43.42it/s] 72%|███████▏  | 57/79 [00:01<00:00, 44.94it/s] 82%|████████▏ | 65/79 [00:01<00:00, 46.15it/s] 92%|█████████▏| 73/79 [00:02<00:00, 47.18it/s]100%|██████████| 79/79 [00:02<00:00, 38.32it/s]
10000 images processed, 2.0993332862854004 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:34,  2.26it/s] 14%|█▍        | 11/79 [00:00<00:02, 25.45it/s] 27%|██▋       | 21/79 [00:00<00:01, 43.75it/s] 39%|███▉      | 31/79 [00:00<00:00, 57.57it/s] 52%|█████▏    | 41/79 [00:00<00:00, 67.94it/s] 65%|██████▍   | 51/79 [00:00<00:00, 75.59it/s] 77%|███████▋  | 61/79 [00:01<00:00, 81.16it/s] 90%|████████▉ | 71/79 [00:01<00:00, 85.41it/s]100%|██████████| 79/79 [00:01<00:00, 62.53it/s]
10000 images processed, 1.2856662273406982 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:28,  2.42it/s] 16%|█▌        | 11/70 [00:00<00:02, 27.00it/s] 30%|███       | 21/70 [00:00<00:01, 45.64it/s] 44%|████▍     | 31/70 [00:00<00:00, 59.44it/s] 59%|█████▊    | 41/70 [00:00<00:00, 69.45it/s] 73%|███████▎  | 51/70 [00:00<00:00, 76.74it/s] 87%|████████▋ | 61/70 [00:01<00:00, 82.22it/s]100%|██████████| 70/70 [00:01<00:00, 60.88it/s]
8925 images processed, 1.1835391521453857 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:41,  1.07it/s]  4%|▍         | 2/45 [00:01<00:19,  2.19it/s] 20%|██        | 9/45 [00:01<00:03, 10.38it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.91it/s] 31%|███       | 14/45 [00:01<00:02, 11.40it/s] 40%|████      | 18/45 [00:02<00:02, 11.08it/s] 49%|████▉     | 22/45 [00:02<00:01, 12.77it/s] 58%|█████▊    | 26/45 [00:02<00:01, 12.59it/s] 67%|██████▋   | 30/45 [00:02<00:01, 12.74it/s] 76%|███████▌  | 34/45 [00:03<00:00, 11.42it/s] 93%|█████████▎| 42/45 [00:04<00:00, 10.87it/s]100%|██████████| 45/45 [00:04<00:00, 10.64it/s]
5640 images processed, 4.256905794143677 seconds used

18.938364028930664
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.53  99.36
places365     67.87  81.18
LSUN          17.60  96.07
iSUN          72.34  81.68
dtd           37.85  91.39
forget        79.50  86.90
AVG           46.28  89.43
Retain-Acc: 0.7407
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
8.453490972518921
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.2 lr=0.0001 epochs=10
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.2, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:17<02:33, 17.04s/it] 20%|██        | 2/10 [00:30<02:01, 15.22s/it] 30%|███       | 3/10 [00:44<01:42, 14.60s/it] 40%|████      | 4/10 [00:59<01:26, 14.43s/it] 50%|█████     | 5/10 [01:13<01:11, 14.29s/it] 60%|██████    | 6/10 [01:27<00:57, 14.27s/it] 70%|███████   | 7/10 [01:41<00:42, 14.15s/it] 80%|████████  | 8/10 [01:55<00:28, 14.11s/it][loss] ep 0 it 0 total=8.2414 mle=1.5709 pcon=5.2950 forget=1.3755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=8.2316 mle=1.5424 pcon=5.2879 forget=1.4013 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=8.3553 mle=1.7005 pcon=5.2809 forget=1.3740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8.5431 mle=1.8997 pcon=5.2738 forget=1.3696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=8.3628 mle=1.7132 pcon=5.2670 forget=1.3826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=8.1408 mle=1.5020 pcon=5.2603 forget=1.3784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=8.1958 mle=1.5598 pcon=5.2541 forget=1.3820 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=8.3181 mle=1.6808 pcon=5.2476 forget=1.3897 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=8.3225 mle=1.6715 pcon=5.2409 forget=1.4101 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=8.2386 mle=1.6369 pcon=5.2346 forget=1.3670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=8.1344 mle=1.5164 pcon=5.2285 forget=1.3895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=8.3922 mle=1.7803 pcon=5.2225 forget=1.3894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=8.3666 mle=1.7645 pcon=5.2167 forget=1.3853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=8.1849 mle=1.6053 pcon=5.2112 forget=1.3683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=8.3079 mle=1.7241 pcon=5.2056 forget=1.3782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=8.3809 mle=1.7945 pcon=5.2003 forget=1.3862 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=8.1248 mle=1.5529 pcon=5.1951 forget=1.3769 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=8.4361 mle=1.8885 pcon=5.1899 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=8.3023 mle=1.7417 pcon=5.1847 forget=1.3759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=8.0711 mle=1.5265 pcon=5.1797 forget=1.3650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=8.1464 mle=1.5858 pcon=5.1746 forget=1.3860 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=8.4247 mle=1.8840 pcon=5.1700 forget=1.3708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=8.1157 mle=1.5927 pcon=5.1652 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=8.3367 mle=1.8113 pcon=5.1606 forget=1.3649 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=8.2329 mle=1.7060 pcon=5.1560 forget=1.3709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=8.2021 mle=1.6708 pcon=5.1519 forget=1.3794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=8.2801 mle=1.7666 pcon=5.1476 forget=1.3660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=8.4245 mle=1.9184 pcon=5.1436 forget=1.3624 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=8.0696 mle=1.5700 pcon=5.1396 forget=1.3600 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=8.3280 mle=1.8362 pcon=5.1354 forget=1.3563 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=8.2097 mle=1.7029 pcon=5.1312 forget=1.3756 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=8.2022 mle=1.6960 pcon=5.1277 forget=1.3785 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=8.1202 mle=1.6542 pcon=5.1238 forget=1.3422 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=8.2060 mle=1.7298 pcon=5.1198 forget=1.3564 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8.1325 mle=1.6500 pcon=5.1162 forget=1.3663 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.9324 mle=1.4538 pcon=5.1127 forget=1.3659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=8.2325 mle=1.7669 pcon=5.1089 forget=1.3567 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.9503 mle=1.4914 pcon=5.1053 forget=1.3536 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=8.3395 mle=1.8722 pcon=5.1019 forget=1.3654 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=8.3234 mle=1.8673 pcon=5.0986 forget=1.3575 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=8.2408 mle=1.7996 pcon=5.0950 forget=1.3463 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=8.0362 mle=1.5840 pcon=5.0918 forget=1.3603 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=8.0431 mle=1.5957 pcon=5.0884 forget=1.3590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=8.1227 mle=1.6911 pcon=5.0855 forget=1.3460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=8.1949 mle=1.7733 pcon=5.0825 forget=1.3391 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=8.0272 mle=1.5964 pcon=5.0795 forget=1.3514 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=8.1726 mle=1.7468 pcon=5.0766 forget=1.3492 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.8474 mle=1.4013 pcon=5.0737 forget=1.3724 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.9360 mle=1.5361 pcon=5.0710 forget=1.3289 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=8.0221 mle=1.5917 pcon=5.0683 forget=1.3621 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=8.0777 mle=1.6656 pcon=5.0656 forget=1.3464 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=8.0356 mle=1.6300 pcon=5.0634 forget=1.3421 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=8.2402 mle=1.8232 pcon=5.0605 forget=1.3565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.9571 mle=1.5346 pcon=5.0582 forget=1.3643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=8.0786 mle=1.6748 pcon=5.0557 forget=1.3481 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=8.0240 mle=1.6308 pcon=5.0531 forget=1.3401 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=8.1318 mle=1.7454 pcon=5.0507 forget=1.3356 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=8.0181 mle=1.6272 pcon=5.0482 forget=1.3427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.9577 mle=1.5825 pcon=5.0459 forget=1.3294 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=8.0513 mle=1.6546 pcon=5.0439 forget=1.3528 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=8.1998 mle=1.7956 pcon=5.0418 forget=1.3624 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=8.1417 mle=1.7419 pcon=5.0394 forget=1.3604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=8.0252 mle=1.6408 pcon=5.0374 forget=1.3470 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 90%|█████████ | 9/10 [02:09<00:14, 14.12s/it]100%|██████████| 10/10 [02:23<00:00, 14.09s/it]100%|██████████| 10/10 [02:23<00:00, 14.34s/it]
[loss] ep 8 it 30 total=7.9333 mle=1.5365 pcon=5.0355 forget=1.3612 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=7.9735 mle=1.5906 pcon=5.0336 forget=1.3493 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.8905 mle=1.5004 pcon=5.0315 forget=1.3586 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=8.0753 mle=1.7033 pcon=5.0296 forget=1.3424 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.8996 mle=1.5307 pcon=5.0281 forget=1.3408 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=8.0355 mle=1.6775 pcon=5.0261 forget=1.3319 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=8.1488 mle=1.7565 pcon=5.0241 forget=1.3682 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.8458 mle=1.4895 pcon=5.0222 forget=1.3341 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.9726 mle=1.6365 pcon=5.0203 forget=1.3157 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=8.0246 mle=1.6694 pcon=5.0183 forget=1.3370 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=8.2115 mle=1.8423 pcon=5.0168 forget=1.3524 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.9320 mle=1.5783 pcon=5.0153 forget=1.3385 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.9230 mle=1.5677 pcon=5.0138 forget=1.3415 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.9189 mle=1.5588 pcon=5.0119 forget=1.3481 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.9064 mle=1.5583 pcon=5.0105 forget=1.3376 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:59,  3.26it/s]  3%|▎         | 11/391 [00:00<00:11, 33.39it/s]  5%|▌         | 21/391 [00:00<00:06, 53.04it/s]  8%|▊         | 31/391 [00:00<00:05, 66.35it/s] 10%|█         | 41/391 [00:00<00:04, 75.38it/s] 13%|█▎        | 51/391 [00:00<00:04, 80.76it/s] 16%|█▌        | 61/391 [00:00<00:03, 84.67it/s] 18%|█▊        | 71/391 [00:01<00:03, 87.84it/s] 21%|██        | 81/391 [00:01<00:03, 89.92it/s] 23%|██▎       | 91/391 [00:01<00:03, 91.44it/s] 26%|██▌       | 101/391 [00:01<00:03, 92.59it/s] 28%|██▊       | 111/391 [00:01<00:03, 93.33it/s] 31%|███       | 121/391 [00:01<00:02, 93.67it/s] 34%|███▎      | 131/391 [00:01<00:02, 93.85it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.15it/s] 39%|███▊      | 151/391 [00:01<00:02, 94.47it/s] 41%|████      | 161/391 [00:01<00:02, 94.42it/s] 44%|████▎     | 171/391 [00:02<00:02, 94.59it/s] 46%|████▋     | 181/391 [00:02<00:02, 94.71it/s] 49%|████▉     | 191/391 [00:02<00:02, 94.77it/s] 51%|█████▏    | 201/391 [00:02<00:02, 94.73it/s] 54%|█████▍    | 211/391 [00:02<00:01, 94.86it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.75it/s] 59%|█████▉    | 231/391 [00:02<00:01, 94.99it/s] 62%|██████▏   | 241/391 [00:02<00:01, 94.95it/s] 64%|██████▍   | 251/391 [00:02<00:01, 94.95it/s] 67%|██████▋   | 261/391 [00:03<00:01, 95.22it/s] 69%|██████▉   | 271/391 [00:03<00:01, 95.06it/s] 72%|███████▏  | 281/391 [00:03<00:01, 93.60it/s] 74%|███████▍  | 291/391 [00:03<00:01, 94.16it/s] 77%|███████▋  | 301/391 [00:03<00:00, 94.26it/s] 80%|███████▉  | 311/391 [00:03<00:00, 94.44it/s] 82%|████████▏ | 321/391 [00:03<00:00, 94.33it/s] 85%|████████▍ | 331/391 [00:03<00:00, 94.48it/s] 87%|████████▋ | 341/391 [00:03<00:00, 94.44it/s] 90%|████████▉ | 351/391 [00:04<00:00, 94.62it/s] 92%|█████████▏| 361/391 [00:04<00:00, 94.65it/s] 95%|█████████▍| 371/391 [00:04<00:00, 94.59it/s] 97%|█████████▋| 381/391 [00:04<00:00, 94.49it/s]100%|██████████| 391/391 [00:04<00:00, 92.05it/s]100%|██████████| 391/391 [00:04<00:00, 88.13it/s]
50000 images processed, 4.539937973022461 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.07it/s] 14%|█▍        | 11/79 [00:00<00:02, 31.96it/s] 27%|██▋       | 21/79 [00:00<00:01, 51.02it/s] 39%|███▉      | 31/79 [00:00<00:00, 63.94it/s] 52%|█████▏    | 41/79 [00:00<00:00, 72.87it/s] 65%|██████▍   | 51/79 [00:00<00:00, 78.72it/s] 77%|███████▋  | 61/79 [00:00<00:00, 82.06it/s] 90%|████████▉ | 71/79 [00:01<00:00, 86.00it/s]100%|██████████| 79/79 [00:01<00:00, 67.28it/s]
10000 images processed, 1.1975364685058594 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:18,  2.60it/s]  5%|▌         | 11/204 [00:00<00:06, 28.43it/s] 10%|█         | 21/204 [00:00<00:03, 46.89it/s] 15%|█▌        | 31/204 [00:00<00:02, 59.90it/s] 20%|██        | 41/204 [00:00<00:02, 69.17it/s] 25%|██▌       | 51/204 [00:00<00:02, 75.73it/s] 30%|██▉       | 61/204 [00:01<00:01, 80.46it/s] 35%|███▍      | 71/204 [00:01<00:01, 83.73it/s] 40%|███▉      | 81/204 [00:01<00:01, 85.97it/s] 45%|████▍     | 91/204 [00:01<00:01, 87.59it/s] 50%|████▉     | 101/204 [00:01<00:01, 88.55it/s] 54%|█████▍    | 111/204 [00:01<00:01, 89.25it/s] 59%|█████▉    | 121/204 [00:01<00:00, 89.90it/s] 64%|██████▍   | 131/204 [00:01<00:00, 90.55it/s] 69%|██████▉   | 141/204 [00:01<00:00, 90.98it/s] 74%|███████▍  | 151/204 [00:02<00:00, 91.51it/s] 79%|███████▉  | 161/204 [00:02<00:00, 91.78it/s] 84%|████████▍ | 171/204 [00:02<00:00, 91.65it/s] 89%|████████▊ | 181/204 [00:02<00:00, 91.55it/s] 94%|█████████▎| 191/204 [00:02<00:00, 91.70it/s] 99%|█████████▊| 201/204 [00:02<00:00, 92.89it/s]100%|██████████| 204/204 [00:02<00:00, 78.42it/s]
26032 images processed, 2.6483867168426514 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:47,  1.64it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.34it/s] 22%|██▏       | 17/79 [00:00<00:02, 25.45it/s] 32%|███▏      | 25/79 [00:01<00:01, 32.32it/s] 42%|████▏     | 33/79 [00:01<00:01, 36.94it/s] 52%|█████▏    | 41/79 [00:01<00:00, 40.85it/s] 62%|██████▏   | 49/79 [00:01<00:00, 43.33it/s] 72%|███████▏  | 57/79 [00:01<00:00, 45.07it/s] 82%|████████▏ | 65/79 [00:01<00:00, 46.46it/s] 92%|█████████▏| 73/79 [00:02<00:00, 47.51it/s]100%|██████████| 79/79 [00:02<00:00, 37.43it/s]
10000 images processed, 2.1479415893554688 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:27,  2.88it/s] 14%|█▍        | 11/79 [00:00<00:02, 30.70it/s] 27%|██▋       | 21/79 [00:00<00:01, 50.12it/s] 39%|███▉      | 31/79 [00:00<00:00, 63.59it/s] 52%|█████▏    | 41/79 [00:00<00:00, 73.03it/s] 65%|██████▍   | 51/79 [00:00<00:00, 79.58it/s] 77%|███████▋  | 61/79 [00:00<00:00, 84.25it/s] 90%|████████▉ | 71/79 [00:01<00:00, 87.87it/s]100%|██████████| 79/79 [00:01<00:00, 68.20it/s]
10000 images processed, 1.1766107082366943 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:23,  2.91it/s] 16%|█▌        | 11/70 [00:00<00:01, 30.91it/s] 30%|███       | 21/70 [00:00<00:00, 50.06it/s] 44%|████▍     | 31/70 [00:00<00:00, 63.23it/s] 59%|█████▊    | 41/70 [00:00<00:00, 72.56it/s] 73%|███████▎  | 51/70 [00:00<00:00, 79.06it/s] 87%|████████▋ | 61/70 [00:00<00:00, 83.98it/s]100%|██████████| 70/70 [00:01<00:00, 64.76it/s]
8925 images processed, 1.1151657104492188 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:39,  1.11it/s]  9%|▉         | 4/45 [00:01<00:08,  4.99it/s] 20%|██        | 9/45 [00:01<00:04,  8.56it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.29it/s] 31%|███       | 14/45 [00:01<00:02, 12.24it/s] 38%|███▊      | 17/45 [00:01<00:02, 11.51it/s] 44%|████▍     | 20/45 [00:02<00:01, 14.42it/s] 51%|█████     | 23/45 [00:02<00:01, 15.39it/s] 56%|█████▌    | 25/45 [00:02<00:01, 11.93it/s] 64%|██████▍   | 29/45 [00:02<00:00, 16.55it/s] 71%|███████   | 32/45 [00:02<00:00, 15.38it/s] 76%|███████▌  | 34/45 [00:03<00:01,  9.83it/s] 89%|████████▉ | 40/45 [00:03<00:00, 15.33it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.95it/s]100%|██████████| 45/45 [00:04<00:00, 11.00it/s]
5640 images processed, 4.114415407180786 seconds used

18.746958017349243
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.86  81.18
LSUN          17.59  96.07
iSUN          72.34  81.68
dtd           37.85  91.39
forget        79.50  86.90
AVG           46.28  89.43
Retain-Acc: 0.7407
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
8.905606031417847
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.2 lr=0.0001 epochs=20
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.2, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:18<05:45, 18.20s/it] 10%|█         | 2/20 [00:32<04:49, 16.11s/it] 15%|█▌        | 3/20 [00:47<04:19, 15.24s/it] 20%|██        | 4/20 [01:01<03:58, 14.91s/it] 25%|██▌       | 5/20 [01:15<03:41, 14.77s/it] 30%|███       | 6/20 [01:30<03:25, 14.67s/it] 35%|███▌      | 7/20 [01:45<03:10, 14.64s/it] 40%|████      | 8/20 [01:59<02:55, 14.59s/it][loss] ep 0 it 0 total=8.2415 mle=1.5709 pcon=5.2950 forget=1.3755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=8.2316 mle=1.5424 pcon=5.2879 forget=1.4013 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=8.3553 mle=1.7004 pcon=5.2809 forget=1.3740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8.5431 mle=1.8997 pcon=5.2738 forget=1.3696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=8.3628 mle=1.7132 pcon=5.2670 forget=1.3826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=8.1407 mle=1.5020 pcon=5.2603 forget=1.3784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=8.1956 mle=1.5596 pcon=5.2540 forget=1.3819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=8.3181 mle=1.6808 pcon=5.2476 forget=1.3897 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=8.3225 mle=1.6716 pcon=5.2409 forget=1.4101 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=8.2388 mle=1.6372 pcon=5.2346 forget=1.3670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=8.1345 mle=1.5166 pcon=5.2285 forget=1.3895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=8.3920 mle=1.7802 pcon=5.2224 forget=1.3894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=8.3664 mle=1.7643 pcon=5.2167 forget=1.3853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=8.1849 mle=1.6053 pcon=5.2112 forget=1.3683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=8.3079 mle=1.7240 pcon=5.2056 forget=1.3783 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=8.3807 mle=1.7943 pcon=5.2003 forget=1.3862 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=8.1249 mle=1.5530 pcon=5.1951 forget=1.3769 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=8.4361 mle=1.8884 pcon=5.1899 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=8.3023 mle=1.7416 pcon=5.1847 forget=1.3759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=8.0710 mle=1.5264 pcon=5.1797 forget=1.3650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=8.1465 mle=1.5859 pcon=5.1746 forget=1.3860 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=8.4245 mle=1.8838 pcon=5.1700 forget=1.3708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=8.1156 mle=1.5926 pcon=5.1652 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=8.3369 mle=1.8114 pcon=5.1605 forget=1.3649 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=8.2329 mle=1.7060 pcon=5.1560 forget=1.3709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=8.2022 mle=1.6709 pcon=5.1519 forget=1.3794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=8.2802 mle=1.7666 pcon=5.1476 forget=1.3660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=8.4244 mle=1.9184 pcon=5.1436 forget=1.3624 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=8.0696 mle=1.5700 pcon=5.1396 forget=1.3600 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=8.3280 mle=1.8362 pcon=5.1354 forget=1.3563 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=8.2099 mle=1.7031 pcon=5.1312 forget=1.3755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=8.2020 mle=1.6958 pcon=5.1276 forget=1.3785 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=8.1201 mle=1.6541 pcon=5.1238 forget=1.3422 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=8.2060 mle=1.7298 pcon=5.1198 forget=1.3564 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8.1326 mle=1.6501 pcon=5.1162 forget=1.3663 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.9324 mle=1.4538 pcon=5.1127 forget=1.3659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=8.2326 mle=1.7671 pcon=5.1088 forget=1.3567 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.9503 mle=1.4914 pcon=5.1053 forget=1.3536 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=8.3394 mle=1.8721 pcon=5.1019 forget=1.3654 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=8.3233 mle=1.8673 pcon=5.0986 forget=1.3575 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=8.2409 mle=1.7996 pcon=5.0950 forget=1.3463 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=8.0363 mle=1.5841 pcon=5.0918 forget=1.3604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=8.0429 mle=1.5955 pcon=5.0883 forget=1.3590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=8.1227 mle=1.6912 pcon=5.0855 forget=1.3460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=8.1948 mle=1.7732 pcon=5.0825 forget=1.3391 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=8.0272 mle=1.5964 pcon=5.0794 forget=1.3514 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=8.1726 mle=1.7467 pcon=5.0766 forget=1.3492 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.8473 mle=1.4012 pcon=5.0737 forget=1.3724 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.9359 mle=1.5360 pcon=5.0710 forget=1.3289 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=8.0220 mle=1.5916 pcon=5.0683 forget=1.3621 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=8.0776 mle=1.6656 pcon=5.0656 forget=1.3464 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=8.0357 mle=1.6301 pcon=5.0634 forget=1.3421 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=8.2403 mle=1.8233 pcon=5.0605 forget=1.3565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.9572 mle=1.5348 pcon=5.0582 forget=1.3643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=8.0785 mle=1.6747 pcon=5.0557 forget=1.3481 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=8.0237 mle=1.6306 pcon=5.0531 forget=1.3401 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=8.1319 mle=1.7455 pcon=5.0507 forget=1.3356 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=8.0181 mle=1.6272 pcon=5.0482 forget=1.3427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.9576 mle=1.5823 pcon=5.0459 forget=1.3294 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=8.0513 mle=1.6546 pcon=5.0439 forget=1.3528 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=8.2000 mle=1.7958 pcon=5.0418 forget=1.3624 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=8.1418 mle=1.7419 pcon=5.0394 forget=1.3605 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=8.0251 mle=1.6407 pcon=5.0374 forget=1.3470 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 45%|████▌     | 9/20 [02:14<02:40, 14.58s/it] 50%|█████     | 10/20 [02:28<02:25, 14.53s/it] 55%|█████▌    | 11/20 [02:42<02:10, 14.47s/it] 60%|██████    | 12/20 [02:57<01:55, 14.44s/it] 65%|██████▌   | 13/20 [03:11<01:39, 14.28s/it] 70%|███████   | 14/20 [03:25<01:25, 14.28s/it] 75%|███████▌  | 15/20 [03:40<01:12, 14.59s/it] 80%|████████  | 16/20 [03:56<01:00, 15.10s/it][loss] ep 8 it 30 total=7.9333 mle=1.5365 pcon=5.0355 forget=1.3612 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=7.9736 mle=1.5906 pcon=5.0336 forget=1.3493 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.8903 mle=1.5002 pcon=5.0315 forget=1.3586 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=8.0752 mle=1.7033 pcon=5.0296 forget=1.3424 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.8996 mle=1.5307 pcon=5.0281 forget=1.3408 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=8.0355 mle=1.6775 pcon=5.0261 forget=1.3319 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=8.1487 mle=1.7565 pcon=5.0241 forget=1.3682 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.8459 mle=1.4896 pcon=5.0222 forget=1.3341 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.9725 mle=1.6364 pcon=5.0203 forget=1.3157 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=8.0248 mle=1.6696 pcon=5.0183 forget=1.3370 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=8.2114 mle=1.8422 pcon=5.0168 forget=1.3524 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.9321 mle=1.5783 pcon=5.0153 forget=1.3385 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.9230 mle=1.5677 pcon=5.0138 forget=1.3415 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.9188 mle=1.5588 pcon=5.0119 forget=1.3481 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.9065 mle=1.5584 pcon=5.0105 forget=1.3376 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=7.9637 mle=1.6070 pcon=5.0087 forget=1.3479 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=7.9550 mle=1.5902 pcon=5.0071 forget=1.3578 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=8.0986 mle=1.7543 pcon=5.0056 forget=1.3387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=8.1865 mle=1.8512 pcon=5.0042 forget=1.3311 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=8.1947 mle=1.8596 pcon=5.0028 forget=1.3322 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=7.9235 mle=1.5823 pcon=5.0015 forget=1.3396 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=7.8884 mle=1.5626 pcon=5.0003 forget=1.3255 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=8.0262 mle=1.6672 pcon=4.9990 forget=1.3600 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=7.9253 mle=1.5764 pcon=4.9976 forget=1.3513 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=7.9512 mle=1.6221 pcon=4.9963 forget=1.3329 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=7.8904 mle=1.5692 pcon=4.9951 forget=1.3262 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=7.9978 mle=1.6659 pcon=4.9935 forget=1.3384 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=8.1207 mle=1.7817 pcon=4.9924 forget=1.3466 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=7.9393 mle=1.6036 pcon=4.9915 forget=1.3442 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=8.2995 mle=1.9669 pcon=4.9902 forget=1.3424 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=7.9791 mle=1.6706 pcon=4.9890 forget=1.3195 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 12 it 20 total=7.9447 mle=1.6200 pcon=4.9876 forget=1.3370 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=8.0233 mle=1.6947 pcon=4.9865 forget=1.3421 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=7.8865 mle=1.5486 pcon=4.9853 forget=1.3526 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=8.0552 mle=1.7439 pcon=4.9843 forget=1.3270 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=7.9139 mle=1.6002 pcon=4.9830 forget=1.3307 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=7.8673 mle=1.5354 pcon=4.9823 forget=1.3496 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=7.8420 mle=1.5206 pcon=4.9813 forget=1.3401 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=8.1982 mle=1.8677 pcon=4.9802 forget=1.3502 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=7.7501 mle=1.4305 pcon=4.9793 forget=1.3403 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=7.8469 mle=1.4955 pcon=4.9783 forget=1.3731 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=7.8881 mle=1.5713 pcon=4.9771 forget=1.3398 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=7.8633 mle=1.5533 pcon=4.9763 forget=1.3338 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=7.9650 mle=1.6447 pcon=4.9754 forget=1.3449 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=7.8862 mle=1.5836 pcon=4.9745 forget=1.3281 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=7.9049 mle=1.5896 pcon=4.9735 forget=1.3418 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=7.8842 mle=1.5783 pcon=4.9726 forget=1.3333 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=8.0634 mle=1.7687 pcon=4.9715 forget=1.3232 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=8.0814 mle=1.7692 pcon=4.9709 forget=1.3413 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=7.9203 mle=1.6157 pcon=4.9705 forget=1.3341 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=8.1919 mle=1.8839 pcon=4.9695 forget=1.3385 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=7.9184 mle=1.6159 pcon=4.9689 forget=1.3336 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=7.9617 mle=1.6702 pcon=4.9681 forget=1.3234 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=7.9731 mle=1.6811 pcon=4.9673 forget=1.3247 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=7.9975 mle=1.6783 pcon=4.9666 forget=1.3526 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=7.7845 mle=1.4935 pcon=4.9658 forget=1.3253 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=7.8642 mle=1.5575 pcon=4.9651 forget=1.3416 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=8.0459 mle=1.7262 pcon=4.9644 forget=1.3553 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=7.9064 mle=1.6169 pcon=4.9638 forget=1.3257 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=8.1769 mle=1.8743 pcon=4.9630 forget=1.3395 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=7.8992 mle=1.5938 pcon=4.9626 forget=1.3429 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=7.8502 mle=1.5447 pcon=4.9619 forget=1.3436 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 16 it 10 total=7.8909 mle=1.5863 pcon=4.9612 forget=1.3435 nr=64 nf=64 protos=540 fproto_sim=NA
 85%|████████▌ | 17/20 [04:13<00:46, 15.44s/it] 90%|█████████ | 18/20 [04:29<00:31, 15.68s/it] 95%|█████████▌| 19/20 [04:45<00:15, 15.78s/it]100%|██████████| 20/20 [05:01<00:00, 15.85s/it]100%|██████████| 20/20 [05:01<00:00, 15.07s/it]
[loss] ep 16 it 60 total=7.7831 mle=1.4934 pcon=4.9606 forget=1.3291 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=7.8056 mle=1.5135 pcon=4.9602 forget=1.3319 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=7.9791 mle=1.6993 pcon=4.9594 forget=1.3204 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=7.8494 mle=1.5732 pcon=4.9587 forget=1.3175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=8.2275 mle=1.9479 pcon=4.9579 forget=1.3217 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=8.0824 mle=1.7943 pcon=4.9573 forget=1.3308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=7.8940 mle=1.6143 pcon=4.9567 forget=1.3230 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 17 it 20 total=8.0717 mle=1.7781 pcon=4.9559 forget=1.3378 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=8.1003 mle=1.8257 pcon=4.9556 forget=1.3189 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=7.8615 mle=1.5745 pcon=4.9549 forget=1.3320 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=7.8694 mle=1.5830 pcon=4.9540 forget=1.3324 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=7.9578 mle=1.6653 pcon=4.9533 forget=1.3392 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=7.7551 mle=1.4685 pcon=4.9532 forget=1.3334 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=8.0820 mle=1.8037 pcon=4.9527 forget=1.3256 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=8.0183 mle=1.7473 pcon=4.9519 forget=1.3191 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=7.8577 mle=1.5876 pcon=4.9518 forget=1.3183 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=7.9773 mle=1.6973 pcon=4.9514 forget=1.3286 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=7.9482 mle=1.6656 pcon=4.9508 forget=1.3318 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=7.8317 mle=1.5480 pcon=4.9504 forget=1.3332 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=7.8576 mle=1.5911 pcon=4.9500 forget=1.3166 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=7.9625 mle=1.6882 pcon=4.9496 forget=1.3247 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=8.0782 mle=1.8074 pcon=4.9491 forget=1.3216 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=7.9839 mle=1.7018 pcon=4.9490 forget=1.3330 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 19 it 40 total=7.7490 mle=1.4660 pcon=4.9488 forget=1.3342 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=7.9943 mle=1.7033 pcon=4.9489 forget=1.3421 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=7.8841 mle=1.6123 pcon=4.9484 forget=1.3234 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=7.9030 mle=1.6441 pcon=4.9479 forget=1.3110 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=8.1218 mle=1.8486 pcon=4.9476 forget=1.3256 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=7.8453 mle=1.5591 pcon=4.9476 forget=1.3386 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=7.9027 mle=1.5975 pcon=4.9475 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:45,  2.36it/s]  3%|▎         | 11/391 [00:00<00:14, 26.54it/s]  5%|▌         | 21/391 [00:00<00:08, 44.83it/s]  8%|▊         | 31/391 [00:00<00:06, 58.87it/s] 10%|█         | 41/391 [00:00<00:05, 69.12it/s] 13%|█▎        | 51/391 [00:00<00:04, 76.45it/s] 16%|█▌        | 61/391 [00:01<00:04, 81.72it/s] 18%|█▊        | 71/391 [00:01<00:03, 85.49it/s] 21%|██        | 81/391 [00:01<00:03, 88.41it/s] 23%|██▎       | 91/391 [00:01<00:03, 90.26it/s] 26%|██▌       | 101/391 [00:01<00:03, 91.48it/s] 28%|██▊       | 111/391 [00:01<00:03, 92.37it/s] 31%|███       | 121/391 [00:01<00:02, 92.87it/s] 34%|███▎      | 131/391 [00:01<00:02, 93.47it/s] 36%|███▌      | 141/391 [00:01<00:02, 93.27it/s] 39%|███▊      | 151/391 [00:02<00:02, 94.06it/s] 41%|████      | 161/391 [00:02<00:02, 94.29it/s] 44%|████▎     | 171/391 [00:02<00:02, 94.67it/s] 46%|████▋     | 181/391 [00:02<00:02, 94.18it/s] 49%|████▉     | 191/391 [00:02<00:02, 94.29it/s] 51%|█████▏    | 201/391 [00:02<00:02, 94.20it/s] 54%|█████▍    | 211/391 [00:02<00:01, 94.29it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.05it/s] 59%|█████▉    | 231/391 [00:02<00:01, 94.31it/s] 62%|██████▏   | 241/391 [00:02<00:01, 94.32it/s] 64%|██████▍   | 251/391 [00:03<00:01, 93.43it/s] 67%|██████▋   | 261/391 [00:03<00:01, 93.62it/s] 69%|██████▉   | 271/391 [00:03<00:01, 93.72it/s] 72%|███████▏  | 281/391 [00:03<00:01, 93.71it/s] 74%|███████▍  | 291/391 [00:03<00:01, 92.03it/s] 77%|███████▋  | 301/391 [00:03<00:00, 92.85it/s] 80%|███████▉  | 311/391 [00:03<00:00, 93.72it/s] 82%|████████▏ | 321/391 [00:03<00:00, 94.19it/s] 85%|████████▍ | 331/391 [00:03<00:00, 94.47it/s] 87%|████████▋ | 341/391 [00:04<00:00, 92.57it/s] 90%|████████▉ | 351/391 [00:04<00:00, 93.10it/s] 92%|█████████▏| 361/391 [00:04<00:00, 93.44it/s] 95%|█████████▍| 371/391 [00:04<00:00, 93.65it/s] 97%|█████████▋| 381/391 [00:04<00:00, 93.39it/s]100%|██████████| 391/391 [00:04<00:00, 91.07it/s]100%|██████████| 391/391 [00:04<00:00, 85.29it/s]
50000 images processed, 4.699670314788818 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:34,  2.27it/s] 13%|█▎        | 10/79 [00:00<00:02, 23.54it/s] 25%|██▌       | 20/79 [00:00<00:01, 42.41it/s] 38%|███▊      | 30/79 [00:00<00:00, 56.57it/s] 51%|█████     | 40/79 [00:00<00:00, 67.05it/s] 63%|██████▎   | 50/79 [00:00<00:00, 74.31it/s] 76%|███████▌  | 60/79 [00:01<00:00, 79.15it/s] 89%|████████▊ | 70/79 [00:01<00:00, 83.76it/s]100%|██████████| 79/79 [00:01<00:00, 61.07it/s]
10000 images processed, 1.3180129528045654 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:20,  2.53it/s]  5%|▌         | 11/204 [00:00<00:06, 27.83it/s] 10%|█         | 21/204 [00:00<00:03, 46.52it/s] 15%|█▌        | 31/204 [00:00<00:02, 60.12it/s] 20%|██        | 41/204 [00:00<00:02, 69.68it/s] 25%|██▌       | 51/204 [00:00<00:02, 76.47it/s] 30%|██▉       | 61/204 [00:01<00:01, 81.18it/s] 35%|███▍      | 71/204 [00:01<00:01, 84.35it/s] 40%|███▉      | 81/204 [00:01<00:01, 85.66it/s] 45%|████▍     | 91/204 [00:01<00:01, 87.58it/s] 50%|████▉     | 101/204 [00:01<00:01, 88.90it/s] 54%|█████▍    | 111/204 [00:01<00:01, 90.00it/s] 59%|█████▉    | 121/204 [00:01<00:00, 90.44it/s] 64%|██████▍   | 131/204 [00:01<00:00, 90.26it/s] 69%|██████▉   | 141/204 [00:01<00:00, 91.13it/s] 74%|███████▍  | 151/204 [00:02<00:00, 91.37it/s] 79%|███████▉  | 161/204 [00:02<00:00, 91.50it/s] 84%|████████▍ | 171/204 [00:02<00:00, 92.13it/s] 89%|████████▊ | 181/204 [00:02<00:00, 92.66it/s] 94%|█████████▎| 191/204 [00:02<00:00, 92.93it/s] 99%|█████████▊| 201/204 [00:02<00:00, 93.80it/s]100%|██████████| 204/204 [00:02<00:00, 78.58it/s]
26032 images processed, 2.639798879623413 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:45,  1.71it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.71it/s] 22%|██▏       | 17/79 [00:00<00:02, 25.21it/s] 32%|███▏      | 25/79 [00:01<00:01, 32.11it/s] 42%|████▏     | 33/79 [00:01<00:01, 37.17it/s] 52%|█████▏    | 41/79 [00:01<00:00, 40.49it/s] 62%|██████▏   | 49/79 [00:01<00:00, 42.73it/s] 72%|███████▏  | 57/79 [00:01<00:00, 44.28it/s] 82%|████████▏ | 65/79 [00:01<00:00, 47.72it/s] 91%|█████████ | 72/79 [00:01<00:00, 52.34it/s] 99%|█████████▊| 78/79 [00:02<00:00, 48.51it/s]100%|██████████| 79/79 [00:02<00:00, 37.23it/s]
10000 images processed, 2.160639524459839 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:32,  2.44it/s] 14%|█▍        | 11/79 [00:00<00:02, 27.11it/s] 27%|██▋       | 21/79 [00:00<00:01, 45.83it/s] 39%|███▉      | 31/79 [00:00<00:00, 59.39it/s] 52%|█████▏    | 41/79 [00:00<00:00, 69.51it/s] 65%|██████▍   | 51/79 [00:00<00:00, 76.85it/s] 77%|███████▋  | 61/79 [00:01<00:00, 82.09it/s] 90%|████████▉ | 71/79 [00:01<00:00, 86.02it/s]100%|██████████| 79/79 [00:01<00:00, 64.26it/s]
10000 images processed, 1.2523524761199951 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:26,  2.59it/s] 14%|█▍        | 10/70 [00:00<00:02, 26.05it/s] 29%|██▊       | 20/70 [00:00<00:01, 45.72it/s] 43%|████▎     | 30/70 [00:00<00:00, 59.91it/s] 57%|█████▋    | 40/70 [00:00<00:00, 69.98it/s] 71%|███████▏  | 50/70 [00:00<00:00, 76.85it/s] 86%|████████▌ | 60/70 [00:01<00:00, 82.25it/s]100%|██████████| 70/70 [00:01<00:00, 84.62it/s]100%|██████████| 70/70 [00:01<00:00, 61.88it/s]
8925 images processed, 1.162381887435913 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:44,  1.00s/it]  4%|▍         | 2/45 [00:01<00:21,  2.03it/s] 20%|██        | 9/45 [00:01<00:04,  8.85it/s] 24%|██▍       | 11/45 [00:01<00:04,  8.49it/s] 38%|███▊      | 17/45 [00:02<00:02, 11.69it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.08it/s] 49%|████▉     | 22/45 [00:02<00:01, 13.40it/s] 56%|█████▌    | 25/45 [00:02<00:01, 14.10it/s] 60%|██████    | 27/45 [00:02<00:01, 12.20it/s] 67%|██████▋   | 30/45 [00:03<00:01, 13.12it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.45it/s] 78%|███████▊  | 35/45 [00:03<00:00, 10.35it/s] 91%|█████████ | 41/45 [00:03<00:00, 13.70it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.49it/s]100%|██████████| 45/45 [00:04<00:00, 10.16it/s]
5640 images processed, 4.451474666595459 seconds used

19.397002696990967
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.91  81.18
LSUN          17.57  96.08
iSUN          72.38  81.67
dtd           37.91  91.39
forget        79.50  86.89
AVG           46.30  89.43
Retain-Acc: 0.7407
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.89 AUIN: 98.36
9.427191972732544
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.2 lr=0.0001 epochs=50
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.2, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:17<14:33, 17.84s/it]  4%|▍         | 2/50 [00:31<12:27, 15.57s/it]  6%|▌         | 3/50 [00:45<11:40, 14.90s/it]  8%|▊         | 4/50 [00:59<11:08, 14.54s/it] 10%|█         | 5/50 [01:14<10:52, 14.50s/it] 12%|█▏        | 6/50 [01:28<10:35, 14.45s/it] 14%|█▍        | 7/50 [01:42<10:16, 14.34s/it] 16%|█▌        | 8/50 [01:57<10:01, 14.33s/it][loss] ep 0 it 0 total=8.2415 mle=1.5709 pcon=5.2950 forget=1.3755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=8.2316 mle=1.5424 pcon=5.2879 forget=1.4013 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=8.3553 mle=1.7004 pcon=5.2809 forget=1.3740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=8.5431 mle=1.8997 pcon=5.2738 forget=1.3696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=8.3628 mle=1.7132 pcon=5.2670 forget=1.3826 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=8.1407 mle=1.5020 pcon=5.2603 forget=1.3784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=8.1956 mle=1.5596 pcon=5.2540 forget=1.3819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=8.3181 mle=1.6808 pcon=5.2476 forget=1.3897 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=8.3225 mle=1.6716 pcon=5.2409 forget=1.4101 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=8.2388 mle=1.6372 pcon=5.2346 forget=1.3670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=8.1345 mle=1.5166 pcon=5.2285 forget=1.3895 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=8.3920 mle=1.7802 pcon=5.2224 forget=1.3894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=8.3664 mle=1.7643 pcon=5.2167 forget=1.3853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=8.1849 mle=1.6053 pcon=5.2112 forget=1.3683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=8.3079 mle=1.7240 pcon=5.2056 forget=1.3783 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=8.3807 mle=1.7943 pcon=5.2003 forget=1.3862 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=8.1249 mle=1.5530 pcon=5.1951 forget=1.3769 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=8.4361 mle=1.8884 pcon=5.1899 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=8.3023 mle=1.7416 pcon=5.1847 forget=1.3759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=8.0710 mle=1.5264 pcon=5.1797 forget=1.3650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=8.1465 mle=1.5859 pcon=5.1746 forget=1.3860 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=8.4245 mle=1.8838 pcon=5.1700 forget=1.3708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=8.1156 mle=1.5926 pcon=5.1652 forget=1.3577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=8.3369 mle=1.8114 pcon=5.1605 forget=1.3649 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=8.2329 mle=1.7060 pcon=5.1560 forget=1.3709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=8.2022 mle=1.6709 pcon=5.1519 forget=1.3794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=8.2802 mle=1.7666 pcon=5.1476 forget=1.3660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=8.4244 mle=1.9184 pcon=5.1436 forget=1.3624 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=8.0696 mle=1.5700 pcon=5.1396 forget=1.3600 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=8.3280 mle=1.8362 pcon=5.1354 forget=1.3563 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=8.2099 mle=1.7031 pcon=5.1312 forget=1.3755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=8.2020 mle=1.6958 pcon=5.1276 forget=1.3785 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=8.1201 mle=1.6541 pcon=5.1238 forget=1.3422 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=8.2060 mle=1.7298 pcon=5.1198 forget=1.3564 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=8.1326 mle=1.6501 pcon=5.1162 forget=1.3663 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.9324 mle=1.4538 pcon=5.1127 forget=1.3659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=8.2326 mle=1.7671 pcon=5.1088 forget=1.3567 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.9503 mle=1.4914 pcon=5.1053 forget=1.3536 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=8.3394 mle=1.8721 pcon=5.1019 forget=1.3654 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=8.3233 mle=1.8673 pcon=5.0986 forget=1.3575 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=8.2409 mle=1.7996 pcon=5.0950 forget=1.3463 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=8.0363 mle=1.5841 pcon=5.0918 forget=1.3604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=8.0429 mle=1.5955 pcon=5.0883 forget=1.3590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=8.1227 mle=1.6912 pcon=5.0855 forget=1.3460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=8.1948 mle=1.7732 pcon=5.0825 forget=1.3391 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=8.0272 mle=1.5964 pcon=5.0794 forget=1.3514 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=8.1726 mle=1.7467 pcon=5.0766 forget=1.3492 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.8473 mle=1.4012 pcon=5.0737 forget=1.3724 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.9359 mle=1.5360 pcon=5.0710 forget=1.3289 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=8.0220 mle=1.5916 pcon=5.0683 forget=1.3621 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=8.0776 mle=1.6656 pcon=5.0656 forget=1.3464 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=8.0357 mle=1.6301 pcon=5.0634 forget=1.3421 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=8.2403 mle=1.8233 pcon=5.0605 forget=1.3565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.9572 mle=1.5348 pcon=5.0582 forget=1.3643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=8.0785 mle=1.6747 pcon=5.0557 forget=1.3481 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=8.0237 mle=1.6306 pcon=5.0531 forget=1.3401 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=8.1319 mle=1.7455 pcon=5.0507 forget=1.3356 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=8.0181 mle=1.6272 pcon=5.0482 forget=1.3427 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.9576 mle=1.5823 pcon=5.0459 forget=1.3294 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=8.0513 mle=1.6546 pcon=5.0439 forget=1.3528 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=8.2000 mle=1.7958 pcon=5.0418 forget=1.3624 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=8.1418 mle=1.7419 pcon=5.0394 forget=1.3605 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=8.0251 mle=1.6407 pcon=5.0374 forget=1.3470 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 18%|█▊        | 9/50 [02:11<09:45, 14.27s/it] 20%|██        | 10/50 [02:25<09:33, 14.34s/it] 22%|██▏       | 11/50 [02:40<09:18, 14.32s/it] 24%|██▍       | 12/50 [02:54<09:04, 14.33s/it] 26%|██▌       | 13/50 [03:08<08:47, 14.25s/it] 28%|██▊       | 14/50 [03:22<08:32, 14.22s/it] 30%|███       | 15/50 [03:36<08:18, 14.24s/it] 32%|███▏      | 16/50 [03:51<08:03, 14.23s/it][loss] ep 8 it 30 total=7.9333 mle=1.5365 pcon=5.0355 forget=1.3612 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=7.9736 mle=1.5906 pcon=5.0336 forget=1.3493 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.8903 mle=1.5002 pcon=5.0315 forget=1.3586 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=8.0752 mle=1.7033 pcon=5.0296 forget=1.3424 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.8996 mle=1.5307 pcon=5.0281 forget=1.3408 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=8.0355 mle=1.6775 pcon=5.0261 forget=1.3319 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=8.1487 mle=1.7565 pcon=5.0241 forget=1.3682 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.8459 mle=1.4896 pcon=5.0222 forget=1.3341 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.9725 mle=1.6364 pcon=5.0203 forget=1.3157 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=8.0248 mle=1.6696 pcon=5.0183 forget=1.3370 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=8.2114 mle=1.8422 pcon=5.0168 forget=1.3524 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.9321 mle=1.5783 pcon=5.0153 forget=1.3385 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.9230 mle=1.5677 pcon=5.0138 forget=1.3415 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.9188 mle=1.5588 pcon=5.0119 forget=1.3481 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.9065 mle=1.5584 pcon=5.0105 forget=1.3376 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=7.9637 mle=1.6070 pcon=5.0087 forget=1.3479 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=7.9549 mle=1.5901 pcon=5.0071 forget=1.3578 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=8.0986 mle=1.7542 pcon=5.0056 forget=1.3387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=8.1863 mle=1.8510 pcon=5.0042 forget=1.3311 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=8.1946 mle=1.8596 pcon=5.0028 forget=1.3322 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=7.9235 mle=1.5823 pcon=5.0015 forget=1.3396 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=7.8885 mle=1.5627 pcon=5.0003 forget=1.3255 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=8.0263 mle=1.6673 pcon=4.9990 forget=1.3600 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=7.9251 mle=1.5762 pcon=4.9976 forget=1.3513 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=7.9511 mle=1.6220 pcon=4.9963 forget=1.3329 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=7.8904 mle=1.5691 pcon=4.9951 forget=1.3261 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=7.9976 mle=1.6657 pcon=4.9935 forget=1.3384 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=8.1205 mle=1.7816 pcon=4.9924 forget=1.3466 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=7.9392 mle=1.6035 pcon=4.9915 forget=1.3442 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=8.2993 mle=1.9667 pcon=4.9902 forget=1.3424 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=7.9790 mle=1.6705 pcon=4.9890 forget=1.3195 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 12 it 20 total=7.9447 mle=1.6200 pcon=4.9876 forget=1.3371 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=8.0233 mle=1.6947 pcon=4.9865 forget=1.3421 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=7.8866 mle=1.5487 pcon=4.9853 forget=1.3526 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=8.0571 mle=1.7458 pcon=4.9843 forget=1.3270 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=7.9140 mle=1.6003 pcon=4.9830 forget=1.3307 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=7.8672 mle=1.5353 pcon=4.9823 forget=1.3495 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=7.8421 mle=1.5207 pcon=4.9813 forget=1.3401 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=8.1982 mle=1.8678 pcon=4.9802 forget=1.3502 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=7.7499 mle=1.4303 pcon=4.9793 forget=1.3403 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=7.8470 mle=1.4956 pcon=4.9783 forget=1.3731 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=7.8882 mle=1.5713 pcon=4.9771 forget=1.3398 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=7.8635 mle=1.5535 pcon=4.9763 forget=1.3338 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=7.9650 mle=1.6446 pcon=4.9754 forget=1.3449 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=7.8859 mle=1.5833 pcon=4.9745 forget=1.3281 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=7.9050 mle=1.5897 pcon=4.9735 forget=1.3418 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=7.8841 mle=1.5782 pcon=4.9726 forget=1.3332 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=8.0637 mle=1.7691 pcon=4.9715 forget=1.3232 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=8.0812 mle=1.7690 pcon=4.9709 forget=1.3412 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=7.9201 mle=1.6156 pcon=4.9705 forget=1.3340 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=8.1917 mle=1.8838 pcon=4.9695 forget=1.3385 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=7.9185 mle=1.6161 pcon=4.9689 forget=1.3335 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=7.9614 mle=1.6700 pcon=4.9681 forget=1.3234 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=7.9728 mle=1.6809 pcon=4.9673 forget=1.3246 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=7.9974 mle=1.6783 pcon=4.9666 forget=1.3526 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=7.7844 mle=1.4934 pcon=4.9657 forget=1.3252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=7.8641 mle=1.5575 pcon=4.9651 forget=1.3415 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=8.0458 mle=1.7261 pcon=4.9644 forget=1.3552 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=7.9062 mle=1.6169 pcon=4.9638 forget=1.3256 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=8.1768 mle=1.8744 pcon=4.9630 forget=1.3394 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=7.8990 mle=1.5937 pcon=4.9625 forget=1.3428 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=7.8499 mle=1.5446 pcon=4.9618 forget=1.3435 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 16 it 10 total=7.8907 mle=1.5862 pcon=4.9612 forget=1.3433 nr=64 nf=64 protos=540 fproto_sim=NA
 34%|███▍      | 17/50 [04:05<07:50, 14.25s/it] 36%|███▌      | 18/50 [04:19<07:38, 14.34s/it] 38%|███▊      | 19/50 [04:34<07:24, 14.34s/it] 40%|████      | 20/50 [04:48<07:09, 14.31s/it] 42%|████▏     | 21/50 [05:02<06:55, 14.32s/it] 44%|████▍     | 22/50 [05:17<06:41, 14.33s/it] 46%|████▌     | 23/50 [05:31<06:25, 14.28s/it] 48%|████▊     | 24/50 [05:45<06:10, 14.25s/it][loss] ep 16 it 60 total=7.7829 mle=1.4933 pcon=4.9606 forget=1.3290 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=7.8054 mle=1.5134 pcon=4.9602 forget=1.3317 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=7.9786 mle=1.6989 pcon=4.9594 forget=1.3203 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=7.8491 mle=1.5731 pcon=4.9587 forget=1.3174 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=8.2272 mle=1.9477 pcon=4.9579 forget=1.3216 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=8.0820 mle=1.7940 pcon=4.9573 forget=1.3307 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=7.8937 mle=1.6142 pcon=4.9567 forget=1.3229 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 17 it 20 total=8.0713 mle=1.7779 pcon=4.9558 forget=1.3377 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=8.0980 mle=1.8237 pcon=4.9555 forget=1.3188 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=7.8612 mle=1.5745 pcon=4.9548 forget=1.3319 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=7.8690 mle=1.5829 pcon=4.9539 forget=1.3322 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=7.9576 mle=1.6654 pcon=4.9532 forget=1.3390 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=7.7545 mle=1.4682 pcon=4.9531 forget=1.3331 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=8.0817 mle=1.8037 pcon=4.9526 forget=1.3255 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=8.0181 mle=1.7474 pcon=4.9518 forget=1.3189 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=7.8570 mle=1.5873 pcon=4.9517 forget=1.3180 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=7.9687 mle=1.6889 pcon=4.9513 forget=1.3285 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=7.9474 mle=1.6651 pcon=4.9507 forget=1.3316 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=7.8312 mle=1.5479 pcon=4.9503 forget=1.3329 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=7.8572 mle=1.5910 pcon=4.9498 forget=1.3164 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=7.9619 mle=1.6880 pcon=4.9494 forget=1.3245 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=8.0776 mle=1.8073 pcon=4.9490 forget=1.3213 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=7.9827 mle=1.7012 pcon=4.9488 forget=1.3327 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 19 it 40 total=7.7485 mle=1.4659 pcon=4.9487 forget=1.3339 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=7.9934 mle=1.7028 pcon=4.9487 forget=1.3419 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=7.8836 mle=1.6121 pcon=4.9482 forget=1.3232 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=7.9026 mle=1.6441 pcon=4.9477 forget=1.3108 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=8.1214 mle=1.8487 pcon=4.9474 forget=1.3253 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=7.8442 mle=1.5586 pcon=4.9474 forget=1.3383 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=7.9023 mle=1.5976 pcon=4.9473 forget=1.3574 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 20 it 0 total=7.9118 mle=1.6448 pcon=4.9469 forget=1.3201 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=7.9482 mle=1.6853 pcon=4.9466 forget=1.3163 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=8.0017 mle=1.7326 pcon=4.9465 forget=1.3226 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=8.0933 mle=1.8369 pcon=4.9460 forget=1.3103 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=7.9192 mle=1.6658 pcon=4.9455 forget=1.3079 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=8.2899 mle=2.0237 pcon=4.9450 forget=1.3212 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=7.9281 mle=1.6578 pcon=4.9445 forget=1.3259 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=7.6561 mle=1.3906 pcon=4.9441 forget=1.3214 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 10 total=7.6826 mle=1.4140 pcon=4.9437 forget=1.3249 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=7.9533 mle=1.6686 pcon=4.9434 forget=1.3413 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=7.7902 mle=1.5042 pcon=4.9432 forget=1.3428 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=7.8139 mle=1.5594 pcon=4.9432 forget=1.3113 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=8.0579 mle=1.7862 pcon=4.9429 forget=1.3288 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=7.9483 mle=1.6853 pcon=4.9427 forget=1.3203 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=7.8901 mle=1.6159 pcon=4.9422 forget=1.3321 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=7.9313 mle=1.6513 pcon=4.9419 forget=1.3380 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 20 total=7.9304 mle=1.6642 pcon=4.9414 forget=1.3247 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=8.0813 mle=1.8233 pcon=4.9410 forget=1.3170 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=7.7958 mle=1.5205 pcon=4.9405 forget=1.3349 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=7.9218 mle=1.6541 pcon=4.9402 forget=1.3275 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=7.9687 mle=1.6929 pcon=4.9400 forget=1.3358 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=7.7828 mle=1.5052 pcon=4.9398 forget=1.3378 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=7.8817 mle=1.6260 pcon=4.9396 forget=1.3160 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=8.3334 mle=2.0748 pcon=4.9391 forget=1.3194 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 30 total=7.8252 mle=1.5552 pcon=4.9394 forget=1.3306 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=7.9617 mle=1.6999 pcon=4.9391 forget=1.3227 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=7.8454 mle=1.5638 pcon=4.9387 forget=1.3430 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=8.0263 mle=1.7651 pcon=4.9384 forget=1.3229 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=7.9326 mle=1.6686 pcon=4.9384 forget=1.3256 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=7.8275 mle=1.5558 pcon=4.9380 forget=1.3337 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=7.9367 mle=1.6811 pcon=4.9378 forget=1.3177 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=7.7929 mle=1.5305 pcon=4.9374 forget=1.3250 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 24 it 40 total=7.7629 mle=1.5168 pcon=4.9374 forget=1.3087 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=7.8789 mle=1.6182 pcon=4.9372 forget=1.3235 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=7.6919 mle=1.4168 pcon=4.9370 forget=1.3381 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=7.9209 mle=1.6642 pcon=4.9370 forget=1.3197 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 240 total=7.8636 mle=1.5949 pcon=4.9369 forget=1.3319 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=8.0730 mle=1.7952 pcon=4.9367 forget=1.3411 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=7.9881 mle=1.7318 pcon=4.9364 forget=1.3198 nr=64 nf=64 protos=540 fproto_sim=NA
 50%|█████     | 25/50 [05:59<05:53, 14.15s/it] 52%|█████▏    | 26/50 [06:13<05:40, 14.18s/it] 54%|█████▍    | 27/50 [06:27<05:26, 14.19s/it] 56%|█████▌    | 28/50 [06:42<05:13, 14.27s/it] 58%|█████▊    | 29/50 [06:56<05:01, 14.35s/it] 60%|██████    | 30/50 [07:11<04:48, 14.41s/it] 62%|██████▏   | 31/50 [07:25<04:34, 14.44s/it] 64%|██████▍   | 32/50 [07:40<04:20, 14.50s/it] 66%|██████▌   | 33/50 [07:55<04:06, 14.48s/it] 68%|██████▊   | 34/50 [08:09<03:53, 14.57s/it][peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 25 it 0 total=7.9434 mle=1.6872 pcon=4.9361 forget=1.3200 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=8.0916 mle=1.8125 pcon=4.9362 forget=1.3430 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=7.8883 mle=1.6480 pcon=4.9359 forget=1.3043 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 150 total=8.0749 mle=1.8204 pcon=4.9362 forget=1.3183 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 200 total=8.0978 mle=1.8390 pcon=4.9360 forget=1.3228 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=8.0556 mle=1.7949 pcon=4.9357 forget=1.3250 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=7.8685 mle=1.6053 pcon=4.9356 forget=1.3276 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=7.8742 mle=1.6084 pcon=4.9357 forget=1.3301 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 26 it 10 total=7.7672 mle=1.5029 pcon=4.9358 forget=1.3285 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=7.8433 mle=1.5677 pcon=4.9357 forget=1.3399 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=8.0267 mle=1.7761 pcon=4.9357 forget=1.3148 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=7.7136 mle=1.4652 pcon=4.9355 forget=1.3129 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=7.9181 mle=1.6552 pcon=4.9352 forget=1.3278 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=7.7991 mle=1.5245 pcon=4.9353 forget=1.3394 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 310 total=8.1700 mle=1.9197 pcon=4.9353 forget=1.3149 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=7.8711 mle=1.6184 pcon=4.9348 forget=1.3178 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 20 total=7.9444 mle=1.6788 pcon=4.9347 forget=1.3308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=7.8338 mle=1.5749 pcon=4.9344 forget=1.3245 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=7.7957 mle=1.5355 pcon=4.9342 forget=1.3260 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=7.8555 mle=1.5898 pcon=4.9339 forget=1.3318 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=7.8393 mle=1.5797 pcon=4.9337 forget=1.3258 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=7.8849 mle=1.6339 pcon=4.9334 forget=1.3175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=7.9868 mle=1.7243 pcon=4.9332 forget=1.3293 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=8.0448 mle=1.7730 pcon=4.9332 forget=1.3386 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 30 total=7.9867 mle=1.7203 pcon=4.9332 forget=1.3332 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=8.0180 mle=1.7521 pcon=4.9333 forget=1.3326 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=7.8242 mle=1.5560 pcon=4.9334 forget=1.3348 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=7.7182 mle=1.4675 pcon=4.9333 forget=1.3174 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=7.6367 mle=1.3891 pcon=4.9329 forget=1.3147 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=7.8127 mle=1.5365 pcon=4.9326 forget=1.3437 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=8.0530 mle=1.7945 pcon=4.9321 forget=1.3265 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=8.0320 mle=1.7585 pcon=4.9319 forget=1.3417 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 40 total=8.0299 mle=1.7722 pcon=4.9315 forget=1.3262 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=7.8584 mle=1.6141 pcon=4.9312 forget=1.3131 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=7.9265 mle=1.6738 pcon=4.9311 forget=1.3216 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=7.8196 mle=1.5712 pcon=4.9310 forget=1.3175 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=7.7320 mle=1.4792 pcon=4.9308 forget=1.3220 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=8.1112 mle=1.8597 pcon=4.9309 forget=1.3206 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=7.9172 mle=1.6577 pcon=4.9309 forget=1.3285 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 0 total=7.8636 mle=1.6104 pcon=4.9310 forget=1.3223 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=7.8864 mle=1.6237 pcon=4.9310 forget=1.3317 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=7.7239 mle=1.4705 pcon=4.9309 forget=1.3225 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=8.1711 mle=1.9178 pcon=4.9307 forget=1.3226 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=7.7813 mle=1.4920 pcon=4.9306 forget=1.3588 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=8.0352 mle=1.7726 pcon=4.9304 forget=1.3323 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=7.7531 mle=1.4884 pcon=4.9304 forget=1.3343 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=8.2485 mle=1.9840 pcon=4.9303 forget=1.3342 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 10 total=7.8624 mle=1.6067 pcon=4.9303 forget=1.3253 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=7.8699 mle=1.6080 pcon=4.9304 forget=1.3315 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=7.8709 mle=1.6023 pcon=4.9304 forget=1.3382 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=7.8406 mle=1.5798 pcon=4.9304 forget=1.3304 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=7.8132 mle=1.5578 pcon=4.9304 forget=1.3250 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 260 total=7.7090 mle=1.4608 pcon=4.9301 forget=1.3181 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=7.8506 mle=1.5852 pcon=4.9301 forget=1.3352 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=7.8372 mle=1.5885 pcon=4.9300 forget=1.3187 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 20 total=7.9976 mle=1.7491 pcon=4.9298 forget=1.3187 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=7.9455 mle=1.6965 pcon=4.9294 forget=1.3197 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=7.7842 mle=1.5141 pcon=4.9293 forget=1.3408 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 170 total=7.8565 mle=1.5984 pcon=4.9295 forget=1.3286 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=8.1057 mle=1.8653 pcon=4.9296 forget=1.3108 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=7.8423 mle=1.5943 pcon=4.9297 forget=1.3183 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=7.7406 mle=1.4921 pcon=4.9296 forget=1.3189 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=7.6833 mle=1.4361 pcon=4.9299 forget=1.3173 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 30 total=7.9576 mle=1.6905 pcon=4.9300 forget=1.3372 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=8.2896 mle=2.0461 pcon=4.9299 forget=1.3137 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=7.7811 mle=1.5307 pcon=4.9297 forget=1.3207 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=8.0788 mle=1.8252 pcon=4.9298 forget=1.3238 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=7.8016 mle=1.5394 pcon=4.9298 forget=1.3324 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=7.8478 mle=1.5847 pcon=4.9298 forget=1.3333 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=7.8842 mle=1.6383 pcon=4.9300 forget=1.3159 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 380 total=7.8150 mle=1.5492 pcon=4.9300 forget=1.3358 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 40 total=8.0343 mle=1.7725 pcon=4.9297 forget=1.3322 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=7.6549 mle=1.4036 pcon=4.9294 forget=1.3219 nr=64 nf=64 protos=540 fproto_sim=NA
 70%|███████   | 35/50 [08:24<03:37, 14.52s/it] 72%|███████▏  | 36/50 [08:39<03:24, 14.60s/it] 74%|███████▍  | 37/50 [08:53<03:09, 14.56s/it] 76%|███████▌  | 38/50 [09:08<02:55, 14.61s/it] 78%|███████▊  | 39/50 [09:22<02:39, 14.52s/it] 80%|████████  | 40/50 [09:37<02:26, 14.61s/it] 82%|████████▏ | 41/50 [09:51<02:11, 14.57s/it] 84%|████████▍ | 42/50 [10:06<01:56, 14.53s/it] 86%|████████▌ | 43/50 [10:20<01:41, 14.46s/it][loss] ep 34 it 140 total=7.8357 mle=1.5974 pcon=4.9295 forget=1.3088 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=7.9776 mle=1.7367 pcon=4.9295 forget=1.3115 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 240 total=8.1548 mle=1.9068 pcon=4.9292 forget=1.3188 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=7.8573 mle=1.5978 pcon=4.9288 forget=1.3306 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=7.8445 mle=1.5808 pcon=4.9288 forget=1.3349 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 0 total=7.9769 mle=1.7312 pcon=4.9287 forget=1.3170 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 50 total=7.7414 mle=1.4960 pcon=4.9288 forget=1.3166 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 100 total=8.0766 mle=1.8240 pcon=4.9284 forget=1.3242 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 150 total=7.9410 mle=1.7020 pcon=4.9283 forget=1.3107 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 200 total=7.9020 mle=1.6354 pcon=4.9283 forget=1.3383 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 250 total=7.9682 mle=1.7130 pcon=4.9280 forget=1.3272 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 300 total=7.7027 mle=1.4734 pcon=4.9280 forget=1.3014 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 350 total=8.0144 mle=1.7644 pcon=4.9279 forget=1.3221 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 36 it 10 total=8.1320 mle=1.8788 pcon=4.9278 forget=1.3254 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 60 total=7.7643 mle=1.5240 pcon=4.9279 forget=1.3123 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 110 total=7.9301 mle=1.6839 pcon=4.9278 forget=1.3184 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 160 total=7.9377 mle=1.6896 pcon=4.9277 forget=1.3204 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 210 total=7.7088 mle=1.4691 pcon=4.9278 forget=1.3119 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 260 total=7.8422 mle=1.5751 pcon=4.9281 forget=1.3390 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 310 total=8.0417 mle=1.7696 pcon=4.9278 forget=1.3443 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 360 total=7.7583 mle=1.5041 pcon=4.9277 forget=1.3264 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 37 it 20 total=7.7228 mle=1.4702 pcon=4.9274 forget=1.3252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 70 total=8.0566 mle=1.7872 pcon=4.9275 forget=1.3420 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 120 total=7.8586 mle=1.6068 pcon=4.9272 forget=1.3247 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 170 total=7.9252 mle=1.6866 pcon=4.9269 forget=1.3117 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 220 total=8.0501 mle=1.7895 pcon=4.9266 forget=1.3340 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 270 total=7.9151 mle=1.6843 pcon=4.9266 forget=1.3043 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 320 total=7.8807 mle=1.6418 pcon=4.9263 forget=1.3127 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 370 total=7.8706 mle=1.6033 pcon=4.9264 forget=1.3409 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 30 total=8.0626 mle=1.8023 pcon=4.9265 forget=1.3338 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 80 total=7.7659 mle=1.5164 pcon=4.9265 forget=1.3229 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 130 total=7.9584 mle=1.7078 pcon=4.9265 forget=1.3241 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 180 total=7.8740 mle=1.6232 pcon=4.9264 forget=1.3245 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 230 total=7.8402 mle=1.5832 pcon=4.9263 forget=1.3308 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 280 total=7.9184 mle=1.6688 pcon=4.9262 forget=1.3233 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 330 total=7.9208 mle=1.6717 pcon=4.9259 forget=1.3232 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 380 total=7.8394 mle=1.5757 pcon=4.9260 forget=1.3376 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 40 total=8.1105 mle=1.8565 pcon=4.9263 forget=1.3277 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 90 total=8.0018 mle=1.7560 pcon=4.9262 forget=1.3197 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 140 total=7.9053 mle=1.6560 pcon=4.9263 forget=1.3230 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 190 total=7.7819 mle=1.5409 pcon=4.9262 forget=1.3148 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 240 total=8.0369 mle=1.7979 pcon=4.9262 forget=1.3128 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 290 total=7.7428 mle=1.4903 pcon=4.9261 forget=1.3264 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 340 total=7.9049 mle=1.6542 pcon=4.9262 forget=1.3245 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 0 total=7.9543 mle=1.6846 pcon=4.9260 forget=1.3437 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 50 total=7.9297 mle=1.6652 pcon=4.9258 forget=1.3387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 100 total=7.8636 mle=1.6267 pcon=4.9255 forget=1.3114 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 150 total=7.7046 mle=1.4597 pcon=4.9256 forget=1.3194 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 200 total=8.0053 mle=1.7432 pcon=4.9254 forget=1.3367 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 250 total=8.1873 mle=1.9418 pcon=4.9254 forget=1.3201 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 300 total=7.7522 mle=1.4879 pcon=4.9256 forget=1.3387 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 350 total=7.6397 mle=1.4079 pcon=4.9257 forget=1.3062 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 10 total=7.7491 mle=1.5113 pcon=4.9258 forget=1.3121 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 60 total=8.2774 mle=2.0116 pcon=4.9259 forget=1.3399 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 110 total=7.8902 mle=1.6357 pcon=4.9257 forget=1.3289 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 160 total=7.7603 mle=1.5026 pcon=4.9259 forget=1.3318 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 210 total=7.7955 mle=1.5452 pcon=4.9259 forget=1.3244 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 260 total=7.7393 mle=1.4955 pcon=4.9262 forget=1.3176 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 310 total=7.7883 mle=1.5445 pcon=4.9264 forget=1.3174 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 360 total=7.9324 mle=1.6845 pcon=4.9266 forget=1.3213 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 42 it 20 total=7.8810 mle=1.6268 pcon=4.9267 forget=1.3275 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 70 total=7.9174 mle=1.6643 pcon=4.9266 forget=1.3266 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 120 total=8.2639 mle=2.0047 pcon=4.9263 forget=1.3329 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 170 total=7.8087 mle=1.5570 pcon=4.9265 forget=1.3252 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 220 total=7.8807 mle=1.6261 pcon=4.9266 forget=1.3279 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 270 total=8.0089 mle=1.7618 pcon=4.9267 forget=1.3205 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 320 total=7.9431 mle=1.6961 pcon=4.9269 forget=1.3201 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 370 total=7.9704 mle=1.7249 pcon=4.9270 forget=1.3186 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 43 it 30 total=7.8470 mle=1.5847 pcon=4.9270 forget=1.3353 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 80 total=7.8194 mle=1.5545 pcon=4.9271 forget=1.3378 nr=64 nf=64 protos=540 fproto_sim=NA
 88%|████████▊ | 44/50 [10:35<01:26, 14.48s/it] 90%|█████████ | 45/50 [10:49<01:12, 14.45s/it] 92%|█████████▏| 46/50 [11:03<00:57, 14.40s/it] 94%|█████████▍| 47/50 [11:18<00:43, 14.41s/it] 96%|█████████▌| 48/50 [11:32<00:28, 14.49s/it] 98%|█████████▊| 49/50 [11:47<00:14, 14.61s/it]100%|██████████| 50/50 [12:02<00:00, 14.64s/it]100%|██████████| 50/50 [12:02<00:00, 14.45s/it]
[loss] ep 43 it 130 total=7.9643 mle=1.7072 pcon=4.9271 forget=1.3300 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 180 total=7.7509 mle=1.4787 pcon=4.9273 forget=1.3449 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 230 total=7.9012 mle=1.6510 pcon=4.9272 forget=1.3230 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 280 total=7.7820 mle=1.5452 pcon=4.9273 forget=1.3094 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 330 total=7.7971 mle=1.5449 pcon=4.9272 forget=1.3250 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 380 total=7.7462 mle=1.5039 pcon=4.9272 forget=1.3151 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 44 it 40 total=8.0718 mle=1.8169 pcon=4.9272 forget=1.3277 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 90 total=7.8284 mle=1.5918 pcon=4.9271 forget=1.3095 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 140 total=8.0498 mle=1.7899 pcon=4.9273 forget=1.3326 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 190 total=7.6311 mle=1.3804 pcon=4.9271 forget=1.3236 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 240 total=7.8702 mle=1.6008 pcon=4.9271 forget=1.3423 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 290 total=8.0716 mle=1.8103 pcon=4.9270 forget=1.3344 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 340 total=8.2259 mle=1.9785 pcon=4.9271 forget=1.3203 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 0 total=7.8159 mle=1.5578 pcon=4.9270 forget=1.3311 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 50 total=7.8493 mle=1.6138 pcon=4.9269 forget=1.3086 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 100 total=7.8470 mle=1.6051 pcon=4.9268 forget=1.3151 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 150 total=7.8342 mle=1.5751 pcon=4.9267 forget=1.3323 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 200 total=7.8536 mle=1.5917 pcon=4.9268 forget=1.3351 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 250 total=7.7165 mle=1.4697 pcon=4.9268 forget=1.3200 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 300 total=7.9371 mle=1.6849 pcon=4.9265 forget=1.3256 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 350 total=7.7421 mle=1.4900 pcon=4.9267 forget=1.3255 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 10 total=7.7364 mle=1.4811 pcon=4.9267 forget=1.3285 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 60 total=7.6895 mle=1.4380 pcon=4.9268 forget=1.3247 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 110 total=7.9498 mle=1.6971 pcon=4.9269 forget=1.3258 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 160 total=7.7311 mle=1.4843 pcon=4.9267 forget=1.3201 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 210 total=7.9706 mle=1.7288 pcon=4.9266 forget=1.3152 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 260 total=7.9117 mle=1.6713 pcon=4.9264 forget=1.3141 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 310 total=7.7560 mle=1.5078 pcon=4.9265 forget=1.3217 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 360 total=7.9324 mle=1.6740 pcon=4.9264 forget=1.3320 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 20 total=7.8848 mle=1.6236 pcon=4.9263 forget=1.3349 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 70 total=7.8340 mle=1.5735 pcon=4.9261 forget=1.3345 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 120 total=7.8850 mle=1.6223 pcon=4.9259 forget=1.3368 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 170 total=7.7867 mle=1.5127 pcon=4.9257 forget=1.3483 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 220 total=7.6998 mle=1.4376 pcon=4.9257 forget=1.3365 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 270 total=7.8312 mle=1.5822 pcon=4.9257 forget=1.3234 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 320 total=7.8740 mle=1.6230 pcon=4.9255 forget=1.3255 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 370 total=7.9486 mle=1.7034 pcon=4.9254 forget=1.3197 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 30 total=8.0129 mle=1.7412 pcon=4.9255 forget=1.3461 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 80 total=7.8748 mle=1.6104 pcon=4.9255 forget=1.3388 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 130 total=7.9702 mle=1.7208 pcon=4.9257 forget=1.3238 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 180 total=7.9556 mle=1.7054 pcon=4.9258 forget=1.3245 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 230 total=8.0302 mle=1.7891 pcon=4.9260 forget=1.3151 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 280 total=7.7452 mle=1.5040 pcon=4.9258 forget=1.3154 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 330 total=7.7531 mle=1.5157 pcon=4.9256 forget=1.3119 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 380 total=7.8488 mle=1.6075 pcon=4.9253 forget=1.3160 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 40 total=7.8043 mle=1.5558 pcon=4.9254 forget=1.3231 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 90 total=7.8458 mle=1.6033 pcon=4.9253 forget=1.3172 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 140 total=7.8840 mle=1.6295 pcon=4.9252 forget=1.3292 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 190 total=7.9958 mle=1.7511 pcon=4.9249 forget=1.3197 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 240 total=7.9146 mle=1.6790 pcon=4.9247 forget=1.3110 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 290 total=7.9080 mle=1.6637 pcon=4.9245 forget=1.3198 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 340 total=7.7127 mle=1.4752 pcon=4.9248 forget=1.3127 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:57,  3.33it/s]  3%|▎         | 11/391 [00:00<00:11, 33.91it/s]  5%|▌         | 21/391 [00:00<00:06, 53.54it/s]  8%|▊         | 31/391 [00:00<00:05, 66.62it/s] 10%|█         | 41/391 [00:00<00:04, 75.35it/s] 13%|█▎        | 51/391 [00:00<00:04, 80.80it/s] 16%|█▌        | 61/391 [00:00<00:03, 85.07it/s] 18%|█▊        | 71/391 [00:01<00:03, 87.98it/s] 21%|██        | 81/391 [00:01<00:03, 90.20it/s] 23%|██▎       | 91/391 [00:01<00:03, 91.53it/s] 26%|██▌       | 101/391 [00:01<00:03, 92.54it/s] 28%|██▊       | 111/391 [00:01<00:03, 93.19it/s] 31%|███       | 121/391 [00:01<00:02, 93.50it/s] 34%|███▎      | 131/391 [00:01<00:02, 94.08it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.59it/s] 39%|███▊      | 151/391 [00:01<00:02, 95.03it/s] 41%|████      | 161/391 [00:01<00:02, 94.89it/s] 44%|████▎     | 171/391 [00:02<00:02, 94.80it/s] 46%|████▋     | 181/391 [00:02<00:02, 93.73it/s] 49%|████▉     | 191/391 [00:02<00:02, 94.07it/s] 51%|█████▏    | 201/391 [00:02<00:02, 94.14it/s] 54%|█████▍    | 211/391 [00:02<00:01, 94.46it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.57it/s] 59%|█████▉    | 231/391 [00:02<00:01, 94.67it/s] 62%|██████▏   | 241/391 [00:02<00:01, 94.81it/s] 64%|██████▍   | 251/391 [00:02<00:01, 94.92it/s] 67%|██████▋   | 261/391 [00:03<00:01, 94.86it/s] 69%|██████▉   | 271/391 [00:03<00:01, 95.00it/s] 72%|███████▏  | 281/391 [00:03<00:01, 94.82it/s] 74%|███████▍  | 291/391 [00:03<00:01, 95.17it/s] 77%|███████▋  | 301/391 [00:03<00:00, 94.02it/s] 80%|███████▉  | 311/391 [00:03<00:00, 94.55it/s] 82%|████████▏ | 321/391 [00:03<00:00, 94.75it/s] 85%|████████▍ | 331/391 [00:03<00:00, 94.61it/s] 87%|████████▋ | 341/391 [00:03<00:00, 94.22it/s] 90%|████████▉ | 351/391 [00:03<00:00, 94.20it/s] 92%|█████████▏| 361/391 [00:04<00:00, 94.30it/s] 95%|█████████▍| 371/391 [00:04<00:00, 94.19it/s] 97%|█████████▋| 381/391 [00:04<00:00, 94.16it/s]100%|██████████| 391/391 [00:04<00:00, 91.67it/s]100%|██████████| 391/391 [00:04<00:00, 88.18it/s]
50000 images processed, 4.520348310470581 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:27,  2.86it/s] 14%|█▍        | 11/79 [00:00<00:02, 30.55it/s] 27%|██▋       | 21/79 [00:00<00:01, 49.84it/s] 39%|███▉      | 31/79 [00:00<00:00, 63.15it/s] 52%|█████▏    | 41/79 [00:00<00:00, 72.64it/s] 65%|██████▍   | 51/79 [00:00<00:00, 79.21it/s] 77%|███████▋  | 61/79 [00:00<00:00, 83.28it/s] 90%|████████▉ | 71/79 [00:01<00:00, 86.92it/s]100%|██████████| 79/79 [00:01<00:00, 66.72it/s]
10000 images processed, 1.207841157913208 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:17,  2.62it/s]  5%|▌         | 11/204 [00:00<00:06, 28.60it/s] 10%|█         | 21/204 [00:00<00:03, 47.28it/s] 15%|█▌        | 31/204 [00:00<00:02, 60.63it/s] 20%|██        | 41/204 [00:00<00:02, 70.52it/s] 25%|██▌       | 51/204 [00:00<00:01, 77.40it/s] 30%|██▉       | 61/204 [00:01<00:01, 82.20it/s] 35%|███▍      | 71/204 [00:01<00:01, 85.70it/s] 40%|███▉      | 81/204 [00:01<00:01, 88.18it/s] 45%|████▍     | 91/204 [00:01<00:01, 89.93it/s] 50%|████▉     | 101/204 [00:01<00:01, 90.94it/s] 54%|█████▍    | 111/204 [00:01<00:01, 91.69it/s] 59%|█████▉    | 121/204 [00:01<00:00, 92.20it/s] 64%|██████▍   | 131/204 [00:01<00:00, 92.70it/s] 69%|██████▉   | 141/204 [00:01<00:00, 93.30it/s] 74%|███████▍  | 151/204 [00:01<00:00, 93.67it/s] 79%|███████▉  | 161/204 [00:02<00:00, 93.96it/s] 84%|████████▍ | 171/204 [00:02<00:00, 94.22it/s] 89%|████████▊ | 181/204 [00:02<00:00, 94.45it/s] 94%|█████████▎| 191/204 [00:02<00:00, 94.71it/s] 99%|█████████▊| 201/204 [00:02<00:00, 95.05it/s]100%|██████████| 204/204 [00:02<00:00, 80.13it/s]
26032 images processed, 2.5910162925720215 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.93it/s] 11%|█▏        | 9/79 [00:00<00:04, 16.67it/s] 22%|██▏       | 17/79 [00:00<00:02, 26.82it/s] 32%|███▏      | 25/79 [00:00<00:01, 33.86it/s] 42%|████▏     | 33/79 [00:01<00:01, 38.76it/s] 52%|█████▏    | 41/79 [00:01<00:00, 42.17it/s] 62%|██████▏   | 49/79 [00:01<00:00, 44.70it/s] 72%|███████▏  | 57/79 [00:01<00:00, 45.26it/s] 82%|████████▏ | 65/79 [00:01<00:00, 46.18it/s] 92%|█████████▏| 73/79 [00:01<00:00, 46.83it/s]100%|██████████| 79/79 [00:02<00:00, 38.83it/s]
10000 images processed, 2.0701303482055664 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:30,  2.57it/s] 14%|█▍        | 11/79 [00:00<00:02, 28.30it/s] 27%|██▋       | 21/79 [00:00<00:01, 47.23it/s] 39%|███▉      | 31/79 [00:00<00:00, 60.92it/s] 52%|█████▏    | 41/79 [00:00<00:00, 70.81it/s] 65%|██████▍   | 51/79 [00:00<00:00, 77.86it/s] 77%|███████▋  | 61/79 [00:01<00:00, 82.88it/s] 90%|████████▉ | 71/79 [00:01<00:00, 86.74it/s]100%|██████████| 79/79 [00:01<00:00, 65.62it/s]
10000 images processed, 1.2238750457763672 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:25,  2.71it/s] 16%|█▌        | 11/70 [00:00<00:02, 29.31it/s] 29%|██▊       | 20/70 [00:00<00:01, 45.67it/s] 43%|████▎     | 30/70 [00:00<00:00, 60.06it/s] 57%|█████▋    | 40/70 [00:00<00:00, 70.27it/s] 71%|███████▏  | 50/70 [00:00<00:00, 77.52it/s] 86%|████████▌ | 60/70 [00:01<00:00, 82.88it/s]100%|██████████| 70/70 [00:01<00:00, 85.03it/s]100%|██████████| 70/70 [00:01<00:00, 62.79it/s]
8925 images processed, 1.1463027000427246 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:40,  1.08it/s]  4%|▍         | 2/45 [00:01<00:19,  2.23it/s] 20%|██        | 9/45 [00:01<00:03,  9.31it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.05it/s] 38%|███▊      | 17/45 [00:01<00:02, 11.89it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.52it/s] 51%|█████     | 23/45 [00:02<00:01, 15.35it/s] 58%|█████▊    | 26/45 [00:02<00:01, 11.05it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.97it/s] 78%|███████▊  | 35/45 [00:03<00:00, 10.95it/s] 91%|█████████ | 41/45 [00:03<00:00, 13.67it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.60it/s]100%|██████████| 45/45 [00:04<00:00, 10.26it/s]
5640 images processed, 4.408182144165039 seconds used

18.94172763824463
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.88  81.18
LSUN          17.48  96.10
iSUN          72.38  81.67
dtd           37.89  91.38
forget        79.50  86.88
AVG           46.28  89.43
Retain-Acc: 0.7407
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.88 AUIN: 98.36
8.819669008255005
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.1 lr=0.001 epochs=5
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.1, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:18<01:12, 18.08s/it] 40%|████      | 2/5 [00:32<00:47, 15.95s/it] 60%|██████    | 3/5 [00:46<00:30, 15.10s/it] 80%|████████  | 4/5 [01:00<00:14, 14.67s/it]100%|██████████| 5/5 [01:14<00:00, 14.45s/it]100%|██████████| 5/5 [01:14<00:00, 14.94s/it]
[loss] ep 0 it 0 total=7.5537 mle=1.5709 pcon=5.2950 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=7.5312 mle=1.5426 pcon=5.2879 forget=0.7007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=7.6682 mle=1.7003 pcon=5.2809 forget=0.6870 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=7.8584 mle=1.8997 pcon=5.2738 forget=0.6848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=7.6714 mle=1.7131 pcon=5.2670 forget=0.6913 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=7.4515 mle=1.5019 pcon=5.2603 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=7.5047 mle=1.5596 pcon=5.2540 forget=0.6910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=7.6234 mle=1.6810 pcon=5.2476 forget=0.6948 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=7.6174 mle=1.6715 pcon=5.2409 forget=0.7050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=7.5551 mle=1.6370 pcon=5.2346 forget=0.6835 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=7.4397 mle=1.5165 pcon=5.2285 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=7.6973 mle=1.7801 pcon=5.2224 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=7.6737 mle=1.7644 pcon=5.2167 forget=0.6926 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=7.5009 mle=1.6055 pcon=5.2112 forget=0.6841 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=7.6186 mle=1.7239 pcon=5.2056 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=7.6876 mle=1.7943 pcon=5.2003 forget=0.6931 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=7.4363 mle=1.5528 pcon=5.1950 forget=0.6884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=7.7570 mle=1.8882 pcon=5.1899 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=7.6143 mle=1.7417 pcon=5.1847 forget=0.6879 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=7.3888 mle=1.5267 pcon=5.1796 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=7.4532 mle=1.5857 pcon=5.1745 forget=0.6929 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=7.7391 mle=1.8839 pcon=5.1699 forget=0.6853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=7.4362 mle=1.5922 pcon=5.1652 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=7.6542 mle=1.8113 pcon=5.1605 forget=0.6824 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=7.5468 mle=1.7055 pcon=5.1560 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=7.5123 mle=1.6709 pcon=5.1519 forget=0.6896 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=7.5973 mle=1.7669 pcon=5.1475 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=7.7427 mle=1.9180 pcon=5.1435 forget=0.6811 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=7.3892 mle=1.5697 pcon=5.1395 forget=0.6799 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=7.6491 mle=1.8357 pcon=5.1353 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=7.5216 mle=1.7029 pcon=5.1311 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=7.5117 mle=1.6951 pcon=5.1275 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=7.4485 mle=1.6538 pcon=5.1236 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=7.5272 mle=1.7295 pcon=5.1196 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=7.4492 mle=1.6502 pcon=5.1160 forget=0.6831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.2485 mle=1.4531 pcon=5.1125 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=7.5533 mle=1.7665 pcon=5.1086 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.2730 mle=1.4912 pcon=5.1051 forget=0.6767 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=7.6555 mle=1.8714 pcon=5.1016 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:28,  2.62it/s]  3%|▎         | 11/391 [00:00<00:13, 28.64it/s]  5%|▌         | 21/391 [00:00<00:07, 47.64it/s]  8%|▊         | 31/391 [00:00<00:05, 61.20it/s] 10%|█         | 41/391 [00:00<00:04, 71.01it/s] 13%|█▎        | 51/391 [00:00<00:04, 77.83it/s] 16%|█▌        | 61/391 [00:01<00:03, 82.81it/s] 18%|█▊        | 71/391 [00:01<00:03, 86.13it/s] 21%|██        | 81/391 [00:01<00:03, 88.27it/s] 23%|██▎       | 91/391 [00:01<00:03, 89.77it/s] 26%|██▌       | 101/391 [00:01<00:03, 91.19it/s] 28%|██▊       | 111/391 [00:01<00:03, 92.25it/s] 31%|███       | 121/391 [00:01<00:02, 92.81it/s] 34%|███▎      | 131/391 [00:01<00:02, 93.55it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.03it/s] 39%|███▊      | 151/391 [00:01<00:02, 94.12it/s] 41%|████      | 161/391 [00:02<00:02, 94.17it/s] 44%|████▎     | 171/391 [00:02<00:02, 94.28it/s] 46%|████▋     | 181/391 [00:02<00:02, 94.19it/s] 49%|████▉     | 191/391 [00:02<00:02, 94.43it/s] 51%|█████▏    | 201/391 [00:02<00:02, 94.65it/s] 54%|█████▍    | 211/391 [00:02<00:01, 94.61it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.75it/s] 59%|█████▉    | 231/391 [00:02<00:01, 94.77it/s] 62%|██████▏   | 241/391 [00:02<00:01, 94.85it/s] 64%|██████▍   | 251/391 [00:03<00:01, 94.74it/s] 67%|██████▋   | 261/391 [00:03<00:01, 94.84it/s] 69%|██████▉   | 271/391 [00:03<00:01, 95.00it/s] 72%|███████▏  | 281/391 [00:03<00:01, 94.97it/s] 74%|███████▍  | 291/391 [00:03<00:01, 94.85it/s] 77%|███████▋  | 301/391 [00:03<00:00, 94.82it/s] 80%|███████▉  | 311/391 [00:03<00:00, 93.09it/s] 82%|████████▏ | 321/391 [00:03<00:00, 93.36it/s] 85%|████████▍ | 331/391 [00:03<00:00, 93.79it/s] 87%|████████▋ | 341/391 [00:03<00:00, 94.16it/s] 90%|████████▉ | 351/391 [00:04<00:00, 94.29it/s] 92%|█████████▏| 361/391 [00:04<00:00, 94.14it/s] 95%|█████████▍| 371/391 [00:04<00:00, 94.33it/s] 97%|█████████▋| 381/391 [00:04<00:00, 94.29it/s]100%|██████████| 391/391 [00:04<00:00, 91.91it/s]100%|██████████| 391/391 [00:04<00:00, 86.43it/s]
50000 images processed, 4.628648281097412 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:27,  2.86it/s] 14%|█▍        | 11/79 [00:00<00:02, 30.29it/s] 27%|██▋       | 21/79 [00:00<00:01, 49.46it/s] 39%|███▉      | 31/79 [00:00<00:00, 62.64it/s] 52%|█████▏    | 41/79 [00:00<00:00, 71.83it/s] 63%|██████▎   | 50/79 [00:00<00:00, 76.86it/s] 76%|███████▌  | 60/79 [00:00<00:00, 81.82it/s] 89%|████████▊ | 70/79 [00:01<00:00, 85.80it/s]100%|██████████| 79/79 [00:01<00:00, 66.14it/s]
10000 images processed, 1.21702241897583 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:12,  2.81it/s]  5%|▌         | 11/204 [00:00<00:06, 30.11it/s] 10%|█         | 21/204 [00:00<00:03, 49.18it/s] 15%|█▌        | 31/204 [00:00<00:02, 62.20it/s] 20%|██        | 41/204 [00:00<00:02, 71.57it/s] 25%|██▌       | 51/204 [00:00<00:01, 78.07it/s] 30%|██▉       | 61/204 [00:00<00:01, 82.47it/s] 35%|███▍      | 71/204 [00:01<00:01, 85.61it/s] 40%|███▉      | 81/204 [00:01<00:01, 87.81it/s] 45%|████▍     | 91/204 [00:01<00:01, 89.28it/s] 50%|████▉     | 101/204 [00:01<00:01, 90.34it/s] 54%|█████▍    | 111/204 [00:01<00:01, 91.17it/s] 59%|█████▉    | 121/204 [00:01<00:00, 91.68it/s] 64%|██████▍   | 131/204 [00:01<00:00, 92.14it/s] 69%|██████▉   | 141/204 [00:01<00:00, 92.42it/s] 74%|███████▍  | 151/204 [00:01<00:00, 92.68it/s] 79%|███████▉  | 161/204 [00:02<00:00, 92.86it/s] 84%|████████▍ | 171/204 [00:02<00:00, 92.11it/s] 89%|████████▊ | 181/204 [00:02<00:00, 92.28it/s] 94%|█████████▎| 191/204 [00:02<00:00, 92.82it/s] 99%|█████████▊| 201/204 [00:02<00:00, 93.69it/s]100%|██████████| 204/204 [00:02<00:00, 80.24it/s]
26032 images processed, 2.5893237590789795 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.91it/s] 11%|█▏        | 9/79 [00:00<00:04, 16.67it/s] 22%|██▏       | 17/79 [00:00<00:02, 26.91it/s] 32%|███▏      | 25/79 [00:00<00:01, 34.04it/s] 42%|████▏     | 33/79 [00:01<00:01, 38.93it/s] 52%|█████▏    | 41/79 [00:01<00:00, 42.21it/s] 62%|██████▏   | 49/79 [00:01<00:00, 44.34it/s] 72%|███████▏  | 57/79 [00:01<00:00, 45.37it/s] 82%|████████▏ | 65/79 [00:01<00:00, 47.24it/s] 92%|█████████▏| 73/79 [00:01<00:00, 48.36it/s] 99%|█████████▊| 78/79 [00:02<00:00, 47.47it/s]100%|██████████| 79/79 [00:02<00:00, 38.09it/s]
10000 images processed, 2.1086597442626953 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:28,  2.77it/s] 14%|█▍        | 11/79 [00:00<00:02, 29.83it/s] 27%|██▋       | 21/79 [00:00<00:01, 49.00it/s] 39%|███▉      | 31/79 [00:00<00:00, 62.55it/s] 52%|█████▏    | 41/79 [00:00<00:00, 72.13it/s] 65%|██████▍   | 51/79 [00:00<00:00, 78.79it/s] 77%|███████▋  | 61/79 [00:00<00:00, 83.45it/s] 90%|████████▉ | 71/79 [00:01<00:00, 87.10it/s]100%|██████████| 79/79 [00:01<00:00, 67.05it/s]
10000 images processed, 1.1991877555847168 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:31,  2.17it/s] 14%|█▍        | 10/70 [00:00<00:02, 22.88it/s] 29%|██▊       | 20/70 [00:00<00:01, 41.71it/s] 43%|████▎     | 30/70 [00:00<00:00, 56.13it/s] 57%|█████▋    | 40/70 [00:00<00:00, 66.87it/s] 71%|███████▏  | 50/70 [00:00<00:00, 74.81it/s] 86%|████████▌ | 60/70 [00:01<00:00, 80.73it/s]100%|██████████| 70/70 [00:01<00:00, 83.52it/s]100%|██████████| 70/70 [00:01<00:00, 58.23it/s]
8925 images processed, 1.2342252731323242 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:39,  1.12it/s]  4%|▍         | 2/45 [00:01<00:19,  2.23it/s] 20%|██        | 9/45 [00:01<00:03,  9.63it/s] 24%|██▍       | 11/45 [00:01<00:04,  8.00it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.87it/s] 42%|████▏     | 19/45 [00:02<00:02,  9.60it/s] 56%|█████▌    | 25/45 [00:02<00:01, 15.37it/s] 62%|██████▏   | 28/45 [00:02<00:01, 11.10it/s] 73%|███████▎  | 33/45 [00:03<00:00, 13.43it/s] 78%|███████▊  | 35/45 [00:03<00:00, 10.74it/s] 91%|█████████ | 41/45 [00:03<00:00, 15.53it/s] 98%|█████████▊| 44/45 [00:04<00:00, 11.44it/s]100%|██████████| 45/45 [00:04<00:00, 10.65it/s]
5640 images processed, 4.24671745300293 seconds used

18.988996744155884
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.51  99.36
places365     67.87  81.18
LSUN          17.54  96.08
iSUN          72.31  81.68
dtd           37.85  91.39
forget        79.50  86.89
AVG           46.27  89.43
Retain-Acc: 0.7407
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.89 AUIN: 98.36
8.864133834838867
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.1 lr=0.001 epochs=10
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.1, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:17<02:40, 17.79s/it] 20%|██        | 2/10 [00:32<02:06, 15.75s/it] 30%|███       | 3/10 [00:46<01:46, 15.18s/it] 40%|████      | 4/10 [01:01<01:29, 14.90s/it] 50%|█████     | 5/10 [01:15<01:13, 14.71s/it] 60%|██████    | 6/10 [01:29<00:57, 14.46s/it] 70%|███████   | 7/10 [01:43<00:43, 14.35s/it] 80%|████████  | 8/10 [01:58<00:29, 14.59s/it][loss] ep 0 it 0 total=7.5537 mle=1.5709 pcon=5.2950 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=7.5312 mle=1.5426 pcon=5.2879 forget=0.7007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=7.6682 mle=1.7003 pcon=5.2809 forget=0.6870 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=7.8584 mle=1.8997 pcon=5.2738 forget=0.6848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=7.6714 mle=1.7131 pcon=5.2670 forget=0.6913 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=7.4515 mle=1.5019 pcon=5.2603 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=7.5047 mle=1.5596 pcon=5.2540 forget=0.6910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=7.6234 mle=1.6810 pcon=5.2476 forget=0.6948 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=7.6174 mle=1.6715 pcon=5.2409 forget=0.7050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=7.5551 mle=1.6370 pcon=5.2346 forget=0.6835 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=7.4397 mle=1.5165 pcon=5.2285 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=7.6973 mle=1.7801 pcon=5.2224 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=7.6737 mle=1.7644 pcon=5.2167 forget=0.6926 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=7.5009 mle=1.6055 pcon=5.2112 forget=0.6841 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=7.6186 mle=1.7239 pcon=5.2056 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=7.6876 mle=1.7943 pcon=5.2003 forget=0.6931 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=7.4363 mle=1.5528 pcon=5.1950 forget=0.6884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=7.7570 mle=1.8883 pcon=5.1899 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=7.6144 mle=1.7418 pcon=5.1847 forget=0.6879 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=7.3887 mle=1.5266 pcon=5.1796 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=7.4532 mle=1.5857 pcon=5.1745 forget=0.6930 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=7.7392 mle=1.8839 pcon=5.1699 forget=0.6853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=7.4362 mle=1.5922 pcon=5.1652 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=7.6540 mle=1.8111 pcon=5.1605 forget=0.6824 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=7.5470 mle=1.7057 pcon=5.1560 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=7.5124 mle=1.6710 pcon=5.1519 forget=0.6896 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=7.5973 mle=1.7669 pcon=5.1475 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=7.7426 mle=1.9180 pcon=5.1435 forget=0.6811 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=7.3894 mle=1.5700 pcon=5.1395 forget=0.6799 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=7.6492 mle=1.8358 pcon=5.1353 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=7.5218 mle=1.7030 pcon=5.1311 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=7.5117 mle=1.6951 pcon=5.1275 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=7.4486 mle=1.6538 pcon=5.1237 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=7.5271 mle=1.7293 pcon=5.1196 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=7.4491 mle=1.6501 pcon=5.1160 forget=0.6831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.2485 mle=1.4532 pcon=5.1125 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=7.5534 mle=1.7666 pcon=5.1086 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.2731 mle=1.4913 pcon=5.1051 forget=0.6767 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=7.6555 mle=1.8714 pcon=5.1016 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=7.6440 mle=1.8670 pcon=5.0983 forget=0.6787 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=7.5676 mle=1.8000 pcon=5.0947 forget=0.6730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=7.3556 mle=1.5840 pcon=5.0915 forget=0.6801 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=7.3611 mle=1.5938 pcon=5.0880 forget=0.6794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=7.4487 mle=1.6907 pcon=5.0851 forget=0.6728 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=7.5281 mle=1.7766 pcon=5.0821 forget=0.6694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=7.3508 mle=1.5963 pcon=5.0790 forget=0.6755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=7.4967 mle=1.7462 pcon=5.0761 forget=0.6744 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.1596 mle=1.4003 pcon=5.0732 forget=0.6860 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.2697 mle=1.5350 pcon=5.0705 forget=0.6643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=7.3400 mle=1.5915 pcon=5.0678 forget=0.6807 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=7.4032 mle=1.6653 pcon=5.0650 forget=0.6729 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=7.3628 mle=1.6292 pcon=5.0627 forget=0.6708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=7.5595 mle=1.8217 pcon=5.0599 forget=0.6779 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.2747 mle=1.5354 pcon=5.0575 forget=0.6818 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=7.4024 mle=1.6737 pcon=5.0549 forget=0.6737 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=7.3529 mle=1.6310 pcon=5.0523 forget=0.6697 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=7.4610 mle=1.7436 pcon=5.0499 forget=0.6675 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=7.3439 mle=1.6257 pcon=5.0473 forget=0.6709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.2904 mle=1.5811 pcon=5.0449 forget=0.6643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=7.3703 mle=1.6515 pcon=5.0429 forget=0.6759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=7.5162 mle=1.7948 pcon=5.0407 forget=0.6807 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=7.4582 mle=1.7404 pcon=5.0383 forget=0.6796 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=7.3483 mle=1.6392 pcon=5.0362 forget=0.6730 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 8 it 30 total=7.2502 mle=1.5361 pcon=5.0342 forget=0.6799 nr=64 nf=64 protos=540 fproto_sim=NA
 90%|█████████ | 9/10 [02:18<00:16, 16.29s/it]100%|██████████| 10/10 [02:34<00:00, 16.21s/it]100%|██████████| 10/10 [02:34<00:00, 15.47s/it]
[loss] ep 8 it 80 total=7.2957 mle=1.5894 pcon=5.0322 forget=0.6741 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.2135 mle=1.5052 pcon=5.0300 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=7.4009 mle=1.7025 pcon=5.0281 forget=0.6704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.2263 mle=1.5302 pcon=5.0265 forget=0.6696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=7.3664 mle=1.6770 pcon=5.0245 forget=0.6650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=7.4607 mle=1.7553 pcon=5.0223 forget=0.6831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.1723 mle=1.4857 pcon=5.0204 forget=0.6662 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.3093 mle=1.6339 pcon=5.0184 forget=0.6571 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=7.3526 mle=1.6686 pcon=5.0162 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=7.5279 mle=1.8380 pcon=5.0146 forget=0.6752 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.2517 mle=1.5710 pcon=5.0130 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.2523 mle=1.5714 pcon=5.0113 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.2464 mle=1.5645 pcon=5.0094 forget=0.6726 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.2360 mle=1.5607 pcon=5.0078 forget=0.6675 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:12,  2.94it/s]  3%|▎         | 11/391 [00:00<00:12, 31.16it/s]  5%|▌         | 21/391 [00:00<00:07, 50.47it/s]  8%|▊         | 31/391 [00:00<00:05, 63.46it/s] 10%|█         | 41/391 [00:00<00:04, 72.48it/s] 13%|█▎        | 51/391 [00:00<00:04, 78.75it/s] 16%|█▌        | 61/391 [00:00<00:03, 83.15it/s] 18%|█▊        | 71/391 [00:01<00:03, 86.15it/s] 21%|██        | 81/391 [00:01<00:03, 88.26it/s] 23%|██▎       | 91/391 [00:01<00:03, 89.73it/s] 26%|██▌       | 101/391 [00:01<00:03, 90.63it/s] 28%|██▊       | 111/391 [00:01<00:03, 91.35it/s] 31%|███       | 121/391 [00:01<00:02, 92.40it/s] 34%|███▎      | 131/391 [00:01<00:02, 93.45it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.00it/s] 39%|███▊      | 151/391 [00:01<00:02, 94.63it/s] 41%|████      | 161/391 [00:02<00:02, 95.18it/s] 44%|████▎     | 171/391 [00:02<00:02, 95.63it/s] 46%|████▋     | 181/391 [00:02<00:02, 95.59it/s] 49%|████▉     | 191/391 [00:02<00:02, 95.58it/s] 51%|█████▏    | 201/391 [00:02<00:01, 95.50it/s] 54%|█████▍    | 211/391 [00:02<00:01, 95.70it/s] 57%|█████▋    | 221/391 [00:02<00:01, 95.43it/s] 59%|█████▉    | 231/391 [00:02<00:01, 95.08it/s] 62%|██████▏   | 241/391 [00:02<00:01, 95.00it/s] 64%|██████▍   | 251/391 [00:02<00:01, 94.90it/s] 67%|██████▋   | 261/391 [00:03<00:01, 94.98it/s] 69%|██████▉   | 271/391 [00:03<00:01, 94.72it/s] 72%|███████▏  | 281/391 [00:03<00:01, 94.65it/s] 74%|███████▍  | 291/391 [00:03<00:01, 94.89it/s] 77%|███████▋  | 301/391 [00:03<00:00, 94.88it/s] 80%|███████▉  | 311/391 [00:03<00:00, 94.91it/s] 82%|████████▏ | 321/391 [00:03<00:00, 95.19it/s] 85%|████████▍ | 331/391 [00:03<00:00, 95.09it/s] 87%|████████▋ | 341/391 [00:03<00:00, 95.34it/s] 90%|████████▉ | 351/391 [00:04<00:00, 95.37it/s] 92%|█████████▏| 361/391 [00:04<00:00, 94.96it/s] 95%|█████████▍| 371/391 [00:04<00:00, 94.87it/s] 97%|█████████▋| 381/391 [00:04<00:00, 89.07it/s]100%|██████████| 391/391 [00:04<00:00, 88.16it/s]100%|██████████| 391/391 [00:04<00:00, 87.01it/s]
50000 images processed, 4.5795793533325195 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:26,  2.91it/s] 14%|█▍        | 11/79 [00:00<00:02, 30.82it/s] 27%|██▋       | 21/79 [00:00<00:01, 50.01it/s] 39%|███▉      | 31/79 [00:00<00:00, 63.13it/s] 52%|█████▏    | 41/79 [00:00<00:00, 72.18it/s] 65%|██████▍   | 51/79 [00:00<00:00, 78.61it/s] 77%|███████▋  | 61/79 [00:00<00:00, 83.47it/s] 90%|████████▉ | 71/79 [00:01<00:00, 87.08it/s]100%|██████████| 79/79 [00:01<00:00, 66.82it/s]
10000 images processed, 1.2110011577606201 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:17,  2.61it/s]  5%|▌         | 11/204 [00:00<00:06, 28.67it/s] 10%|█         | 21/204 [00:00<00:03, 47.66it/s] 15%|█▌        | 31/204 [00:00<00:02, 61.35it/s] 20%|██        | 41/204 [00:00<00:02, 71.14it/s] 25%|██▌       | 51/204 [00:00<00:01, 78.09it/s] 30%|██▉       | 61/204 [00:01<00:01, 82.99it/s] 35%|███▍      | 71/204 [00:01<00:01, 86.26it/s] 40%|███▉      | 81/204 [00:01<00:01, 88.40it/s] 45%|████▍     | 91/204 [00:01<00:01, 89.93it/s] 50%|████▉     | 101/204 [00:01<00:01, 90.90it/s] 54%|█████▍    | 111/204 [00:01<00:01, 91.30it/s] 59%|█████▉    | 121/204 [00:01<00:00, 91.95it/s] 64%|██████▍   | 131/204 [00:01<00:00, 92.66it/s] 69%|██████▉   | 141/204 [00:01<00:00, 93.07it/s] 74%|███████▍  | 151/204 [00:01<00:00, 93.37it/s] 79%|███████▉  | 161/204 [00:02<00:00, 93.60it/s] 84%|████████▍ | 171/204 [00:02<00:00, 93.76it/s] 89%|████████▊ | 181/204 [00:02<00:00, 93.61it/s] 94%|█████████▎| 191/204 [00:02<00:00, 93.86it/s] 99%|█████████▊| 201/204 [00:02<00:00, 94.47it/s]100%|██████████| 204/204 [00:02<00:00, 80.16it/s]
26032 images processed, 2.5881741046905518 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:43,  1.78it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.25it/s] 22%|██▏       | 17/79 [00:00<00:02, 24.17it/s] 32%|███▏      | 25/79 [00:01<00:01, 30.45it/s] 42%|████▏     | 33/79 [00:01<00:01, 34.78it/s] 52%|█████▏    | 41/79 [00:01<00:00, 38.01it/s] 62%|██████▏   | 49/79 [00:01<00:00, 39.98it/s] 72%|███████▏  | 57/79 [00:01<00:00, 41.25it/s] 82%|████████▏ | 65/79 [00:01<00:00, 42.31it/s] 92%|█████████▏| 73/79 [00:02<00:00, 42.82it/s]100%|██████████| 79/79 [00:02<00:00, 35.39it/s]
10000 images processed, 2.266909599304199 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:27,  2.84it/s] 14%|█▍        | 11/79 [00:00<00:02, 30.33it/s] 27%|██▋       | 21/79 [00:00<00:01, 49.43it/s] 39%|███▉      | 31/79 [00:00<00:00, 62.84it/s] 52%|█████▏    | 41/79 [00:00<00:00, 72.10it/s] 65%|██████▍   | 51/79 [00:00<00:00, 78.63it/s] 77%|███████▋  | 61/79 [00:00<00:00, 83.30it/s] 90%|████████▉ | 71/79 [00:01<00:00, 86.89it/s]100%|██████████| 79/79 [00:01<00:00, 67.36it/s]
10000 images processed, 1.1950562000274658 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:25,  2.71it/s] 16%|█▌        | 11/70 [00:00<00:02, 29.37it/s] 29%|██▊       | 20/70 [00:00<00:01, 46.36it/s] 43%|████▎     | 30/70 [00:00<00:00, 60.54it/s] 57%|█████▋    | 40/70 [00:00<00:00, 69.93it/s] 71%|███████▏  | 50/70 [00:00<00:00, 77.18it/s] 86%|████████▌ | 60/70 [00:01<00:00, 81.99it/s]100%|██████████| 70/70 [00:01<00:00, 84.51it/s]100%|██████████| 70/70 [00:01<00:00, 62.69it/s]
8925 images processed, 1.1483399868011475 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:39,  1.11it/s]  4%|▍         | 2/45 [00:01<00:19,  2.23it/s] 20%|██        | 9/45 [00:01<00:03,  9.78it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.16it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.12it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.69it/s] 49%|████▉     | 22/45 [00:02<00:01, 13.62it/s] 53%|█████▎    | 24/45 [00:02<00:01, 14.47it/s] 58%|█████▊    | 26/45 [00:02<00:01, 10.78it/s] 67%|██████▋   | 30/45 [00:02<00:01, 13.73it/s] 71%|███████   | 32/45 [00:03<00:00, 14.41it/s] 76%|███████▌  | 34/45 [00:03<00:01,  9.37it/s] 87%|████████▋ | 39/45 [00:03<00:00, 14.85it/s] 93%|█████████▎| 42/45 [00:04<00:00,  8.74it/s]100%|██████████| 45/45 [00:04<00:00, 10.46it/s]
5640 images processed, 4.322267055511475 seconds used

19.07654118537903
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.95  81.18
LSUN          17.28  96.12
iSUN          72.25  81.65
dtd           37.82  91.36
forget        79.40  86.86
AVG           46.20  89.42
Retain-Acc: 0.7412
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.40 AUROC: 86.86 AUIN: 98.36
9.484450101852417
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.1 lr=0.001 epochs=20
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.1, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:19<06:07, 19.34s/it] 10%|█         | 2/20 [00:34<05:03, 16.85s/it] 15%|█▌        | 3/20 [00:48<04:26, 15.67s/it] 20%|██        | 4/20 [01:02<04:01, 15.09s/it] 25%|██▌       | 5/20 [01:17<03:42, 14.83s/it] 30%|███       | 6/20 [01:31<03:25, 14.65s/it] 35%|███▌      | 7/20 [01:45<03:08, 14.53s/it] 40%|████      | 8/20 [02:01<02:56, 14.74s/it][loss] ep 0 it 0 total=7.5537 mle=1.5709 pcon=5.2950 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=7.5312 mle=1.5426 pcon=5.2879 forget=0.7007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=7.6682 mle=1.7003 pcon=5.2809 forget=0.6870 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=7.8584 mle=1.8997 pcon=5.2738 forget=0.6848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=7.6714 mle=1.7131 pcon=5.2670 forget=0.6913 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=7.4515 mle=1.5019 pcon=5.2603 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=7.5047 mle=1.5596 pcon=5.2540 forget=0.6910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=7.6234 mle=1.6810 pcon=5.2476 forget=0.6948 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=7.6174 mle=1.6715 pcon=5.2409 forget=0.7050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=7.5551 mle=1.6370 pcon=5.2346 forget=0.6835 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=7.4397 mle=1.5165 pcon=5.2285 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=7.6973 mle=1.7801 pcon=5.2224 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=7.6737 mle=1.7644 pcon=5.2167 forget=0.6926 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=7.5009 mle=1.6055 pcon=5.2112 forget=0.6841 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=7.6186 mle=1.7239 pcon=5.2056 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=7.6876 mle=1.7943 pcon=5.2003 forget=0.6931 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=7.4363 mle=1.5528 pcon=5.1950 forget=0.6884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=7.7570 mle=1.8882 pcon=5.1899 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=7.6143 mle=1.7417 pcon=5.1847 forget=0.6879 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=7.3888 mle=1.5267 pcon=5.1796 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=7.4532 mle=1.5857 pcon=5.1745 forget=0.6929 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=7.7391 mle=1.8839 pcon=5.1699 forget=0.6853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=7.4362 mle=1.5922 pcon=5.1652 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=7.6542 mle=1.8113 pcon=5.1605 forget=0.6824 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=7.5468 mle=1.7055 pcon=5.1560 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=7.5123 mle=1.6709 pcon=5.1519 forget=0.6896 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=7.5973 mle=1.7669 pcon=5.1475 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=7.7427 mle=1.9180 pcon=5.1435 forget=0.6811 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=7.3892 mle=1.5698 pcon=5.1395 forget=0.6799 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=7.6492 mle=1.8358 pcon=5.1353 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=7.5216 mle=1.7028 pcon=5.1311 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=7.5118 mle=1.6951 pcon=5.1275 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=7.4485 mle=1.6538 pcon=5.1236 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=7.5272 mle=1.7295 pcon=5.1196 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=7.4493 mle=1.6502 pcon=5.1160 forget=0.6831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.2486 mle=1.4533 pcon=5.1125 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=7.5533 mle=1.7665 pcon=5.1086 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.2732 mle=1.4914 pcon=5.1051 forget=0.6767 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=7.6553 mle=1.8712 pcon=5.1016 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=7.6441 mle=1.8672 pcon=5.0983 forget=0.6787 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=7.5674 mle=1.7998 pcon=5.0947 forget=0.6730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=7.3556 mle=1.5841 pcon=5.0915 forget=0.6801 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=7.3612 mle=1.5939 pcon=5.0880 forget=0.6794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=7.4485 mle=1.6906 pcon=5.0851 forget=0.6728 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=7.5279 mle=1.7764 pcon=5.0821 forget=0.6694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=7.3509 mle=1.5964 pcon=5.0790 forget=0.6754 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=7.4968 mle=1.7462 pcon=5.0761 forget=0.6744 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.1594 mle=1.4001 pcon=5.0732 forget=0.6860 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.2695 mle=1.5348 pcon=5.0705 forget=0.6643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=7.3399 mle=1.5914 pcon=5.0678 forget=0.6807 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=7.4031 mle=1.6652 pcon=5.0650 forget=0.6729 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=7.3630 mle=1.6295 pcon=5.0627 forget=0.6708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=7.5593 mle=1.8215 pcon=5.0599 forget=0.6779 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.2747 mle=1.5354 pcon=5.0574 forget=0.6818 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=7.4024 mle=1.6738 pcon=5.0549 forget=0.6737 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=7.3531 mle=1.6312 pcon=5.0523 forget=0.6697 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=7.4609 mle=1.7435 pcon=5.0499 forget=0.6675 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=7.3440 mle=1.6259 pcon=5.0473 forget=0.6708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.2905 mle=1.5812 pcon=5.0449 forget=0.6643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=7.3704 mle=1.6515 pcon=5.0429 forget=0.6759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=7.5163 mle=1.7949 pcon=5.0407 forget=0.6807 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=7.4579 mle=1.7401 pcon=5.0383 forget=0.6796 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=7.3484 mle=1.6393 pcon=5.0362 forget=0.6729 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 8 it 30 total=7.2501 mle=1.5360 pcon=5.0342 forget=0.6799 nr=64 nf=64 protos=540 fproto_sim=NA
 45%|████▌     | 9/20 [02:15<02:42, 14.75s/it] 50%|█████     | 10/20 [02:30<02:26, 14.62s/it] 55%|█████▌    | 11/20 [02:44<02:10, 14.50s/it] 60%|██████    | 12/20 [02:58<01:55, 14.46s/it] 65%|██████▌   | 13/20 [03:12<01:40, 14.38s/it] 70%|███████   | 14/20 [03:27<01:26, 14.34s/it] 75%|███████▌  | 15/20 [03:41<01:11, 14.24s/it] 80%|████████  | 16/20 [03:55<00:56, 14.21s/it][loss] ep 8 it 80 total=7.2956 mle=1.5892 pcon=5.0322 forget=0.6741 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.2134 mle=1.5051 pcon=5.0300 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=7.4009 mle=1.7025 pcon=5.0281 forget=0.6704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.2262 mle=1.5301 pcon=5.0265 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=7.3665 mle=1.6771 pcon=5.0245 forget=0.6650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=7.4608 mle=1.7554 pcon=5.0223 forget=0.6831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.1723 mle=1.4858 pcon=5.0204 forget=0.6662 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.3095 mle=1.6341 pcon=5.0184 forget=0.6571 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=7.3526 mle=1.6687 pcon=5.0162 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=7.5278 mle=1.8380 pcon=5.0146 forget=0.6752 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.2518 mle=1.5711 pcon=5.0130 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.2522 mle=1.5713 pcon=5.0113 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.2467 mle=1.5648 pcon=5.0094 forget=0.6726 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.2359 mle=1.5607 pcon=5.0078 forget=0.6675 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=7.2841 mle=1.6061 pcon=5.0059 forget=0.6721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=7.2695 mle=1.5884 pcon=5.0040 forget=0.6771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=7.4221 mle=1.7525 pcon=5.0024 forget=0.6672 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=7.5070 mle=1.8427 pcon=5.0009 forget=0.6635 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=7.5165 mle=1.8528 pcon=4.9992 forget=0.6644 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=7.2468 mle=1.5812 pcon=4.9977 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=7.2208 mle=1.5641 pcon=4.9964 forget=0.6604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=7.3356 mle=1.6636 pcon=4.9948 forget=0.6773 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=7.2319 mle=1.5650 pcon=4.9932 forget=0.6737 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=7.2737 mle=1.6188 pcon=4.9916 forget=0.6634 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=7.2192 mle=1.5688 pcon=4.9901 forget=0.6603 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=7.3124 mle=1.6586 pcon=4.9882 forget=0.6656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=7.4342 mle=1.7788 pcon=4.9867 forget=0.6687 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=7.2562 mle=1.6023 pcon=4.9856 forget=0.6682 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=7.6200 mle=1.9696 pcon=4.9839 forget=0.6665 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=7.3053 mle=1.6661 pcon=4.9823 forget=0.6569 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 12 it 20 total=7.2693 mle=1.6239 pcon=4.9806 forget=0.6648 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=7.3204 mle=1.6758 pcon=4.9790 forget=0.6656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=7.1945 mle=1.5448 pcon=4.9774 forget=0.6724 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=7.3731 mle=1.7373 pcon=4.9758 forget=0.6600 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=7.2202 mle=1.5843 pcon=4.9741 forget=0.6618 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=7.1641 mle=1.5212 pcon=4.9729 forget=0.6700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=7.1334 mle=1.4989 pcon=4.9713 forget=0.6632 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=7.4975 mle=1.8586 pcon=4.9697 forget=0.6692 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=7.0574 mle=1.4246 pcon=4.9682 forget=0.6646 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=7.1246 mle=1.4758 pcon=4.9665 forget=0.6823 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=7.1775 mle=1.5474 pcon=4.9647 forget=0.6654 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=7.1621 mle=1.5387 pcon=4.9633 forget=0.6602 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=7.2850 mle=1.6578 pcon=4.9618 forget=0.6654 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=7.1818 mle=1.5635 pcon=4.9601 forget=0.6581 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=7.2014 mle=1.5771 pcon=4.9584 forget=0.6659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=7.1842 mle=1.5670 pcon=4.9568 forget=0.6604 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=7.3781 mle=1.7663 pcon=4.9549 forget=0.6569 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=7.3938 mle=1.7738 pcon=4.9535 forget=0.6664 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=7.2032 mle=1.5889 pcon=4.9523 forget=0.6620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=7.4738 mle=1.8619 pcon=4.9505 forget=0.6614 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=7.2212 mle=1.6119 pcon=4.9490 forget=0.6603 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=7.2379 mle=1.6355 pcon=4.9473 forget=0.6551 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=7.2811 mle=1.6770 pcon=4.9456 forget=0.6585 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=7.3010 mle=1.6865 pcon=4.9440 forget=0.6704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=7.0642 mle=1.4633 pcon=4.9422 forget=0.6587 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=7.1574 mle=1.5542 pcon=4.9405 forget=0.6626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=7.3104 mle=1.7005 pcon=4.9388 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=7.2077 mle=1.6116 pcon=4.9372 forget=0.6589 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=7.4978 mle=1.8957 pcon=4.9354 forget=0.6667 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=7.1821 mle=1.5824 pcon=4.9339 forget=0.6659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=7.1008 mle=1.5022 pcon=4.9322 forget=0.6664 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 16 it 10 total=7.1740 mle=1.5764 pcon=4.9305 forget=0.6672 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=7.0848 mle=1.4951 pcon=4.9288 forget=0.6609 nr=64 nf=64 protos=540 fproto_sim=NA
 85%|████████▌ | 17/20 [04:09<00:42, 14.29s/it] 90%|█████████ | 18/20 [04:24<00:28, 14.29s/it] 95%|█████████▌| 19/20 [04:38<00:14, 14.26s/it]100%|██████████| 20/20 [04:52<00:00, 14.25s/it]100%|██████████| 20/20 [04:52<00:00, 14.63s/it]
[loss] ep 16 it 110 total=7.0930 mle=1.5039 pcon=4.9275 forget=0.6617 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=7.2497 mle=1.6634 pcon=4.9256 forget=0.6607 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=7.1758 mle=1.5955 pcon=4.9237 forget=0.6566 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=7.4836 mle=1.9047 pcon=4.9219 forget=0.6570 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=7.3501 mle=1.7650 pcon=4.9202 forget=0.6649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=7.1770 mle=1.5947 pcon=4.9185 forget=0.6638 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 17 it 20 total=7.3362 mle=1.7523 pcon=4.9166 forget=0.6672 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=7.3778 mle=1.8042 pcon=4.9153 forget=0.6583 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=7.1328 mle=1.5539 pcon=4.9135 forget=0.6655 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=7.1493 mle=1.5746 pcon=4.9115 forget=0.6632 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=7.2244 mle=1.6434 pcon=4.9098 forget=0.6712 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=7.0193 mle=1.4454 pcon=4.9086 forget=0.6653 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=7.3413 mle=1.7717 pcon=4.9071 forget=0.6625 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=7.3083 mle=1.7414 pcon=4.9052 forget=0.6617 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 18 it 30 total=7.1119 mle=1.5514 pcon=4.9042 forget=0.6563 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=7.2284 mle=1.6549 pcon=4.9027 forget=0.6708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=7.1881 mle=1.6176 pcon=4.9012 forget=0.6692 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=7.1195 mle=1.5530 pcon=4.8999 forget=0.6667 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=7.1695 mle=1.6075 pcon=4.8985 forget=0.6635 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=7.1985 mle=1.6353 pcon=4.8972 forget=0.6660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=7.3286 mle=1.7702 pcon=4.8959 forget=0.6626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=7.2447 mle=1.6791 pcon=4.8949 forget=0.6706 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 19 it 40 total=7.0107 mle=1.4451 pcon=4.8939 forget=0.6717 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=7.2623 mle=1.6928 pcon=4.8930 forget=0.6765 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=7.1178 mle=1.5580 pcon=4.8917 forget=0.6680 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=7.1848 mle=1.6314 pcon=4.8905 forget=0.6629 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=7.4203 mle=1.8653 pcon=4.8894 forget=0.6656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=7.1166 mle=1.5547 pcon=4.8885 forget=0.6733 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=7.1419 mle=1.5732 pcon=4.8877 forget=0.6811 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:28,  2.63it/s]  3%|▎         | 11/391 [00:00<00:13, 28.73it/s]  5%|▌         | 21/391 [00:00<00:07, 47.69it/s]  8%|▊         | 31/391 [00:00<00:05, 61.35it/s] 10%|█         | 41/391 [00:00<00:04, 71.01it/s] 13%|█▎        | 51/391 [00:00<00:04, 77.89it/s] 16%|█▌        | 61/391 [00:01<00:03, 82.97it/s] 18%|█▊        | 71/391 [00:01<00:03, 86.54it/s] 21%|██        | 81/391 [00:01<00:03, 89.18it/s] 23%|██▎       | 91/391 [00:01<00:03, 90.95it/s] 26%|██▌       | 101/391 [00:01<00:03, 92.03it/s] 28%|██▊       | 111/391 [00:01<00:03, 92.98it/s] 31%|███       | 121/391 [00:01<00:02, 93.50it/s] 34%|███▎      | 131/391 [00:01<00:02, 93.96it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.18it/s] 39%|███▊      | 151/391 [00:01<00:02, 94.36it/s] 41%|████      | 161/391 [00:02<00:02, 94.19it/s] 44%|████▎     | 171/391 [00:02<00:02, 94.44it/s] 46%|████▋     | 181/391 [00:02<00:02, 94.58it/s] 49%|████▉     | 191/391 [00:02<00:02, 94.74it/s] 51%|█████▏    | 201/391 [00:02<00:02, 94.86it/s] 54%|█████▍    | 211/391 [00:02<00:01, 94.92it/s] 57%|█████▋    | 221/391 [00:02<00:01, 94.75it/s] 59%|█████▉    | 231/391 [00:02<00:01, 94.95it/s] 62%|██████▏   | 241/391 [00:02<00:01, 94.98it/s] 64%|██████▍   | 251/391 [00:03<00:01, 94.72it/s] 67%|██████▋   | 261/391 [00:03<00:01, 94.87it/s] 69%|██████▉   | 271/391 [00:03<00:01, 94.99it/s] 72%|███████▏  | 281/391 [00:03<00:01, 93.46it/s] 74%|███████▍  | 291/391 [00:03<00:01, 93.84it/s] 77%|███████▋  | 301/391 [00:03<00:00, 94.07it/s] 80%|███████▉  | 311/391 [00:03<00:00, 94.38it/s] 82%|████████▏ | 321/391 [00:03<00:00, 94.45it/s] 85%|████████▍ | 331/391 [00:03<00:00, 94.50it/s] 87%|████████▋ | 341/391 [00:03<00:00, 94.54it/s] 90%|████████▉ | 351/391 [00:04<00:00, 94.72it/s] 92%|█████████▏| 361/391 [00:04<00:00, 94.65it/s] 95%|█████████▍| 371/391 [00:04<00:00, 94.60it/s] 97%|█████████▋| 381/391 [00:04<00:00, 94.55it/s]100%|██████████| 391/391 [00:04<00:00, 92.10it/s]100%|██████████| 391/391 [00:04<00:00, 86.67it/s]
50000 images processed, 4.61579442024231 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:29,  2.66it/s] 14%|█▍        | 11/79 [00:00<00:02, 28.89it/s] 27%|██▋       | 21/79 [00:00<00:01, 47.68it/s] 39%|███▉      | 31/79 [00:00<00:00, 61.10it/s] 52%|█████▏    | 41/79 [00:00<00:00, 70.28it/s] 65%|██████▍   | 51/79 [00:00<00:00, 76.58it/s] 77%|███████▋  | 61/79 [00:01<00:00, 81.42it/s] 90%|████████▉ | 71/79 [00:01<00:00, 85.43it/s]100%|██████████| 79/79 [00:01<00:00, 64.65it/s]
10000 images processed, 1.2451276779174805 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:29,  2.28it/s]  5%|▌         | 11/204 [00:00<00:07, 25.80it/s] 10%|█         | 21/204 [00:00<00:04, 43.75it/s] 15%|█▌        | 31/204 [00:00<00:03, 57.30it/s] 20%|██        | 41/204 [00:00<00:02, 67.32it/s] 25%|██▌       | 51/204 [00:00<00:02, 74.43it/s] 30%|██▉       | 61/204 [00:01<00:01, 79.80it/s] 35%|███▍      | 71/204 [00:01<00:01, 83.56it/s] 40%|███▉      | 81/204 [00:01<00:01, 86.16it/s] 45%|████▍     | 91/204 [00:01<00:01, 87.85it/s] 50%|████▉     | 101/204 [00:01<00:01, 89.16it/s] 54%|█████▍    | 111/204 [00:01<00:01, 89.91it/s] 59%|█████▉    | 121/204 [00:01<00:00, 90.39it/s] 64%|██████▍   | 131/204 [00:01<00:00, 90.95it/s] 69%|██████▉   | 141/204 [00:01<00:00, 91.33it/s] 74%|███████▍  | 151/204 [00:02<00:00, 91.69it/s] 79%|███████▉  | 161/204 [00:02<00:00, 91.89it/s] 84%|████████▍ | 171/204 [00:02<00:00, 92.12it/s] 89%|████████▊ | 181/204 [00:02<00:00, 91.93it/s] 94%|█████████▎| 191/204 [00:02<00:00, 92.51it/s] 99%|█████████▊| 201/204 [00:02<00:00, 93.42it/s]100%|██████████| 204/204 [00:02<00:00, 77.27it/s]
26032 images processed, 2.6988468170166016 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:43,  1.78it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.58it/s] 22%|██▏       | 17/79 [00:00<00:02, 25.44it/s] 32%|███▏      | 25/79 [00:01<00:01, 33.16it/s] 42%|████▏     | 33/79 [00:01<00:01, 38.60it/s] 52%|█████▏    | 41/79 [00:01<00:00, 42.16it/s] 62%|██████▏   | 49/79 [00:01<00:00, 44.29it/s] 72%|███████▏  | 57/79 [00:01<00:00, 45.34it/s] 82%|████████▏ | 65/79 [00:01<00:00, 46.46it/s] 92%|█████████▏| 73/79 [00:02<00:00, 47.33it/s]100%|██████████| 79/79 [00:02<00:00, 38.25it/s]
10000 images processed, 2.101490020751953 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:32,  2.42it/s] 14%|█▍        | 11/79 [00:00<00:02, 26.95it/s] 27%|██▋       | 21/79 [00:00<00:01, 45.58it/s] 39%|███▉      | 31/79 [00:00<00:00, 59.40it/s] 52%|█████▏    | 41/79 [00:00<00:00, 69.56it/s] 65%|██████▍   | 51/79 [00:00<00:00, 76.86it/s] 77%|███████▋  | 61/79 [00:01<00:00, 82.14it/s] 90%|████████▉ | 71/79 [00:01<00:00, 86.17it/s]100%|██████████| 79/79 [00:01<00:00, 64.19it/s]
10000 images processed, 1.2523436546325684 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:30,  2.27it/s] 16%|█▌        | 11/70 [00:00<00:02, 25.72it/s] 30%|███       | 21/70 [00:00<00:01, 44.05it/s] 44%|████▍     | 31/70 [00:00<00:00, 57.92it/s] 59%|█████▊    | 41/70 [00:00<00:00, 68.02it/s] 73%|███████▎  | 51/70 [00:00<00:00, 75.57it/s] 87%|████████▋ | 61/70 [00:01<00:00, 81.27it/s]100%|██████████| 70/70 [00:01<00:00, 59.40it/s]
8925 images processed, 1.211866855621338 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:43,  1.02it/s]  4%|▍         | 2/45 [00:01<00:20,  2.10it/s] 20%|██        | 9/45 [00:01<00:03,  9.06it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.92it/s] 36%|███▌      | 16/45 [00:01<00:01, 14.62it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.17it/s] 49%|████▉     | 22/45 [00:02<00:01, 12.97it/s] 56%|█████▌    | 25/45 [00:02<00:01, 13.01it/s] 60%|██████    | 27/45 [00:02<00:01, 12.48it/s] 67%|██████▋   | 30/45 [00:02<00:01, 13.18it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.02it/s] 78%|███████▊  | 35/45 [00:03<00:00, 11.15it/s] 91%|█████████ | 41/45 [00:03<00:00, 13.60it/s] 96%|█████████▌| 43/45 [00:04<00:00, 10.00it/s]100%|██████████| 45/45 [00:04<00:00, 10.48it/s]
5640 images processed, 4.315919399261475 seconds used

19.184685707092285
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           3.11  99.28
places365     70.00  80.56
LSUN          20.99  95.67
iSUN          74.48  80.57
dtd           40.90  90.71
forget        75.30  87.19
AVG           47.46  88.99
Retain-Acc: 0.7409
Forget-as-OOD (retain known vs forget novel):
  FPR: 75.30 AUROC: 87.19 AUIN: 98.39
9.972179889678955
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.1 lr=0.001 epochs=50
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.1, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:17<14:28, 17.73s/it]  4%|▍         | 2/50 [00:32<12:35, 15.74s/it]  6%|▌         | 3/50 [00:46<11:46, 15.02s/it]  8%|▊         | 4/50 [01:00<11:19, 14.77s/it] 10%|█         | 5/50 [01:14<10:54, 14.54s/it] 12%|█▏        | 6/50 [01:28<10:33, 14.40s/it] 14%|█▍        | 7/50 [01:43<10:16, 14.33s/it] 16%|█▌        | 8/50 [01:57<10:01, 14.33s/it][loss] ep 0 it 0 total=7.5538 mle=1.5710 pcon=5.2950 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=7.5309 mle=1.5423 pcon=5.2879 forget=0.7007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=7.6683 mle=1.7004 pcon=5.2809 forget=0.6870 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=7.8585 mle=1.8998 pcon=5.2738 forget=0.6848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=7.6712 mle=1.7129 pcon=5.2670 forget=0.6913 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=7.4516 mle=1.5020 pcon=5.2603 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=7.5046 mle=1.5596 pcon=5.2540 forget=0.6910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=7.6234 mle=1.6810 pcon=5.2476 forget=0.6948 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=7.6174 mle=1.6716 pcon=5.2409 forget=0.7050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=7.5552 mle=1.6371 pcon=5.2346 forget=0.6835 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=7.4397 mle=1.5165 pcon=5.2284 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=7.6973 mle=1.7802 pcon=5.2224 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=7.6736 mle=1.7643 pcon=5.2167 forget=0.6926 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=7.5008 mle=1.6055 pcon=5.2112 forget=0.6841 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=7.6186 mle=1.7239 pcon=5.2056 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=7.6877 mle=1.7943 pcon=5.2003 forget=0.6931 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=7.4362 mle=1.5528 pcon=5.1950 forget=0.6884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=7.7570 mle=1.8883 pcon=5.1899 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=7.6142 mle=1.7416 pcon=5.1847 forget=0.6879 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=7.3887 mle=1.5266 pcon=5.1796 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=7.4533 mle=1.5858 pcon=5.1745 forget=0.6930 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=7.7392 mle=1.8840 pcon=5.1699 forget=0.6853 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=7.4362 mle=1.5922 pcon=5.1652 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=7.6540 mle=1.8111 pcon=5.1605 forget=0.6824 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=7.5470 mle=1.7057 pcon=5.1560 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=7.5122 mle=1.6708 pcon=5.1519 forget=0.6896 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=7.5973 mle=1.7670 pcon=5.1475 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=7.7428 mle=1.9182 pcon=5.1435 forget=0.6811 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=7.3893 mle=1.5699 pcon=5.1395 forget=0.6799 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=7.6492 mle=1.8358 pcon=5.1353 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=7.5217 mle=1.7029 pcon=5.1311 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=7.5118 mle=1.6952 pcon=5.1275 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=7.4485 mle=1.6538 pcon=5.1237 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=7.5271 mle=1.7294 pcon=5.1196 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=7.4493 mle=1.6503 pcon=5.1160 forget=0.6831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.2486 mle=1.4532 pcon=5.1125 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=7.5534 mle=1.7665 pcon=5.1086 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.2731 mle=1.4914 pcon=5.1051 forget=0.6767 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=7.6554 mle=1.8713 pcon=5.1016 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=7.6440 mle=1.8671 pcon=5.0983 forget=0.6787 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=7.5674 mle=1.7998 pcon=5.0947 forget=0.6730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=7.3558 mle=1.5842 pcon=5.0915 forget=0.6801 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=7.3610 mle=1.5936 pcon=5.0880 forget=0.6794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=7.4485 mle=1.6906 pcon=5.0851 forget=0.6728 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=7.5281 mle=1.7766 pcon=5.0821 forget=0.6694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=7.3508 mle=1.5963 pcon=5.0790 forget=0.6755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=7.4969 mle=1.7463 pcon=5.0761 forget=0.6744 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.1594 mle=1.4002 pcon=5.0732 forget=0.6860 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.2695 mle=1.5348 pcon=5.0705 forget=0.6643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=7.3399 mle=1.5914 pcon=5.0678 forget=0.6807 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=7.4030 mle=1.6652 pcon=5.0650 forget=0.6729 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=7.3628 mle=1.6292 pcon=5.0627 forget=0.6708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=7.5593 mle=1.8215 pcon=5.0599 forget=0.6779 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.2746 mle=1.5353 pcon=5.0575 forget=0.6818 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=7.4023 mle=1.6737 pcon=5.0549 forget=0.6737 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=7.3532 mle=1.6313 pcon=5.0523 forget=0.6697 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=7.4611 mle=1.7437 pcon=5.0499 forget=0.6675 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=7.3439 mle=1.6257 pcon=5.0473 forget=0.6708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.2903 mle=1.5810 pcon=5.0449 forget=0.6643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=7.3704 mle=1.6515 pcon=5.0429 forget=0.6759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=7.5163 mle=1.7949 pcon=5.0407 forget=0.6807 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=7.4580 mle=1.7402 pcon=5.0383 forget=0.6796 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=7.3529 mle=1.6438 pcon=5.0362 forget=0.6729 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 8 it 30 total=7.2501 mle=1.5360 pcon=5.0342 forget=0.6799 nr=64 nf=64 protos=540 fproto_sim=NA
 18%|█▊        | 9/50 [02:11<09:47, 14.33s/it] 20%|██        | 10/50 [02:25<09:31, 14.29s/it] 22%|██▏       | 11/50 [02:40<09:22, 14.42s/it] 24%|██▍       | 12/50 [02:55<09:08, 14.42s/it] 26%|██▌       | 13/50 [03:09<08:50, 14.33s/it] 28%|██▊       | 14/50 [03:23<08:36, 14.34s/it] 30%|███       | 15/50 [03:37<08:18, 14.24s/it] 32%|███▏      | 16/50 [03:51<08:02, 14.20s/it][loss] ep 8 it 80 total=7.2955 mle=1.5891 pcon=5.0322 forget=0.6741 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.2135 mle=1.5052 pcon=5.0300 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=7.4009 mle=1.7025 pcon=5.0281 forget=0.6703 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.2263 mle=1.5302 pcon=5.0265 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=7.3662 mle=1.6767 pcon=5.0245 forget=0.6650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=7.4607 mle=1.7553 pcon=5.0223 forget=0.6831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.1723 mle=1.4857 pcon=5.0204 forget=0.6662 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.3094 mle=1.6340 pcon=5.0184 forget=0.6571 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=7.3529 mle=1.6689 pcon=5.0162 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=7.5278 mle=1.8379 pcon=5.0146 forget=0.6752 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.2520 mle=1.5712 pcon=5.0130 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.2523 mle=1.5714 pcon=5.0113 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.2466 mle=1.5647 pcon=5.0094 forget=0.6725 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.2359 mle=1.5606 pcon=5.0078 forget=0.6675 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=7.2842 mle=1.6062 pcon=5.0059 forget=0.6721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=7.2695 mle=1.5883 pcon=5.0041 forget=0.6771 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=7.4221 mle=1.7525 pcon=5.0024 forget=0.6672 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=7.5069 mle=1.8426 pcon=5.0009 forget=0.6635 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=7.5164 mle=1.8528 pcon=4.9992 forget=0.6644 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=7.2468 mle=1.5812 pcon=4.9978 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=7.2207 mle=1.5639 pcon=4.9964 forget=0.6604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=7.3357 mle=1.6636 pcon=4.9948 forget=0.6773 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=7.2318 mle=1.5649 pcon=4.9932 forget=0.6736 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=7.2736 mle=1.6187 pcon=4.9916 forget=0.6634 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=7.2189 mle=1.5685 pcon=4.9901 forget=0.6603 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=7.3125 mle=1.6587 pcon=4.9882 forget=0.6656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=7.4339 mle=1.7786 pcon=4.9867 forget=0.6686 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=7.2560 mle=1.6023 pcon=4.9856 forget=0.6682 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=7.6224 mle=1.9721 pcon=4.9839 forget=0.6664 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=7.3052 mle=1.6661 pcon=4.9823 forget=0.6568 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 12 it 20 total=7.2687 mle=1.6235 pcon=4.9805 forget=0.6647 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=7.3198 mle=1.6755 pcon=4.9789 forget=0.6654 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=7.1940 mle=1.5446 pcon=4.9773 forget=0.6722 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=7.3860 mle=1.7505 pcon=4.9757 forget=0.6598 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=7.2180 mle=1.5825 pcon=4.9740 forget=0.6616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=7.1625 mle=1.5201 pcon=4.9727 forget=0.6697 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=7.1313 mle=1.4976 pcon=4.9710 forget=0.6628 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=7.4957 mle=1.8575 pcon=4.9693 forget=0.6688 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=7.0563 mle=1.4244 pcon=4.9678 forget=0.6641 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=7.1209 mle=1.4729 pcon=4.9661 forget=0.6819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=7.1732 mle=1.5441 pcon=4.9642 forget=0.6650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=7.1594 mle=1.5373 pcon=4.9626 forget=0.6595 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=7.2827 mle=1.6571 pcon=4.9609 forget=0.6647 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=7.1757 mle=1.5590 pcon=4.9591 forget=0.6576 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=7.1973 mle=1.5750 pcon=4.9572 forget=0.6652 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=7.1814 mle=1.5663 pcon=4.9554 forget=0.6597 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=7.3739 mle=1.7642 pcon=4.9532 forget=0.6565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=7.3866 mle=1.7691 pcon=4.9515 forget=0.6660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=7.1936 mle=1.5820 pcon=4.9500 forget=0.6616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=7.4603 mle=1.8519 pcon=4.9478 forget=0.6606 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=7.2265 mle=1.6208 pcon=4.9459 forget=0.6598 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=7.2192 mle=1.6206 pcon=4.9438 forget=0.6548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=7.2817 mle=1.6818 pcon=4.9415 forget=0.6583 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=7.3052 mle=1.6959 pcon=4.9394 forget=0.6699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=7.0568 mle=1.4615 pcon=4.9369 forget=0.6584 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=7.1568 mle=1.5603 pcon=4.9345 forget=0.6620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=7.3006 mle=1.6975 pcon=4.9320 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=7.1933 mle=1.6040 pcon=4.9295 forget=0.6598 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=7.5074 mle=1.9128 pcon=4.9267 forget=0.6679 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=7.1560 mle=1.5659 pcon=4.9242 forget=0.6659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=7.0865 mle=1.4978 pcon=4.9214 forget=0.6673 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 16 it 10 total=7.1659 mle=1.5789 pcon=4.9185 forget=0.6685 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 60 total=7.0711 mle=1.4934 pcon=4.9156 forget=0.6621 nr=64 nf=64 protos=540 fproto_sim=NA
 34%|███▍      | 17/50 [04:05<07:47, 14.17s/it] 36%|███▌      | 18/50 [04:20<07:34, 14.19s/it] 38%|███▊      | 19/50 [04:34<07:18, 14.13s/it] 40%|████      | 20/50 [04:48<07:03, 14.10s/it] 42%|████▏     | 21/50 [05:02<06:48, 14.08s/it] 44%|████▍     | 22/50 [05:16<06:33, 14.06s/it] 46%|████▌     | 23/50 [05:30<06:20, 14.11s/it] 48%|████▊     | 24/50 [05:44<06:09, 14.20s/it][loss] ep 16 it 110 total=7.0866 mle=1.5101 pcon=4.9128 forget=0.6637 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=7.2005 mle=1.6277 pcon=4.9094 forget=0.6633 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=7.1530 mle=1.5877 pcon=4.9060 forget=0.6593 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=7.4223 mle=1.8595 pcon=4.9025 forget=0.6603 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=7.3111 mle=1.7434 pcon=4.8991 forget=0.6686 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=7.1475 mle=1.5845 pcon=4.8957 forget=0.6673 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 17 it 20 total=7.3203 mle=1.7580 pcon=4.8919 forget=0.6704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=7.2982 mle=1.7452 pcon=4.8886 forget=0.6643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=7.0957 mle=1.5395 pcon=4.8849 forget=0.6712 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=7.0845 mle=1.5348 pcon=4.8810 forget=0.6687 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=7.1380 mle=1.5820 pcon=4.8773 forget=0.6787 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=6.9983 mle=1.4531 pcon=4.8740 forget=0.6712 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=7.3003 mle=1.7610 pcon=4.8704 forget=0.6689 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=7.2507 mle=1.7153 pcon=4.8666 forget=0.6688 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 18 it 30 total=7.0250 mle=1.4963 pcon=4.8634 forget=0.6653 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=7.1339 mle=1.5943 pcon=4.8600 forget=0.6796 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=7.1297 mle=1.5943 pcon=4.8565 forget=0.6789 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=7.0584 mle=1.5303 pcon=4.8532 forget=0.6750 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=7.1805 mle=1.6562 pcon=4.8497 forget=0.6746 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=7.1204 mle=1.5972 pcon=4.8464 forget=0.6768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=7.2157 mle=1.7003 pcon=4.8433 forget=0.6722 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=7.1985 mle=1.6738 pcon=4.8403 forget=0.6843 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 19 it 40 total=7.0583 mle=1.5348 pcon=4.8373 forget=0.6862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=7.1057 mle=1.5811 pcon=4.8344 forget=0.6902 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=7.0802 mle=1.5679 pcon=4.8313 forget=0.6810 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=7.1300 mle=1.6238 pcon=4.8283 forget=0.6778 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=7.2992 mle=1.7941 pcon=4.8255 forget=0.6795 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=7.1407 mle=1.6303 pcon=4.8230 forget=0.6874 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=7.0258 mle=1.5133 pcon=4.8203 forget=0.6921 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 20 it 0 total=7.1163 mle=1.6170 pcon=4.8177 forget=0.6816 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=7.1360 mle=1.6422 pcon=4.8150 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=7.2060 mle=1.7007 pcon=4.8126 forget=0.6928 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=7.2622 mle=1.7684 pcon=4.8098 forget=0.6840 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=7.0311 mle=1.5444 pcon=4.8071 forget=0.6795 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=7.3597 mle=1.8742 pcon=4.8045 forget=0.6810 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=7.2239 mle=1.7311 pcon=4.8020 forget=0.6908 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=6.9531 mle=1.4670 pcon=4.7995 forget=0.6866 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 21 it 10 total=6.9355 mle=1.4512 pcon=4.7971 forget=0.6873 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=7.0907 mle=1.6066 pcon=4.7947 forget=0.6894 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=7.0346 mle=1.5462 pcon=4.7925 forget=0.6959 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=7.0360 mle=1.5631 pcon=4.7904 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=7.1905 mle=1.7126 pcon=4.7882 forget=0.6898 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=7.0960 mle=1.6238 pcon=4.7860 forget=0.6862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=7.0014 mle=1.5301 pcon=4.7838 forget=0.6875 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=7.0273 mle=1.5506 pcon=4.7815 forget=0.6952 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 22 it 20 total=7.0548 mle=1.5885 pcon=4.7794 forget=0.6869 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=7.1065 mle=1.6468 pcon=4.7773 forget=0.6824 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=7.0172 mle=1.5458 pcon=4.7751 forget=0.6964 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=7.0479 mle=1.5793 pcon=4.7731 forget=0.6956 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=7.0163 mle=1.5489 pcon=4.7713 forget=0.6961 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=6.9363 mle=1.4780 pcon=4.7693 forget=0.6890 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=7.1307 mle=1.6796 pcon=4.7676 forget=0.6835 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=7.3295 mle=1.8703 pcon=4.7656 forget=0.6935 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 23 it 30 total=6.9493 mle=1.4980 pcon=4.7642 forget=0.6871 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=7.0519 mle=1.6076 pcon=4.7624 forget=0.6819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=7.0016 mle=1.5497 pcon=4.7605 forget=0.6914 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=7.1457 mle=1.7090 pcon=4.7587 forget=0.6780 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=7.0340 mle=1.5915 pcon=4.7571 forget=0.6855 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=7.0050 mle=1.5632 pcon=4.7553 forget=0.6865 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=7.0404 mle=1.6059 pcon=4.7536 forget=0.6810 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=6.9885 mle=1.5501 pcon=4.7519 forget=0.6865 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 24 it 40 total=6.9509 mle=1.5256 pcon=4.7504 forget=0.6749 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=7.0282 mle=1.5962 pcon=4.7488 forget=0.6832 nr=64 nf=64 protos=540 fproto_sim=NA
 50%|█████     | 25/50 [05:58<05:54, 14.18s/it] 52%|█████▏    | 26/50 [06:13<05:41, 14.21s/it] 54%|█████▍    | 27/50 [06:27<05:25, 14.16s/it] 56%|█████▌    | 28/50 [06:41<05:10, 14.12s/it] 58%|█████▊    | 29/50 [06:55<04:57, 14.18s/it] 60%|██████    | 30/50 [07:09<04:43, 14.17s/it] 62%|██████▏   | 31/50 [07:23<04:29, 14.17s/it] 64%|██████▍   | 32/50 [07:38<04:16, 14.23s/it][loss] ep 24 it 140 total=6.9819 mle=1.5459 pcon=4.7472 forget=0.6888 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=7.0459 mle=1.6212 pcon=4.7457 forget=0.6790 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 240 total=6.9116 mle=1.4853 pcon=4.7442 forget=0.6820 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=7.1177 mle=1.6902 pcon=4.7428 forget=0.6847 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=6.9824 mle=1.5602 pcon=4.7413 forget=0.6809 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 25 it 0 total=6.9901 mle=1.5671 pcon=4.7399 forget=0.6831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=7.1488 mle=1.7236 pcon=4.7386 forget=0.6865 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=6.9943 mle=1.5869 pcon=4.7372 forget=0.6701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 150 total=7.1136 mle=1.7044 pcon=4.7361 forget=0.6731 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 200 total=7.2227 mle=1.8073 pcon=4.7347 forget=0.6807 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=7.1135 mle=1.6973 pcon=4.7333 forget=0.6830 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=6.9882 mle=1.5814 pcon=4.7320 forget=0.6748 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=6.9033 mle=1.4974 pcon=4.7310 forget=0.6750 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 26 it 10 total=7.0329 mle=1.6246 pcon=4.7298 forget=0.6786 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=6.9537 mle=1.5432 pcon=4.7287 forget=0.6819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=7.0739 mle=1.6750 pcon=4.7275 forget=0.6714 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=6.8903 mle=1.4928 pcon=4.7263 forget=0.6712 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=7.0245 mle=1.6238 pcon=4.7250 forget=0.6757 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=6.8709 mle=1.4723 pcon=4.7239 forget=0.6747 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 310 total=7.2698 mle=1.8716 pcon=4.7228 forget=0.6754 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=6.9385 mle=1.5474 pcon=4.7216 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 27 it 20 total=7.0183 mle=1.6268 pcon=4.7205 forget=0.6710 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=6.9322 mle=1.5410 pcon=4.7192 forget=0.6719 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=6.9296 mle=1.5415 pcon=4.7181 forget=0.6699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=6.9160 mle=1.5286 pcon=4.7169 forget=0.6705 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=6.8672 mle=1.4806 pcon=4.7158 forget=0.6708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=6.9611 mle=1.5758 pcon=4.7146 forget=0.6708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=7.0535 mle=1.6684 pcon=4.7136 forget=0.6716 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=7.1036 mle=1.7078 pcon=4.7126 forget=0.6832 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 28 it 30 total=7.0126 mle=1.6292 pcon=4.7117 forget=0.6718 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=7.0520 mle=1.6737 pcon=4.7108 forget=0.6675 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=6.9447 mle=1.5668 pcon=4.7100 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=6.9268 mle=1.5558 pcon=4.7091 forget=0.6620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=6.8034 mle=1.4357 pcon=4.7080 forget=0.6597 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=6.9367 mle=1.5659 pcon=4.7070 forget=0.6638 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=7.1200 mle=1.7453 pcon=4.7058 forget=0.6689 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=7.0634 mle=1.6906 pcon=4.7048 forget=0.6680 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 29 it 40 total=7.0508 mle=1.6823 pcon=4.7037 forget=0.6648 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=6.8862 mle=1.5320 pcon=4.7026 forget=0.6516 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=7.0064 mle=1.6438 pcon=4.7017 forget=0.6609 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=6.9711 mle=1.6048 pcon=4.7009 forget=0.6654 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=6.9418 mle=1.5791 pcon=4.7001 forget=0.6626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=7.0310 mle=1.6799 pcon=4.6994 forget=0.6518 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=6.8970 mle=1.5375 pcon=4.6987 forget=0.6609 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 30 it 0 total=6.9530 mle=1.6006 pcon=4.6978 forget=0.6546 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=6.9773 mle=1.6115 pcon=4.6971 forget=0.6687 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=6.9114 mle=1.5534 pcon=4.6964 forget=0.6615 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=7.1146 mle=1.7664 pcon=4.6956 forget=0.6525 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=6.8436 mle=1.4763 pcon=4.6948 forget=0.6724 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=7.1037 mle=1.7487 pcon=4.6941 forget=0.6609 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=6.8124 mle=1.4610 pcon=4.6934 forget=0.6580 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=7.0946 mle=1.7388 pcon=4.6928 forget=0.6631 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 31 it 10 total=6.9771 mle=1.6324 pcon=4.6921 forget=0.6525 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=6.8744 mle=1.5291 pcon=4.6916 forget=0.6538 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=6.9002 mle=1.5492 pcon=4.6909 forget=0.6600 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=6.8846 mle=1.5386 pcon=4.6903 forget=0.6557 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=6.8868 mle=1.5445 pcon=4.6896 forget=0.6527 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 260 total=6.8369 mle=1.4985 pcon=4.6889 forget=0.6494 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=6.9476 mle=1.6030 pcon=4.6882 forget=0.6563 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=6.8835 mle=1.5468 pcon=4.6875 forget=0.6491 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 32 it 20 total=7.0152 mle=1.6789 pcon=4.6869 forget=0.6495 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=6.8838 mle=1.5514 pcon=4.6862 forget=0.6462 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=6.9072 mle=1.5601 pcon=4.6856 forget=0.6615 nr=64 nf=64 protos=540 fproto_sim=NA
 66%|██████▌   | 33/50 [07:52<04:02, 14.25s/it] 68%|██████▊   | 34/50 [08:06<03:47, 14.22s/it] 70%|███████   | 35/50 [08:21<03:34, 14.31s/it] 72%|███████▏  | 36/50 [08:35<03:20, 14.35s/it] 74%|███████▍  | 37/50 [08:50<03:07, 14.44s/it] 76%|███████▌  | 38/50 [09:04<02:53, 14.43s/it] 78%|███████▊  | 39/50 [09:19<02:39, 14.53s/it] 80%|████████  | 40/50 [09:34<02:26, 14.63s/it][loss] ep 32 it 170 total=6.9094 mle=1.5682 pcon=4.6851 forget=0.6561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=6.9578 mle=1.6262 pcon=4.6846 forget=0.6470 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=6.8853 mle=1.5531 pcon=4.6841 forget=0.6480 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=6.8136 mle=1.4845 pcon=4.6836 forget=0.6454 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=6.7883 mle=1.4608 pcon=4.6832 forget=0.6442 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 33 it 30 total=6.9790 mle=1.6450 pcon=4.6828 forget=0.6512 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=7.1974 mle=1.8702 pcon=4.6823 forget=0.6448 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=6.8204 mle=1.4902 pcon=4.6818 forget=0.6484 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=6.9730 mle=1.6371 pcon=4.6814 forget=0.6545 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=6.9465 mle=1.6080 pcon=4.6810 forget=0.6576 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=7.0232 mle=1.6935 pcon=4.6805 forget=0.6492 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=7.0146 mle=1.6856 pcon=4.6801 forget=0.6489 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 380 total=6.9132 mle=1.5783 pcon=4.6796 forget=0.6552 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 34 it 40 total=7.0309 mle=1.7013 pcon=4.6791 forget=0.6505 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=6.8371 mle=1.5039 pcon=4.6785 forget=0.6546 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 140 total=6.9029 mle=1.5836 pcon=4.6781 forget=0.6412 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=6.9645 mle=1.6382 pcon=4.6778 forget=0.6486 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 240 total=7.0089 mle=1.6862 pcon=4.6773 forget=0.6455 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=6.9318 mle=1.6061 pcon=4.6768 forget=0.6490 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=6.8554 mle=1.5228 pcon=4.6763 forget=0.6562 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 35 it 0 total=6.9165 mle=1.5946 pcon=4.6759 forget=0.6461 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 50 total=6.8904 mle=1.5690 pcon=4.6754 forget=0.6460 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 100 total=7.0778 mle=1.7465 pcon=4.6749 forget=0.6565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 150 total=6.9300 mle=1.6112 pcon=4.6745 forget=0.6444 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 200 total=6.8799 mle=1.5521 pcon=4.6741 forget=0.6537 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 250 total=7.0050 mle=1.6807 pcon=4.6737 forget=0.6506 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 300 total=6.8672 mle=1.5522 pcon=4.6732 forget=0.6418 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 350 total=7.0195 mle=1.7021 pcon=4.6729 forget=0.6446 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 36 it 10 total=7.0796 mle=1.7559 pcon=4.6726 forget=0.6511 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 60 total=6.8506 mle=1.5332 pcon=4.6723 forget=0.6450 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 110 total=6.8894 mle=1.5672 pcon=4.6720 forget=0.6502 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 160 total=6.9611 mle=1.6432 pcon=4.6716 forget=0.6463 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 210 total=6.8117 mle=1.4935 pcon=4.6713 forget=0.6469 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 260 total=6.9291 mle=1.5985 pcon=4.6711 forget=0.6595 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 310 total=6.9784 mle=1.6480 pcon=4.6707 forget=0.6596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 360 total=6.9579 mle=1.6349 pcon=4.6703 forget=0.6528 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 37 it 20 total=6.8106 mle=1.4875 pcon=4.6699 forget=0.6532 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 70 total=6.9667 mle=1.6395 pcon=4.6696 forget=0.6576 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 120 total=6.9236 mle=1.5990 pcon=4.6693 forget=0.6553 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 170 total=6.8838 mle=1.5658 pcon=4.6689 forget=0.6491 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 220 total=7.0501 mle=1.7220 pcon=4.6685 forget=0.6596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 270 total=6.9272 mle=1.6107 pcon=4.6681 forget=0.6484 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 320 total=6.9572 mle=1.6381 pcon=4.6678 forget=0.6513 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 370 total=6.9403 mle=1.6055 pcon=4.6676 forget=0.6672 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 30 total=6.9865 mle=1.6553 pcon=4.6675 forget=0.6637 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 80 total=6.7854 mle=1.4621 pcon=4.6673 forget=0.6560 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 130 total=6.9321 mle=1.6092 pcon=4.6670 forget=0.6559 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 180 total=6.8697 mle=1.5484 pcon=4.6668 forget=0.6546 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 230 total=6.8351 mle=1.5108 pcon=4.6665 forget=0.6578 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 280 total=6.9795 mle=1.6529 pcon=4.6663 forget=0.6603 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 330 total=6.9072 mle=1.5891 pcon=4.6659 forget=0.6522 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 380 total=6.8140 mle=1.4972 pcon=4.6658 forget=0.6510 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 39 it 40 total=7.0033 mle=1.6812 pcon=4.6658 forget=0.6563 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 90 total=6.9312 mle=1.6045 pcon=4.6656 forget=0.6612 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 140 total=6.9097 mle=1.5935 pcon=4.6654 forget=0.6509 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 190 total=6.7946 mle=1.4733 pcon=4.6652 forget=0.6561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 240 total=6.9640 mle=1.6428 pcon=4.6650 forget=0.6562 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 290 total=6.8104 mle=1.4884 pcon=4.6649 forget=0.6571 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 340 total=6.8873 mle=1.5629 pcon=4.6648 forget=0.6596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 0 total=6.8670 mle=1.5400 pcon=4.6645 forget=0.6625 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 50 total=6.9156 mle=1.5890 pcon=4.6643 forget=0.6623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 100 total=6.9290 mle=1.6102 pcon=4.6640 forget=0.6548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 150 total=6.7600 mle=1.4411 pcon=4.6639 forget=0.6550 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 200 total=7.0587 mle=1.7324 pcon=4.6637 forget=0.6627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 250 total=7.1038 mle=1.7785 pcon=4.6635 forget=0.6618 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 300 total=6.8612 mle=1.5399 pcon=4.6634 forget=0.6579 nr=64 nf=64 protos=540 fproto_sim=NA
 82%|████████▏ | 41/50 [09:50<02:15, 15.04s/it] 84%|████████▍ | 42/50 [10:05<02:01, 15.22s/it] 86%|████████▌ | 43/50 [10:21<01:47, 15.42s/it] 88%|████████▊ | 44/50 [10:37<01:33, 15.57s/it] 90%|█████████ | 45/50 [10:58<01:25, 17.11s/it] 92%|█████████▏| 46/50 [11:21<01:15, 18.88s/it] 94%|█████████▍| 47/50 [11:39<00:56, 18.67s/it] 96%|█████████▌| 48/50 [11:55<00:35, 17.73s/it] 98%|█████████▊| 49/50 [12:11<00:17, 17.21s/it]100%|██████████| 50/50 [12:31<00:00, 18.29s/it]100%|██████████| 50/50 [12:31<00:00, 15.04s/it]
[loss] ep 40 it 350 total=6.7678 mle=1.4510 pcon=4.6632 forget=0.6535 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 10 total=6.8355 mle=1.5188 pcon=4.6631 forget=0.6535 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 60 total=7.1588 mle=1.8299 pcon=4.6631 forget=0.6658 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 110 total=6.8933 mle=1.5673 pcon=4.6629 forget=0.6630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 160 total=6.8782 mle=1.5556 pcon=4.6629 forget=0.6597 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 210 total=6.9442 mle=1.6209 pcon=4.6628 forget=0.6604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 260 total=6.8513 mle=1.5322 pcon=4.6630 forget=0.6561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 310 total=6.8281 mle=1.5068 pcon=4.6630 forget=0.6582 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 360 total=7.0067 mle=1.6816 pcon=4.6630 forget=0.6621 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 42 it 20 total=6.9699 mle=1.6409 pcon=4.6630 forget=0.6660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 70 total=6.8958 mle=1.5702 pcon=4.6628 forget=0.6629 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 120 total=7.1651 mle=1.8339 pcon=4.6626 forget=0.6685 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 170 total=6.9247 mle=1.6039 pcon=4.6626 forget=0.6582 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 220 total=6.9592 mle=1.6334 pcon=4.6625 forget=0.6633 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 270 total=7.0492 mle=1.7276 pcon=4.6624 forget=0.6592 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 320 total=6.9748 mle=1.6498 pcon=4.6624 forget=0.6626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 370 total=6.9208 mle=1.5958 pcon=4.6624 forget=0.6625 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 30 total=6.9450 mle=1.6111 pcon=4.6623 forget=0.6716 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 80 total=6.9065 mle=1.5799 pcon=4.6622 forget=0.6644 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 130 total=6.9792 mle=1.6504 pcon=4.6621 forget=0.6667 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 180 total=6.8231 mle=1.4849 pcon=4.6621 forget=0.6761 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 230 total=6.8383 mle=1.5177 pcon=4.6619 forget=0.6587 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 280 total=6.8467 mle=1.5272 pcon=4.6619 forget=0.6576 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 330 total=6.8572 mle=1.5281 pcon=4.6616 forget=0.6675 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 380 total=6.8181 mle=1.4993 pcon=4.6616 forget=0.6573 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 40 total=6.9932 mle=1.6679 pcon=4.6614 forget=0.6639 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 90 total=6.8639 mle=1.5403 pcon=4.6614 forget=0.6623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 140 total=6.9645 mle=1.6349 pcon=4.6613 forget=0.6683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 190 total=6.8215 mle=1.4971 pcon=4.6612 forget=0.6631 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 240 total=6.8500 mle=1.5179 pcon=4.6612 forget=0.6709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 290 total=7.0937 mle=1.7657 pcon=4.6612 forget=0.6668 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 340 total=7.0919 mle=1.7619 pcon=4.6611 forget=0.6689 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 0 total=6.8875 mle=1.5514 pcon=4.6610 forget=0.6750 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 50 total=6.8433 mle=1.5218 pcon=4.6610 forget=0.6606 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 100 total=6.8684 mle=1.5415 pcon=4.6609 forget=0.6660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 150 total=6.8484 mle=1.5228 pcon=4.6608 forget=0.6648 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 200 total=6.9177 mle=1.5883 pcon=4.6608 forget=0.6686 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 250 total=6.8567 mle=1.5283 pcon=4.6607 forget=0.6677 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 300 total=6.9124 mle=1.5885 pcon=4.6604 forget=0.6634 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 350 total=6.8612 mle=1.5346 pcon=4.6604 forget=0.6661 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 10 total=6.8020 mle=1.4752 pcon=4.6604 forget=0.6664 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 60 total=6.8455 mle=1.5130 pcon=4.6603 forget=0.6721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 110 total=6.9743 mle=1.6443 pcon=4.6603 forget=0.6697 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 160 total=6.7869 mle=1.4647 pcon=4.6602 forget=0.6620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 210 total=7.0498 mle=1.7280 pcon=4.6601 forget=0.6616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 260 total=6.9503 mle=1.6319 pcon=4.6601 forget=0.6584 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 310 total=6.8484 mle=1.5230 pcon=4.6601 forget=0.6653 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 360 total=7.0381 mle=1.7063 pcon=4.6600 forget=0.6717 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 20 total=6.8672 mle=1.5406 pcon=4.6598 forget=0.6668 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 70 total=6.8390 mle=1.5142 pcon=4.6598 forget=0.6651 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 120 total=6.9810 mle=1.6502 pcon=4.6596 forget=0.6712 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 170 total=6.8774 mle=1.5398 pcon=4.6594 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 220 total=6.7774 mle=1.4503 pcon=4.6594 forget=0.6677 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 270 total=6.8945 mle=1.5647 pcon=4.6594 forget=0.6704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 320 total=6.8742 mle=1.5374 pcon=4.6593 forget=0.6774 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 370 total=6.9636 mle=1.6328 pcon=4.6593 forget=0.6716 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 30 total=6.8917 mle=1.5580 pcon=4.6593 forget=0.6743 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 80 total=6.8908 mle=1.5535 pcon=4.6592 forget=0.6780 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 130 total=6.9544 mle=1.6306 pcon=4.6592 forget=0.6645 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 180 total=6.9759 mle=1.6417 pcon=4.6592 forget=0.6749 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 230 total=6.9668 mle=1.6446 pcon=4.6592 forget=0.6629 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 280 total=6.8623 mle=1.5425 pcon=4.6591 forget=0.6607 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 330 total=6.8839 mle=1.5677 pcon=4.6591 forget=0.6570 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 380 total=6.8472 mle=1.5201 pcon=4.6589 forget=0.6682 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 40 total=6.8947 mle=1.5571 pcon=4.6590 forget=0.6787 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 90 total=6.9493 mle=1.6180 pcon=4.6590 forget=0.6723 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 140 total=6.9380 mle=1.6103 pcon=4.6590 forget=0.6688 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 190 total=7.0490 mle=1.7222 pcon=4.6587 forget=0.6681 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 240 total=6.9748 mle=1.6528 pcon=4.6586 forget=0.6634 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 290 total=6.9116 mle=1.5853 pcon=4.6585 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 340 total=6.8513 mle=1.5227 pcon=4.6586 forget=0.6700 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:22,  2.73it/s]  2%|▏         | 8/391 [00:00<00:18, 21.26it/s]  5%|▍         | 18/391 [00:00<00:08, 42.77it/s]  7%|▋         | 28/391 [00:00<00:06, 58.23it/s]  9%|▉         | 37/391 [00:00<00:05, 65.66it/s] 12%|█▏        | 47/391 [00:00<00:04, 74.44it/s] 14%|█▍        | 56/391 [00:01<00:04, 67.67it/s] 17%|█▋        | 65/391 [00:01<00:04, 72.17it/s] 19%|█▉        | 75/391 [00:01<00:04, 78.38it/s] 21%|██▏       | 84/391 [00:01<00:03, 80.40it/s] 24%|██▍       | 94/391 [00:01<00:03, 84.47it/s] 27%|██▋       | 104/391 [00:01<00:03, 87.55it/s] 29%|██▉       | 114/391 [00:01<00:03, 89.58it/s] 32%|███▏      | 124/391 [00:01<00:03, 83.15it/s] 34%|███▍      | 134/391 [00:01<00:02, 86.46it/s] 37%|███▋      | 144/391 [00:02<00:02, 88.90it/s] 39%|███▉      | 154/391 [00:02<00:02, 90.55it/s] 42%|████▏     | 164/391 [00:02<00:02, 91.73it/s] 45%|████▍     | 174/391 [00:02<00:02, 92.65it/s] 47%|████▋     | 184/391 [00:02<00:02, 93.20it/s] 50%|████▉     | 194/391 [00:02<00:02, 93.65it/s] 52%|█████▏    | 204/391 [00:02<00:01, 94.06it/s] 55%|█████▍    | 214/391 [00:02<00:01, 94.34it/s] 57%|█████▋    | 224/391 [00:02<00:01, 94.59it/s] 60%|█████▉    | 234/391 [00:02<00:01, 94.75it/s] 62%|██████▏   | 244/391 [00:03<00:01, 94.65it/s] 65%|██████▍   | 254/391 [00:03<00:01, 94.33it/s] 68%|██████▊   | 264/391 [00:03<00:01, 94.48it/s] 70%|███████   | 274/391 [00:03<00:01, 93.58it/s] 73%|███████▎  | 284/391 [00:03<00:01, 93.88it/s] 75%|███████▌  | 294/391 [00:03<00:01, 94.20it/s] 78%|███████▊  | 304/391 [00:03<00:00, 94.06it/s] 80%|████████  | 314/391 [00:03<00:00, 93.96it/s] 83%|████████▎ | 324/391 [00:03<00:00, 93.32it/s] 85%|████████▌ | 334/391 [00:04<00:00, 93.66it/s] 88%|████████▊ | 344/391 [00:04<00:00, 94.08it/s] 91%|█████████ | 354/391 [00:04<00:00, 94.12it/s] 93%|█████████▎| 364/391 [00:04<00:00, 94.26it/s] 96%|█████████▌| 374/391 [00:04<00:00, 94.13it/s] 98%|█████████▊| 384/391 [00:04<00:00, 94.25it/s]100%|██████████| 391/391 [00:04<00:00, 83.73it/s]
50000 images processed, 4.793994903564453 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:30,  2.54it/s] 14%|█▍        | 11/79 [00:00<00:02, 27.86it/s] 25%|██▌       | 20/79 [00:00<00:01, 44.64it/s] 38%|███▊      | 30/79 [00:00<00:00, 58.82it/s] 51%|█████     | 40/79 [00:00<00:00, 69.13it/s] 63%|██████▎   | 50/79 [00:00<00:00, 76.22it/s] 76%|███████▌  | 60/79 [00:01<00:00, 81.33it/s] 89%|████████▊ | 70/79 [00:01<00:00, 85.20it/s]100%|██████████| 79/79 [00:01<00:00, 63.63it/s]
10000 images processed, 1.266662359237671 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:13,  2.77it/s]  5%|▌         | 11/204 [00:00<00:06, 29.79it/s] 10%|█         | 21/204 [00:00<00:03, 48.93it/s] 15%|█▌        | 31/204 [00:00<00:02, 62.54it/s] 20%|██        | 41/204 [00:00<00:02, 72.10it/s] 25%|██▌       | 51/204 [00:00<00:01, 78.66it/s] 30%|██▉       | 61/204 [00:00<00:01, 83.21it/s] 35%|███▍      | 71/204 [00:01<00:01, 86.66it/s] 40%|███▉      | 81/204 [00:01<00:01, 88.93it/s] 45%|████▍     | 91/204 [00:01<00:01, 90.56it/s] 50%|████▉     | 101/204 [00:01<00:01, 91.66it/s] 54%|█████▍    | 111/204 [00:01<00:01, 92.32it/s] 59%|█████▉    | 121/204 [00:01<00:00, 92.62it/s] 64%|██████▍   | 131/204 [00:01<00:00, 93.16it/s] 69%|██████▉   | 141/204 [00:01<00:00, 93.54it/s] 74%|███████▍  | 151/204 [00:01<00:00, 93.61it/s] 79%|███████▉  | 161/204 [00:02<00:00, 93.72it/s] 84%|████████▍ | 171/204 [00:02<00:00, 93.93it/s] 89%|████████▊ | 181/204 [00:02<00:00, 93.96it/s] 94%|█████████▎| 191/204 [00:02<00:00, 94.11it/s] 99%|█████████▊| 201/204 [00:02<00:00, 94.61it/s]100%|██████████| 204/204 [00:02<00:00, 80.94it/s]
26032 images processed, 2.5678627490997314 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:45,  1.72it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.30it/s] 22%|██▏       | 17/79 [00:00<00:02, 25.06it/s] 32%|███▏      | 25/79 [00:01<00:01, 32.02it/s] 42%|████▏     | 33/79 [00:01<00:01, 36.97it/s] 51%|█████     | 40/79 [00:01<00:00, 43.41it/s] 58%|█████▊    | 46/79 [00:01<00:00, 41.22it/s] 67%|██████▋   | 53/79 [00:01<00:00, 41.58it/s] 77%|███████▋  | 61/79 [00:01<00:00, 44.08it/s] 87%|████████▋ | 69/79 [00:01<00:00, 45.74it/s] 97%|█████████▋| 77/79 [00:02<00:00, 47.00it/s]100%|██████████| 79/79 [00:02<00:00, 36.48it/s]
10000 images processed, 2.2023160457611084 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:28,  2.73it/s] 14%|█▍        | 11/79 [00:00<00:02, 29.37it/s] 27%|██▋       | 21/79 [00:00<00:01, 48.53it/s] 39%|███▉      | 31/79 [00:00<00:00, 62.15it/s] 52%|█████▏    | 41/79 [00:00<00:00, 71.29it/s] 65%|██████▍   | 51/79 [00:00<00:00, 78.15it/s] 77%|███████▋  | 61/79 [00:01<00:00, 83.11it/s] 90%|████████▉ | 71/79 [00:01<00:00, 86.76it/s]100%|██████████| 79/79 [00:01<00:00, 66.61it/s]
10000 images processed, 1.2083685398101807 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:28,  2.41it/s] 16%|█▌        | 11/70 [00:00<00:02, 27.07it/s] 30%|███       | 21/70 [00:00<00:01, 45.76it/s] 44%|████▍     | 31/70 [00:00<00:00, 59.60it/s] 59%|█████▊    | 41/70 [00:00<00:00, 69.69it/s] 73%|███████▎  | 51/70 [00:00<00:00, 76.15it/s] 87%|████████▋ | 61/70 [00:01<00:00, 81.79it/s]100%|██████████| 70/70 [00:01<00:00, 60.80it/s]
8925 images processed, 1.1837553977966309 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:43,  1.00it/s]  4%|▍         | 2/45 [00:01<00:21,  2.03it/s] 20%|██        | 9/45 [00:01<00:04,  8.80it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.61it/s] 38%|███▊      | 17/45 [00:02<00:02, 11.44it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.75it/s] 51%|█████     | 23/45 [00:02<00:01, 14.85it/s] 56%|█████▌    | 25/45 [00:02<00:01, 12.52it/s] 60%|██████    | 27/45 [00:02<00:01, 12.25it/s] 69%|██████▉   | 31/45 [00:02<00:00, 15.67it/s] 73%|███████▎  | 33/45 [00:03<00:01,  9.98it/s] 78%|███████▊  | 35/45 [00:03<00:00, 10.61it/s] 91%|█████████ | 41/45 [00:03<00:00, 13.79it/s] 96%|█████████▌| 43/45 [00:04<00:00, 10.11it/s]100%|██████████| 45/45 [00:04<00:00, 10.39it/s]
5640 images processed, 4.358013868331909 seconds used

19.40941023826599
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           5.02  98.83
places365     74.80  79.17
LSUN          24.80  95.14
iSUN          77.76  78.87
dtd           42.98  90.03
forget        40.40  91.39
AVG           44.29  88.90
Retain-Acc: 0.7337
Forget-as-OOD (retain known vs forget novel):
  FPR: 40.40 AUROC: 91.39 AUIN: 98.86
9.321239233016968
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.1 lr=0.0005 epochs=5
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.1, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:28<01:52, 28.18s/it] 40%|████      | 2/5 [00:52<01:18, 26.20s/it] 60%|██████    | 3/5 [01:22<00:55, 27.57s/it] 80%|████████  | 4/5 [01:48<00:26, 27.00s/it]100%|██████████| 5/5 [02:14<00:00, 26.61s/it]100%|██████████| 5/5 [02:14<00:00, 26.85s/it]
[loss] ep 0 it 0 total=7.5538 mle=1.5710 pcon=5.2950 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=7.5309 mle=1.5424 pcon=5.2879 forget=0.7007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=7.6684 mle=1.7005 pcon=5.2809 forget=0.6870 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=7.8584 mle=1.8997 pcon=5.2738 forget=0.6848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=7.6715 mle=1.7132 pcon=5.2670 forget=0.6913 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=7.4514 mle=1.5019 pcon=5.2603 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=7.5047 mle=1.5597 pcon=5.2540 forget=0.6910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=7.6234 mle=1.6809 pcon=5.2476 forget=0.6948 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=7.6175 mle=1.6716 pcon=5.2409 forget=0.7050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=7.5551 mle=1.6370 pcon=5.2346 forget=0.6835 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=7.4398 mle=1.5166 pcon=5.2284 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=7.6974 mle=1.7803 pcon=5.2224 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=7.6739 mle=1.7645 pcon=5.2167 forget=0.6927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=7.5008 mle=1.6054 pcon=5.2112 forget=0.6842 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=7.6188 mle=1.7241 pcon=5.2056 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=7.6878 mle=1.7944 pcon=5.2003 forget=0.6931 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=7.4365 mle=1.5531 pcon=5.1950 forget=0.6884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=7.7571 mle=1.8883 pcon=5.1899 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=7.6142 mle=1.7416 pcon=5.1847 forget=0.6879 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=7.3886 mle=1.5264 pcon=5.1797 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=7.4534 mle=1.5858 pcon=5.1746 forget=0.6930 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=7.7391 mle=1.8838 pcon=5.1699 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=7.4363 mle=1.5923 pcon=5.1652 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=7.6543 mle=1.8114 pcon=5.1605 forget=0.6824 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=7.5471 mle=1.7057 pcon=5.1560 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=7.5124 mle=1.6708 pcon=5.1519 forget=0.6897 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=7.5975 mle=1.7670 pcon=5.1476 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=7.7431 mle=1.9184 pcon=5.1436 forget=0.6812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=7.3893 mle=1.5698 pcon=5.1395 forget=0.6800 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=7.6496 mle=1.8361 pcon=5.1354 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=7.5218 mle=1.7029 pcon=5.1312 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=7.5123 mle=1.6955 pcon=5.1276 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=7.4487 mle=1.6539 pcon=5.1237 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=7.5273 mle=1.7295 pcon=5.1197 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=7.4494 mle=1.6502 pcon=5.1161 forget=0.6831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.2489 mle=1.4534 pcon=5.1126 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=7.5537 mle=1.7667 pcon=5.1088 forget=0.6783 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.2732 mle=1.4912 pcon=5.1052 forget=0.6768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=7.6563 mle=1.8719 pcon=5.1018 forget=0.6826 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:19,  2.79it/s]  2%|▏         | 8/391 [00:00<00:17, 21.64it/s]  4%|▍         | 17/391 [00:00<00:09, 41.14it/s]  7%|▋         | 26/391 [00:00<00:06, 55.02it/s]  9%|▉         | 35/391 [00:00<00:05, 65.06it/s] 11%|█         | 43/391 [00:00<00:05, 62.39it/s] 13%|█▎        | 51/391 [00:01<00:06, 49.53it/s] 15%|█▍        | 57/391 [00:01<00:07, 45.89it/s] 16%|█▌        | 63/391 [00:01<00:07, 46.00it/s] 18%|█▊        | 69/391 [00:01<00:06, 46.07it/s] 19%|█▉        | 74/391 [00:01<00:06, 46.14it/s] 20%|██        | 79/391 [00:01<00:06, 46.22it/s] 21%|██▏       | 84/391 [00:01<00:06, 46.27it/s] 23%|██▎       | 90/391 [00:01<00:06, 47.48it/s] 24%|██▍       | 95/391 [00:02<00:06, 47.13it/s] 26%|██▌       | 100/391 [00:02<00:06, 46.90it/s] 27%|██▋       | 105/391 [00:02<00:06, 46.66it/s] 28%|██▊       | 110/391 [00:02<00:06, 42.81it/s] 29%|██▉       | 115/391 [00:02<00:07, 39.29it/s] 31%|███       | 120/391 [00:02<00:07, 38.13it/s] 32%|███▏      | 124/391 [00:02<00:06, 38.39it/s] 33%|███▎      | 129/391 [00:02<00:06, 40.55it/s] 34%|███▍      | 134/391 [00:03<00:06, 42.15it/s] 36%|███▌      | 139/391 [00:03<00:05, 43.41it/s] 37%|███▋      | 145/391 [00:03<00:05, 45.69it/s] 38%|███▊      | 150/391 [00:03<00:05, 45.92it/s] 40%|███▉      | 155/391 [00:03<00:05, 46.01it/s] 41%|████      | 160/391 [00:03<00:05, 46.10it/s] 42%|████▏     | 165/391 [00:03<00:04, 46.13it/s] 43%|████▎     | 170/391 [00:03<00:04, 46.39it/s] 45%|████▍     | 175/391 [00:03<00:04, 47.33it/s] 46%|████▌     | 180/391 [00:04<00:05, 42.04it/s] 47%|████▋     | 185/391 [00:04<00:05, 38.90it/s] 49%|████▊     | 190/391 [00:04<00:05, 37.94it/s] 50%|████▉     | 195/391 [00:04<00:04, 39.60it/s] 51%|█████     | 200/391 [00:04<00:04, 41.40it/s] 52%|█████▏    | 205/391 [00:04<00:04, 42.74it/s] 54%|█████▎    | 210/391 [00:04<00:04, 43.68it/s] 55%|█████▍    | 215/391 [00:04<00:03, 44.91it/s] 56%|█████▋    | 220/391 [00:05<00:03, 46.27it/s] 58%|█████▊    | 225/391 [00:05<00:03, 46.31it/s] 59%|█████▉    | 230/391 [00:05<00:03, 46.29it/s] 60%|██████    | 235/391 [00:05<00:03, 46.28it/s] 61%|██████▏   | 240/391 [00:05<00:03, 46.26it/s] 63%|██████▎   | 245/391 [00:05<00:03, 44.08it/s] 64%|██████▍   | 250/391 [00:05<00:03, 40.02it/s] 65%|██████▌   | 255/391 [00:05<00:03, 38.98it/s] 66%|██████▌   | 259/391 [00:05<00:03, 37.11it/s] 68%|██████▊   | 266/391 [00:06<00:02, 45.04it/s] 70%|███████   | 275/391 [00:06<00:02, 56.60it/s] 73%|███████▎  | 284/391 [00:06<00:01, 65.43it/s] 75%|███████▍  | 293/391 [00:06<00:01, 71.84it/s] 77%|███████▋  | 301/391 [00:06<00:01, 63.49it/s] 79%|███████▉  | 308/391 [00:06<00:01, 62.11it/s] 81%|████████  | 315/391 [00:06<00:01, 60.78it/s] 83%|████████▎ | 324/391 [00:06<00:00, 67.76it/s] 85%|████████▌ | 333/391 [00:07<00:00, 73.07it/s] 87%|████████▋ | 342/391 [00:07<00:00, 77.27it/s] 90%|████████▉ | 351/391 [00:07<00:00, 79.93it/s] 92%|█████████▏| 360/391 [00:07<00:00, 77.60it/s] 94%|█████████▍| 368/391 [00:07<00:00, 63.79it/s] 96%|█████████▌| 375/391 [00:07<00:00, 53.39it/s] 97%|█████████▋| 381/391 [00:07<00:00, 51.47it/s] 99%|█████████▉| 387/391 [00:07<00:00, 50.01it/s]100%|██████████| 391/391 [00:08<00:00, 48.56it/s]
50000 images processed, 8.155872106552124 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:22,  3.46it/s]  6%|▋         | 5/79 [00:00<00:04, 15.12it/s] 11%|█▏        | 9/79 [00:00<00:03, 22.91it/s] 18%|█▊        | 14/79 [00:00<00:02, 30.82it/s] 24%|██▍       | 19/79 [00:00<00:01, 35.79it/s] 30%|███       | 24/79 [00:00<00:01, 39.04it/s] 37%|███▋      | 29/79 [00:00<00:01, 41.22it/s] 43%|████▎     | 34/79 [00:01<00:01, 42.79it/s] 54%|█████▍    | 43/79 [00:01<00:00, 55.64it/s] 63%|██████▎   | 50/79 [00:01<00:00, 59.47it/s] 72%|███████▏  | 57/79 [00:01<00:00, 54.42it/s] 80%|███████▉  | 63/79 [00:01<00:00, 55.66it/s] 89%|████████▊ | 70/79 [00:01<00:00, 56.56it/s] 96%|█████████▌| 76/79 [00:01<00:00, 53.21it/s]100%|██████████| 79/79 [00:01<00:00, 43.69it/s]
10000 images processed, 1.8353605270385742 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:19,  2.56it/s]  3%|▎         | 6/204 [00:00<00:13, 14.45it/s]  5%|▌         | 11/204 [00:00<00:08, 23.33it/s]  9%|▉         | 18/204 [00:00<00:05, 35.87it/s] 13%|█▎        | 26/204 [00:00<00:03, 46.52it/s] 16%|█▌        | 32/204 [00:00<00:03, 46.53it/s] 19%|█▊        | 38/204 [00:01<00:03, 46.57it/s] 22%|██▏       | 44/204 [00:01<00:03, 46.60it/s] 24%|██▍       | 49/204 [00:01<00:03, 46.56it/s] 26%|██▋       | 54/204 [00:01<00:03, 46.60it/s] 29%|██▉       | 59/204 [00:01<00:03, 43.50it/s] 31%|███▏      | 64/204 [00:01<00:03, 38.04it/s] 33%|███▎      | 68/204 [00:01<00:03, 36.48it/s] 35%|███▌      | 72/204 [00:01<00:03, 35.78it/s] 38%|███▊      | 77/204 [00:02<00:03, 38.65it/s] 40%|████      | 82/204 [00:02<00:02, 40.83it/s] 43%|████▎     | 87/204 [00:02<00:02, 42.45it/s] 45%|████▌     | 92/204 [00:02<00:02, 43.65it/s] 49%|████▉     | 100/204 [00:02<00:01, 52.26it/s] 53%|█████▎    | 109/204 [00:02<00:01, 61.90it/s] 57%|█████▋    | 116/204 [00:02<00:01, 58.05it/s] 60%|█████▉    | 122/204 [00:02<00:01, 55.65it/s] 63%|██████▎   | 128/204 [00:02<00:01, 54.50it/s] 67%|██████▋   | 137/204 [00:03<00:01, 63.01it/s] 72%|███████▏  | 146/204 [00:03<00:00, 69.55it/s] 76%|███████▌  | 155/204 [00:03<00:00, 74.38it/s] 80%|███████▉  | 163/204 [00:03<00:00, 68.34it/s] 83%|████████▎ | 170/204 [00:03<00:00, 56.18it/s] 87%|████████▋ | 177/204 [00:03<00:00, 51.32it/s] 91%|█████████ | 186/204 [00:03<00:00, 59.39it/s] 95%|█████████▌| 194/204 [00:04<00:00, 63.92it/s]100%|█████████▉| 203/204 [00:04<00:00, 70.28it/s]100%|██████████| 204/204 [00:04<00:00, 49.38it/s]
26032 images processed, 4.173515319824219 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:43,  1.80it/s] 11%|█▏        | 9/79 [00:00<00:04, 15.69it/s] 16%|█▋        | 13/79 [00:00<00:03, 20.42it/s] 23%|██▎       | 18/79 [00:00<00:02, 26.92it/s] 28%|██▊       | 22/79 [00:01<00:01, 29.59it/s] 34%|███▍      | 27/79 [00:01<00:01, 34.12it/s] 46%|████▌     | 36/79 [00:01<00:00, 47.41it/s] 53%|█████▎    | 42/79 [00:01<00:00, 42.29it/s] 59%|█████▉    | 47/79 [00:01<00:00, 43.02it/s] 67%|██████▋   | 53/79 [00:01<00:00, 47.18it/s] 77%|███████▋  | 61/79 [00:01<00:00, 54.89it/s] 85%|████████▍ | 67/79 [00:01<00:00, 46.92it/s] 92%|█████████▏| 73/79 [00:02<00:00, 48.88it/s]100%|██████████| 79/79 [00:02<00:00, 37.83it/s]
10000 images processed, 2.1214966773986816 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:24,  3.16it/s] 14%|█▍        | 11/79 [00:00<00:02, 32.68it/s] 27%|██▋       | 21/79 [00:00<00:01, 52.24it/s] 39%|███▉      | 31/79 [00:00<00:00, 65.48it/s] 52%|█████▏    | 41/79 [00:00<00:00, 73.58it/s] 65%|██████▍   | 51/79 [00:00<00:00, 79.78it/s] 77%|███████▋  | 61/79 [00:00<00:00, 84.23it/s] 90%|████████▉ | 71/79 [00:01<00:00, 87.68it/s]100%|██████████| 79/79 [00:01<00:00, 69.58it/s]
10000 images processed, 1.1535065174102783 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:27,  2.50it/s] 16%|█▌        | 11/70 [00:00<00:02, 27.45it/s] 30%|███       | 21/70 [00:00<00:01, 45.46it/s] 41%|████▏     | 29/70 [00:00<00:00, 49.65it/s] 54%|█████▍    | 38/70 [00:00<00:00, 59.58it/s] 67%|██████▋   | 47/70 [00:00<00:00, 67.33it/s] 80%|████████  | 56/70 [00:01<00:00, 73.02it/s] 93%|█████████▎| 65/70 [00:01<00:00, 77.57it/s]100%|██████████| 70/70 [00:01<00:00, 56.76it/s]
8925 images processed, 1.271125316619873 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:41,  1.05it/s]  4%|▍         | 2/45 [00:01<00:20,  2.11it/s] 20%|██        | 9/45 [00:01<00:03,  9.05it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.63it/s] 38%|███▊      | 17/45 [00:02<00:02, 12.03it/s] 42%|████▏     | 19/45 [00:02<00:02, 12.89it/s] 49%|████▉     | 22/45 [00:02<00:01, 15.24it/s] 53%|█████▎    | 24/45 [00:02<00:01, 14.55it/s] 58%|█████▊    | 26/45 [00:02<00:01, 12.77it/s] 67%|██████▋   | 30/45 [00:02<00:01, 13.51it/s] 71%|███████   | 32/45 [00:03<00:00, 13.25it/s] 76%|███████▌  | 34/45 [00:03<00:01,  9.77it/s] 87%|████████▋ | 39/45 [00:03<00:00, 14.12it/s] 91%|█████████ | 41/45 [00:03<00:00, 11.41it/s] 96%|█████████▌| 43/45 [00:04<00:00, 11.84it/s]100%|██████████| 45/45 [00:04<00:00, 11.05it/s]
5640 images processed, 4.097119569778442 seconds used

24.450178861618042
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.86  81.18
LSUN          17.56  96.07
iSUN          72.34  81.68
dtd           37.87  91.39
forget        79.50  86.90
AVG           46.27  89.43
Retain-Acc: 0.7407
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
24.555143117904663
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.1 lr=0.0005 epochs=10
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.1, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:27<04:07, 27.47s/it] 20%|██        | 2/10 [00:50<03:17, 24.70s/it] 30%|███       | 3/10 [01:14<02:52, 24.65s/it] 40%|████      | 4/10 [01:37<02:23, 23.93s/it] 50%|█████     | 5/10 [02:00<01:58, 23.64s/it] 60%|██████    | 6/10 [02:29<01:40, 25.21s/it] 70%|███████   | 7/10 [02:54<01:16, 25.42s/it] 80%|████████  | 8/10 [03:20<00:51, 25.50s/it][loss] ep 0 it 0 total=7.5537 mle=1.5709 pcon=5.2950 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=7.5309 mle=1.5423 pcon=5.2879 forget=0.7007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=7.6683 mle=1.7004 pcon=5.2809 forget=0.6870 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=7.8582 mle=1.8996 pcon=5.2738 forget=0.6848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=7.6714 mle=1.7131 pcon=5.2670 forget=0.6913 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=7.4515 mle=1.5020 pcon=5.2603 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=7.5046 mle=1.5596 pcon=5.2540 forget=0.6910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=7.6233 mle=1.6809 pcon=5.2476 forget=0.6948 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=7.6174 mle=1.6715 pcon=5.2409 forget=0.7050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=7.5551 mle=1.6370 pcon=5.2346 forget=0.6835 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=7.4397 mle=1.5165 pcon=5.2284 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=7.6975 mle=1.7804 pcon=5.2224 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=7.6740 mle=1.7646 pcon=5.2167 forget=0.6927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=7.5008 mle=1.6054 pcon=5.2112 forget=0.6842 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=7.6189 mle=1.7241 pcon=5.2056 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=7.6878 mle=1.7944 pcon=5.2003 forget=0.6931 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=7.4366 mle=1.5531 pcon=5.1950 forget=0.6884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=7.7572 mle=1.8884 pcon=5.1899 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=7.6143 mle=1.7416 pcon=5.1847 forget=0.6879 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=7.3886 mle=1.5264 pcon=5.1797 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=7.4534 mle=1.5858 pcon=5.1746 forget=0.6930 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=7.7391 mle=1.8838 pcon=5.1699 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=7.4364 mle=1.5924 pcon=5.1652 forget=0.6789 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=7.6543 mle=1.8113 pcon=5.1605 forget=0.6824 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=7.5473 mle=1.7059 pcon=5.1560 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=7.5125 mle=1.6709 pcon=5.1519 forget=0.6897 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=7.5973 mle=1.7668 pcon=5.1475 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=7.7432 mle=1.9185 pcon=5.1436 forget=0.6812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=7.3893 mle=1.5698 pcon=5.1395 forget=0.6800 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=7.6496 mle=1.8361 pcon=5.1354 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=7.5219 mle=1.7030 pcon=5.1312 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=7.5122 mle=1.6954 pcon=5.1276 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=7.4488 mle=1.6539 pcon=5.1237 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=7.5275 mle=1.7296 pcon=5.1197 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=7.4492 mle=1.6500 pcon=5.1161 forget=0.6831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.2491 mle=1.4536 pcon=5.1126 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=7.5538 mle=1.7667 pcon=5.1088 forget=0.6783 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.2731 mle=1.4912 pcon=5.1052 forget=0.6768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=7.6563 mle=1.8719 pcon=5.1018 forget=0.6826 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=7.6446 mle=1.8674 pcon=5.0984 forget=0.6787 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=7.5677 mle=1.7998 pcon=5.0948 forget=0.6731 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=7.3558 mle=1.5840 pcon=5.0917 forget=0.6801 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=7.3630 mle=1.5953 pcon=5.0882 forget=0.6795 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=7.4493 mle=1.6911 pcon=5.0853 forget=0.6729 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=7.5250 mle=1.7732 pcon=5.0823 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=7.3513 mle=1.5964 pcon=5.0793 forget=0.6756 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=7.4977 mle=1.7468 pcon=5.0764 forget=0.6745 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.1606 mle=1.4010 pcon=5.0735 forget=0.6861 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.2708 mle=1.5357 pcon=5.0708 forget=0.6644 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=7.3406 mle=1.5915 pcon=5.0681 forget=0.6809 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=7.4038 mle=1.6653 pcon=5.0654 forget=0.6731 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=7.3639 mle=1.6298 pcon=5.0631 forget=0.6710 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=7.5611 mle=1.8227 pcon=5.0603 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.2749 mle=1.5350 pcon=5.0579 forget=0.6821 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=7.4039 mle=1.6746 pcon=5.0554 forget=0.6739 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=7.3537 mle=1.6310 pcon=5.0528 forget=0.6699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=7.4632 mle=1.7450 pcon=5.0504 forget=0.6677 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=7.3457 mle=1.6266 pcon=5.0479 forget=0.6712 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.2921 mle=1.5820 pcon=5.0455 forget=0.6646 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=7.3732 mle=1.6534 pcon=5.0436 forget=0.6763 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=7.5192 mle=1.7968 pcon=5.0414 forget=0.6810 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=7.4603 mle=1.7413 pcon=5.0390 forget=0.6800 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=7.3506 mle=1.6403 pcon=5.0369 forget=0.6734 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 90%|█████████ | 9/10 [03:49<00:26, 26.69s/it]100%|██████████| 10/10 [04:12<00:00, 25.41s/it]100%|██████████| 10/10 [04:12<00:00, 25.24s/it]
[loss] ep 8 it 30 total=7.2519 mle=1.5364 pcon=5.0350 forget=0.6804 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=7.2980 mle=1.5903 pcon=5.0331 forget=0.6745 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.2093 mle=1.4994 pcon=5.0309 forget=0.6790 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=7.4031 mle=1.7031 pcon=5.0290 forget=0.6710 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.2283 mle=1.5306 pcon=5.0275 forget=0.6702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=7.3684 mle=1.6771 pcon=5.0255 forget=0.6657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=7.4629 mle=1.7556 pcon=5.0235 forget=0.6839 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.1767 mle=1.4883 pcon=5.0216 forget=0.6668 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.3130 mle=1.6356 pcon=5.0196 forget=0.6577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=7.3543 mle=1.6684 pcon=5.0175 forget=0.6683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=7.5325 mle=1.8404 pcon=5.0161 forget=0.6760 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.2612 mle=1.5777 pcon=5.0145 forget=0.6690 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.2496 mle=1.5661 pcon=5.0129 forget=0.6705 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.2435 mle=1.5586 pcon=5.0111 forget=0.6738 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.2411 mle=1.5629 pcon=5.0096 forget=0.6686 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<01:50,  3.53it/s]  3%|▎         | 10/391 [00:00<00:12, 31.57it/s]  4%|▍         | 16/391 [00:00<00:09, 39.99it/s]  6%|▋         | 25/391 [00:00<00:06, 55.00it/s]  9%|▊         | 34/391 [00:00<00:05, 65.11it/s] 11%|█         | 43/391 [00:00<00:04, 71.88it/s] 13%|█▎        | 52/391 [00:00<00:04, 74.89it/s] 15%|█▌        | 60/391 [00:01<00:05, 62.97it/s] 17%|█▋        | 67/391 [00:01<00:05, 60.44it/s] 19%|█▉        | 75/391 [00:01<00:04, 64.75it/s] 21%|██▏       | 84/391 [00:01<00:04, 71.30it/s] 24%|██▍       | 93/391 [00:01<00:03, 76.26it/s] 26%|██▌       | 102/391 [00:01<00:03, 79.92it/s] 28%|██▊       | 111/391 [00:01<00:03, 82.47it/s] 31%|███       | 120/391 [00:01<00:03, 72.50it/s] 33%|███▎      | 128/391 [00:02<00:04, 64.21it/s] 35%|███▍      | 135/391 [00:02<00:03, 64.50it/s] 37%|███▋      | 144/391 [00:02<00:03, 70.77it/s] 39%|███▉      | 153/391 [00:02<00:03, 75.66it/s] 41%|████▏     | 162/391 [00:02<00:02, 79.23it/s] 44%|████▎     | 171/391 [00:02<00:02, 81.89it/s] 46%|████▌     | 180/391 [00:02<00:02, 74.48it/s] 48%|████▊     | 188/391 [00:02<00:03, 67.23it/s] 50%|█████     | 196/391 [00:02<00:03, 63.89it/s] 52%|█████▏    | 205/391 [00:03<00:02, 70.18it/s] 55%|█████▍    | 214/391 [00:03<00:02, 75.08it/s] 57%|█████▋    | 223/391 [00:03<00:02, 78.77it/s] 59%|█████▉    | 232/391 [00:03<00:01, 81.56it/s] 62%|██████▏   | 241/391 [00:03<00:01, 79.24it/s] 64%|██████▍   | 250/391 [00:03<00:02, 66.59it/s] 66%|██████▌   | 258/391 [00:03<00:02, 63.77it/s] 68%|██████▊   | 267/391 [00:03<00:01, 69.86it/s] 71%|███████   | 276/391 [00:04<00:01, 74.82it/s] 73%|███████▎  | 285/391 [00:04<00:01, 78.77it/s] 75%|███████▌  | 294/391 [00:04<00:01, 81.75it/s] 77%|███████▋  | 303/391 [00:04<00:01, 83.56it/s] 80%|███████▉  | 312/391 [00:04<00:01, 69.44it/s] 82%|████████▏ | 320/391 [00:04<00:01, 63.96it/s] 84%|████████▍ | 328/391 [00:04<00:00, 67.29it/s] 86%|████████▌ | 337/391 [00:04<00:00, 73.00it/s] 88%|████████▊ | 346/391 [00:04<00:00, 77.44it/s] 91%|█████████ | 355/391 [00:05<00:00, 80.86it/s] 93%|█████████▎| 364/391 [00:05<00:00, 82.88it/s] 95%|█████████▌| 373/391 [00:05<00:00, 76.82it/s] 97%|█████████▋| 381/391 [00:05<00:00, 66.73it/s] 99%|█████████▉| 389/391 [00:05<00:00, 63.92it/s]100%|██████████| 391/391 [00:05<00:00, 69.18it/s]
50000 images processed, 5.72402811050415 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:21,  3.61it/s]  8%|▊         | 6/79 [00:00<00:03, 18.91it/s] 16%|█▋        | 13/79 [00:00<00:01, 34.73it/s] 24%|██▍       | 19/79 [00:00<00:01, 41.48it/s] 35%|███▌      | 28/79 [00:00<00:00, 55.49it/s] 47%|████▋     | 37/79 [00:00<00:00, 65.18it/s] 58%|█████▊    | 46/79 [00:00<00:00, 71.81it/s] 68%|██████▊   | 54/79 [00:01<00:00, 72.25it/s] 78%|███████▊  | 62/79 [00:01<00:00, 61.51it/s] 87%|████████▋ | 69/79 [00:01<00:00, 59.56it/s] 99%|█████████▊| 78/79 [00:01<00:00, 66.24it/s]100%|██████████| 79/79 [00:01<00:00, 54.67it/s]
10000 images processed, 1.4686994552612305 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:15,  2.71it/s]  5%|▍         | 10/204 [00:00<00:07, 26.80it/s]  9%|▉         | 19/204 [00:00<00:04, 44.14it/s] 13%|█▎        | 26/204 [00:00<00:03, 47.87it/s] 16%|█▌        | 33/204 [00:00<00:03, 48.54it/s] 19%|█▉        | 39/204 [00:00<00:03, 49.26it/s] 24%|██▎       | 48/204 [00:01<00:02, 59.24it/s] 28%|██▊       | 57/204 [00:01<00:02, 66.90it/s] 32%|███▏      | 66/204 [00:01<00:01, 72.61it/s] 37%|███▋      | 75/204 [00:01<00:01, 76.82it/s] 41%|████      | 84/204 [00:01<00:01, 61.75it/s] 45%|████▍     | 91/204 [00:01<00:01, 59.75it/s] 49%|████▉     | 100/204 [00:01<00:01, 66.26it/s] 53%|█████▎    | 109/204 [00:01<00:01, 71.45it/s] 58%|█████▊    | 118/204 [00:02<00:01, 75.46it/s] 62%|██████▏   | 127/204 [00:02<00:00, 77.24it/s] 66%|██████▌   | 135/204 [00:02<00:01, 62.68it/s] 70%|██████▉   | 142/204 [00:02<00:01, 60.31it/s] 74%|███████▍  | 151/204 [00:02<00:00, 67.13it/s] 78%|███████▊  | 160/204 [00:02<00:00, 72.21it/s] 83%|████████▎ | 169/204 [00:02<00:00, 76.05it/s] 87%|████████▋ | 178/204 [00:02<00:00, 77.14it/s] 91%|█████████ | 186/204 [00:03<00:00, 62.90it/s] 95%|█████████▍| 193/204 [00:03<00:00, 60.50it/s] 99%|█████████▉| 202/204 [00:03<00:00, 67.33it/s]100%|██████████| 204/204 [00:03<00:00, 61.35it/s]
26032 images processed, 3.37117600440979 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:41,  1.87it/s] 10%|█         | 8/79 [00:00<00:04, 15.91it/s] 19%|█▉        | 15/79 [00:00<00:02, 27.97it/s] 27%|██▋       | 21/79 [00:00<00:01, 29.00it/s] 33%|███▎      | 26/79 [00:01<00:01, 33.09it/s] 42%|████▏     | 33/79 [00:01<00:01, 37.47it/s] 48%|████▊     | 38/79 [00:01<00:01, 38.66it/s] 57%|█████▋    | 45/79 [00:01<00:00, 43.90it/s] 66%|██████▌   | 52/79 [00:01<00:00, 49.03it/s] 73%|███████▎  | 58/79 [00:01<00:00, 43.56it/s] 80%|███████▉  | 63/79 [00:01<00:00, 44.89it/s] 86%|████████▌ | 68/79 [00:01<00:00, 45.24it/s] 96%|█████████▌| 76/79 [00:02<00:00, 44.48it/s]100%|██████████| 79/79 [00:02<00:00, 36.27it/s]
10000 images processed, 2.2184462547302246 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:31,  2.47it/s]  9%|▉         | 7/79 [00:00<00:04, 17.15it/s] 20%|██        | 16/79 [00:00<00:01, 36.33it/s] 32%|███▏      | 25/79 [00:00<00:01, 50.64it/s] 43%|████▎     | 34/79 [00:00<00:00, 61.04it/s] 54%|█████▍    | 43/79 [00:00<00:00, 68.70it/s] 65%|██████▍   | 51/79 [00:01<00:00, 61.25it/s] 73%|███████▎  | 58/79 [00:01<00:00, 60.49it/s] 82%|████████▏ | 65/79 [00:01<00:00, 59.45it/s] 94%|█████████▎| 74/79 [00:01<00:00, 67.03it/s]100%|██████████| 79/79 [00:01<00:00, 53.41it/s]
10000 images processed, 1.4989633560180664 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:27,  2.53it/s] 13%|█▎        | 9/70 [00:00<00:02, 22.44it/s] 20%|██        | 14/70 [00:00<00:01, 28.37it/s] 29%|██▊       | 20/70 [00:00<00:01, 35.22it/s] 39%|███▊      | 27/70 [00:00<00:00, 44.28it/s] 51%|█████▏    | 36/70 [00:00<00:00, 56.57it/s] 64%|██████▍   | 45/70 [00:01<00:00, 65.63it/s] 77%|███████▋  | 54/70 [00:01<00:00, 71.53it/s] 89%|████████▊ | 62/70 [00:01<00:00, 71.98it/s]100%|██████████| 70/70 [00:01<00:00, 60.29it/s]100%|██████████| 70/70 [00:01<00:00, 48.43it/s]
8925 images processed, 1.473717451095581 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:41,  1.06it/s]  9%|▉         | 4/45 [00:01<00:08,  4.77it/s] 20%|██        | 9/45 [00:01<00:04,  8.39it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.93it/s] 36%|███▌      | 16/45 [00:01<00:01, 14.62it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.93it/s] 49%|████▉     | 22/45 [00:02<00:01, 14.08it/s] 56%|█████▌    | 25/45 [00:02<00:01, 10.43it/s] 67%|██████▋   | 30/45 [00:02<00:00, 15.28it/s] 73%|███████▎  | 33/45 [00:03<00:01,  9.21it/s] 91%|█████████ | 41/45 [00:04<00:00, 11.14it/s]100%|██████████| 45/45 [00:04<00:00, 11.00it/s]
5640 images processed, 4.117511987686157 seconds used

21.607064247131348
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.84  81.18
LSUN          17.47  96.10
iSUN          72.34  81.67
dtd           37.82  91.38
forget        79.50  86.88
AVG           46.25  89.43
Retain-Acc: 0.7408
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.88 AUIN: 98.36
10.311003923416138
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.1 lr=0.0005 epochs=20
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.1, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:26<08:31, 26.91s/it] 10%|█         | 2/20 [00:50<07:27, 24.85s/it] 15%|█▌        | 3/20 [01:13<06:53, 24.31s/it] 20%|██        | 4/20 [01:40<06:45, 25.35s/it] 25%|██▌       | 5/20 [02:05<06:17, 25.15s/it] 30%|███       | 6/20 [02:30<05:49, 24.98s/it] 35%|███▌      | 7/20 [02:58<05:39, 26.10s/it] 40%|████      | 8/20 [03:23<05:08, 25.73s/it][loss] ep 0 it 0 total=7.5537 mle=1.5709 pcon=5.2950 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=7.5309 mle=1.5423 pcon=5.2879 forget=0.7007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=7.6683 mle=1.7004 pcon=5.2809 forget=0.6870 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=7.8582 mle=1.8996 pcon=5.2738 forget=0.6848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=7.6714 mle=1.7131 pcon=5.2670 forget=0.6913 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=7.4515 mle=1.5020 pcon=5.2603 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=7.5046 mle=1.5596 pcon=5.2540 forget=0.6910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=7.6233 mle=1.6809 pcon=5.2476 forget=0.6948 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=7.6174 mle=1.6715 pcon=5.2409 forget=0.7050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=7.5551 mle=1.6370 pcon=5.2346 forget=0.6835 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=7.4397 mle=1.5165 pcon=5.2284 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=7.6975 mle=1.7804 pcon=5.2224 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=7.6739 mle=1.7646 pcon=5.2167 forget=0.6927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=7.5007 mle=1.6054 pcon=5.2112 forget=0.6842 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=7.6187 mle=1.7239 pcon=5.2056 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=7.6876 mle=1.7943 pcon=5.2003 forget=0.6931 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=7.4364 mle=1.5530 pcon=5.1950 forget=0.6884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=7.7573 mle=1.8885 pcon=5.1899 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=7.6143 mle=1.7416 pcon=5.1847 forget=0.6879 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=7.3885 mle=1.5264 pcon=5.1797 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=7.4532 mle=1.5857 pcon=5.1746 forget=0.6930 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=7.7392 mle=1.8839 pcon=5.1699 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=7.4364 mle=1.5924 pcon=5.1652 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=7.6543 mle=1.8113 pcon=5.1605 forget=0.6824 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=7.5474 mle=1.7060 pcon=5.1560 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=7.5126 mle=1.6710 pcon=5.1519 forget=0.6897 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=7.5974 mle=1.7669 pcon=5.1475 forget=0.6830 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=7.7433 mle=1.9185 pcon=5.1436 forget=0.6812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=7.3893 mle=1.5697 pcon=5.1395 forget=0.6800 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=7.6496 mle=1.8361 pcon=5.1354 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=7.5218 mle=1.7029 pcon=5.1312 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=7.5122 mle=1.6954 pcon=5.1276 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=7.4488 mle=1.6539 pcon=5.1237 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=7.5275 mle=1.7297 pcon=5.1197 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=7.4494 mle=1.6502 pcon=5.1161 forget=0.6831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.2491 mle=1.4536 pcon=5.1126 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=7.5537 mle=1.7666 pcon=5.1088 forget=0.6783 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.2735 mle=1.4915 pcon=5.1052 forget=0.6768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=7.6562 mle=1.8719 pcon=5.1018 forget=0.6826 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=7.6445 mle=1.8673 pcon=5.0984 forget=0.6787 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=7.5676 mle=1.7997 pcon=5.0948 forget=0.6731 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=7.3560 mle=1.5842 pcon=5.0917 forget=0.6801 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=7.3631 mle=1.5954 pcon=5.0882 forget=0.6794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=7.4494 mle=1.6911 pcon=5.0853 forget=0.6729 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=7.5253 mle=1.7735 pcon=5.0823 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=7.3513 mle=1.5964 pcon=5.0793 forget=0.6756 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=7.4978 mle=1.7469 pcon=5.0764 forget=0.6745 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.1605 mle=1.4008 pcon=5.0735 forget=0.6861 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.2711 mle=1.5359 pcon=5.0707 forget=0.6644 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=7.3405 mle=1.5915 pcon=5.0681 forget=0.6809 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=7.4037 mle=1.6652 pcon=5.0654 forget=0.6731 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=7.3641 mle=1.6300 pcon=5.0631 forget=0.6710 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=7.5612 mle=1.8228 pcon=5.0602 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.2749 mle=1.5350 pcon=5.0579 forget=0.6821 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=7.4036 mle=1.6743 pcon=5.0554 forget=0.6739 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=7.3537 mle=1.6310 pcon=5.0528 forget=0.6699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=7.4631 mle=1.7450 pcon=5.0504 forget=0.6677 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=7.3457 mle=1.6267 pcon=5.0479 forget=0.6712 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.2923 mle=1.5821 pcon=5.0455 forget=0.6646 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=7.3734 mle=1.6536 pcon=5.0435 forget=0.6763 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=7.5191 mle=1.7967 pcon=5.0414 forget=0.6810 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=7.4603 mle=1.7413 pcon=5.0390 forget=0.6800 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=7.3506 mle=1.6403 pcon=5.0369 forget=0.6734 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 45%|████▌     | 9/20 [03:46<04:33, 24.82s/it] 50%|█████     | 10/20 [04:10<04:04, 24.43s/it] 55%|█████▌    | 11/20 [04:35<03:42, 24.75s/it] 60%|██████    | 12/20 [05:00<03:17, 24.72s/it] 65%|██████▌   | 13/20 [05:26<02:55, 25.08s/it] 70%|███████   | 14/20 [05:52<02:32, 25.47s/it] 75%|███████▌  | 15/20 [06:18<02:07, 25.50s/it] 80%|████████  | 16/20 [06:46<01:45, 26.50s/it][loss] ep 8 it 30 total=7.2519 mle=1.5365 pcon=5.0350 forget=0.6804 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=7.2979 mle=1.5902 pcon=5.0331 forget=0.6745 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.2094 mle=1.4994 pcon=5.0309 forget=0.6790 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=7.4029 mle=1.7029 pcon=5.0290 forget=0.6710 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.2282 mle=1.5305 pcon=5.0275 forget=0.6702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=7.3684 mle=1.6771 pcon=5.0255 forget=0.6657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=7.4629 mle=1.7556 pcon=5.0235 forget=0.6839 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.1767 mle=1.4883 pcon=5.0216 forget=0.6668 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.3130 mle=1.6356 pcon=5.0196 forget=0.6577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=7.3544 mle=1.6685 pcon=5.0175 forget=0.6683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=7.5325 mle=1.8404 pcon=5.0161 forget=0.6760 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.2611 mle=1.5777 pcon=5.0145 forget=0.6690 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.2497 mle=1.5662 pcon=5.0129 forget=0.6705 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.2434 mle=1.5586 pcon=5.0111 forget=0.6738 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.2410 mle=1.5629 pcon=5.0096 forget=0.6686 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=7.2878 mle=1.6063 pcon=5.0078 forget=0.6736 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=7.2744 mle=1.5897 pcon=5.0061 forget=0.6786 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=7.4278 mle=1.7542 pcon=5.0046 forget=0.6690 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=7.5172 mle=1.8487 pcon=5.0032 forget=0.6652 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=7.5254 mle=1.8578 pcon=5.0017 forget=0.6658 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=7.2521 mle=1.5822 pcon=5.0004 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=7.2237 mle=1.5621 pcon=4.9992 forget=0.6624 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=7.3450 mle=1.6675 pcon=4.9978 forget=0.6796 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=7.2461 mle=1.5743 pcon=4.9964 forget=0.6754 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=7.2823 mle=1.6213 pcon=4.9950 forget=0.6660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=7.2255 mle=1.5690 pcon=4.9938 forget=0.6627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=7.3250 mle=1.6642 pcon=4.9921 forget=0.6687 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=7.4446 mle=1.7810 pcon=4.9909 forget=0.6727 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=7.2732 mle=1.6116 pcon=4.9901 forget=0.6716 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=7.6279 mle=1.9686 pcon=4.9887 forget=0.6706 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=7.3167 mle=1.6699 pcon=4.9874 forget=0.6595 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 12 it 20 total=7.2739 mle=1.6199 pcon=4.9860 forget=0.6681 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=7.3489 mle=1.6937 pcon=4.9848 forget=0.6704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=7.2067 mle=1.5473 pcon=4.9836 forget=0.6758 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=7.3937 mle=1.7481 pcon=4.9825 forget=0.6630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=7.2441 mle=1.5980 pcon=4.9812 forget=0.6649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=7.1874 mle=1.5329 pcon=4.9804 forget=0.6742 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=7.1657 mle=1.5172 pcon=4.9793 forget=0.6692 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=7.5134 mle=1.8609 pcon=4.9782 forget=0.6743 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=7.0755 mle=1.4288 pcon=4.9772 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=7.1549 mle=1.4928 pcon=4.9761 forget=0.6860 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=7.2131 mle=1.5689 pcon=4.9749 forget=0.6693 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=7.1904 mle=1.5504 pcon=4.9740 forget=0.6660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=7.2877 mle=1.6431 pcon=4.9731 forget=0.6716 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=7.2163 mle=1.5811 pcon=4.9721 forget=0.6632 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=7.2284 mle=1.5871 pcon=4.9710 forget=0.6702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=7.2123 mle=1.5766 pcon=4.9701 forget=0.6657 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=7.3960 mle=1.7663 pcon=4.9689 forget=0.6608 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=7.4043 mle=1.7662 pcon=4.9682 forget=0.6699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=7.2436 mle=1.6097 pcon=4.9678 forget=0.6661 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=7.5156 mle=1.8809 pcon=4.9667 forget=0.6680 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=7.2466 mle=1.6148 pcon=4.9660 forget=0.6657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=7.2930 mle=1.6674 pcon=4.9651 forget=0.6604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=7.3063 mle=1.6806 pcon=4.9642 forget=0.6614 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=7.3245 mle=1.6857 pcon=4.9635 forget=0.6753 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=7.1145 mle=1.4902 pcon=4.9626 forget=0.6617 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=7.1870 mle=1.5558 pcon=4.9619 forget=0.6693 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=7.3603 mle=1.7229 pcon=4.9611 forget=0.6763 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=7.2368 mle=1.6147 pcon=4.9604 forget=0.6617 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=7.5111 mle=1.8828 pcon=4.9595 forget=0.6688 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=7.2313 mle=1.6021 pcon=4.9590 forget=0.6702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=7.1695 mle=1.5408 pcon=4.9582 forget=0.6705 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 16 it 10 total=7.2122 mle=1.5843 pcon=4.9575 forget=0.6704 nr=64 nf=64 protos=540 fproto_sim=NA
 85%|████████▌ | 17/20 [07:17<01:22, 27.64s/it] 90%|█████████ | 18/20 [07:42<00:54, 27.04s/it] 95%|█████████▌| 19/20 [08:07<00:26, 26.45s/it]100%|██████████| 20/20 [08:34<00:00, 26.36s/it]100%|██████████| 20/20 [08:34<00:00, 25.70s/it]
[loss] ep 16 it 60 total=7.1127 mle=1.4925 pcon=4.9568 forget=0.6633 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=7.1339 mle=1.5129 pcon=4.9563 forget=0.6646 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=7.3155 mle=1.7006 pcon=4.9555 forget=0.6594 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=7.1943 mle=1.5821 pcon=4.9547 forget=0.6575 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=7.5573 mle=1.9442 pcon=4.9538 forget=0.6593 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=7.4042 mle=1.7867 pcon=4.9532 forget=0.6644 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=7.2258 mle=1.6125 pcon=4.9525 forget=0.6608 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 17 it 20 total=7.3945 mle=1.7753 pcon=4.9515 forget=0.6677 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=7.4331 mle=1.8239 pcon=4.9512 forget=0.6580 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=7.1894 mle=1.5742 pcon=4.9504 forget=0.6648 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=7.1945 mle=1.5805 pcon=4.9494 forget=0.6647 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=7.2823 mle=1.6649 pcon=4.9486 forget=0.6687 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=7.0790 mle=1.4652 pcon=4.9485 forget=0.6652 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=7.4116 mle=1.8022 pcon=4.9479 forget=0.6615 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=7.3513 mle=1.7457 pcon=4.9470 forget=0.6586 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=7.1965 mle=1.5923 pcon=4.9469 forget=0.6572 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=7.3046 mle=1.6945 pcon=4.9464 forget=0.6637 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=7.2721 mle=1.6613 pcon=4.9458 forget=0.6650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=7.1604 mle=1.5500 pcon=4.9453 forget=0.6651 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=7.1936 mle=1.5914 pcon=4.9448 forget=0.6575 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=7.2912 mle=1.6855 pcon=4.9444 forget=0.6614 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=7.4107 mle=1.8070 pcon=4.9439 forget=0.6597 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=7.3077 mle=1.6986 pcon=4.9437 forget=0.6654 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 19 it 40 total=7.0733 mle=1.4640 pcon=4.9435 forget=0.6658 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=7.3247 mle=1.7110 pcon=4.9434 forget=0.6702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=7.2123 mle=1.6084 pcon=4.9429 forget=0.6610 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=7.2400 mle=1.6428 pcon=4.9424 forget=0.6548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=7.4541 mle=1.8509 pcon=4.9420 forget=0.6612 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=7.1639 mle=1.5540 pcon=4.9419 forget=0.6680 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=7.2156 mle=1.5961 pcon=4.9418 forget=0.6777 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:12,  2.95it/s]  2%|▏         | 7/391 [00:00<00:19, 19.56it/s]  3%|▎         | 12/391 [00:00<00:13, 28.06it/s]  5%|▌         | 21/391 [00:00<00:08, 45.45it/s]  8%|▊         | 30/391 [00:00<00:06, 58.07it/s] 10%|▉         | 39/391 [00:00<00:05, 66.87it/s] 12%|█▏        | 48/391 [00:00<00:04, 73.05it/s] 15%|█▍        | 57/391 [00:01<00:04, 77.50it/s] 17%|█▋        | 66/391 [00:01<00:04, 68.87it/s] 19%|█▉        | 74/391 [00:01<00:04, 64.26it/s] 21%|██        | 81/391 [00:01<00:04, 62.90it/s] 23%|██▎       | 90/391 [00:01<00:04, 69.46it/s] 25%|██▌       | 99/391 [00:01<00:03, 74.55it/s] 28%|██▊       | 108/391 [00:01<00:03, 78.28it/s] 30%|██▉       | 117/391 [00:01<00:03, 81.05it/s] 32%|███▏      | 126/391 [00:01<00:03, 83.10it/s] 35%|███▍      | 135/391 [00:02<00:03, 67.74it/s] 37%|███▋      | 143/391 [00:02<00:04, 62.00it/s] 39%|███▉      | 152/391 [00:02<00:03, 67.86it/s] 41%|████      | 161/391 [00:02<00:03, 72.82it/s] 43%|████▎     | 170/391 [00:02<00:02, 76.92it/s] 46%|████▌     | 179/391 [00:02<00:02, 79.89it/s] 48%|████▊     | 188/391 [00:02<00:02, 78.92it/s] 50%|█████     | 197/391 [00:03<00:02, 65.65it/s] 52%|█████▏    | 205/391 [00:03<00:02, 63.04it/s] 55%|█████▍    | 214/391 [00:03<00:02, 69.20it/s] 57%|█████▋    | 223/391 [00:03<00:02, 74.01it/s] 59%|█████▉    | 232/391 [00:03<00:02, 77.77it/s] 62%|██████▏   | 241/391 [00:03<00:01, 80.46it/s] 64%|██████▍   | 250/391 [00:03<00:01, 73.40it/s] 66%|██████▌   | 258/391 [00:03<00:01, 66.62it/s] 68%|██████▊   | 265/391 [00:04<00:01, 63.43it/s] 70%|███████   | 274/391 [00:04<00:01, 69.43it/s] 72%|███████▏  | 283/391 [00:04<00:01, 74.55it/s] 75%|███████▍  | 292/391 [00:04<00:01, 78.40it/s] 77%|███████▋  | 301/391 [00:04<00:01, 81.35it/s] 79%|███████▉  | 310/391 [00:04<00:01, 69.19it/s] 81%|████████▏ | 318/391 [00:04<00:01, 64.74it/s] 83%|████████▎ | 325/391 [00:04<00:01, 65.55it/s] 85%|████████▌ | 334/391 [00:04<00:00, 71.01it/s] 88%|████████▊ | 343/391 [00:05<00:00, 75.43it/s] 90%|█████████ | 352/391 [00:05<00:00, 79.03it/s] 92%|█████████▏| 361/391 [00:05<00:00, 78.77it/s] 95%|█████████▍| 370/391 [00:05<00:00, 65.05it/s] 97%|█████████▋| 378/391 [00:05<00:00, 62.83it/s] 99%|█████████▉| 388/391 [00:05<00:00, 69.91it/s]100%|██████████| 391/391 [00:05<00:00, 67.70it/s]
50000 images processed, 5.849204063415527 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:21,  3.55it/s]  9%|▉         | 7/79 [00:00<00:03, 21.61it/s] 16%|█▋        | 13/79 [00:00<00:02, 32.79it/s] 28%|██▊       | 22/79 [00:00<00:01, 49.60it/s] 39%|███▉      | 31/79 [00:00<00:00, 61.05it/s] 51%|█████     | 40/79 [00:00<00:00, 69.19it/s] 61%|██████    | 48/79 [00:00<00:00, 64.47it/s] 70%|██████▉   | 55/79 [00:01<00:00, 60.77it/s] 78%|███████▊  | 62/79 [00:01<00:00, 58.89it/s] 90%|████████▉ | 71/79 [00:01<00:00, 66.76it/s]100%|██████████| 79/79 [00:01<00:00, 55.89it/s]
10000 images processed, 1.437049388885498 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:06,  3.04it/s]  5%|▍         | 10/204 [00:00<00:06, 28.75it/s]  9%|▉         | 19/204 [00:00<00:03, 46.47it/s] 14%|█▎        | 28/204 [00:00<00:02, 58.69it/s] 18%|█▊        | 36/204 [00:00<00:03, 51.65it/s] 21%|██        | 43/204 [00:00<00:03, 50.94it/s] 25%|██▌       | 52/204 [00:01<00:02, 59.99it/s] 30%|██▉       | 61/204 [00:01<00:02, 67.15it/s] 34%|███▍      | 70/204 [00:01<00:01, 72.69it/s] 39%|███▊      | 79/204 [00:01<00:01, 76.70it/s] 43%|████▎     | 88/204 [00:01<00:01, 63.04it/s] 47%|████▋     | 95/204 [00:01<00:01, 59.12it/s] 51%|█████     | 104/204 [00:01<00:01, 65.29it/s] 55%|█████▌    | 113/204 [00:01<00:01, 70.50it/s] 60%|█████▉    | 122/204 [00:02<00:01, 74.31it/s] 64%|██████▎   | 130/204 [00:02<00:01, 63.09it/s] 67%|██████▋   | 137/204 [00:02<00:01, 60.99it/s] 71%|███████   | 144/204 [00:02<00:00, 62.85it/s] 75%|███████▌  | 153/204 [00:02<00:00, 69.34it/s] 79%|███████▉  | 162/204 [00:02<00:00, 74.11it/s] 84%|████████▍ | 171/204 [00:02<00:00, 77.94it/s] 88%|████████▊ | 179/204 [00:02<00:00, 76.21it/s] 92%|█████████▏| 187/204 [00:03<00:00, 63.69it/s] 95%|█████████▌| 194/204 [00:03<00:00, 60.97it/s]100%|█████████▉| 203/204 [00:03<00:00, 67.92it/s]100%|██████████| 204/204 [00:03<00:00, 62.07it/s]
26032 images processed, 3.326967239379883 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:46,  1.67it/s]  8%|▊         | 6/79 [00:00<00:06, 10.58it/s] 13%|█▎        | 10/79 [00:00<00:04, 16.37it/s] 22%|██▏       | 17/79 [00:00<00:02, 28.32it/s] 32%|███▏      | 25/79 [00:01<00:01, 36.10it/s] 38%|███▊      | 30/79 [00:01<00:01, 36.49it/s] 44%|████▍     | 35/79 [00:01<00:01, 38.64it/s] 53%|█████▎    | 42/79 [00:01<00:00, 38.28it/s] 59%|█████▉    | 47/79 [00:01<00:00, 40.72it/s] 66%|██████▌   | 52/79 [00:01<00:00, 42.31it/s] 73%|███████▎  | 58/79 [00:01<00:00, 42.35it/s] 80%|███████▉  | 63/79 [00:01<00:00, 42.23it/s] 86%|████████▌ | 68/79 [00:02<00:00, 43.16it/s] 94%|█████████▎| 74/79 [00:02<00:00, 46.77it/s]100%|██████████| 79/79 [00:02<00:00, 35.08it/s]
10000 images processed, 2.283045768737793 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:26,  3.00it/s]  8%|▊         | 6/79 [00:00<00:04, 16.30it/s] 14%|█▍        | 11/79 [00:00<00:02, 25.85it/s] 20%|██        | 16/79 [00:00<00:01, 32.71it/s] 32%|███▏      | 25/79 [00:00<00:01, 48.32it/s] 43%|████▎     | 34/79 [00:00<00:00, 59.84it/s] 54%|█████▍    | 43/79 [00:00<00:00, 68.16it/s] 66%|██████▌   | 52/79 [00:01<00:00, 73.91it/s] 76%|███████▌  | 60/79 [00:01<00:00, 70.89it/s] 86%|████████▌ | 68/79 [00:01<00:00, 62.46it/s] 95%|█████████▍| 75/79 [00:01<00:00, 59.95it/s]100%|██████████| 79/79 [00:01<00:00, 51.87it/s]
10000 images processed, 1.5444939136505127 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:22,  3.12it/s] 11%|█▏        | 8/70 [00:00<00:02, 23.52it/s] 24%|██▍       | 17/70 [00:00<00:01, 43.26it/s] 37%|███▋      | 26/70 [00:00<00:00, 56.78it/s] 50%|█████     | 35/70 [00:00<00:00, 66.16it/s] 61%|██████▏   | 43/70 [00:00<00:00, 67.92it/s] 73%|███████▎  | 51/70 [00:01<00:00, 58.06it/s] 83%|████████▎ | 58/70 [00:01<00:00, 56.87it/s] 96%|█████████▌| 67/70 [00:01<00:00, 64.88it/s]100%|██████████| 70/70 [00:01<00:00, 54.02it/s]
8925 images processed, 1.3290975093841553 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:43,  1.02it/s]  7%|▋         | 3/45 [00:01<00:12,  3.41it/s] 20%|██        | 9/45 [00:01<00:04,  8.44it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.79it/s] 36%|███▌      | 16/45 [00:01<00:02, 14.29it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.47it/s] 49%|████▉     | 22/45 [00:02<00:01, 13.98it/s] 56%|█████▌    | 25/45 [00:02<00:01, 14.09it/s] 60%|██████    | 27/45 [00:02<00:01, 12.18it/s] 67%|██████▋   | 30/45 [00:02<00:01, 14.41it/s] 71%|███████   | 32/45 [00:02<00:00, 15.26it/s] 76%|███████▌  | 34/45 [00:03<00:01,  8.81it/s] 89%|████████▉ | 40/45 [00:03<00:00, 15.51it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.52it/s]100%|██████████| 45/45 [00:04<00:00, 10.56it/s]
5640 images processed, 4.281657457351685 seconds used

21.660104513168335
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
Evaluating forget
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.51  99.36
places365     67.95  81.18
LSUN          17.35  96.12
iSUN          72.30  81.63
dtd           37.84  91.36
forget        79.50  86.85
AVG           46.24  89.42
Retain-Acc: 0.7409
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.85 AUIN: 98.35
22.174927473068237
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.1 lr=0.0005 epochs=50
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.0005, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.1, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:31<25:45, 31.54s/it]  4%|▍         | 2/50 [00:56<22:11, 27.74s/it]  6%|▌         | 3/50 [01:21<20:40, 26.40s/it]  8%|▊         | 4/50 [01:50<21:10, 27.62s/it] 10%|█         | 5/50 [02:19<21:02, 28.06s/it] 12%|█▏        | 6/50 [02:45<19:57, 27.21s/it] 14%|█▍        | 7/50 [03:12<19:35, 27.34s/it] 16%|█▌        | 8/50 [03:40<19:11, 27.42s/it][loss] ep 0 it 0 total=7.5536 mle=1.5708 pcon=5.2951 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=7.5312 mle=1.5427 pcon=5.2879 forget=0.7006 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=7.6680 mle=1.7001 pcon=5.2809 forget=0.6870 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=7.8582 mle=1.8995 pcon=5.2738 forget=0.6848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=7.6715 mle=1.7131 pcon=5.2670 forget=0.6913 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=7.4513 mle=1.5018 pcon=5.2603 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=7.5044 mle=1.5594 pcon=5.2540 forget=0.6910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=7.6234 mle=1.6809 pcon=5.2476 forget=0.6948 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=7.6173 mle=1.6714 pcon=5.2409 forget=0.7050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=7.5554 mle=1.6373 pcon=5.2346 forget=0.6835 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=7.4398 mle=1.5166 pcon=5.2285 forget=0.6948 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=7.6973 mle=1.7802 pcon=5.2224 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=7.6740 mle=1.7646 pcon=5.2167 forget=0.6927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=7.5010 mle=1.6055 pcon=5.2112 forget=0.6842 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=7.6187 mle=1.7239 pcon=5.2056 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=7.6875 mle=1.7941 pcon=5.2003 forget=0.6931 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=7.4364 mle=1.5529 pcon=5.1951 forget=0.6884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=7.7570 mle=1.8882 pcon=5.1899 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=7.6143 mle=1.7416 pcon=5.1847 forget=0.6879 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=7.3887 mle=1.5266 pcon=5.1797 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=7.4534 mle=1.5858 pcon=5.1746 forget=0.6930 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=7.7390 mle=1.8836 pcon=5.1699 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=7.4363 mle=1.5923 pcon=5.1652 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=7.6542 mle=1.8112 pcon=5.1605 forget=0.6824 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=7.5471 mle=1.7057 pcon=5.1560 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=7.5122 mle=1.6706 pcon=5.1519 forget=0.6897 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=7.5973 mle=1.7668 pcon=5.1476 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=7.7432 mle=1.9185 pcon=5.1436 forget=0.6812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=7.3894 mle=1.5699 pcon=5.1396 forget=0.6800 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=7.6497 mle=1.8361 pcon=5.1354 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=7.5220 mle=1.7031 pcon=5.1312 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=7.5125 mle=1.6957 pcon=5.1276 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=7.4489 mle=1.6541 pcon=5.1238 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=7.5280 mle=1.7300 pcon=5.1197 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=7.4492 mle=1.6500 pcon=5.1161 forget=0.6831 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.2493 mle=1.4537 pcon=5.1126 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=7.5543 mle=1.7672 pcon=5.1088 forget=0.6783 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.2732 mle=1.4912 pcon=5.1052 forget=0.6768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=7.6562 mle=1.8718 pcon=5.1018 forget=0.6826 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=7.6445 mle=1.8673 pcon=5.0985 forget=0.6787 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=7.5675 mle=1.7996 pcon=5.0949 forget=0.6731 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=7.3559 mle=1.5840 pcon=5.0917 forget=0.6801 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=7.3630 mle=1.5953 pcon=5.0882 forget=0.6795 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=7.4495 mle=1.6912 pcon=5.0853 forget=0.6730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=7.5253 mle=1.7734 pcon=5.0823 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=7.3513 mle=1.5963 pcon=5.0793 forget=0.6756 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=7.4976 mle=1.7467 pcon=5.0764 forget=0.6745 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.1606 mle=1.4009 pcon=5.0736 forget=0.6861 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.2706 mle=1.5354 pcon=5.0708 forget=0.6644 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=7.3405 mle=1.5914 pcon=5.0681 forget=0.6809 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=7.4040 mle=1.6655 pcon=5.0654 forget=0.6731 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=7.3642 mle=1.6301 pcon=5.0631 forget=0.6710 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=7.5612 mle=1.8228 pcon=5.0603 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.2750 mle=1.5350 pcon=5.0579 forget=0.6820 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=7.4040 mle=1.6747 pcon=5.0554 forget=0.6739 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=7.3538 mle=1.6311 pcon=5.0528 forget=0.6699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=7.4632 mle=1.7450 pcon=5.0504 forget=0.6677 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=7.3458 mle=1.6267 pcon=5.0479 forget=0.6712 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.2923 mle=1.5821 pcon=5.0455 forget=0.6646 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=7.3732 mle=1.6533 pcon=5.0436 forget=0.6763 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=7.5178 mle=1.7954 pcon=5.0414 forget=0.6810 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=7.4603 mle=1.7413 pcon=5.0390 forget=0.6800 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=7.3503 mle=1.6400 pcon=5.0369 forget=0.6734 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 18%|█▊        | 9/50 [04:07<18:33, 27.17s/it] 20%|██        | 10/50 [04:33<17:53, 26.84s/it] 22%|██▏       | 11/50 [04:59<17:16, 26.57s/it] 24%|██▍       | 12/50 [05:27<17:13, 27.20s/it] 26%|██▌       | 13/50 [05:56<17:06, 27.76s/it] 28%|██▊       | 14/50 [06:23<16:25, 27.37s/it] 30%|███       | 15/50 [06:50<15:56, 27.34s/it] 32%|███▏      | 16/50 [07:16<15:10, 26.78s/it][loss] ep 8 it 30 total=7.2519 mle=1.5364 pcon=5.0350 forget=0.6804 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=7.2980 mle=1.5904 pcon=5.0331 forget=0.6745 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.2096 mle=1.4997 pcon=5.0309 forget=0.6790 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=7.4029 mle=1.7029 pcon=5.0290 forget=0.6710 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.2283 mle=1.5306 pcon=5.0275 forget=0.6702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=7.3686 mle=1.6773 pcon=5.0256 forget=0.6657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=7.4631 mle=1.7558 pcon=5.0235 forget=0.6839 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.1767 mle=1.4882 pcon=5.0216 forget=0.6669 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.3130 mle=1.6356 pcon=5.0197 forget=0.6577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=7.3545 mle=1.6686 pcon=5.0176 forget=0.6683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=7.5327 mle=1.8406 pcon=5.0161 forget=0.6760 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.2610 mle=1.5775 pcon=5.0145 forget=0.6690 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.2497 mle=1.5663 pcon=5.0130 forget=0.6705 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.2434 mle=1.5585 pcon=5.0111 forget=0.6738 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.2361 mle=1.5579 pcon=5.0096 forget=0.6686 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=7.2880 mle=1.6066 pcon=5.0078 forget=0.6736 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=7.2746 mle=1.5900 pcon=5.0061 forget=0.6785 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=7.4272 mle=1.7536 pcon=5.0046 forget=0.6690 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=7.5171 mle=1.8486 pcon=5.0032 forget=0.6652 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=7.5253 mle=1.8577 pcon=5.0017 forget=0.6658 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=7.2528 mle=1.5828 pcon=5.0004 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=7.2237 mle=1.5621 pcon=4.9992 forget=0.6624 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=7.3449 mle=1.6675 pcon=4.9978 forget=0.6796 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=7.2463 mle=1.5745 pcon=4.9964 forget=0.6754 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=7.2822 mle=1.6212 pcon=4.9950 forget=0.6660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=7.2259 mle=1.5694 pcon=4.9938 forget=0.6627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=7.3249 mle=1.6641 pcon=4.9921 forget=0.6687 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=7.4447 mle=1.7811 pcon=4.9909 forget=0.6727 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=7.2626 mle=1.6009 pcon=4.9901 forget=0.6716 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=7.6253 mle=1.9660 pcon=4.9887 forget=0.6706 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=7.3168 mle=1.6699 pcon=4.9874 forget=0.6594 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 12 it 20 total=7.2737 mle=1.6196 pcon=4.9860 forget=0.6681 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=7.3489 mle=1.6937 pcon=4.9848 forget=0.6704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=7.2066 mle=1.5472 pcon=4.9836 forget=0.6758 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=7.3914 mle=1.7459 pcon=4.9825 forget=0.6630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=7.2442 mle=1.5981 pcon=4.9812 forget=0.6649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=7.1872 mle=1.5326 pcon=4.9804 forget=0.6742 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=7.1658 mle=1.5173 pcon=4.9793 forget=0.6692 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=7.5122 mle=1.8598 pcon=4.9782 forget=0.6742 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=7.0753 mle=1.4287 pcon=4.9772 forget=0.6694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=7.1547 mle=1.4926 pcon=4.9761 forget=0.6860 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=7.2126 mle=1.5685 pcon=4.9748 forget=0.6692 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=7.1904 mle=1.5505 pcon=4.9740 forget=0.6659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=7.2876 mle=1.6431 pcon=4.9731 forget=0.6714 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=7.2161 mle=1.5811 pcon=4.9720 forget=0.6631 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=7.2281 mle=1.5870 pcon=4.9710 forget=0.6701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=7.2117 mle=1.5762 pcon=4.9700 forget=0.6655 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=7.3967 mle=1.7673 pcon=4.9688 forget=0.6607 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=7.4037 mle=1.7658 pcon=4.9681 forget=0.6698 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=7.2423 mle=1.6088 pcon=4.9676 forget=0.6659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=7.5145 mle=1.8803 pcon=4.9665 forget=0.6677 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=7.2458 mle=1.6145 pcon=4.9658 forget=0.6655 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=7.2918 mle=1.6668 pcon=4.9649 forget=0.6601 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=7.3060 mle=1.6809 pcon=4.9640 forget=0.6612 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=7.3234 mle=1.6851 pcon=4.9633 forget=0.6750 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=7.1135 mle=1.4898 pcon=4.9623 forget=0.6614 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=7.1857 mle=1.5554 pcon=4.9616 forget=0.6688 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=7.3584 mle=1.7219 pcon=4.9607 forget=0.6758 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=7.2349 mle=1.6138 pcon=4.9600 forget=0.6612 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=7.5127 mle=1.8853 pcon=4.9591 forget=0.6683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=7.2294 mle=1.6013 pcon=4.9585 forget=0.6696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=7.1665 mle=1.5390 pcon=4.9577 forget=0.6698 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 16 it 10 total=7.2099 mle=1.5833 pcon=4.9569 forget=0.6696 nr=64 nf=64 protos=540 fproto_sim=NA
 34%|███▍      | 17/50 [07:45<15:12, 27.64s/it] 36%|███▌      | 18/50 [08:11<14:30, 27.21s/it] 38%|███▊      | 19/50 [08:37<13:48, 26.74s/it] 40%|████      | 20/50 [09:09<14:12, 28.41s/it] 42%|████▏     | 21/50 [09:35<13:19, 27.56s/it] 44%|████▍     | 22/50 [10:04<13:03, 28.00s/it] 46%|████▌     | 23/50 [10:30<12:21, 27.46s/it] 48%|████▊     | 24/50 [10:56<11:37, 26.84s/it][loss] ep 16 it 60 total=7.1111 mle=1.4923 pcon=4.9562 forget=0.6626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=7.1322 mle=1.5128 pcon=4.9557 forget=0.6637 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=7.3135 mle=1.7000 pcon=4.9548 forget=0.6587 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=7.1938 mle=1.5834 pcon=4.9538 forget=0.6566 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=7.5538 mle=1.9426 pcon=4.9530 forget=0.6582 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=7.4008 mle=1.7852 pcon=4.9522 forget=0.6634 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=7.2217 mle=1.6101 pcon=4.9515 forget=0.6601 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 17 it 20 total=7.3908 mle=1.7738 pcon=4.9504 forget=0.6666 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=7.4278 mle=1.8211 pcon=4.9500 forget=0.6567 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=7.1865 mle=1.5739 pcon=4.9491 forget=0.6635 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=7.1835 mle=1.5724 pcon=4.9480 forget=0.6631 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=7.2789 mle=1.6643 pcon=4.9472 forget=0.6674 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=7.0727 mle=1.4622 pcon=4.9470 forget=0.6636 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=7.4048 mle=1.7987 pcon=4.9462 forget=0.6599 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=7.3471 mle=1.7447 pcon=4.9452 forget=0.6571 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=7.1890 mle=1.5892 pcon=4.9450 forget=0.6549 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=7.2978 mle=1.6911 pcon=4.9443 forget=0.6624 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=7.2736 mle=1.6667 pcon=4.9436 forget=0.6633 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=7.1548 mle=1.5489 pcon=4.9429 forget=0.6629 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=7.1899 mle=1.5919 pcon=4.9423 forget=0.6557 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=7.2829 mle=1.6818 pcon=4.9417 forget=0.6594 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=7.3976 mle=1.7991 pcon=4.9410 forget=0.6575 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=7.2997 mle=1.6958 pcon=4.9407 forget=0.6633 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 19 it 40 total=7.0704 mle=1.4667 pcon=4.9403 forget=0.6633 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=7.3182 mle=1.7099 pcon=4.9401 forget=0.6682 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=7.2010 mle=1.6026 pcon=4.9394 forget=0.6590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=7.2280 mle=1.6366 pcon=4.9387 forget=0.6527 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=7.4492 mle=1.8533 pcon=4.9381 forget=0.6577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=7.1512 mle=1.5484 pcon=4.9378 forget=0.6650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=7.2055 mle=1.5933 pcon=4.9374 forget=0.6748 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 20 it 0 total=7.2475 mle=1.6540 pcon=4.9368 forget=0.6567 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=7.2795 mle=1.6885 pcon=4.9362 forget=0.6548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=7.3211 mle=1.7270 pcon=4.9358 forget=0.6583 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=7.4217 mle=1.8345 pcon=4.9351 forget=0.6521 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=7.2437 mle=1.6598 pcon=4.9342 forget=0.6497 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=7.6077 mle=2.0194 pcon=4.9334 forget=0.6548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=7.2568 mle=1.6648 pcon=4.9326 forget=0.6594 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=6.9902 mle=1.4022 pcon=4.9319 forget=0.6561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 10 total=6.9888 mle=1.3996 pcon=4.9312 forget=0.6581 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=7.2715 mle=1.6766 pcon=4.9305 forget=0.6643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=7.1009 mle=1.5055 pcon=4.9300 forget=0.6654 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=7.1343 mle=1.5540 pcon=4.9296 forget=0.6507 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=7.3671 mle=1.7771 pcon=4.9291 forget=0.6610 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=7.2569 mle=1.6714 pcon=4.9284 forget=0.6570 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=7.2075 mle=1.6195 pcon=4.9276 forget=0.6605 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=7.2287 mle=1.6375 pcon=4.9269 forget=0.6643 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 22 it 20 total=7.2352 mle=1.6522 pcon=4.9260 forget=0.6570 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=7.3829 mle=1.8051 pcon=4.9252 forget=0.6526 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=7.1065 mle=1.5191 pcon=4.9243 forget=0.6632 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=7.2302 mle=1.6481 pcon=4.9237 forget=0.6585 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=7.2552 mle=1.6695 pcon=4.9231 forget=0.6627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=7.0986 mle=1.5109 pcon=4.9224 forget=0.6653 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=7.2135 mle=1.6376 pcon=4.9218 forget=0.6541 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=7.6377 mle=2.0594 pcon=4.9209 forget=0.6574 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 23 it 30 total=7.1181 mle=1.5384 pcon=4.9207 forget=0.6591 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=7.2700 mle=1.6939 pcon=4.9199 forget=0.6562 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=7.1365 mle=1.5510 pcon=4.9191 forget=0.6664 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=7.3243 mle=1.7524 pcon=4.9183 forget=0.6536 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=7.2332 mle=1.6569 pcon=4.9178 forget=0.6585 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=7.1214 mle=1.5422 pcon=4.9169 forget=0.6622 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=7.2212 mle=1.6512 pcon=4.9162 forget=0.6538 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=7.1018 mle=1.5256 pcon=4.9154 forget=0.6608 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 24 it 40 total=7.0869 mle=1.5207 pcon=4.9148 forget=0.6514 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=7.1792 mle=1.6082 pcon=4.9141 forget=0.6568 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=6.9982 mle=1.4209 pcon=4.9133 forget=0.6640 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=7.2105 mle=1.6419 pcon=4.9128 forget=0.6558 nr=64 nf=64 protos=540 fproto_sim=NA
 50%|█████     | 25/50 [11:24<11:21, 27.26s/it] 52%|█████▏    | 26/50 [11:51<10:53, 27.24s/it] 54%|█████▍    | 27/50 [12:18<10:26, 27.25s/it] 56%|█████▌    | 28/50 [12:47<10:10, 27.77s/it] 58%|█████▊    | 29/50 [13:13<09:28, 27.05s/it] 60%|██████    | 30/50 [13:40<09:04, 27.24s/it] 62%|██████▏   | 31/50 [14:07<08:32, 26.96s/it] 64%|██████▍   | 32/50 [14:31<07:52, 26.22s/it] 66%|██████▌   | 33/50 [14:57<07:21, 25.98s/it][loss] ep 24 it 240 total=7.1598 mle=1.5875 pcon=4.9121 forget=0.6601 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=7.3627 mle=1.7875 pcon=4.9114 forget=0.6638 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=7.2726 mle=1.7076 pcon=4.9106 forget=0.6543 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 25 it 0 total=7.2259 mle=1.6576 pcon=4.9097 forget=0.6586 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=7.3794 mle=1.8023 pcon=4.9091 forget=0.6679 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=7.1757 mle=1.6170 pcon=4.9083 forget=0.6504 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 150 total=7.3440 mle=1.7817 pcon=4.9080 forget=0.6544 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 200 total=7.3889 mle=1.8213 pcon=4.9072 forget=0.6604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=7.3237 mle=1.7562 pcon=4.9064 forget=0.6611 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=7.1560 mle=1.5921 pcon=4.9056 forget=0.6583 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=7.1608 mle=1.5958 pcon=4.9051 forget=0.6599 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 26 it 10 total=7.0861 mle=1.5175 pcon=4.9045 forget=0.6642 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=7.1135 mle=1.5428 pcon=4.9038 forget=0.6669 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=7.3471 mle=1.7903 pcon=4.9030 forget=0.6538 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=7.0301 mle=1.4724 pcon=4.9022 forget=0.6554 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=7.2157 mle=1.6533 pcon=4.9012 forget=0.6612 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=7.0981 mle=1.5336 pcon=4.9005 forget=0.6640 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 310 total=7.4475 mle=1.8903 pcon=4.8999 forget=0.6574 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=7.1354 mle=1.5789 pcon=4.8987 forget=0.6578 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 20 total=7.2318 mle=1.6720 pcon=4.8978 forget=0.6620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=7.1065 mle=1.5485 pcon=4.8967 forget=0.6613 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=7.0890 mle=1.5289 pcon=4.8957 forget=0.6643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=7.1238 mle=1.5623 pcon=4.8947 forget=0.6669 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=7.1271 mle=1.5709 pcon=4.8937 forget=0.6625 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=7.1806 mle=1.6273 pcon=4.8926 forget=0.6607 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=7.2970 mle=1.7431 pcon=4.8917 forget=0.6622 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=7.3149 mle=1.7537 pcon=4.8908 forget=0.6704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 30 total=7.2399 mle=1.6849 pcon=4.8899 forget=0.6651 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=7.2808 mle=1.7276 pcon=4.8892 forget=0.6640 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=7.0844 mle=1.5291 pcon=4.8884 forget=0.6669 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=7.0219 mle=1.4772 pcon=4.8875 forget=0.6571 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=6.9286 mle=1.3836 pcon=4.8863 forget=0.6586 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=7.0899 mle=1.5360 pcon=4.8852 forget=0.6687 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=7.2916 mle=1.7424 pcon=4.8838 forget=0.6654 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=7.2955 mle=1.7427 pcon=4.8826 forget=0.6702 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 29 it 40 total=7.2941 mle=1.7478 pcon=4.8813 forget=0.6650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=7.0883 mle=1.5490 pcon=4.8800 forget=0.6592 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=7.2008 mle=1.6568 pcon=4.8791 forget=0.6649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=7.1239 mle=1.5804 pcon=4.8780 forget=0.6655 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=7.0345 mle=1.4915 pcon=4.8769 forget=0.6660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=7.3845 mle=1.8501 pcon=4.8760 forget=0.6585 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=7.1487 mle=1.6080 pcon=4.8750 forget=0.6657 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 0 total=7.1588 mle=1.6240 pcon=4.8740 forget=0.6607 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=7.1760 mle=1.6334 pcon=4.8731 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=7.0040 mle=1.4671 pcon=4.8720 forget=0.6649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=7.3693 mle=1.8337 pcon=4.8708 forget=0.6648 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=7.0163 mle=1.4638 pcon=4.8697 forget=0.6828 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=7.2909 mle=1.7533 pcon=4.8685 forget=0.6690 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=7.0382 mle=1.4988 pcon=4.8675 forget=0.6719 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=7.4848 mle=1.9478 pcon=4.8664 forget=0.6706 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 31 it 10 total=7.1236 mle=1.5910 pcon=4.8654 forget=0.6673 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=7.0800 mle=1.5478 pcon=4.8644 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=7.1141 mle=1.5785 pcon=4.8634 forget=0.6723 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=7.0470 mle=1.5146 pcon=4.8623 forget=0.6701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=7.0837 mle=1.5575 pcon=4.8612 forget=0.6650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 260 total=6.9906 mle=1.4680 pcon=4.8599 forget=0.6627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=7.1323 mle=1.6024 pcon=4.8587 forget=0.6712 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=7.0642 mle=1.5440 pcon=4.8575 forget=0.6627 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 32 it 20 total=7.2627 mle=1.7406 pcon=4.8563 forget=0.6658 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=7.1181 mle=1.6028 pcon=4.8548 forget=0.6605 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=7.0629 mle=1.5315 pcon=4.8536 forget=0.6779 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 170 total=7.0825 mle=1.5592 pcon=4.8526 forget=0.6707 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=7.2643 mle=1.7491 pcon=4.8515 forget=0.6636 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=7.0768 mle=1.5608 pcon=4.8505 forget=0.6655 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=6.9757 mle=1.4608 pcon=4.8493 forget=0.6656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=6.9335 mle=1.4167 pcon=4.8484 forget=0.6684 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 68%|██████▊   | 34/50 [15:25<07:06, 26.64s/it] 70%|███████   | 35/50 [15:51<06:39, 26.62s/it] 72%|███████▏  | 36/50 [16:17<06:08, 26.34s/it] 74%|███████▍  | 37/50 [16:48<05:59, 27.67s/it] 76%|███████▌  | 38/50 [17:15<05:30, 27.55s/it] 78%|███████▊  | 39/50 [17:41<04:59, 27.19s/it] 80%|████████  | 40/50 [18:07<04:26, 26.66s/it] 82%|████████▏ | 41/50 [18:31<03:53, 26.00s/it][loss] ep 33 it 30 total=7.1694 mle=1.6488 pcon=4.8473 forget=0.6732 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=7.5280 mle=2.0221 pcon=4.8462 forget=0.6597 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=6.9831 mle=1.4682 pcon=4.8450 forget=0.6699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=7.3131 mle=1.7932 pcon=4.8438 forget=0.6760 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=7.0652 mle=1.5508 pcon=4.8427 forget=0.6718 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=7.1361 mle=1.6255 pcon=4.8415 forget=0.6690 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=7.1305 mle=1.6242 pcon=4.8406 forget=0.6656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 380 total=7.0747 mle=1.5581 pcon=4.8394 forget=0.6772 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 34 it 40 total=7.2226 mle=1.7125 pcon=4.8380 forget=0.6721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=6.9219 mle=1.4144 pcon=4.8366 forget=0.6709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 140 total=7.0923 mle=1.5967 pcon=4.8355 forget=0.6600 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=7.1675 mle=1.6661 pcon=4.8344 forget=0.6670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 240 total=7.2841 mle=1.7835 pcon=4.8331 forget=0.6675 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=7.0796 mle=1.5760 pcon=4.8316 forget=0.6719 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=7.0465 mle=1.5391 pcon=4.8305 forget=0.6770 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 35 it 0 total=7.1781 mle=1.6821 pcon=4.8292 forget=0.6668 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 50 total=7.0122 mle=1.5165 pcon=4.8281 forget=0.6676 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 100 total=7.3287 mle=1.8292 pcon=4.8267 forget=0.6729 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 150 total=7.1089 mle=1.6195 pcon=4.8255 forget=0.6639 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 200 total=7.0851 mle=1.5817 pcon=4.8244 forget=0.6790 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 250 total=7.1845 mle=1.6887 pcon=4.8231 forget=0.6727 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 300 total=6.9501 mle=1.4634 pcon=4.8219 forget=0.6649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 350 total=7.2514 mle=1.7607 pcon=4.8207 forget=0.6699 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 36 it 10 total=7.3062 mle=1.8129 pcon=4.8196 forget=0.6737 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 60 total=7.0028 mle=1.5177 pcon=4.8186 forget=0.6665 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 110 total=7.1246 mle=1.6363 pcon=4.8174 forget=0.6709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 160 total=7.1345 mle=1.6498 pcon=4.8162 forget=0.6684 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 210 total=6.9486 mle=1.4674 pcon=4.8152 forget=0.6660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 260 total=7.0423 mle=1.5492 pcon=4.8144 forget=0.6787 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 310 total=7.1844 mle=1.6899 pcon=4.8131 forget=0.6814 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 360 total=7.0628 mle=1.5765 pcon=4.8119 forget=0.6743 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 37 it 20 total=6.9391 mle=1.4569 pcon=4.8105 forget=0.6717 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 70 total=7.2117 mle=1.7248 pcon=4.8095 forget=0.6774 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 120 total=7.0014 mle=1.5166 pcon=4.8082 forget=0.6765 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 170 total=7.0923 mle=1.6140 pcon=4.8070 forget=0.6713 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 220 total=7.2243 mle=1.7395 pcon=4.8056 forget=0.6791 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 270 total=7.0867 mle=1.6153 pcon=4.8045 forget=0.6668 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 320 total=7.0935 mle=1.6179 pcon=4.8033 forget=0.6723 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 370 total=7.0590 mle=1.5698 pcon=4.8024 forget=0.6868 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 38 it 30 total=7.1478 mle=1.6639 pcon=4.8015 forget=0.6824 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 80 total=6.9053 mle=1.4319 pcon=4.8005 forget=0.6729 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 130 total=7.1400 mle=1.6657 pcon=4.7994 forget=0.6748 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 180 total=7.0576 mle=1.5841 pcon=4.7984 forget=0.6751 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 230 total=7.0050 mle=1.5303 pcon=4.7973 forget=0.6774 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 280 total=7.1375 mle=1.6689 pcon=4.7964 forget=0.6722 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 330 total=7.0687 mle=1.5996 pcon=4.7951 forget=0.6740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 380 total=6.9368 mle=1.4655 pcon=4.7943 forget=0.6770 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 39 it 40 total=7.2819 mle=1.8143 pcon=4.7936 forget=0.6739 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 90 total=7.1655 mle=1.6936 pcon=4.7926 forget=0.6792 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 140 total=7.1350 mle=1.6717 pcon=4.7918 forget=0.6716 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 190 total=6.9423 mle=1.4781 pcon=4.7909 forget=0.6733 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 240 total=7.1369 mle=1.6723 pcon=4.7900 forget=0.6746 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 290 total=6.9014 mle=1.4368 pcon=4.7891 forget=0.6754 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 340 total=7.0878 mle=1.6236 pcon=4.7883 forget=0.6759 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 40 it 0 total=7.0632 mle=1.5940 pcon=4.7873 forget=0.6819 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 50 total=7.0656 mle=1.5994 pcon=4.7862 forget=0.6800 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 100 total=7.0600 mle=1.6065 pcon=4.7851 forget=0.6684 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 150 total=6.9156 mle=1.4590 pcon=4.7844 forget=0.6723 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 200 total=7.2339 mle=1.7695 pcon=4.7834 forget=0.6810 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 250 total=7.2667 mle=1.8072 pcon=4.7825 forget=0.6770 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 300 total=6.9515 mle=1.4945 pcon=4.7818 forget=0.6752 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 350 total=6.8587 mle=1.4054 pcon=4.7810 forget=0.6722 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 41 it 10 total=6.9696 mle=1.5207 pcon=4.7802 forget=0.6687 nr=64 nf=64 protos=540 fproto_sim=NA
 84%|████████▍ | 42/50 [18:58<03:28, 26.10s/it] 86%|████████▌ | 43/50 [19:24<03:04, 26.29s/it] 88%|████████▊ | 44/50 [19:51<02:38, 26.45s/it] 90%|█████████ | 45/50 [20:19<02:14, 26.82s/it] 92%|█████████▏| 46/50 [20:44<01:45, 26.39s/it] 94%|█████████▍| 47/50 [21:13<01:20, 26.97s/it] 96%|█████████▌| 48/50 [21:39<00:53, 26.96s/it] 98%|█████████▊| 49/50 [22:05<00:26, 26.65s/it][loss] ep 41 it 60 total=7.3744 mle=1.9108 pcon=4.7796 forget=0.6840 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 110 total=7.0175 mle=1.5600 pcon=4.7786 forget=0.6789 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 160 total=6.9685 mle=1.5166 pcon=4.7780 forget=0.6739 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 210 total=7.0328 mle=1.5796 pcon=4.7773 forget=0.6759 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 260 total=6.9084 mle=1.4641 pcon=4.7769 forget=0.6674 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 310 total=6.9475 mle=1.5001 pcon=4.7764 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 360 total=7.0606 mle=1.6121 pcon=4.7759 forget=0.6726 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 42 it 20 total=7.1080 mle=1.6561 pcon=4.7753 forget=0.6767 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 70 total=7.0652 mle=1.6153 pcon=4.7744 forget=0.6756 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 120 total=7.3965 mle=1.9406 pcon=4.7735 forget=0.6824 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 170 total=7.0301 mle=1.5772 pcon=4.7730 forget=0.6799 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 220 total=7.0616 mle=1.6117 pcon=4.7724 forget=0.6775 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 270 total=7.2016 mle=1.7549 pcon=4.7718 forget=0.6749 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 320 total=7.1168 mle=1.6720 pcon=4.7713 forget=0.6735 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 370 total=7.0790 mle=1.6328 pcon=4.7708 forget=0.6754 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 43 it 30 total=7.0533 mle=1.6010 pcon=4.7701 forget=0.6821 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 80 total=7.0118 mle=1.5620 pcon=4.7695 forget=0.6803 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 130 total=7.1282 mle=1.6764 pcon=4.7689 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 180 total=6.9536 mle=1.5006 pcon=4.7684 forget=0.6845 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 230 total=6.9848 mle=1.5454 pcon=4.7677 forget=0.6716 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 280 total=6.9733 mle=1.5328 pcon=4.7673 forget=0.6733 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 330 total=6.9587 mle=1.5165 pcon=4.7665 forget=0.6756 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 380 total=6.9375 mle=1.5001 pcon=4.7659 forget=0.6716 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 44 it 40 total=7.1679 mle=1.7216 pcon=4.7653 forget=0.6810 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 90 total=6.9738 mle=1.5378 pcon=4.7647 forget=0.6713 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 140 total=7.0719 mle=1.6225 pcon=4.7643 forget=0.6851 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 190 total=6.8939 mle=1.4535 pcon=4.7638 forget=0.6766 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 240 total=6.9678 mle=1.5168 pcon=4.7633 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 290 total=7.1965 mle=1.7559 pcon=4.7627 forget=0.6779 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 340 total=7.2930 mle=1.8533 pcon=4.7623 forget=0.6774 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 45 it 0 total=6.9631 mle=1.5219 pcon=4.7617 forget=0.6794 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 50 total=7.0392 mle=1.6072 pcon=4.7613 forget=0.6707 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 100 total=6.9785 mle=1.5427 pcon=4.7607 forget=0.6750 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 150 total=6.9645 mle=1.5266 pcon=4.7602 forget=0.6777 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 200 total=7.0237 mle=1.5825 pcon=4.7598 forget=0.6814 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 250 total=6.9570 mle=1.5228 pcon=4.7593 forget=0.6748 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 300 total=7.0729 mle=1.6421 pcon=4.7587 forget=0.6722 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 350 total=6.9457 mle=1.5110 pcon=4.7583 forget=0.6764 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 10 total=6.8940 mle=1.4544 pcon=4.7579 forget=0.6817 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 60 total=6.9131 mle=1.4807 pcon=4.7575 forget=0.6749 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 110 total=7.0856 mle=1.6483 pcon=4.7570 forget=0.6803 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 160 total=6.9138 mle=1.4818 pcon=4.7565 forget=0.6755 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 210 total=7.1607 mle=1.7322 pcon=4.7560 forget=0.6725 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 260 total=7.0903 mle=1.6642 pcon=4.7555 forget=0.6705 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 310 total=6.9603 mle=1.5269 pcon=4.7552 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 360 total=7.1264 mle=1.6927 pcon=4.7548 forget=0.6789 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 47 it 20 total=6.9633 mle=1.5282 pcon=4.7542 forget=0.6809 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 70 total=6.9857 mle=1.5538 pcon=4.7538 forget=0.6781 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 120 total=7.1269 mle=1.6901 pcon=4.7532 forget=0.6836 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 170 total=6.9485 mle=1.5102 pcon=4.7526 forget=0.6856 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 220 total=6.8819 mle=1.4468 pcon=4.7522 forget=0.6828 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 270 total=7.0240 mle=1.5904 pcon=4.7519 forget=0.6817 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 320 total=7.0116 mle=1.5816 pcon=4.7515 forget=0.6785 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 370 total=7.0688 mle=1.6385 pcon=4.7511 forget=0.6792 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 30 total=7.0619 mle=1.6201 pcon=4.7509 forget=0.6909 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 80 total=7.0113 mle=1.5754 pcon=4.7505 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 130 total=7.0665 mle=1.6377 pcon=4.7502 forget=0.6786 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 180 total=7.0559 mle=1.6224 pcon=4.7500 forget=0.6834 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 230 total=7.0837 mle=1.6646 pcon=4.7498 forget=0.6693 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 280 total=6.9518 mle=1.5303 pcon=4.7494 forget=0.6721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 330 total=6.9671 mle=1.5519 pcon=4.7490 forget=0.6663 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 380 total=6.9727 mle=1.5497 pcon=4.7485 forget=0.6745 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 49 it 40 total=7.0016 mle=1.5775 pcon=4.7483 forget=0.6758 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 90 total=7.0451 mle=1.6208 pcon=4.7480 forget=0.6762 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 140 total=7.0244 mle=1.5980 pcon=4.7477 forget=0.6787 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 190 total=7.1382 mle=1.7162 pcon=4.7472 forget=0.6748 nr=64 nf=64 protos=540 fproto_sim=NA
100%|██████████| 50/50 [22:31<00:00, 26.28s/it]100%|██████████| 50/50 [22:31<00:00, 27.03s/it]
[loss] ep 49 it 240 total=7.0921 mle=1.6722 pcon=4.7468 forget=0.6731 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 290 total=7.0176 mle=1.5966 pcon=4.7464 forget=0.6746 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 340 total=6.9016 mle=1.4800 pcon=4.7463 forget=0.6753 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:19,  2.79it/s]  2%|▏         | 6/391 [00:00<00:24, 15.89it/s]  3%|▎         | 12/391 [00:00<00:13, 27.61it/s]  5%|▌         | 21/391 [00:00<00:08, 43.95it/s]  8%|▊         | 30/391 [00:00<00:06, 56.79it/s] 10%|▉         | 39/391 [00:00<00:05, 65.97it/s] 12%|█▏        | 48/391 [00:00<00:04, 72.77it/s] 14%|█▍        | 56/391 [00:01<00:05, 63.18it/s] 16%|█▌        | 63/391 [00:01<00:05, 61.69it/s] 18%|█▊        | 70/391 [00:01<00:05, 60.77it/s] 20%|██        | 79/391 [00:01<00:04, 68.26it/s] 23%|██▎       | 88/391 [00:01<00:04, 73.98it/s] 25%|██▍       | 97/391 [00:01<00:03, 78.29it/s] 27%|██▋       | 106/391 [00:01<00:03, 81.45it/s] 29%|██▉       | 115/391 [00:01<00:03, 75.51it/s] 31%|███▏      | 123/391 [00:02<00:04, 66.76it/s] 33%|███▎      | 130/391 [00:02<00:04, 63.28it/s] 36%|███▌      | 139/391 [00:02<00:03, 69.66it/s] 38%|███▊      | 148/391 [00:02<00:03, 74.75it/s] 40%|████      | 157/391 [00:02<00:02, 78.63it/s] 42%|████▏     | 166/391 [00:02<00:02, 81.41it/s] 45%|████▍     | 175/391 [00:02<00:02, 73.50it/s] 47%|████▋     | 183/391 [00:02<00:03, 64.58it/s] 49%|████▉     | 191/391 [00:03<00:02, 68.05it/s] 51%|█████     | 200/391 [00:03<00:02, 73.51it/s] 53%|█████▎    | 209/391 [00:03<00:02, 77.83it/s] 56%|█████▌    | 218/391 [00:03<00:02, 81.09it/s] 58%|█████▊    | 227/391 [00:03<00:02, 73.20it/s] 60%|██████    | 235/391 [00:03<00:02, 73.91it/s] 63%|██████▎   | 245/391 [00:03<00:01, 80.11it/s] 65%|██████▌   | 255/391 [00:03<00:01, 84.78it/s] 68%|██████▊   | 265/391 [00:03<00:01, 88.01it/s] 70%|███████   | 275/391 [00:04<00:01, 90.35it/s] 73%|███████▎  | 285/391 [00:04<00:01, 90.61it/s] 75%|███████▌  | 295/391 [00:04<00:01, 92.13it/s] 78%|███████▊  | 305/391 [00:04<00:00, 93.32it/s] 81%|████████  | 315/391 [00:04<00:00, 94.19it/s] 83%|████████▎ | 325/391 [00:04<00:00, 94.60it/s] 86%|████████▌ | 335/391 [00:04<00:00, 94.76it/s] 88%|████████▊ | 345/391 [00:04<00:00, 95.09it/s] 91%|█████████ | 355/391 [00:04<00:00, 95.17it/s] 93%|█████████▎| 365/391 [00:04<00:00, 95.29it/s] 96%|█████████▌| 375/391 [00:05<00:00, 94.79it/s] 98%|█████████▊| 385/391 [00:05<00:00, 95.53it/s]100%|██████████| 391/391 [00:05<00:00, 74.61it/s]
50000 images processed, 5.355364799499512 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:23,  3.34it/s] 13%|█▎        | 10/79 [00:00<00:02, 31.20it/s] 24%|██▍       | 19/79 [00:00<00:01, 49.45it/s] 35%|███▌      | 28/79 [00:00<00:00, 58.28it/s] 46%|████▌     | 36/79 [00:00<00:00, 53.33it/s] 54%|█████▍    | 43/79 [00:00<00:00, 54.32it/s] 66%|██████▌   | 52/79 [00:01<00:00, 62.06it/s] 77%|███████▋  | 61/79 [00:01<00:00, 69.16it/s] 89%|████████▊ | 70/79 [00:01<00:00, 74.74it/s]100%|██████████| 79/79 [00:01<00:00, 77.91it/s]100%|██████████| 79/79 [00:01<00:00, 59.16it/s]
10000 images processed, 1.3563456535339355 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:02,  3.27it/s]  3%|▎         | 6/204 [00:00<00:11, 17.10it/s]  6%|▌         | 12/204 [00:00<00:06, 29.66it/s]  9%|▉         | 19/204 [00:00<00:04, 40.63it/s] 14%|█▎        | 28/204 [00:00<00:03, 54.17it/s] 18%|█▊        | 37/204 [00:00<00:02, 63.89it/s] 23%|██▎       | 46/204 [00:00<00:02, 70.73it/s] 27%|██▋       | 55/204 [00:01<00:01, 75.57it/s] 31%|███       | 63/204 [00:01<00:02, 61.56it/s] 34%|███▍      | 70/204 [00:01<00:02, 57.34it/s] 39%|███▊      | 79/204 [00:01<00:01, 64.56it/s] 43%|████▎     | 88/204 [00:01<00:01, 70.50it/s] 48%|████▊     | 97/204 [00:01<00:01, 74.88it/s] 52%|█████▏    | 106/204 [00:01<00:01, 78.13it/s] 56%|█████▋    | 115/204 [00:01<00:01, 75.80it/s] 60%|██████    | 123/204 [00:02<00:01, 62.52it/s] 64%|██████▎   | 130/204 [00:02<00:01, 60.44it/s] 68%|██████▊   | 139/204 [00:02<00:00, 67.21it/s] 73%|███████▎  | 148/204 [00:02<00:00, 72.53it/s] 77%|███████▋  | 157/204 [00:02<00:00, 76.59it/s] 81%|████████▏ | 166/204 [00:02<00:00, 79.43it/s] 86%|████████▌ | 175/204 [00:02<00:00, 72.55it/s] 90%|████████▉ | 183/204 [00:02<00:00, 62.05it/s] 93%|█████████▎| 190/204 [00:03<00:00, 62.71it/s] 98%|█████████▊| 199/204 [00:03<00:00, 69.18it/s]100%|██████████| 204/204 [00:03<00:00, 62.78it/s]
26032 images processed, 3.297290563583374 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:38,  2.04it/s] 11%|█▏        | 9/79 [00:00<00:04, 16.80it/s] 19%|█▉        | 15/79 [00:00<00:02, 25.74it/s] 28%|██▊       | 22/79 [00:00<00:01, 35.26it/s] 34%|███▍      | 27/79 [00:01<00:01, 34.44it/s] 41%|████      | 32/79 [00:01<00:01, 36.68it/s] 47%|████▋     | 37/79 [00:01<00:01, 38.75it/s] 54%|█████▍    | 43/79 [00:01<00:00, 38.12it/s] 61%|██████    | 48/79 [00:01<00:00, 39.17it/s] 67%|██████▋   | 53/79 [00:01<00:00, 40.23it/s] 75%|███████▍  | 59/79 [00:01<00:00, 37.14it/s] 82%|████████▏ | 65/79 [00:01<00:00, 41.43it/s] 90%|████████▉ | 71/79 [00:02<00:00, 45.83it/s] 96%|█████████▌| 76/79 [00:02<00:00, 42.50it/s]100%|██████████| 79/79 [00:02<00:00, 35.08it/s]
10000 images processed, 2.2838597297668457 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:30,  2.58it/s]  9%|▉         | 7/79 [00:00<00:04, 17.79it/s] 20%|██        | 16/79 [00:00<00:01, 37.26it/s] 32%|███▏      | 25/79 [00:00<00:01, 51.31it/s] 43%|████▎     | 34/79 [00:00<00:00, 61.54it/s] 53%|█████▎    | 42/79 [00:00<00:00, 58.01it/s] 62%|██████▏   | 49/79 [00:01<00:00, 54.40it/s] 71%|███████   | 56/79 [00:01<00:00, 56.49it/s] 82%|████████▏ | 65/79 [00:01<00:00, 64.41it/s] 94%|█████████▎| 74/79 [00:01<00:00, 70.96it/s]100%|██████████| 79/79 [00:01<00:00, 53.61it/s]
10000 images processed, 1.4983000755310059 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:26,  2.60it/s]  9%|▊         | 6/70 [00:00<00:04, 15.18it/s] 17%|█▋        | 12/70 [00:00<00:02, 26.34it/s] 30%|███       | 21/70 [00:00<00:01, 43.46it/s] 43%|████▎     | 30/70 [00:00<00:00, 56.09it/s] 56%|█████▌    | 39/70 [00:00<00:00, 65.22it/s] 69%|██████▊   | 48/70 [00:01<00:00, 71.60it/s] 80%|████████  | 56/70 [00:01<00:00, 63.00it/s] 90%|█████████ | 63/70 [00:01<00:00, 57.94it/s]100%|██████████| 70/70 [00:01<00:00, 60.35it/s]100%|██████████| 70/70 [00:01<00:00, 48.99it/s]
8925 images processed, 1.4609973430633545 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:39,  1.13it/s]  4%|▍         | 2/45 [00:00<00:18,  2.34it/s] 18%|█▊        | 8/45 [00:01<00:03, 11.82it/s] 27%|██▋       | 12/45 [00:01<00:03,  9.65it/s] 38%|███▊      | 17/45 [00:01<00:02, 13.46it/s] 44%|████▍     | 20/45 [00:02<00:02, 11.45it/s] 56%|█████▌    | 25/45 [00:02<00:01, 15.87it/s] 62%|██████▏   | 28/45 [00:02<00:01, 12.54it/s] 69%|██████▉   | 31/45 [00:02<00:00, 14.60it/s] 76%|███████▌  | 34/45 [00:03<00:01, 10.11it/s] 89%|████████▉ | 40/45 [00:03<00:00, 15.94it/s] 96%|█████████▌| 43/45 [00:04<00:00,  9.70it/s]100%|██████████| 45/45 [00:04<00:00, 10.83it/s]
5640 images processed, 4.176763534545898 seconds used

21.205464839935303
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           4.79  98.96
places365     77.97  78.26
LSUN          30.69  94.29
iSUN          81.22  78.28
dtd           46.99  89.48
AVG           48.33  87.85
Retain-Acc: 0.7411
Forget-as-OOD (retain known vs forget novel):
  FPR: 55.80 AUROC: 89.04 AUIN: 98.60
18.29291343688965
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.0005-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.1 lr=0.0001 epochs=5
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.1, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:28<01:55, 28.91s/it] 40%|████      | 2/5 [00:53<01:19, 26.55s/it] 60%|██████    | 3/5 [01:25<00:57, 28.70s/it] 80%|████████  | 4/5 [01:52<00:28, 28.21s/it]100%|██████████| 5/5 [02:17<00:00, 27.09s/it]100%|██████████| 5/5 [02:17<00:00, 27.52s/it]
[loss] ep 0 it 0 total=7.5533 mle=1.5706 pcon=5.2950 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=7.5307 mle=1.5422 pcon=5.2879 forget=0.7007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=7.6681 mle=1.7002 pcon=5.2809 forget=0.6870 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=7.8584 mle=1.8997 pcon=5.2738 forget=0.6848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=7.6713 mle=1.7129 pcon=5.2670 forget=0.6913 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=7.4516 mle=1.5021 pcon=5.2603 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=7.5048 mle=1.5598 pcon=5.2540 forget=0.6910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=7.6233 mle=1.6809 pcon=5.2476 forget=0.6948 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=7.6176 mle=1.6716 pcon=5.2409 forget=0.7050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=7.5549 mle=1.6368 pcon=5.2346 forget=0.6835 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=7.4399 mle=1.5167 pcon=5.2285 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=7.6975 mle=1.7803 pcon=5.2225 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=7.6741 mle=1.7648 pcon=5.2167 forget=0.6927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=7.5010 mle=1.6056 pcon=5.2113 forget=0.6842 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=7.6189 mle=1.7241 pcon=5.2056 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=7.6877 mle=1.7943 pcon=5.2003 forget=0.6931 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=7.4361 mle=1.5526 pcon=5.1951 forget=0.6884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=7.7568 mle=1.8881 pcon=5.1899 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=7.6143 mle=1.7416 pcon=5.1847 forget=0.6879 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=7.3888 mle=1.5267 pcon=5.1797 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=7.4533 mle=1.5857 pcon=5.1746 forget=0.6930 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=7.7391 mle=1.8838 pcon=5.1699 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=7.4368 mle=1.5927 pcon=5.1652 forget=0.6789 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=7.6542 mle=1.8111 pcon=5.1605 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=7.5473 mle=1.7059 pcon=5.1560 forget=0.6855 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=7.5124 mle=1.6707 pcon=5.1519 forget=0.6897 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=7.5974 mle=1.7668 pcon=5.1476 forget=0.6830 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=7.7431 mle=1.9183 pcon=5.1436 forget=0.6812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=7.3897 mle=1.5702 pcon=5.1396 forget=0.6800 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=7.6497 mle=1.8361 pcon=5.1354 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=7.5217 mle=1.7027 pcon=5.1312 forget=0.6878 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=7.5130 mle=1.6962 pcon=5.1276 forget=0.6893 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=7.4492 mle=1.6542 pcon=5.1238 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=7.5278 mle=1.7298 pcon=5.1198 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=7.4489 mle=1.6496 pcon=5.1161 forget=0.6832 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.2492 mle=1.4536 pcon=5.1127 forget=0.6830 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=7.5539 mle=1.7667 pcon=5.1088 forget=0.6784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.2737 mle=1.4916 pcon=5.1053 forget=0.6768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=7.6565 mle=1.8719 pcon=5.1019 forget=0.6827 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:25,  2.68it/s]  2%|▏         | 6/391 [00:00<00:24, 15.46it/s]  3%|▎         | 12/391 [00:00<00:14, 26.83it/s]  5%|▍         | 18/391 [00:00<00:10, 35.44it/s]  7%|▋         | 27/391 [00:00<00:07, 49.57it/s]  9%|▉         | 36/391 [00:00<00:05, 60.30it/s] 12%|█▏        | 45/391 [00:01<00:05, 67.99it/s] 14%|█▎        | 53/391 [00:01<00:05, 60.40it/s] 15%|█▌        | 60/391 [00:01<00:05, 58.71it/s] 17%|█▋        | 67/391 [00:01<00:05, 59.97it/s] 19%|█▉        | 76/391 [00:01<00:04, 67.57it/s] 22%|██▏       | 85/391 [00:01<00:04, 73.33it/s] 24%|██▍       | 94/391 [00:01<00:03, 77.50it/s] 26%|██▋       | 103/391 [00:01<00:03, 80.41it/s] 29%|██▊       | 112/391 [00:02<00:04, 65.72it/s] 31%|███       | 120/391 [00:02<00:04, 61.47it/s] 33%|███▎      | 129/391 [00:02<00:03, 67.99it/s] 35%|███▌      | 138/391 [00:02<00:03, 73.28it/s] 38%|███▊      | 147/391 [00:02<00:03, 76.99it/s] 40%|███▉      | 156/391 [00:02<00:02, 80.24it/s] 42%|████▏     | 165/391 [00:02<00:03, 67.43it/s] 44%|████▍     | 173/391 [00:02<00:03, 62.05it/s] 47%|████▋     | 182/391 [00:03<00:03, 67.40it/s] 49%|████▉     | 191/391 [00:03<00:02, 72.56it/s] 51%|█████     | 200/391 [00:03<00:02, 76.67it/s] 53%|█████▎    | 209/391 [00:03<00:02, 79.87it/s] 56%|█████▌    | 218/391 [00:03<00:02, 72.24it/s] 58%|█████▊    | 226/391 [00:03<00:02, 66.65it/s] 60%|█████▉    | 233/391 [00:03<00:02, 63.40it/s] 62%|██████▏   | 242/391 [00:03<00:02, 69.66it/s] 64%|██████▍   | 251/391 [00:03<00:01, 74.53it/s] 66%|██████▋   | 260/391 [00:04<00:01, 78.14it/s] 69%|██████▉   | 269/391 [00:04<00:01, 79.04it/s] 71%|███████   | 278/391 [00:04<00:01, 65.17it/s] 73%|███████▎  | 286/391 [00:04<00:01, 62.91it/s] 75%|███████▌  | 295/391 [00:04<00:01, 68.82it/s] 78%|███████▊  | 304/391 [00:04<00:01, 73.51it/s] 80%|████████  | 313/391 [00:04<00:01, 77.52it/s] 82%|████████▏ | 322/391 [00:04<00:00, 80.59it/s] 85%|████████▍ | 331/391 [00:05<00:00, 74.65it/s] 87%|████████▋ | 339/391 [00:05<00:00, 66.75it/s] 88%|████████▊ | 346/391 [00:05<00:00, 63.24it/s] 91%|█████████ | 355/391 [00:05<00:00, 69.23it/s] 93%|█████████▎| 364/391 [00:05<00:00, 73.80it/s] 95%|█████████▌| 373/391 [00:05<00:00, 76.97it/s] 97%|█████████▋| 381/391 [00:05<00:00, 70.14it/s] 99%|█████████▉| 389/391 [00:05<00:00, 63.48it/s]100%|██████████| 391/391 [00:05<00:00, 65.27it/s]
50000 images processed, 6.092747688293457 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:28,  2.72it/s] 10%|█         | 8/79 [00:00<00:03, 21.51it/s] 22%|██▏       | 17/79 [00:00<00:01, 40.14it/s] 33%|███▎      | 26/79 [00:00<00:00, 53.79it/s] 44%|████▍     | 35/79 [00:00<00:00, 62.45it/s] 54%|█████▍    | 43/79 [00:00<00:00, 55.93it/s] 63%|██████▎   | 50/79 [00:01<00:00, 55.92it/s] 73%|███████▎  | 58/79 [00:01<00:00, 60.93it/s] 85%|████████▍ | 67/79 [00:01<00:00, 67.26it/s] 96%|█████████▌| 76/79 [00:01<00:00, 73.23it/s]100%|██████████| 79/79 [00:01<00:00, 54.57it/s]
10000 images processed, 1.4712059497833252 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:26,  2.35it/s]  3%|▎         | 6/204 [00:00<00:14, 13.67it/s]  6%|▌         | 12/204 [00:00<00:07, 24.80it/s] 10%|▉         | 20/204 [00:00<00:04, 38.80it/s] 14%|█▍        | 29/204 [00:00<00:03, 51.20it/s] 19%|█▊        | 38/204 [00:00<00:02, 60.29it/s] 23%|██▎       | 46/204 [00:01<00:02, 52.69it/s] 26%|██▌       | 53/204 [00:01<00:02, 54.75it/s] 30%|███       | 62/204 [00:01<00:02, 62.68it/s] 35%|███▍      | 71/204 [00:01<00:01, 69.06it/s] 39%|███▉      | 80/204 [00:01<00:01, 73.64it/s] 43%|████▎     | 88/204 [00:01<00:01, 64.92it/s] 48%|████▊     | 98/204 [00:01<00:01, 72.49it/s] 53%|█████▎    | 108/204 [00:01<00:01, 78.35it/s] 58%|█████▊    | 118/204 [00:02<00:01, 82.81it/s] 63%|██████▎   | 128/204 [00:02<00:00, 86.07it/s] 68%|██████▊   | 138/204 [00:02<00:00, 88.51it/s] 73%|███████▎  | 148/204 [00:02<00:00, 90.28it/s] 77%|███████▋  | 158/204 [00:02<00:00, 91.42it/s] 82%|████████▏ | 168/204 [00:02<00:00, 92.01it/s] 87%|████████▋ | 178/204 [00:02<00:00, 91.56it/s] 92%|█████████▏| 188/204 [00:02<00:00, 92.38it/s] 97%|█████████▋| 198/204 [00:02<00:00, 93.35it/s]100%|██████████| 204/204 [00:02<00:00, 68.02it/s]
26032 images processed, 3.053889036178589 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:42,  1.83it/s]  8%|▊         | 6/79 [00:00<00:06, 11.74it/s] 13%|█▎        | 10/79 [00:00<00:03, 17.83it/s] 22%|██▏       | 17/79 [00:00<00:02, 25.11it/s] 29%|██▉       | 23/79 [00:01<00:01, 31.54it/s] 38%|███▊      | 30/79 [00:01<00:01, 39.98it/s] 44%|████▍     | 35/79 [00:01<00:01, 35.56it/s] 52%|█████▏    | 41/79 [00:01<00:01, 37.31it/s] 62%|██████▏   | 49/79 [00:01<00:00, 39.17it/s] 68%|██████▊   | 54/79 [00:01<00:00, 39.85it/s] 75%|███████▍  | 59/79 [00:01<00:00, 41.82it/s] 82%|████████▏ | 65/79 [00:02<00:00, 42.40it/s] 92%|█████████▏| 73/79 [00:02<00:00, 42.12it/s]100%|██████████| 79/79 [00:02<00:00, 45.96it/s]100%|██████████| 79/79 [00:02<00:00, 33.83it/s]
10000 images processed, 2.3659260272979736 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.07it/s] 11%|█▏        | 9/79 [00:00<00:02, 25.79it/s] 23%|██▎       | 18/79 [00:00<00:01, 44.13it/s] 32%|███▏      | 25/79 [00:00<00:01, 51.46it/s] 41%|████      | 32/79 [00:00<00:00, 48.02it/s] 48%|████▊     | 38/79 [00:00<00:00, 49.74it/s] 59%|█████▉    | 47/79 [00:01<00:00, 59.12it/s] 71%|███████   | 56/79 [00:01<00:00, 66.94it/s] 82%|████████▏ | 65/79 [00:01<00:00, 72.85it/s] 94%|█████████▎| 74/79 [00:01<00:00, 77.22it/s]100%|██████████| 79/79 [00:01<00:00, 57.15it/s]
10000 images processed, 1.4057526588439941 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:32,  2.10it/s] 10%|█         | 7/70 [00:00<00:04, 14.86it/s] 23%|██▎       | 16/70 [00:00<00:01, 32.67it/s] 36%|███▌      | 25/70 [00:00<00:00, 46.91it/s] 49%|████▊     | 34/70 [00:00<00:00, 57.95it/s] 61%|██████▏   | 43/70 [00:01<00:00, 64.53it/s] 73%|███████▎  | 51/70 [00:01<00:00, 55.09it/s] 83%|████████▎ | 58/70 [00:01<00:00, 53.91it/s] 96%|█████████▌| 67/70 [00:01<00:00, 62.09it/s]100%|██████████| 70/70 [00:01<00:00, 47.06it/s]
8925 images processed, 1.5260353088378906 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:45,  1.03s/it]  4%|▍         | 2/45 [00:01<00:21,  2.01it/s] 20%|██        | 9/45 [00:01<00:03,  9.10it/s] 24%|██▍       | 11/45 [00:01<00:03,  8.90it/s] 31%|███       | 14/45 [00:01<00:02, 11.71it/s] 38%|███▊      | 17/45 [00:01<00:02, 13.12it/s] 42%|████▏     | 19/45 [00:02<00:02, 10.76it/s] 49%|████▉     | 22/45 [00:02<00:01, 12.74it/s] 58%|█████▊    | 26/45 [00:02<00:01, 11.76it/s] 67%|██████▋   | 30/45 [00:02<00:01, 14.51it/s] 73%|███████▎  | 33/45 [00:03<00:00, 13.42it/s] 78%|███████▊  | 35/45 [00:03<00:00, 10.08it/s] 91%|█████████ | 41/45 [00:03<00:00, 16.35it/s] 98%|█████████▊| 44/45 [00:04<00:00,  9.78it/s]100%|██████████| 45/45 [00:04<00:00, 10.16it/s]
5640 images processed, 4.460571527481079 seconds used

22.08796000480652
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.86  81.18
LSUN          17.60  96.07
iSUN          72.34  81.68
dtd           37.85  91.39
AVG           39.63  89.93
Retain-Acc: 0.7406
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
11.548843622207642
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.1 lr=0.0001 epochs=10
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=10, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.1, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/10 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 10%|█         | 1/10 [00:30<04:30, 30.10s/it] 20%|██        | 2/10 [00:56<03:41, 27.67s/it] 30%|███       | 3/10 [01:25<03:17, 28.26s/it] 40%|████      | 4/10 [01:52<02:47, 27.97s/it] 50%|█████     | 5/10 [02:19<02:18, 27.71s/it] 60%|██████    | 6/10 [02:51<01:56, 29.06s/it] 70%|███████   | 7/10 [03:20<01:27, 29.18s/it] 80%|████████  | 8/10 [03:47<00:56, 28.37s/it][loss] ep 0 it 0 total=7.5533 mle=1.5706 pcon=5.2950 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=7.5307 mle=1.5422 pcon=5.2879 forget=0.7007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=7.6681 mle=1.7002 pcon=5.2809 forget=0.6870 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=7.8584 mle=1.8997 pcon=5.2738 forget=0.6848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=7.6713 mle=1.7129 pcon=5.2670 forget=0.6913 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=7.4516 mle=1.5021 pcon=5.2603 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=7.5048 mle=1.5598 pcon=5.2540 forget=0.6910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=7.6233 mle=1.6809 pcon=5.2476 forget=0.6948 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=7.6176 mle=1.6716 pcon=5.2409 forget=0.7050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=7.5549 mle=1.6368 pcon=5.2346 forget=0.6835 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=7.4399 mle=1.5167 pcon=5.2285 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=7.6975 mle=1.7803 pcon=5.2225 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=7.6741 mle=1.7647 pcon=5.2167 forget=0.6927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=7.5010 mle=1.6055 pcon=5.2112 forget=0.6842 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=7.6188 mle=1.7240 pcon=5.2056 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=7.6875 mle=1.7941 pcon=5.2003 forget=0.6931 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=7.4362 mle=1.5527 pcon=5.1951 forget=0.6884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=7.7568 mle=1.8880 pcon=5.1899 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=7.6144 mle=1.7417 pcon=5.1847 forget=0.6880 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=7.3887 mle=1.5265 pcon=5.1797 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=7.4533 mle=1.5857 pcon=5.1746 forget=0.6930 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=7.7392 mle=1.8839 pcon=5.1700 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=7.4367 mle=1.5926 pcon=5.1652 forget=0.6789 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=7.6543 mle=1.8113 pcon=5.1606 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=7.5472 mle=1.7058 pcon=5.1560 forget=0.6855 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=7.5123 mle=1.6707 pcon=5.1519 forget=0.6897 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=7.5973 mle=1.7668 pcon=5.1476 forget=0.6830 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=7.7431 mle=1.9183 pcon=5.1436 forget=0.6812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=7.3897 mle=1.5701 pcon=5.1396 forget=0.6800 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=7.6498 mle=1.8362 pcon=5.1354 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=7.5220 mle=1.7030 pcon=5.1312 forget=0.6878 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=7.5129 mle=1.6960 pcon=5.1276 forget=0.6893 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=7.4491 mle=1.6542 pcon=5.1238 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=7.5279 mle=1.7299 pcon=5.1198 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=7.4491 mle=1.6498 pcon=5.1161 forget=0.6832 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.2491 mle=1.4534 pcon=5.1127 forget=0.6830 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=7.5539 mle=1.7667 pcon=5.1089 forget=0.6784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.2736 mle=1.4915 pcon=5.1053 forget=0.6768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=7.6565 mle=1.8719 pcon=5.1019 forget=0.6827 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=7.6442 mle=1.8669 pcon=5.0986 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=7.5680 mle=1.7998 pcon=5.0950 forget=0.6732 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=7.3560 mle=1.5840 pcon=5.0918 forget=0.6802 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=7.3638 mle=1.5959 pcon=5.0884 forget=0.6795 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=7.4500 mle=1.6915 pcon=5.0855 forget=0.6730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=7.5255 mle=1.7734 pcon=5.0825 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=7.3516 mle=1.5964 pcon=5.0795 forget=0.6757 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=7.4975 mle=1.7462 pcon=5.0766 forget=0.6746 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.1610 mle=1.4010 pcon=5.0737 forget=0.6862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.2719 mle=1.5364 pcon=5.0710 forget=0.6645 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=7.3411 mle=1.5917 pcon=5.0683 forget=0.6811 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=7.4038 mle=1.6649 pcon=5.0656 forget=0.6733 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=7.3650 mle=1.6305 pcon=5.0634 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=7.5624 mle=1.8236 pcon=5.0605 forget=0.6783 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.2751 mle=1.5348 pcon=5.0582 forget=0.6822 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=7.4046 mle=1.6748 pcon=5.0557 forget=0.6741 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=7.3540 mle=1.6308 pcon=5.0531 forget=0.6700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=7.4638 mle=1.7452 pcon=5.0508 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=7.3471 mle=1.6275 pcon=5.0482 forget=0.6714 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.2931 mle=1.5825 pcon=5.0459 forget=0.6647 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=7.3750 mle=1.6546 pcon=5.0440 forget=0.6764 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=7.5190 mle=1.7959 pcon=5.0418 forget=0.6812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=7.4537 mle=1.7340 pcon=5.0394 forget=0.6803 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=7.3517 mle=1.6408 pcon=5.0374 forget=0.6735 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 90%|█████████ | 9/10 [04:14<00:27, 27.93s/it]100%|██████████| 10/10 [04:41<00:00, 27.64s/it]100%|██████████| 10/10 [04:41<00:00, 28.15s/it]
[loss] ep 8 it 30 total=7.2527 mle=1.5365 pcon=5.0355 forget=0.6807 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=7.2989 mle=1.5906 pcon=5.0336 forget=0.6747 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.2114 mle=1.5005 pcon=5.0315 forget=0.6793 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=7.4042 mle=1.7033 pcon=5.0296 forget=0.6712 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.2291 mle=1.5306 pcon=5.0281 forget=0.6704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=7.3697 mle=1.6776 pcon=5.0262 forget=0.6660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=7.4649 mle=1.7567 pcon=5.0241 forget=0.6841 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.1789 mle=1.4895 pcon=5.0223 forget=0.6671 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.3144 mle=1.6361 pcon=5.0203 forget=0.6579 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=7.3564 mle=1.6696 pcon=5.0183 forget=0.6685 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=7.5356 mle=1.8425 pcon=5.0168 forget=0.6763 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.2628 mle=1.5782 pcon=5.0153 forget=0.6693 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.2522 mle=1.5676 pcon=5.0138 forget=0.6708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.2443 mle=1.5582 pcon=5.0119 forget=0.6741 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.2378 mle=1.5585 pcon=5.0105 forget=0.6688 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:19,  2.79it/s]  2%|▏         | 6/391 [00:00<00:23, 16.07it/s]  3%|▎         | 12/391 [00:00<00:13, 28.24it/s]  5%|▌         | 20/391 [00:00<00:08, 43.11it/s]  7%|▋         | 29/391 [00:00<00:06, 56.60it/s] 10%|▉         | 38/391 [00:00<00:05, 65.80it/s] 12%|█▏        | 47/391 [00:00<00:04, 72.37it/s] 14%|█▍        | 56/391 [00:01<00:04, 77.35it/s] 17%|█▋        | 65/391 [00:01<00:04, 72.84it/s] 19%|█▊        | 73/391 [00:01<00:04, 64.79it/s] 20%|██        | 80/391 [00:01<00:05, 61.59it/s] 23%|██▎       | 89/391 [00:01<00:04, 68.67it/s] 25%|██▌       | 98/391 [00:01<00:03, 74.19it/s] 27%|██▋       | 107/391 [00:01<00:03, 78.21it/s] 30%|██▉       | 116/391 [00:01<00:03, 81.12it/s] 32%|███▏      | 125/391 [00:02<00:03, 83.28it/s] 34%|███▍      | 134/391 [00:02<00:03, 69.22it/s] 36%|███▋      | 142/391 [00:02<00:03, 64.16it/s] 38%|███▊      | 150/391 [00:02<00:03, 67.41it/s] 41%|████      | 159/391 [00:02<00:03, 72.98it/s] 43%|████▎     | 168/391 [00:02<00:02, 77.28it/s] 45%|████▌     | 177/391 [00:02<00:02, 80.69it/s] 48%|████▊     | 186/391 [00:02<00:02, 82.97it/s] 50%|████▉     | 195/391 [00:02<00:02, 78.29it/s] 52%|█████▏    | 204/391 [00:03<00:02, 66.96it/s] 54%|█████▍    | 212/391 [00:03<00:02, 63.88it/s] 57%|█████▋    | 221/391 [00:03<00:02, 69.71it/s] 59%|█████▉    | 230/391 [00:03<00:02, 74.61it/s] 61%|██████    | 239/391 [00:03<00:01, 78.35it/s] 63%|██████▎   | 248/391 [00:03<00:01, 80.97it/s] 66%|██████▌   | 257/391 [00:03<00:01, 82.86it/s] 68%|██████▊   | 266/391 [00:03<00:01, 68.77it/s] 70%|███████   | 274/391 [00:04<00:01, 63.24it/s] 72%|███████▏  | 283/391 [00:04<00:01, 68.26it/s] 75%|███████▍  | 292/391 [00:04<00:01, 73.00it/s] 77%|███████▋  | 301/391 [00:04<00:01, 77.28it/s] 79%|███████▉  | 310/391 [00:04<00:01, 80.41it/s] 82%|████████▏ | 319/391 [00:04<00:00, 82.80it/s] 84%|████████▍ | 328/391 [00:04<00:00, 72.52it/s] 86%|████████▌ | 336/391 [00:04<00:00, 66.77it/s] 88%|████████▊ | 343/391 [00:05<00:00, 64.80it/s] 90%|█████████ | 352/391 [00:05<00:00, 70.89it/s] 92%|█████████▏| 361/391 [00:05<00:00, 75.42it/s] 95%|█████████▍| 370/391 [00:05<00:00, 78.86it/s] 97%|█████████▋| 379/391 [00:05<00:00, 80.90it/s] 99%|█████████▉| 388/391 [00:05<00:00, 81.96it/s]100%|██████████| 391/391 [00:05<00:00, 69.06it/s]
50000 images processed, 5.773923635482788 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:27,  2.85it/s] 13%|█▎        | 10/79 [00:00<00:02, 27.82it/s] 24%|██▍       | 19/79 [00:00<00:01, 45.66it/s] 35%|███▌      | 28/79 [00:00<00:00, 58.25it/s] 46%|████▌     | 36/79 [00:00<00:00, 61.84it/s] 56%|█████▌    | 44/79 [00:00<00:00, 55.98it/s] 65%|██████▍   | 51/79 [00:01<00:00, 55.15it/s] 76%|███████▌  | 60/79 [00:01<00:00, 63.44it/s] 87%|████████▋ | 69/79 [00:01<00:00, 70.30it/s]100%|██████████| 79/79 [00:01<00:00, 75.18it/s]100%|██████████| 79/79 [00:01<00:00, 56.69it/s]
10000 images processed, 1.414247989654541 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:08,  2.97it/s]  3%|▎         | 6/204 [00:00<00:11, 16.87it/s]  6%|▋         | 13/204 [00:00<00:05, 32.08it/s] 11%|█         | 22/204 [00:00<00:03, 48.58it/s] 15%|█▌        | 31/204 [00:00<00:02, 59.88it/s] 20%|█▉        | 40/204 [00:00<00:02, 67.90it/s] 24%|██▍       | 49/204 [00:00<00:02, 73.53it/s] 28%|██▊       | 57/204 [00:01<00:02, 63.98it/s] 32%|███▏      | 65/204 [00:01<00:02, 59.09it/s] 36%|███▌      | 73/204 [00:01<00:02, 63.20it/s] 40%|████      | 82/204 [00:01<00:01, 69.44it/s] 45%|████▍     | 91/204 [00:01<00:01, 74.18it/s] 49%|████▉     | 100/204 [00:01<00:01, 77.75it/s] 53%|█████▎    | 109/204 [00:01<00:01, 80.59it/s] 58%|█████▊    | 118/204 [00:01<00:01, 75.42it/s] 62%|██████▏   | 126/204 [00:02<00:01, 65.68it/s] 65%|██████▌   | 133/204 [00:02<00:01, 62.28it/s] 70%|██████▉   | 142/204 [00:02<00:00, 68.45it/s] 74%|███████▍  | 151/204 [00:02<00:00, 73.23it/s] 78%|███████▊  | 160/204 [00:02<00:00, 76.84it/s] 83%|████████▎ | 169/204 [00:02<00:00, 79.82it/s] 87%|████████▋ | 178/204 [00:02<00:00, 81.91it/s] 92%|█████████▏| 187/204 [00:02<00:00, 67.85it/s] 96%|█████████▌| 195/204 [00:03<00:00, 62.25it/s]100%|██████████| 204/204 [00:03<00:00, 67.30it/s]100%|██████████| 204/204 [00:03<00:00, 63.77it/s]
26032 images processed, 3.23992657661438 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:37,  2.07it/s]  8%|▊         | 6/79 [00:00<00:05, 12.90it/s] 18%|█▊        | 14/79 [00:00<00:02, 28.57it/s] 24%|██▍       | 19/79 [00:00<00:01, 30.13it/s] 30%|███       | 24/79 [00:00<00:01, 33.89it/s] 37%|███▋      | 29/79 [00:01<00:01, 37.17it/s] 44%|████▍     | 35/79 [00:01<00:01, 39.77it/s] 53%|█████▎    | 42/79 [00:01<00:00, 45.96it/s] 61%|██████    | 48/79 [00:01<00:00, 40.24it/s] 70%|██████▉   | 55/79 [00:01<00:00, 46.01it/s] 77%|███████▋  | 61/79 [00:01<00:00, 39.96it/s] 85%|████████▍ | 67/79 [00:01<00:00, 41.86it/s] 95%|█████████▍| 75/79 [00:02<00:00, 42.12it/s]100%|██████████| 79/79 [00:02<00:00, 35.84it/s]
10000 images processed, 2.238654851913452 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:25,  3.06it/s]  8%|▊         | 6/79 [00:00<00:04, 16.36it/s] 15%|█▌        | 12/79 [00:00<00:02, 27.81it/s] 27%|██▋       | 21/79 [00:00<00:01, 44.12it/s] 38%|███▊      | 30/79 [00:00<00:00, 56.55it/s] 49%|████▉     | 39/79 [00:00<00:00, 65.35it/s] 61%|██████    | 48/79 [00:00<00:00, 71.69it/s] 71%|███████   | 56/79 [00:01<00:00, 73.91it/s] 81%|████████  | 64/79 [00:01<00:00, 60.45it/s] 90%|████████▉ | 71/79 [00:01<00:00, 58.86it/s]100%|██████████| 79/79 [00:01<00:00, 53.37it/s]
10000 images processed, 1.498361587524414 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:26,  2.64it/s]  9%|▊         | 6/70 [00:00<00:04, 15.42it/s] 21%|██▏       | 15/70 [00:00<00:01, 35.62it/s] 34%|███▍      | 24/70 [00:00<00:00, 50.45it/s] 47%|████▋     | 33/70 [00:00<00:00, 61.06it/s] 60%|██████    | 42/70 [00:00<00:00, 68.61it/s] 71%|███████▏  | 50/70 [00:01<00:00, 69.36it/s] 83%|████████▎ | 58/70 [00:01<00:00, 59.91it/s] 93%|█████████▎| 65/70 [00:01<00:00, 58.41it/s]100%|██████████| 70/70 [00:01<00:00, 50.91it/s]
8925 images processed, 1.405895471572876 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:41,  1.07it/s]  4%|▍         | 2/45 [00:01<00:19,  2.17it/s] 16%|█▌        | 7/45 [00:01<00:04,  9.36it/s] 22%|██▏       | 10/45 [00:01<00:04,  8.11it/s] 33%|███▎      | 15/45 [00:01<00:02, 13.48it/s] 40%|████      | 18/45 [00:02<00:02, 11.02it/s] 53%|█████▎    | 24/45 [00:02<00:01, 17.38it/s] 62%|██████▏   | 28/45 [00:02<00:01, 13.06it/s] 69%|██████▉   | 31/45 [00:02<00:00, 14.02it/s] 76%|███████▌  | 34/45 [00:03<00:00, 11.27it/s] 91%|█████████ | 41/45 [00:03<00:00, 15.72it/s] 98%|█████████▊| 44/45 [00:04<00:00, 11.72it/s]100%|██████████| 45/45 [00:04<00:00, 11.13it/s]
5640 images processed, 4.06625771522522 seconds used

21.168633937835693
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.89  81.18
LSUN          17.59  96.07
iSUN          72.35  81.67
dtd           37.87  91.39
AVG           39.64  89.93
Retain-Acc: 0.7407
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.90 AUIN: 98.36
20.85540008544922
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e10-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.1 lr=0.0001 epochs=20
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=20, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.1, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/20 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  5%|▌         | 1/20 [00:28<09:09, 28.94s/it] 10%|█         | 2/20 [00:54<08:03, 26.89s/it] 15%|█▌        | 3/20 [01:25<08:10, 28.86s/it] 20%|██        | 4/20 [01:48<07:07, 26.70s/it] 25%|██▌       | 5/20 [02:16<06:47, 27.15s/it] 30%|███       | 6/20 [02:44<06:21, 27.22s/it] 35%|███▌      | 7/20 [03:10<05:49, 26.90s/it] 40%|████      | 8/20 [03:39<05:28, 27.41s/it][loss] ep 0 it 0 total=7.5533 mle=1.5706 pcon=5.2950 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=7.5307 mle=1.5422 pcon=5.2879 forget=0.7007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=7.6681 mle=1.7002 pcon=5.2809 forget=0.6870 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=7.8584 mle=1.8997 pcon=5.2738 forget=0.6848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=7.6713 mle=1.7129 pcon=5.2670 forget=0.6913 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=7.4516 mle=1.5021 pcon=5.2603 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=7.5048 mle=1.5598 pcon=5.2540 forget=0.6910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=7.6233 mle=1.6809 pcon=5.2476 forget=0.6948 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=7.6176 mle=1.6716 pcon=5.2409 forget=0.7050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=7.5549 mle=1.6368 pcon=5.2346 forget=0.6835 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=7.4399 mle=1.5167 pcon=5.2285 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=7.6975 mle=1.7803 pcon=5.2225 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=7.6741 mle=1.7648 pcon=5.2167 forget=0.6927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=7.5010 mle=1.6056 pcon=5.2113 forget=0.6842 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=7.6189 mle=1.7241 pcon=5.2056 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=7.6877 mle=1.7943 pcon=5.2003 forget=0.6931 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=7.4361 mle=1.5526 pcon=5.1951 forget=0.6884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=7.7568 mle=1.8881 pcon=5.1899 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=7.6143 mle=1.7416 pcon=5.1847 forget=0.6879 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=7.3888 mle=1.5267 pcon=5.1797 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=7.4533 mle=1.5857 pcon=5.1746 forget=0.6930 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=7.7391 mle=1.8838 pcon=5.1699 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=7.4368 mle=1.5927 pcon=5.1652 forget=0.6789 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=7.6542 mle=1.8111 pcon=5.1605 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=7.5473 mle=1.7059 pcon=5.1560 forget=0.6855 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=7.5124 mle=1.6707 pcon=5.1519 forget=0.6897 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=7.5974 mle=1.7668 pcon=5.1476 forget=0.6830 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=7.7431 mle=1.9183 pcon=5.1436 forget=0.6812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=7.3897 mle=1.5702 pcon=5.1396 forget=0.6800 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=7.6497 mle=1.8361 pcon=5.1354 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=7.5217 mle=1.7027 pcon=5.1312 forget=0.6878 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=7.5130 mle=1.6962 pcon=5.1276 forget=0.6893 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=7.4492 mle=1.6542 pcon=5.1238 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=7.5278 mle=1.7298 pcon=5.1198 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=7.4489 mle=1.6496 pcon=5.1161 forget=0.6832 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.2492 mle=1.4536 pcon=5.1127 forget=0.6830 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=7.5539 mle=1.7667 pcon=5.1088 forget=0.6784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.2737 mle=1.4916 pcon=5.1053 forget=0.6768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=7.6565 mle=1.8719 pcon=5.1019 forget=0.6827 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=7.6440 mle=1.8667 pcon=5.0986 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=7.5681 mle=1.8000 pcon=5.0950 forget=0.6732 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=7.3560 mle=1.5840 pcon=5.0918 forget=0.6802 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=7.3635 mle=1.5957 pcon=5.0883 forget=0.6795 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=7.4499 mle=1.6914 pcon=5.0855 forget=0.6730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=7.5254 mle=1.7734 pcon=5.0825 forget=0.6696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=7.3516 mle=1.5965 pcon=5.0794 forget=0.6757 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=7.4975 mle=1.7463 pcon=5.0766 forget=0.6746 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.1610 mle=1.4010 pcon=5.0737 forget=0.6862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.2716 mle=1.5361 pcon=5.0710 forget=0.6645 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=7.3410 mle=1.5916 pcon=5.0683 forget=0.6811 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=7.4039 mle=1.6650 pcon=5.0656 forget=0.6733 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=7.3654 mle=1.6309 pcon=5.0634 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=7.5625 mle=1.8237 pcon=5.0605 forget=0.6783 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.2750 mle=1.5347 pcon=5.0582 forget=0.6821 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=7.4046 mle=1.6748 pcon=5.0557 forget=0.6741 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=7.3538 mle=1.6307 pcon=5.0531 forget=0.6701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=7.4637 mle=1.7452 pcon=5.0507 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=7.3471 mle=1.6275 pcon=5.0482 forget=0.6714 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.2932 mle=1.5826 pcon=5.0459 forget=0.6647 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=7.3749 mle=1.6545 pcon=5.0439 forget=0.6764 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=7.5189 mle=1.7959 pcon=5.0418 forget=0.6812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=7.4536 mle=1.7339 pcon=5.0394 forget=0.6803 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=7.3516 mle=1.6407 pcon=5.0374 forget=0.6735 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 45%|████▌     | 9/20 [04:05<04:58, 27.12s/it] 50%|█████     | 10/20 [04:31<04:27, 26.73s/it] 55%|█████▌    | 11/20 [05:02<04:11, 27.97s/it] 60%|██████    | 12/20 [05:27<03:36, 27.03s/it] 65%|██████▌   | 13/20 [05:52<03:06, 26.59s/it] 70%|███████   | 14/20 [06:21<02:43, 27.31s/it] 75%|███████▌  | 15/20 [06:47<02:14, 26.84s/it] 80%|████████  | 16/20 [07:11<01:44, 26.10s/it][loss] ep 8 it 30 total=7.2527 mle=1.5365 pcon=5.0355 forget=0.6807 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=7.2989 mle=1.5906 pcon=5.0336 forget=0.6747 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.2115 mle=1.5006 pcon=5.0315 forget=0.6793 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=7.4042 mle=1.7033 pcon=5.0296 forget=0.6712 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.2292 mle=1.5307 pcon=5.0281 forget=0.6704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=7.3697 mle=1.6776 pcon=5.0262 forget=0.6660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=7.4646 mle=1.7564 pcon=5.0241 forget=0.6841 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.1789 mle=1.4895 pcon=5.0223 forget=0.6671 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.3144 mle=1.6362 pcon=5.0204 forget=0.6579 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=7.3563 mle=1.6695 pcon=5.0183 forget=0.6685 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=7.5355 mle=1.8424 pcon=5.0168 forget=0.6763 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.2627 mle=1.5781 pcon=5.0153 forget=0.6693 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.2522 mle=1.5676 pcon=5.0138 forget=0.6708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.2445 mle=1.5584 pcon=5.0120 forget=0.6741 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.2377 mle=1.5583 pcon=5.0105 forget=0.6688 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=7.2897 mle=1.6069 pcon=5.0088 forget=0.6740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=7.2766 mle=1.5906 pcon=5.0071 forget=0.6789 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=7.4290 mle=1.7540 pcon=5.0056 forget=0.6694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=7.5208 mle=1.8509 pcon=5.0043 forget=0.6656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=7.5286 mle=1.8596 pcon=5.0028 forget=0.6662 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=7.2537 mle=1.5823 pcon=5.0015 forget=0.6699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=7.2257 mle=1.5625 pcon=5.0004 forget=0.6628 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=7.3461 mle=1.6670 pcon=4.9990 forget=0.6800 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=7.2499 mle=1.5764 pcon=4.9977 forget=0.6757 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=7.2847 mle=1.6219 pcon=4.9963 forget=0.6665 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=7.2271 mle=1.5689 pcon=4.9951 forget=0.6631 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=7.3283 mle=1.6655 pcon=4.9935 forget=0.6693 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=7.4474 mle=1.7817 pcon=4.9924 forget=0.6734 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=7.2673 mle=1.6036 pcon=4.9916 forget=0.6722 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=7.6285 mle=1.9669 pcon=4.9902 forget=0.6713 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=7.3196 mle=1.6708 pcon=4.9890 forget=0.6598 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 12 it 20 total=7.2762 mle=1.6200 pcon=4.9877 forget=0.6686 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=7.3526 mle=1.6949 pcon=4.9865 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=7.2103 mle=1.5486 pcon=4.9854 forget=0.6763 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=7.3935 mle=1.7456 pcon=4.9843 forget=0.6636 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=7.2488 mle=1.6003 pcon=4.9831 forget=0.6654 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=7.1924 mle=1.5352 pcon=4.9824 forget=0.6749 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=7.1724 mle=1.5209 pcon=4.9813 forget=0.6701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=7.5230 mle=1.8675 pcon=4.9803 forget=0.6752 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=7.0799 mle=1.4303 pcon=4.9794 forget=0.6702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=7.1604 mle=1.4954 pcon=4.9783 forget=0.6866 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=7.2182 mle=1.5711 pcon=4.9771 forget=0.6700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=7.1967 mle=1.5534 pcon=4.9763 forget=0.6670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=7.2927 mle=1.6447 pcon=4.9755 forget=0.6726 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=7.2224 mle=1.5838 pcon=4.9746 forget=0.6641 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=7.2347 mle=1.5901 pcon=4.9736 forget=0.6710 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=7.2184 mle=1.5790 pcon=4.9727 forget=0.6667 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=7.4024 mle=1.7692 pcon=4.9716 forget=0.6616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=7.4111 mle=1.7694 pcon=4.9710 forget=0.6707 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=7.2530 mle=1.6153 pcon=4.9706 forget=0.6671 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=7.5222 mle=1.8833 pcon=4.9696 forget=0.6693 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=7.2515 mle=1.6157 pcon=4.9690 forget=0.6668 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=7.3002 mle=1.6703 pcon=4.9682 forget=0.6618 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=7.3106 mle=1.6808 pcon=4.9673 forget=0.6624 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=7.3213 mle=1.6782 pcon=4.9667 forget=0.6764 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=7.1222 mle=1.4936 pcon=4.9658 forget=0.6627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=7.1938 mle=1.5577 pcon=4.9652 forget=0.6709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=7.3687 mle=1.7265 pcon=4.9645 forget=0.6778 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=7.2437 mle=1.6169 pcon=4.9639 forget=0.6630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=7.5073 mle=1.8743 pcon=4.9631 forget=0.6698 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=7.2278 mle=1.5937 pcon=4.9626 forget=0.6715 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=7.1786 mle=1.5448 pcon=4.9620 forget=0.6719 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 16 it 10 total=7.2191 mle=1.5860 pcon=4.9613 forget=0.6718 nr=64 nf=64 protos=540 fproto_sim=NA
 85%|████████▌ | 17/20 [07:40<01:21, 27.01s/it] 90%|█████████ | 18/20 [08:07<00:53, 26.96s/it] 95%|█████████▌| 19/20 [08:35<00:27, 27.16s/it]100%|██████████| 20/20 [09:05<00:00, 28.19s/it]100%|██████████| 20/20 [09:05<00:00, 27.29s/it]
[loss] ep 16 it 60 total=7.1185 mle=1.4931 pcon=4.9607 forget=0.6647 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=7.1400 mle=1.5137 pcon=4.9603 forget=0.6660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=7.3186 mle=1.6988 pcon=4.9595 forget=0.6602 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=7.1907 mle=1.5731 pcon=4.9588 forget=0.6588 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=7.5669 mle=1.9479 pcon=4.9580 forget=0.6610 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=7.4138 mle=1.7909 pcon=4.9574 forget=0.6655 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=7.2328 mle=1.6145 pcon=4.9569 forget=0.6615 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 17 it 20 total=7.4027 mle=1.7778 pcon=4.9559 forget=0.6690 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=7.4388 mle=1.8235 pcon=4.9557 forget=0.6595 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=7.1956 mle=1.5745 pcon=4.9550 forget=0.6661 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=7.2032 mle=1.5828 pcon=4.9540 forget=0.6663 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=7.2882 mle=1.6651 pcon=4.9534 forget=0.6697 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=7.0886 mle=1.4685 pcon=4.9533 forget=0.6667 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=7.4189 mle=1.8033 pcon=4.9528 forget=0.6629 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=7.3588 mle=1.7472 pcon=4.9520 forget=0.6596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=7.1985 mle=1.5874 pcon=4.9519 forget=0.6592 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=7.3045 mle=1.6887 pcon=4.9515 forget=0.6643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=7.2824 mle=1.6655 pcon=4.9509 forget=0.6659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=7.1652 mle=1.5479 pcon=4.9505 forget=0.6667 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=7.1993 mle=1.5909 pcon=4.9501 forget=0.6583 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=7.3001 mle=1.6879 pcon=4.9497 forget=0.6624 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=7.4190 mle=1.8088 pcon=4.9493 forget=0.6608 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=7.3175 mle=1.7018 pcon=4.9491 forget=0.6666 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 19 it 40 total=7.0821 mle=1.4660 pcon=4.9490 forget=0.6672 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=7.3229 mle=1.7028 pcon=4.9490 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=7.2227 mle=1.6124 pcon=4.9485 forget=0.6617 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=7.2475 mle=1.6439 pcon=4.9480 forget=0.6555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=7.4580 mle=1.8474 pcon=4.9477 forget=0.6629 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=7.1763 mle=1.5592 pcon=4.9477 forget=0.6694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=7.2245 mle=1.5980 pcon=4.9476 forget=0.6789 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:45,  2.35it/s]  2%|▏         | 8/391 [00:00<00:20, 18.44it/s]  3%|▎         | 13/391 [00:00<00:14, 26.00it/s]  5%|▍         | 19/391 [00:00<00:11, 33.39it/s]  7%|▋         | 26/391 [00:00<00:08, 42.54it/s]  9%|▉         | 35/391 [00:00<00:06, 54.59it/s] 11%|█▏        | 44/391 [00:01<00:05, 63.85it/s] 14%|█▎        | 53/391 [00:01<00:04, 70.25it/s] 16%|█▌        | 61/391 [00:01<00:04, 72.10it/s] 18%|█▊        | 69/391 [00:01<00:05, 61.65it/s] 19%|█▉        | 76/391 [00:01<00:05, 60.14it/s] 21%|██▏       | 84/391 [00:01<00:04, 64.19it/s] 24%|██▍       | 93/391 [00:01<00:04, 70.39it/s] 26%|██▌       | 102/391 [00:01<00:03, 75.30it/s] 28%|██▊       | 111/391 [00:02<00:03, 77.38it/s] 30%|███       | 119/391 [00:02<00:03, 69.96it/s] 32%|███▏      | 127/391 [00:02<00:04, 65.41it/s] 34%|███▍      | 134/391 [00:02<00:04, 62.20it/s] 37%|███▋      | 143/391 [00:02<00:03, 68.56it/s] 39%|███▉      | 152/391 [00:02<00:03, 73.30it/s] 41%|████      | 161/391 [00:02<00:02, 77.07it/s] 43%|████▎     | 170/391 [00:02<00:02, 79.71it/s] 46%|████▌     | 179/391 [00:03<00:03, 66.92it/s] 48%|████▊     | 187/391 [00:03<00:03, 63.09it/s] 50%|████▉     | 195/391 [00:03<00:02, 65.44it/s] 52%|█████▏    | 204/391 [00:03<00:02, 70.74it/s] 54%|█████▍    | 213/391 [00:03<00:02, 74.61it/s] 57%|█████▋    | 222/391 [00:03<00:02, 76.26it/s] 59%|█████▉    | 230/391 [00:03<00:02, 64.65it/s] 61%|██████    | 237/391 [00:03<00:02, 62.73it/s] 63%|██████▎   | 245/391 [00:04<00:02, 65.62it/s] 65%|██████▍   | 254/391 [00:04<00:01, 70.78it/s] 67%|██████▋   | 263/391 [00:04<00:01, 75.05it/s] 69%|██████▉   | 271/391 [00:04<00:01, 72.24it/s] 71%|███████▏  | 279/391 [00:04<00:01, 62.02it/s] 73%|███████▎  | 286/391 [00:04<00:01, 59.99it/s] 75%|███████▌  | 295/391 [00:04<00:01, 66.66it/s] 78%|███████▊  | 304/391 [00:04<00:01, 72.19it/s] 80%|████████  | 313/391 [00:04<00:01, 76.62it/s] 82%|████████▏ | 322/391 [00:05<00:00, 80.08it/s] 85%|████████▍ | 331/391 [00:05<00:00, 73.53it/s] 87%|████████▋ | 339/391 [00:05<00:00, 65.73it/s] 88%|████████▊ | 346/391 [00:05<00:00, 62.78it/s] 91%|█████████ | 355/391 [00:05<00:00, 69.09it/s] 93%|█████████▎| 364/391 [00:05<00:00, 73.58it/s] 95%|█████████▌| 373/391 [00:05<00:00, 76.83it/s] 97%|█████████▋| 381/391 [00:05<00:00, 75.38it/s] 99%|█████████▉| 389/391 [00:06<00:00, 63.95it/s]100%|██████████| 391/391 [00:06<00:00, 64.05it/s]
50000 images processed, 6.249156951904297 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:29,  2.65it/s]  8%|▊         | 6/79 [00:00<00:04, 15.40it/s] 19%|█▉        | 15/79 [00:00<00:01, 35.73it/s] 30%|███       | 24/79 [00:00<00:01, 50.39it/s] 42%|████▏     | 33/79 [00:00<00:00, 60.97it/s] 53%|█████▎    | 42/79 [00:00<00:00, 68.70it/s] 63%|██████▎   | 50/79 [00:01<00:00, 66.36it/s] 73%|███████▎  | 58/79 [00:01<00:00, 60.83it/s] 82%|████████▏ | 65/79 [00:01<00:00, 58.37it/s] 94%|█████████▎| 74/79 [00:01<00:00, 66.16it/s]100%|██████████| 79/79 [00:01<00:00, 53.26it/s]
10000 images processed, 1.5099678039550781 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:17,  2.64it/s]  4%|▍         | 9/204 [00:00<00:08, 23.79it/s]  9%|▉         | 18/204 [00:00<00:04, 41.99it/s] 13%|█▎        | 27/204 [00:00<00:03, 54.66it/s] 18%|█▊        | 36/204 [00:00<00:02, 63.86it/s] 22%|██▏       | 44/204 [00:00<00:02, 62.13it/s] 25%|██▌       | 52/204 [00:01<00:02, 56.61it/s] 29%|██▉       | 59/204 [00:01<00:02, 57.51it/s] 33%|███▎      | 68/204 [00:01<00:02, 65.07it/s] 38%|███▊      | 77/204 [00:01<00:01, 70.08it/s] 42%|████▏     | 86/204 [00:01<00:01, 74.29it/s] 46%|████▌     | 94/204 [00:01<00:01, 68.56it/s] 50%|█████     | 102/204 [00:01<00:01, 58.90it/s] 53%|█████▎    | 109/204 [00:01<00:01, 60.82it/s] 58%|█████▊    | 118/204 [00:02<00:01, 67.32it/s] 62%|██████▏   | 127/204 [00:02<00:01, 72.27it/s] 67%|██████▋   | 136/204 [00:02<00:00, 75.90it/s] 71%|███████   | 144/204 [00:02<00:00, 72.19it/s] 75%|███████▍  | 152/204 [00:02<00:00, 62.93it/s] 78%|███████▊  | 159/204 [00:02<00:00, 60.70it/s] 82%|████████▏ | 168/204 [00:02<00:00, 67.42it/s] 87%|████████▋ | 177/204 [00:02<00:00, 72.42it/s] 91%|█████████ | 186/204 [00:03<00:00, 76.41it/s] 96%|█████████▌| 195/204 [00:03<00:00, 79.67it/s]100%|██████████| 204/204 [00:03<00:00, 67.68it/s]100%|██████████| 204/204 [00:03<00:00, 62.12it/s]
26032 images processed, 3.3424324989318848 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.91it/s]  9%|▉         | 7/79 [00:00<00:05, 13.78it/s] 19%|█▉        | 15/79 [00:00<00:02, 28.67it/s] 27%|██▋       | 21/79 [00:00<00:02, 28.52it/s] 34%|███▍      | 27/79 [00:01<00:01, 32.55it/s] 43%|████▎     | 34/79 [00:01<00:01, 39.82it/s] 49%|████▉     | 39/79 [00:01<00:01, 36.63it/s] 56%|█████▌    | 44/79 [00:01<00:00, 38.66it/s] 63%|██████▎   | 50/79 [00:01<00:00, 42.77it/s] 71%|███████   | 56/79 [00:01<00:00, 40.46it/s] 77%|███████▋  | 61/79 [00:01<00:00, 40.86it/s] 84%|████████▎ | 66/79 [00:01<00:00, 41.61it/s] 91%|█████████ | 72/79 [00:02<00:00, 42.08it/s] 97%|█████████▋| 77/79 [00:02<00:00, 42.06it/s]100%|██████████| 79/79 [00:02<00:00, 34.63it/s]
10000 images processed, 2.325545072555542 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:38,  2.03it/s]  8%|▊         | 6/79 [00:00<00:05, 12.30it/s] 16%|█▋        | 13/79 [00:00<00:02, 24.61it/s] 25%|██▌       | 20/79 [00:00<00:01, 34.63it/s] 37%|███▋      | 29/79 [00:00<00:01, 47.40it/s] 48%|████▊     | 38/79 [00:01<00:00, 58.00it/s] 59%|█████▉    | 47/79 [00:01<00:00, 66.06it/s] 70%|██████▉   | 55/79 [00:01<00:00, 61.38it/s] 78%|███████▊  | 62/79 [00:01<00:00, 58.17it/s] 87%|████████▋ | 69/79 [00:01<00:00, 58.77it/s] 99%|█████████▊| 78/79 [00:01<00:00, 66.23it/s]100%|██████████| 79/79 [00:01<00:00, 47.26it/s]
10000 images processed, 1.6959588527679443 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:31,  2.18it/s] 10%|█         | 7/70 [00:00<00:04, 15.26it/s] 20%|██        | 14/70 [00:00<00:01, 28.16it/s] 33%|███▎      | 23/70 [00:00<00:01, 43.71it/s] 46%|████▌     | 32/70 [00:00<00:00, 55.58it/s] 59%|█████▊    | 41/70 [00:00<00:00, 64.58it/s] 71%|███████▏  | 50/70 [00:01<00:00, 71.10it/s] 84%|████████▍ | 59/70 [00:01<00:00, 63.52it/s] 96%|█████████▌| 67/70 [00:01<00:00, 59.60it/s]100%|██████████| 70/70 [00:01<00:00, 47.23it/s]
8925 images processed, 1.5146448612213135 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:45,  1.04s/it]  4%|▍         | 2/45 [00:01<00:22,  1.91it/s] 20%|██        | 9/45 [00:01<00:04,  8.80it/s] 24%|██▍       | 11/45 [00:01<00:04,  7.95it/s] 38%|███▊      | 17/45 [00:02<00:02, 11.63it/s] 42%|████▏     | 19/45 [00:02<00:02, 10.38it/s] 56%|█████▌    | 25/45 [00:02<00:01, 14.26it/s] 60%|██████    | 27/45 [00:03<00:01, 10.89it/s] 73%|███████▎  | 33/45 [00:03<00:00, 12.78it/s] 78%|███████▊  | 35/45 [00:03<00:01,  9.81it/s] 91%|█████████ | 41/45 [00:03<00:00, 15.07it/s] 98%|█████████▊| 44/45 [00:04<00:00,  8.96it/s]100%|██████████| 45/45 [00:04<00:00,  9.51it/s]
5640 images processed, 4.7534074783325195 seconds used

23.069448709487915
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.91  81.18
LSUN          17.57  96.08
iSUN          72.36  81.67
dtd           37.87  91.39
AVG           39.65  89.93
Retain-Acc: 0.7407
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.89 AUIN: 98.36
26.246034145355225
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e20-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Run] lambda=0.1 lr=0.0001 epochs=50
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.0001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter', adapter_load_path=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_lambda=0.1, forget_margin=100.0, centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:108: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [00:32<26:56, 32.99s/it]  4%|▍         | 2/50 [00:57<22:33, 28.19s/it]  6%|▌         | 3/50 [01:23<21:04, 26.90s/it]  8%|▊         | 4/50 [01:52<21:22, 27.88s/it] 10%|█         | 5/50 [02:19<20:42, 27.62s/it] 12%|█▏        | 6/50 [02:45<19:41, 26.84s/it] 14%|█▍        | 7/50 [03:13<19:36, 27.37s/it] 16%|█▌        | 8/50 [03:38<18:44, 26.77s/it][loss] ep 0 it 0 total=7.5537 mle=1.5709 pcon=5.2950 forget=0.6877 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 50 total=7.5309 mle=1.5424 pcon=5.2879 forget=0.7007 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 100 total=7.6684 mle=1.7005 pcon=5.2809 forget=0.6870 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 150 total=7.8585 mle=1.8998 pcon=5.2738 forget=0.6848 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 200 total=7.6714 mle=1.7131 pcon=5.2670 forget=0.6913 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 250 total=7.4515 mle=1.5019 pcon=5.2603 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 300 total=7.5048 mle=1.5598 pcon=5.2540 forget=0.6910 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 0 it 350 total=7.6233 mle=1.6809 pcon=5.2476 forget=0.6948 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 1 it 10 total=7.6175 mle=1.6716 pcon=5.2409 forget=0.7050 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 60 total=7.5552 mle=1.6371 pcon=5.2346 forget=0.6835 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 110 total=7.4397 mle=1.5165 pcon=5.2285 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 160 total=7.6972 mle=1.7801 pcon=5.2225 forget=0.6947 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 210 total=7.6740 mle=1.7647 pcon=5.2167 forget=0.6927 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 260 total=7.5007 mle=1.6053 pcon=5.2112 forget=0.6842 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 310 total=7.6188 mle=1.7241 pcon=5.2056 forget=0.6891 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 1 it 360 total=7.6876 mle=1.7942 pcon=5.2003 forget=0.6931 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 2 it 20 total=7.4365 mle=1.5531 pcon=5.1951 forget=0.6884 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 70 total=7.7572 mle=1.8884 pcon=5.1899 forget=0.6789 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 120 total=7.6143 mle=1.7417 pcon=5.1847 forget=0.6879 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 170 total=7.3885 mle=1.5263 pcon=5.1797 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 220 total=7.4535 mle=1.5859 pcon=5.1746 forget=0.6930 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 270 total=7.7391 mle=1.8838 pcon=5.1699 forget=0.6854 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 320 total=7.4367 mle=1.5926 pcon=5.1652 forget=0.6789 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 2 it 370 total=7.6544 mle=1.8113 pcon=5.1606 forget=0.6825 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 3 it 30 total=7.5476 mle=1.7061 pcon=5.1560 forget=0.6855 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 80 total=7.5125 mle=1.6709 pcon=5.1519 forget=0.6897 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 130 total=7.5975 mle=1.7669 pcon=5.1476 forget=0.6830 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 180 total=7.7431 mle=1.9183 pcon=5.1436 forget=0.6812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 230 total=7.3895 mle=1.5699 pcon=5.1396 forget=0.6800 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 280 total=7.6498 mle=1.8362 pcon=5.1354 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 330 total=7.5220 mle=1.7030 pcon=5.1312 forget=0.6878 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 3 it 380 total=7.5129 mle=1.6960 pcon=5.1276 forget=0.6892 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 4 it 40 total=7.4490 mle=1.6541 pcon=5.1238 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 90 total=7.5280 mle=1.7300 pcon=5.1198 forget=0.6782 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 140 total=7.4490 mle=1.6497 pcon=5.1161 forget=0.6832 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 190 total=7.2493 mle=1.4536 pcon=5.1127 forget=0.6829 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 240 total=7.5546 mle=1.7674 pcon=5.1089 forget=0.6784 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 290 total=7.2734 mle=1.4913 pcon=5.1053 forget=0.6768 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 4 it 340 total=7.6567 mle=1.8721 pcon=5.1019 forget=0.6827 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 5 it 0 total=7.6444 mle=1.8670 pcon=5.0986 forget=0.6788 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 50 total=7.5677 mle=1.7996 pcon=5.0950 forget=0.6731 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 100 total=7.3562 mle=1.5842 pcon=5.0918 forget=0.6802 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 150 total=7.3635 mle=1.5957 pcon=5.0883 forget=0.6795 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 200 total=7.4498 mle=1.6913 pcon=5.0855 forget=0.6730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 250 total=7.5251 mle=1.7731 pcon=5.0825 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 300 total=7.3516 mle=1.5964 pcon=5.0795 forget=0.6757 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 5 it 350 total=7.4979 mle=1.7467 pcon=5.0766 forget=0.6746 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 6 it 10 total=7.1612 mle=1.4013 pcon=5.0737 forget=0.6862 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 60 total=7.2717 mle=1.5362 pcon=5.0710 forget=0.6645 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 110 total=7.3410 mle=1.5916 pcon=5.0683 forget=0.6811 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 160 total=7.4043 mle=1.6654 pcon=5.0656 forget=0.6732 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 210 total=7.3648 mle=1.6303 pcon=5.0634 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 260 total=7.5623 mle=1.8235 pcon=5.0605 forget=0.6783 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 310 total=7.2750 mle=1.5346 pcon=5.0582 forget=0.6822 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 6 it 360 total=7.4047 mle=1.6749 pcon=5.0557 forget=0.6741 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 7 it 20 total=7.3541 mle=1.6310 pcon=5.0531 forget=0.6701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 70 total=7.4639 mle=1.7453 pcon=5.0507 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 120 total=7.3468 mle=1.6272 pcon=5.0482 forget=0.6714 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 170 total=7.2931 mle=1.5825 pcon=5.0459 forget=0.6647 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 220 total=7.3750 mle=1.6546 pcon=5.0439 forget=0.6764 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 270 total=7.5190 mle=1.7960 pcon=5.0418 forget=0.6812 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 320 total=7.4614 mle=1.7417 pcon=5.0394 forget=0.6803 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 7 it 370 total=7.3516 mle=1.6407 pcon=5.0374 forget=0.6735 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
 18%|█▊        | 9/50 [04:02<17:37, 25.80s/it] 20%|██        | 10/50 [04:29<17:26, 26.17s/it] 22%|██▏       | 11/50 [04:55<16:57, 26.09s/it] 24%|██▍       | 12/50 [05:21<16:35, 26.19s/it] 26%|██▌       | 13/50 [05:49<16:25, 26.64s/it] 28%|██▊       | 14/50 [06:14<15:38, 26.06s/it] 30%|███       | 15/50 [06:38<14:55, 25.58s/it] 32%|███▏      | 16/50 [07:09<15:16, 26.97s/it][loss] ep 8 it 30 total=7.2527 mle=1.5365 pcon=5.0355 forget=0.6806 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 80 total=7.2988 mle=1.5905 pcon=5.0336 forget=0.6747 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 130 total=7.2109 mle=1.5001 pcon=5.0315 forget=0.6793 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 180 total=7.4043 mle=1.7035 pcon=5.0296 forget=0.6712 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 230 total=7.2292 mle=1.5307 pcon=5.0281 forget=0.6704 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 280 total=7.3695 mle=1.6774 pcon=5.0262 forget=0.6660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 330 total=7.4646 mle=1.7564 pcon=5.0241 forget=0.6841 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 8 it 380 total=7.1788 mle=1.4894 pcon=5.0222 forget=0.6671 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 9 it 40 total=7.3145 mle=1.6363 pcon=5.0203 forget=0.6579 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 90 total=7.3565 mle=1.6697 pcon=5.0183 forget=0.6685 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 140 total=7.5353 mle=1.8422 pcon=5.0168 forget=0.6763 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 190 total=7.2628 mle=1.5783 pcon=5.0153 forget=0.6693 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 240 total=7.2522 mle=1.5677 pcon=5.0138 forget=0.6708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 290 total=7.2445 mle=1.5585 pcon=5.0119 forget=0.6741 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 9 it 340 total=7.2377 mle=1.5584 pcon=5.0105 forget=0.6688 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 10 it 0 total=7.2898 mle=1.6070 pcon=5.0088 forget=0.6740 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 50 total=7.2759 mle=1.5899 pcon=5.0071 forget=0.6789 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 100 total=7.4292 mle=1.7542 pcon=5.0056 forget=0.6694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 150 total=7.5211 mle=1.8512 pcon=5.0043 forget=0.6656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 200 total=7.5286 mle=1.8596 pcon=5.0028 forget=0.6662 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 250 total=7.2538 mle=1.5824 pcon=5.0015 forget=0.6699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 300 total=7.2255 mle=1.5624 pcon=5.0004 forget=0.6628 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 10 it 350 total=7.3463 mle=1.6672 pcon=4.9990 forget=0.6800 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 11 it 10 total=7.2497 mle=1.5763 pcon=4.9977 forget=0.6757 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 60 total=7.2851 mle=1.6223 pcon=4.9963 forget=0.6665 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 110 total=7.2272 mle=1.5690 pcon=4.9951 forget=0.6631 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 160 total=7.3284 mle=1.6656 pcon=4.9935 forget=0.6693 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 210 total=7.4475 mle=1.7817 pcon=4.9924 forget=0.6734 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 260 total=7.2673 mle=1.6036 pcon=4.9916 forget=0.6721 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 310 total=7.6282 mle=1.9667 pcon=4.9902 forget=0.6713 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 11 it 360 total=7.3195 mle=1.6707 pcon=4.9890 forget=0.6598 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 12 it 20 total=7.2762 mle=1.6200 pcon=4.9877 forget=0.6686 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 70 total=7.3526 mle=1.6950 pcon=4.9865 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 120 total=7.2105 mle=1.5488 pcon=4.9854 forget=0.6763 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 170 total=7.3937 mle=1.7458 pcon=4.9843 forget=0.6636 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 220 total=7.2486 mle=1.6001 pcon=4.9831 forget=0.6654 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 270 total=7.1928 mle=1.5356 pcon=4.9824 forget=0.6749 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 320 total=7.1722 mle=1.5207 pcon=4.9813 forget=0.6701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 12 it 370 total=7.5232 mle=1.8677 pcon=4.9803 forget=0.6752 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 13 it 30 total=7.0798 mle=1.4302 pcon=4.9794 forget=0.6702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 80 total=7.1605 mle=1.4956 pcon=4.9783 forget=0.6866 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 130 total=7.2184 mle=1.5713 pcon=4.9771 forget=0.6699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 180 total=7.1964 mle=1.5531 pcon=4.9763 forget=0.6670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 230 total=7.2927 mle=1.6447 pcon=4.9755 forget=0.6726 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 280 total=7.2220 mle=1.5833 pcon=4.9745 forget=0.6641 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 330 total=7.2342 mle=1.5896 pcon=4.9736 forget=0.6710 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 13 it 380 total=7.2180 mle=1.5786 pcon=4.9727 forget=0.6667 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 14 it 40 total=7.4021 mle=1.7689 pcon=4.9716 forget=0.6616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 90 total=7.4106 mle=1.7690 pcon=4.9710 forget=0.6707 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 140 total=7.2533 mle=1.6156 pcon=4.9706 forget=0.6671 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 190 total=7.5224 mle=1.8835 pcon=4.9696 forget=0.6693 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 240 total=7.2518 mle=1.6160 pcon=4.9690 forget=0.6668 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 290 total=7.2999 mle=1.6699 pcon=4.9682 forget=0.6618 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 14 it 340 total=7.3108 mle=1.6811 pcon=4.9673 forget=0.6624 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 15 it 0 total=7.3212 mle=1.6782 pcon=4.9667 forget=0.6764 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 50 total=7.1220 mle=1.4935 pcon=4.9658 forget=0.6627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 100 total=7.1938 mle=1.5577 pcon=4.9652 forget=0.6709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 150 total=7.3686 mle=1.7263 pcon=4.9645 forget=0.6778 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 200 total=7.2436 mle=1.6168 pcon=4.9639 forget=0.6629 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 250 total=7.5075 mle=1.8745 pcon=4.9631 forget=0.6698 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 300 total=7.2278 mle=1.5937 pcon=4.9626 forget=0.6715 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 15 it 350 total=7.1784 mle=1.5446 pcon=4.9619 forget=0.6719 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 16 it 10 total=7.2192 mle=1.5862 pcon=4.9613 forget=0.6718 nr=64 nf=64 protos=540 fproto_sim=NA
 34%|███▍      | 17/50 [07:31<14:03, 25.57s/it] 36%|███▌      | 18/50 [07:54<13:16, 24.90s/it] 38%|███▊      | 19/50 [08:26<13:59, 27.08s/it] 40%|████      | 20/50 [08:52<13:18, 26.62s/it] 42%|████▏     | 21/50 [09:16<12:27, 25.77s/it] 44%|████▍     | 22/50 [09:43<12:17, 26.35s/it] 46%|████▌     | 23/50 [10:08<11:38, 25.88s/it] 48%|████▊     | 24/50 [10:33<11:05, 25.61s/it][loss] ep 16 it 60 total=7.1184 mle=1.4931 pcon=4.9607 forget=0.6646 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 110 total=7.1399 mle=1.5136 pcon=4.9603 forget=0.6660 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 160 total=7.3187 mle=1.6990 pcon=4.9595 forget=0.6602 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 210 total=7.1908 mle=1.5732 pcon=4.9587 forget=0.6588 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 260 total=7.5667 mle=1.9477 pcon=4.9580 forget=0.6609 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 310 total=7.4170 mle=1.7941 pcon=4.9574 forget=0.6655 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 16 it 360 total=7.2327 mle=1.6144 pcon=4.9568 forget=0.6615 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 17 it 20 total=7.4026 mle=1.7778 pcon=4.9559 forget=0.6689 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 70 total=7.4389 mle=1.8237 pcon=4.9557 forget=0.6595 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 120 total=7.1954 mle=1.5744 pcon=4.9549 forget=0.6661 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 170 total=7.2033 mle=1.5830 pcon=4.9540 forget=0.6663 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 220 total=7.2885 mle=1.6655 pcon=4.9533 forget=0.6696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 270 total=7.0884 mle=1.4684 pcon=4.9533 forget=0.6667 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 320 total=7.4191 mle=1.8035 pcon=4.9527 forget=0.6628 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 17 it 370 total=7.3589 mle=1.7473 pcon=4.9519 forget=0.6596 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 30 total=7.1987 mle=1.5876 pcon=4.9519 forget=0.6592 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 80 total=7.3047 mle=1.6890 pcon=4.9514 forget=0.6643 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 130 total=7.2820 mle=1.6652 pcon=4.9508 forget=0.6659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 180 total=7.1649 mle=1.5478 pcon=4.9504 forget=0.6667 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 230 total=7.1994 mle=1.5911 pcon=4.9500 forget=0.6583 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 280 total=7.3000 mle=1.6880 pcon=4.9496 forget=0.6624 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 330 total=7.4186 mle=1.8086 pcon=4.9492 forget=0.6608 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 18 it 380 total=7.3172 mle=1.7016 pcon=4.9490 forget=0.6665 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 19 it 40 total=7.0817 mle=1.4657 pcon=4.9489 forget=0.6671 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 90 total=7.3231 mle=1.7031 pcon=4.9489 forget=0.6711 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 140 total=7.2224 mle=1.6123 pcon=4.9484 forget=0.6617 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 190 total=7.2475 mle=1.6441 pcon=4.9479 forget=0.6555 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 240 total=7.4591 mle=1.8487 pcon=4.9476 forget=0.6628 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 290 total=7.1758 mle=1.5589 pcon=4.9476 forget=0.6693 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 19 it 340 total=7.2237 mle=1.5974 pcon=4.9475 forget=0.6789 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 20 it 0 total=7.2521 mle=1.6448 pcon=4.9471 forget=0.6602 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 50 total=7.2907 mle=1.6856 pcon=4.9468 forget=0.6582 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 100 total=7.3405 mle=1.7324 pcon=4.9467 forget=0.6614 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 150 total=7.4382 mle=1.8367 pcon=4.9463 forget=0.6553 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 200 total=7.2594 mle=1.6595 pcon=4.9457 forget=0.6541 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 250 total=7.6297 mle=2.0237 pcon=4.9452 forget=0.6608 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 300 total=7.2654 mle=1.6576 pcon=4.9447 forget=0.6631 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 20 it 350 total=6.9958 mle=1.3906 pcon=4.9443 forget=0.6609 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 10 total=7.0206 mle=1.4141 pcon=4.9439 forget=0.6626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 60 total=7.2827 mle=1.6682 pcon=4.9436 forget=0.6709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 110 total=7.1192 mle=1.5041 pcon=4.9435 forget=0.6717 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 160 total=7.1587 mle=1.5594 pcon=4.9435 forget=0.6559 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 210 total=7.3941 mle=1.7864 pcon=4.9432 forget=0.6645 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 260 total=7.2885 mle=1.6853 pcon=4.9429 forget=0.6603 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 310 total=7.2247 mle=1.6160 pcon=4.9424 forget=0.6663 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 21 it 360 total=7.2624 mle=1.6509 pcon=4.9422 forget=0.6693 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 20 total=7.2684 mle=1.6641 pcon=4.9417 forget=0.6626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 70 total=7.4288 mle=1.8288 pcon=4.9413 forget=0.6587 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 120 total=7.1289 mle=1.5205 pcon=4.9407 forget=0.6676 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 170 total=7.2585 mle=1.6540 pcon=4.9405 forget=0.6640 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 220 total=7.3022 mle=1.6937 pcon=4.9403 forget=0.6682 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 270 total=7.1143 mle=1.5052 pcon=4.9401 forget=0.6691 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 320 total=7.2241 mle=1.6259 pcon=4.9399 forget=0.6582 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 22 it 370 total=7.6744 mle=2.0751 pcon=4.9394 forget=0.6599 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 30 total=7.1582 mle=1.5529 pcon=4.9397 forget=0.6656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 80 total=7.3010 mle=1.7000 pcon=4.9394 forget=0.6616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 130 total=7.1746 mle=1.5638 pcon=4.9390 forget=0.6718 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 180 total=7.3659 mle=1.7655 pcon=4.9387 forget=0.6617 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 230 total=7.2699 mle=1.6681 pcon=4.9387 forget=0.6631 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 280 total=7.1612 mle=1.5557 pcon=4.9384 forget=0.6671 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 330 total=7.2791 mle=1.6818 pcon=4.9382 forget=0.6591 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 23 it 380 total=7.1306 mle=1.5301 pcon=4.9378 forget=0.6626 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 24 it 40 total=7.1090 mle=1.5167 pcon=4.9377 forget=0.6545 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 90 total=7.2179 mle=1.6183 pcon=4.9376 forget=0.6620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 140 total=7.0233 mle=1.4165 pcon=4.9374 forget=0.6694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 190 total=7.2616 mle=1.6641 pcon=4.9374 forget=0.6601 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 240 total=7.1983 mle=1.5948 pcon=4.9372 forget=0.6663 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 290 total=7.4035 mle=1.7955 pcon=4.9371 forget=0.6709 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 24 it 340 total=7.3199 mle=1.7228 pcon=4.9368 forget=0.6602 nr=64 nf=64 protos=540 fproto_sim=NA
 50%|█████     | 25/50 [11:05<11:24, 27.40s/it] 52%|█████▏    | 26/50 [11:31<10:46, 26.94s/it] 54%|█████▍    | 27/50 [11:57<10:15, 26.77s/it] 56%|█████▌    | 28/50 [12:25<09:59, 27.27s/it] 58%|█████▊    | 29/50 [12:51<09:22, 26.80s/it] 60%|██████    | 30/50 [13:19<09:02, 27.11s/it] 62%|██████▏   | 31/50 [13:44<08:26, 26.64s/it] 64%|██████▍   | 32/50 [14:13<08:11, 27.31s/it] 66%|██████▌   | 33/50 [14:45<08:04, 28.52s/it] 68%|██████▊   | 34/50 [15:10<07:20, 27.50s/it][peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 25 it 0 total=7.2845 mle=1.6878 pcon=4.9365 forget=0.6601 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 50 total=7.4211 mle=1.8127 pcon=4.9366 forget=0.6718 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 100 total=7.2371 mle=1.6484 pcon=4.9363 forget=0.6524 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 150 total=7.4154 mle=1.8194 pcon=4.9366 forget=0.6595 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 200 total=7.4372 mle=1.8393 pcon=4.9364 forget=0.6616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 250 total=7.3938 mle=1.7950 pcon=4.9361 forget=0.6626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 300 total=7.2058 mle=1.6056 pcon=4.9360 forget=0.6641 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 25 it 350 total=7.2098 mle=1.6082 pcon=4.9362 forget=0.6654 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 26 it 10 total=7.0951 mle=1.4944 pcon=4.9362 forget=0.6645 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 60 total=7.1743 mle=1.5679 pcon=4.9362 forget=0.6702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 110 total=7.3696 mle=1.7757 pcon=4.9361 forget=0.6578 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 160 total=7.0583 mle=1.4656 pcon=4.9360 forget=0.6567 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 210 total=7.2544 mle=1.6546 pcon=4.9356 forget=0.6641 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 260 total=7.1302 mle=1.5243 pcon=4.9358 forget=0.6702 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 310 total=7.5129 mle=1.9195 pcon=4.9358 forget=0.6577 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 26 it 360 total=7.2132 mle=1.6186 pcon=4.9354 forget=0.6592 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 20 total=7.2795 mle=1.6785 pcon=4.9352 forget=0.6658 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 70 total=7.1726 mle=1.5751 pcon=4.9349 forget=0.6626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 120 total=7.1335 mle=1.5356 pcon=4.9347 forget=0.6632 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 170 total=7.1903 mle=1.5898 pcon=4.9344 forget=0.6662 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 220 total=7.1772 mle=1.5797 pcon=4.9342 forget=0.6632 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 270 total=7.2224 mle=1.6294 pcon=4.9339 forget=0.6590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 320 total=7.3230 mle=1.7241 pcon=4.9338 forget=0.6651 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 27 it 370 total=7.3765 mle=1.7731 pcon=4.9338 forget=0.6697 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 30 total=7.3211 mle=1.7204 pcon=4.9338 forget=0.6670 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 80 total=7.3528 mle=1.7521 pcon=4.9339 forget=0.6667 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 130 total=7.1577 mle=1.5560 pcon=4.9339 forget=0.6678 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 180 total=7.0606 mle=1.4677 pcon=4.9338 forget=0.6591 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 230 total=6.9803 mle=1.3892 pcon=4.9335 forget=0.6576 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 280 total=7.1422 mle=1.5368 pcon=4.9332 forget=0.6723 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 330 total=7.3912 mle=1.7949 pcon=4.9327 forget=0.6636 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 28 it 380 total=7.3625 mle=1.7587 pcon=4.9324 forget=0.6713 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 40 total=7.3677 mle=1.7723 pcon=4.9321 forget=0.6634 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 90 total=7.2037 mle=1.6150 pcon=4.9318 forget=0.6569 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 140 total=7.2673 mle=1.6745 pcon=4.9317 forget=0.6611 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 190 total=7.1620 mle=1.5714 pcon=4.9316 forget=0.6590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 240 total=7.0726 mle=1.4800 pcon=4.9315 forget=0.6611 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 290 total=7.4517 mle=1.8595 pcon=4.9315 forget=0.6608 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 29 it 340 total=7.2541 mle=1.6579 pcon=4.9316 forget=0.6646 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 0 total=7.2083 mle=1.6151 pcon=4.9316 forget=0.6616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 50 total=7.2221 mle=1.6242 pcon=4.9317 forget=0.6662 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 100 total=7.0643 mle=1.4711 pcon=4.9315 forget=0.6616 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 150 total=7.5112 mle=1.9182 pcon=4.9313 forget=0.6617 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 200 total=7.1035 mle=1.4923 pcon=4.9313 forget=0.6799 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 250 total=7.3699 mle=1.7724 pcon=4.9311 forget=0.6665 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 300 total=7.0872 mle=1.4886 pcon=4.9311 forget=0.6676 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 30 it 350 total=7.5826 mle=1.9841 pcon=4.9310 forget=0.6675 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 10 total=7.2011 mle=1.6069 pcon=4.9310 forget=0.6631 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 60 total=7.2077 mle=1.6104 pcon=4.9311 forget=0.6663 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 110 total=7.2024 mle=1.6017 pcon=4.9311 forget=0.6696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 160 total=7.1771 mle=1.5803 pcon=4.9311 forget=0.6656 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 210 total=7.1513 mle=1.5572 pcon=4.9311 forget=0.6630 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 260 total=7.0512 mle=1.4608 pcon=4.9309 forget=0.6595 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 310 total=7.1840 mle=1.5851 pcon=4.9309 forget=0.6680 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 31 it 360 total=7.1789 mle=1.5882 pcon=4.9307 forget=0.6600 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 20 total=7.3391 mle=1.7487 pcon=4.9306 forget=0.6598 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 70 total=7.2876 mle=1.6971 pcon=4.9302 forget=0.6604 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 120 total=7.1154 mle=1.5145 pcon=4.9301 forget=0.6708 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 170 total=7.1929 mle=1.5978 pcon=4.9303 forget=0.6648 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 220 total=7.4530 mle=1.8668 pcon=4.9304 forget=0.6558 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 270 total=7.1848 mle=1.5948 pcon=4.9305 forget=0.6595 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 320 total=7.0821 mle=1.4918 pcon=4.9304 forget=0.6599 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 32 it 370 total=7.0260 mle=1.4363 pcon=4.9307 forget=0.6590 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 30 total=7.2907 mle=1.6908 pcon=4.9308 forget=0.6692 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 80 total=7.6352 mle=2.0471 pcon=4.9307 forget=0.6574 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 130 total=7.1231 mle=1.5319 pcon=4.9306 forget=0.6607 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 180 total=7.4177 mle=1.8251 pcon=4.9306 forget=0.6620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 230 total=7.1369 mle=1.5393 pcon=4.9306 forget=0.6669 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 280 total=7.1829 mle=1.5851 pcon=4.9306 forget=0.6672 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 330 total=7.2280 mle=1.6388 pcon=4.9309 forget=0.6583 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 33 it 380 total=7.1489 mle=1.5497 pcon=4.9309 forget=0.6683 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 40 total=7.3691 mle=1.7718 pcon=4.9306 forget=0.6666 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 90 total=6.9956 mle=1.4040 pcon=4.9303 forget=0.6613 nr=64 nf=64 protos=540 fproto_sim=NA
 70%|███████   | 35/50 [15:32<06:26, 25.79s/it] 72%|███████▏  | 36/50 [15:58<06:05, 26.09s/it] 74%|███████▍  | 37/50 [16:24<05:37, 25.96s/it] 76%|███████▌  | 38/50 [16:48<05:04, 25.38s/it] 78%|███████▊  | 39/50 [17:19<04:58, 27.14s/it] 80%|████████  | 40/50 [17:46<04:28, 26.87s/it] 82%|████████▏ | 41/50 [18:11<03:57, 26.41s/it] 84%|████████▍ | 42/50 [18:40<03:37, 27.18s/it] 86%|████████▌ | 43/50 [19:05<03:05, 26.52s/it][loss] ep 34 it 140 total=7.1828 mle=1.5976 pcon=4.9304 forget=0.6548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 190 total=7.3246 mle=1.7381 pcon=4.9304 forget=0.6561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 240 total=7.4967 mle=1.9068 pcon=4.9302 forget=0.6597 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 290 total=7.1937 mle=1.5981 pcon=4.9298 forget=0.6658 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 34 it 340 total=7.1780 mle=1.5802 pcon=4.9297 forget=0.6680 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 0 total=7.3196 mle=1.7311 pcon=4.9297 forget=0.6589 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 50 total=7.0837 mle=1.4953 pcon=4.9297 forget=0.6587 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 100 total=7.4170 mle=1.8252 pcon=4.9293 forget=0.6626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 150 total=7.2875 mle=1.7024 pcon=4.9293 forget=0.6558 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 200 total=7.2346 mle=1.6358 pcon=4.9293 forget=0.6695 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 250 total=7.3062 mle=1.7132 pcon=4.9290 forget=0.6639 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 300 total=7.0535 mle=1.4738 pcon=4.9290 forget=0.6508 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 35 it 350 total=7.3558 mle=1.7654 pcon=4.9289 forget=0.6615 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 10 total=7.4709 mle=1.8788 pcon=4.9289 forget=0.6632 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 60 total=7.1097 mle=1.5241 pcon=4.9290 forget=0.6566 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 110 total=7.2726 mle=1.6842 pcon=4.9288 forget=0.6595 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 160 total=7.2906 mle=1.7010 pcon=4.9287 forget=0.6608 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 210 total=7.0540 mle=1.4688 pcon=4.9288 forget=0.6564 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 260 total=7.1745 mle=1.5754 pcon=4.9291 forget=0.6700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 310 total=7.3714 mle=1.7700 pcon=4.9288 forget=0.6726 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 36 it 360 total=7.0964 mle=1.5039 pcon=4.9288 forget=0.6637 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 37 it 20 total=7.0611 mle=1.4693 pcon=4.9284 forget=0.6634 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 70 total=7.3886 mle=1.7883 pcon=4.9285 forget=0.6718 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 120 total=7.1975 mle=1.6066 pcon=4.9283 forget=0.6627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 170 total=7.2711 mle=1.6868 pcon=4.9280 forget=0.6562 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 220 total=7.3845 mle=1.7892 pcon=4.9277 forget=0.6676 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 270 total=7.2646 mle=1.6845 pcon=4.9276 forget=0.6525 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 320 total=7.2251 mle=1.6411 pcon=4.9274 forget=0.6566 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 37 it 370 total=7.2016 mle=1.6035 pcon=4.9275 forget=0.6707 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 30 total=7.3972 mle=1.8022 pcon=4.9276 forget=0.6674 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 80 total=7.1064 mle=1.5167 pcon=4.9276 forget=0.6621 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 130 total=7.2989 mle=1.7085 pcon=4.9276 forget=0.6627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 180 total=7.2139 mle=1.6237 pcon=4.9275 forget=0.6627 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 230 total=7.1773 mle=1.5840 pcon=4.9274 forget=0.6659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 280 total=7.2587 mle=1.6689 pcon=4.9274 forget=0.6625 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 330 total=7.2617 mle=1.6725 pcon=4.9270 forget=0.6622 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 38 it 380 total=7.1727 mle=1.5761 pcon=4.9272 forget=0.6694 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 40 total=7.4487 mle=1.8568 pcon=4.9274 forget=0.6644 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 90 total=7.3439 mle=1.7565 pcon=4.9273 forget=0.6601 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 140 total=7.2445 mle=1.6550 pcon=4.9274 forget=0.6620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 190 total=7.1265 mle=1.5416 pcon=4.9273 forget=0.6576 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 240 total=7.3833 mle=1.7994 pcon=4.9273 forget=0.6565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 290 total=7.0813 mle=1.4905 pcon=4.9273 forget=0.6635 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 39 it 340 total=7.2508 mle=1.6607 pcon=4.9273 forget=0.6628 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 0 total=7.2839 mle=1.6845 pcon=4.9271 forget=0.6722 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 50 total=7.2624 mle=1.6654 pcon=4.9270 forget=0.6700 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 100 total=7.2105 mle=1.6277 pcon=4.9267 forget=0.6561 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 150 total=7.0468 mle=1.4599 pcon=4.9268 forget=0.6601 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 200 total=7.3389 mle=1.7434 pcon=4.9266 forget=0.6690 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 250 total=7.5307 mle=1.9439 pcon=4.9266 forget=0.6603 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 300 total=7.0850 mle=1.4881 pcon=4.9268 forget=0.6701 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 40 it 350 total=6.9883 mle=1.4082 pcon=4.9269 forget=0.6532 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 10 total=7.0945 mle=1.5109 pcon=4.9270 forget=0.6566 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 60 total=7.6105 mle=2.0129 pcon=4.9271 forget=0.6705 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 110 total=7.2261 mle=1.6342 pcon=4.9269 forget=0.6650 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 160 total=7.0971 mle=1.5035 pcon=4.9271 forget=0.6665 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 210 total=7.1354 mle=1.5458 pcon=4.9271 forget=0.6626 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 260 total=7.0829 mle=1.4961 pcon=4.9274 forget=0.6594 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 310 total=7.1319 mle=1.5451 pcon=4.9276 forget=0.6591 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 41 it 360 total=7.2739 mle=1.6847 pcon=4.9278 forget=0.6614 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 42 it 20 total=7.2185 mle=1.6262 pcon=4.9279 forget=0.6644 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 70 total=7.2566 mle=1.6649 pcon=4.9278 forget=0.6639 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 120 total=7.6005 mle=2.0060 pcon=4.9276 forget=0.6669 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 170 total=7.1473 mle=1.5567 pcon=4.9277 forget=0.6629 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 220 total=7.2174 mle=1.6251 pcon=4.9278 forget=0.6645 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 270 total=7.3498 mle=1.7611 pcon=4.9279 forget=0.6608 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 320 total=7.2851 mle=1.6965 pcon=4.9281 forget=0.6605 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 42 it 370 total=7.3122 mle=1.7243 pcon=4.9282 forget=0.6597 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 43 it 30 total=7.1805 mle=1.5843 pcon=4.9282 forget=0.6680 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 80 total=7.1518 mle=1.5538 pcon=4.9283 forget=0.6696 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 130 total=7.3021 mle=1.7085 pcon=4.9283 forget=0.6652 nr=64 nf=64 protos=540 fproto_sim=NA
 88%|████████▊ | 44/50 [19:30<02:37, 26.22s/it] 90%|█████████ | 45/50 [19:57<02:11, 26.34s/it] 92%|█████████▏| 46/50 [20:24<01:46, 26.50s/it] 94%|█████████▍| 47/50 [20:48<01:17, 25.92s/it] 96%|█████████▌| 48/50 [21:17<00:53, 26.70s/it] 98%|█████████▊| 49/50 [21:42<00:26, 26.32s/it]100%|██████████| 50/50 [22:07<00:00, 25.93s/it]100%|██████████| 50/50 [22:07<00:00, 26.56s/it]
[loss] ep 43 it 180 total=7.0809 mle=1.4793 pcon=4.9285 forget=0.6730 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 230 total=7.2407 mle=1.6501 pcon=4.9284 forget=0.6622 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 280 total=7.1287 mle=1.5453 pcon=4.9286 forget=0.6548 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 330 total=7.1366 mle=1.5450 pcon=4.9285 forget=0.6631 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 43 it 380 total=7.0905 mle=1.5039 pcon=4.9285 forget=0.6581 nr=64 nf=64 protos=540 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
[loss] ep 44 it 40 total=7.4103 mle=1.8178 pcon=4.9285 forget=0.6640 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 90 total=7.1748 mle=1.5913 pcon=4.9284 forget=0.6551 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 140 total=7.3859 mle=1.7906 pcon=4.9286 forget=0.6667 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 190 total=6.9819 mle=1.3912 pcon=4.9284 forget=0.6623 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 240 total=7.2004 mle=1.6008 pcon=4.9284 forget=0.6713 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 290 total=7.4066 mle=1.8105 pcon=4.9283 forget=0.6679 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 44 it 340 total=7.5682 mle=1.9792 pcon=4.9284 forget=0.6606 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 0 total=7.1520 mle=1.5578 pcon=4.9283 forget=0.6659 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 50 total=7.1954 mle=1.6125 pcon=4.9282 forget=0.6547 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 100 total=7.1903 mle=1.6045 pcon=4.9281 forget=0.6578 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 150 total=7.1707 mle=1.5760 pcon=4.9280 forget=0.6666 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 200 total=7.1882 mle=1.5922 pcon=4.9281 forget=0.6679 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 250 total=7.0584 mle=1.4699 pcon=4.9281 forget=0.6603 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 300 total=7.2756 mle=1.6844 pcon=4.9278 forget=0.6633 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 45 it 350 total=7.0813 mle=1.4902 pcon=4.9280 forget=0.6631 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 10 total=7.0734 mle=1.4808 pcon=4.9280 forget=0.6646 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 60 total=7.0286 mle=1.4376 pcon=4.9282 forget=0.6629 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 110 total=7.2889 mle=1.6975 pcon=4.9282 forget=0.6632 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 160 total=7.0732 mle=1.4845 pcon=4.9281 forget=0.6607 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 210 total=7.3139 mle=1.7279 pcon=4.9280 forget=0.6579 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 260 total=7.2561 mle=1.6709 pcon=4.9277 forget=0.6575 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 310 total=7.0969 mle=1.5078 pcon=4.9278 forget=0.6612 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 46 it 360 total=7.2684 mle=1.6739 pcon=4.9278 forget=0.6667 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 20 total=7.2199 mle=1.6243 pcon=4.9276 forget=0.6680 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 70 total=7.1683 mle=1.5729 pcon=4.9274 forget=0.6679 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 120 total=7.2191 mle=1.6229 pcon=4.9273 forget=0.6690 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 170 total=7.1150 mle=1.5132 pcon=4.9270 forget=0.6747 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 220 total=7.0333 mle=1.4376 pcon=4.9270 forget=0.6687 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 270 total=7.1718 mle=1.5829 pcon=4.9271 forget=0.6618 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 320 total=7.2134 mle=1.6236 pcon=4.9268 forget=0.6629 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 47 it 370 total=7.3010 mle=1.7140 pcon=4.9267 forget=0.6603 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 30 total=7.3417 mle=1.7415 pcon=4.9269 forget=0.6733 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 80 total=7.2086 mle=1.6118 pcon=4.9269 forget=0.6699 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 130 total=7.3105 mle=1.7214 pcon=4.9271 forget=0.6620 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 180 total=7.2966 mle=1.7070 pcon=4.9271 forget=0.6625 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 230 total=7.3781 mle=1.7928 pcon=4.9273 forget=0.6579 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 280 total=7.0861 mle=1.5009 pcon=4.9271 forget=0.6581 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 330 total=7.0986 mle=1.5152 pcon=4.9270 forget=0.6565 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 48 it 380 total=7.1923 mle=1.6075 pcon=4.9267 forget=0.6582 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 40 total=7.1456 mle=1.5566 pcon=4.9268 forget=0.6622 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 90 total=7.1882 mle=1.6025 pcon=4.9267 forget=0.6591 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 140 total=7.2221 mle=1.6305 pcon=4.9266 forget=0.6649 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 190 total=7.3381 mle=1.7515 pcon=4.9263 forget=0.6602 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 240 total=7.2611 mle=1.6792 pcon=4.9261 forget=0.6558 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 290 total=7.2496 mle=1.6631 pcon=4.9259 forget=0.6606 nr=64 nf=64 protos=540 fproto_sim=NA
[loss] ep 49 it 340 total=7.0581 mle=1.4754 pcon=4.9262 forget=0.6565 nr=64 nf=64 protos=540 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid-planB_adapter
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:28,  2.62it/s]  3%|▎         | 10/391 [00:00<00:14, 26.05it/s]  5%|▍         | 19/391 [00:00<00:08, 43.18it/s]  7%|▋         | 28/391 [00:00<00:06, 55.66it/s]  9%|▉         | 36/391 [00:00<00:06, 56.92it/s] 11%|█         | 43/391 [00:00<00:06, 53.44it/s] 13%|█▎        | 50/391 [00:01<00:06, 53.55it/s] 15%|█▌        | 59/391 [00:01<00:05, 62.14it/s] 17%|█▋        | 68/391 [00:01<00:04, 69.18it/s] 20%|█▉        | 77/391 [00:01<00:04, 74.18it/s] 22%|██▏       | 86/391 [00:01<00:03, 78.32it/s] 24%|██▍       | 95/391 [00:01<00:04, 73.08it/s] 26%|██▋       | 103/391 [00:01<00:04, 63.56it/s] 28%|██▊       | 110/391 [00:02<00:05, 53.37it/s] 30%|██▉       | 116/391 [00:02<00:05, 51.66it/s] 31%|███       | 122/391 [00:02<00:05, 50.34it/s] 33%|███▎      | 128/391 [00:02<00:05, 49.43it/s] 34%|███▍      | 134/391 [00:02<00:05, 48.71it/s] 36%|███▌      | 139/391 [00:02<00:05, 48.25it/s] 37%|███▋      | 144/391 [00:02<00:05, 47.90it/s] 38%|███▊      | 149/391 [00:02<00:05, 47.65it/s] 39%|███▉      | 154/391 [00:02<00:04, 47.49it/s] 41%|████      | 159/391 [00:03<00:04, 47.38it/s] 42%|████▏     | 164/391 [00:03<00:04, 47.53it/s] 43%|████▎     | 169/391 [00:03<00:04, 46.95it/s] 45%|████▍     | 174/391 [00:03<00:05, 40.87it/s] 46%|████▌     | 179/391 [00:03<00:05, 37.51it/s] 47%|████▋     | 183/391 [00:03<00:05, 37.06it/s] 48%|████▊     | 187/391 [00:03<00:05, 36.95it/s] 49%|████▉     | 192/391 [00:03<00:05, 39.58it/s] 50%|█████     | 197/391 [00:04<00:04, 42.07it/s] 52%|█████▏    | 202/391 [00:04<00:04, 43.42it/s] 53%|█████▎    | 207/391 [00:04<00:04, 44.39it/s] 54%|█████▍    | 212/391 [00:04<00:03, 45.36it/s] 55%|█████▌    | 217/391 [00:04<00:03, 45.86it/s] 57%|█████▋    | 222/391 [00:04<00:03, 46.21it/s] 58%|█████▊    | 227/391 [00:04<00:03, 46.45it/s] 59%|█████▉    | 232/391 [00:04<00:03, 46.64it/s] 61%|██████    | 237/391 [00:04<00:03, 46.71it/s] 62%|██████▏   | 242/391 [00:04<00:03, 46.84it/s] 63%|██████▎   | 247/391 [00:05<00:03, 45.45it/s] 64%|██████▍   | 252/391 [00:05<00:03, 39.99it/s] 66%|██████▌   | 257/391 [00:05<00:03, 37.74it/s] 67%|██████▋   | 261/391 [00:05<00:03, 35.83it/s] 68%|██████▊   | 266/391 [00:05<00:03, 37.51it/s] 69%|██████▉   | 271/391 [00:05<00:03, 39.93it/s] 71%|███████   | 276/391 [00:05<00:02, 41.82it/s] 72%|███████▏  | 281/391 [00:05<00:02, 43.14it/s] 73%|███████▎  | 286/391 [00:06<00:02, 44.26it/s] 74%|███████▍  | 291/391 [00:06<00:02, 45.02it/s] 76%|███████▌  | 296/391 [00:06<00:02, 45.52it/s] 77%|███████▋  | 301/391 [00:06<00:01, 45.60it/s] 78%|███████▊  | 306/391 [00:06<00:01, 45.89it/s] 80%|███████▉  | 311/391 [00:06<00:01, 46.08it/s] 81%|████████  | 316/391 [00:06<00:01, 46.23it/s] 82%|████████▏ | 321/391 [00:06<00:01, 46.33it/s] 83%|████████▎ | 326/391 [00:06<00:01, 43.07it/s] 85%|████████▍ | 331/391 [00:07<00:01, 38.63it/s] 86%|████████▌ | 335/391 [00:07<00:01, 37.87it/s] 87%|████████▋ | 339/391 [00:07<00:01, 35.79it/s] 88%|████████▊ | 344/391 [00:07<00:01, 37.92it/s] 89%|████████▉ | 349/391 [00:07<00:01, 40.29it/s] 91%|█████████ | 354/391 [00:07<00:00, 42.04it/s] 92%|█████████▏| 359/391 [00:07<00:00, 43.36it/s] 93%|█████████▎| 364/391 [00:07<00:00, 44.30it/s] 94%|█████████▍| 369/391 [00:08<00:00, 45.23it/s] 96%|█████████▌| 374/391 [00:08<00:00, 45.64it/s] 97%|█████████▋| 379/391 [00:08<00:00, 46.26it/s] 98%|█████████▊| 384/391 [00:08<00:00, 46.41it/s] 99%|█████████▉| 389/391 [00:08<00:00, 46.51it/s]100%|██████████| 391/391 [00:08<00:00, 46.06it/s]
50000 images processed, 8.590235710144043 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:20,  3.86it/s]  6%|▋         | 5/79 [00:00<00:04, 15.41it/s] 13%|█▎        | 10/79 [00:00<00:02, 25.88it/s] 19%|█▉        | 15/79 [00:00<00:01, 32.64it/s] 25%|██▌       | 20/79 [00:00<00:01, 37.08it/s] 32%|███▏      | 25/79 [00:00<00:01, 40.10it/s] 38%|███▊      | 30/79 [00:00<00:01, 42.11it/s] 44%|████▍     | 35/79 [00:01<00:01, 43.30it/s] 51%|█████     | 40/79 [00:01<00:00, 44.35it/s] 57%|█████▋    | 45/79 [00:01<00:00, 45.07it/s] 63%|██████▎   | 50/79 [00:01<00:00, 46.18it/s] 70%|██████▉   | 55/79 [00:01<00:00, 46.45it/s] 76%|███████▌  | 60/79 [00:01<00:00, 46.97it/s] 82%|████████▏ | 65/79 [00:01<00:00, 46.80it/s] 89%|████████▊ | 70/79 [00:01<00:00, 40.61it/s] 95%|█████████▍| 75/79 [00:01<00:00, 37.25it/s]100%|██████████| 79/79 [00:02<00:00, 36.97it/s]100%|██████████| 79/79 [00:02<00:00, 37.77it/s]
10000 images processed, 2.1141815185546875 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:11,  2.84it/s]  3%|▎         | 6/204 [00:00<00:12, 15.58it/s]  5%|▌         | 11/204 [00:00<00:07, 24.67it/s]  8%|▊         | 16/204 [00:00<00:06, 31.09it/s] 10%|█         | 21/204 [00:00<00:05, 35.68it/s] 13%|█▎        | 26/204 [00:00<00:04, 38.86it/s] 15%|█▌        | 31/204 [00:01<00:04, 41.10it/s] 18%|█▊        | 36/204 [00:01<00:03, 42.64it/s] 20%|██        | 41/204 [00:01<00:03, 42.66it/s] 23%|██▎       | 46/204 [00:01<00:03, 43.50it/s] 25%|██▌       | 51/204 [00:01<00:03, 44.28it/s] 27%|██▋       | 56/204 [00:01<00:03, 38.01it/s] 30%|██▉       | 61/204 [00:01<00:03, 37.97it/s] 33%|███▎      | 67/204 [00:01<00:03, 41.75it/s] 37%|███▋      | 76/204 [00:01<00:02, 53.76it/s] 42%|████▏     | 85/204 [00:02<00:01, 63.06it/s] 46%|████▌     | 94/204 [00:02<00:01, 69.98it/s] 50%|█████     | 103/204 [00:02<00:01, 75.24it/s] 55%|█████▍    | 112/204 [00:02<00:01, 79.03it/s] 59%|█████▉    | 121/204 [00:02<00:01, 63.92it/s] 63%|██████▎   | 129/204 [00:02<00:01, 57.31it/s] 67%|██████▋   | 136/204 [00:02<00:01, 54.03it/s] 70%|██████▉   | 142/204 [00:03<00:01, 52.26it/s] 73%|███████▎  | 148/204 [00:03<00:01, 50.63it/s] 75%|███████▌  | 154/204 [00:03<00:01, 49.43it/s] 78%|███████▊  | 160/204 [00:03<00:00, 48.57it/s] 81%|████████  | 165/204 [00:03<00:00, 48.36it/s] 83%|████████▎ | 170/204 [00:03<00:00, 47.88it/s] 86%|████████▌ | 175/204 [00:03<00:00, 47.57it/s] 88%|████████▊ | 180/204 [00:03<00:00, 46.21it/s] 91%|█████████ | 185/204 [00:04<00:00, 39.58it/s] 93%|█████████▎| 190/204 [00:04<00:00, 36.62it/s] 95%|█████████▌| 194/204 [00:04<00:00, 35.72it/s] 98%|█████████▊| 199/204 [00:04<00:00, 38.46it/s]100%|██████████| 204/204 [00:04<00:00, 40.48it/s]100%|██████████| 204/204 [00:04<00:00, 45.02it/s]
26032 images processed, 4.592225790023804 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:40,  1.92it/s]  8%|▊         | 6/79 [00:00<00:06, 12.11it/s] 14%|█▍        | 11/79 [00:00<00:03, 20.56it/s] 20%|██        | 16/79 [00:00<00:02, 27.32it/s] 27%|██▋       | 21/79 [00:00<00:01, 32.40it/s] 33%|███▎      | 26/79 [00:01<00:01, 35.40it/s] 42%|████▏     | 33/79 [00:01<00:01, 42.35it/s] 48%|████▊     | 38/79 [00:01<00:00, 43.06it/s] 54%|█████▍    | 43/79 [00:01<00:00, 40.11it/s] 62%|██████▏   | 49/79 [00:01<00:00, 44.68it/s] 70%|██████▉   | 55/79 [00:01<00:00, 48.31it/s] 77%|███████▋  | 61/79 [00:01<00:00, 42.42it/s] 84%|████████▎ | 66/79 [00:01<00:00, 43.59it/s] 91%|█████████ | 72/79 [00:02<00:00, 46.30it/s]100%|██████████| 79/79 [00:02<00:00, 37.05it/s]
10000 images processed, 2.170550584793091 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:31,  2.46it/s]  5%|▌         | 4/79 [00:00<00:07,  9.44it/s] 11%|█▏        | 9/79 [00:00<00:03, 19.56it/s] 18%|█▊        | 14/79 [00:00<00:02, 27.21it/s] 24%|██▍       | 19/79 [00:00<00:01, 32.61it/s] 30%|███       | 24/79 [00:00<00:01, 36.71it/s] 37%|███▋      | 29/79 [00:01<00:01, 39.62it/s] 43%|████▎     | 34/79 [00:01<00:01, 41.73it/s] 49%|████▉     | 39/79 [00:01<00:00, 43.20it/s] 56%|█████▌    | 44/79 [00:01<00:00, 44.25it/s] 62%|██████▏   | 49/79 [00:01<00:00, 44.98it/s] 68%|██████▊   | 54/79 [00:01<00:00, 45.52it/s] 75%|███████▍  | 59/79 [00:01<00:00, 40.07it/s] 81%|████████  | 64/79 [00:01<00:00, 39.42it/s] 87%|████████▋ | 69/79 [00:01<00:00, 41.86it/s] 99%|█████████▊| 78/79 [00:02<00:00, 53.90it/s]100%|██████████| 79/79 [00:02<00:00, 37.68it/s]
10000 images processed, 2.1251089572906494 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:28,  2.44it/s]  9%|▊         | 6/70 [00:00<00:04, 14.51it/s] 16%|█▌        | 11/70 [00:00<00:02, 23.52it/s] 23%|██▎       | 16/70 [00:00<00:01, 30.13it/s] 30%|███       | 21/70 [00:00<00:01, 34.93it/s] 37%|███▋      | 26/70 [00:00<00:01, 38.32it/s] 44%|████▍     | 31/70 [00:01<00:00, 40.78it/s] 51%|█████▏    | 36/70 [00:01<00:00, 42.47it/s] 59%|█████▊    | 41/70 [00:01<00:00, 43.71it/s] 66%|██████▌   | 46/70 [00:01<00:00, 43.33it/s] 73%|███████▎  | 51/70 [00:01<00:00, 37.62it/s] 79%|███████▊  | 55/70 [00:01<00:00, 36.25it/s] 84%|████████▍ | 59/70 [00:01<00:00, 34.02it/s] 91%|█████████▏| 64/70 [00:01<00:00, 37.30it/s]100%|██████████| 70/70 [00:02<00:00, 42.49it/s]100%|██████████| 70/70 [00:02<00:00, 34.49it/s]
8925 images processed, 2.063464641571045 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:36,  1.22it/s]  4%|▍         | 2/45 [00:00<00:18,  2.32it/s] 16%|█▌        | 7/45 [00:01<00:03, 10.05it/s] 22%|██▏       | 10/45 [00:01<00:03,  8.86it/s] 31%|███       | 14/45 [00:01<00:02, 13.04it/s] 38%|███▊      | 17/45 [00:01<00:02, 12.05it/s] 44%|████▍     | 20/45 [00:01<00:01, 14.64it/s] 51%|█████     | 23/45 [00:02<00:01, 14.17it/s] 56%|█████▌    | 25/45 [00:02<00:01, 12.81it/s] 64%|██████▍   | 29/45 [00:02<00:01, 13.73it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.35it/s] 89%|████████▉ | 40/45 [00:03<00:00, 16.62it/s] 93%|█████████▎| 42/45 [00:03<00:00, 10.11it/s]100%|██████████| 45/45 [00:04<00:00, 11.20it/s]
5640 images processed, 4.040027141571045 seconds used

27.682569980621338
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.52  99.36
places365     67.88  81.18
LSUN          17.51  96.09
iSUN          72.36  81.66
dtd           37.89  91.38
AVG           39.63  89.93
Retain-Acc: 0.7408
Forget-as-OOD (retain known vs forget novel):
  FPR: 79.50 AUROC: 86.88 AUIN: 98.36
10.65542459487915
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e50-lr0.0001-wd1e-4-ltboth-bfmbalanced-fl0.1-lora_r8a32d0.05-temp0.08-fullgrid_rf.png
[Full Grid] Completed. Manifest saved to: evaluation_results/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-fullgrid_runs.csv
