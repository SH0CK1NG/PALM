nohup: ignoring input
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, palm_enable=True, palm_mle_mode='all', palm_retain_only=False, pcon_inc=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:168: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:15<01:03, 15.79s/it] 40%|████      | 2/5 [00:27<00:39, 13.28s/it] 60%|██████    | 3/5 [00:38<00:24, 12.36s/it] 80%|████████  | 4/5 [00:48<00:11, 11.54s/it]100%|██████████| 5/5 [00:59<00:00, 11.15s/it]100%|██████████| 5/5 [00:59<00:00, 11.86s/it]
[loss] ep 0 it 0 total=9.2464 mle=1.5918 pcon=5.2950 forget=2.3595 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=9.2172 mle=1.5427 pcon=5.2893 forget=2.3852 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=9.1528 mle=1.4807 pcon=5.2835 forget=2.3886 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=9.1409 mle=1.5235 pcon=5.2780 forget=2.3394 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=9.1266 mle=1.5433 pcon=5.2723 forget=2.3110 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=9.2534 mle=1.5947 pcon=5.2669 forget=2.3918 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=9.1123 mle=1.4607 pcon=5.2616 forget=2.3901 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=9.0806 mle=1.4115 pcon=5.2563 forget=2.4128 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 1 it 10 total=9.1707 mle=1.5101 pcon=5.2514 forget=2.4092 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=9.0501 mle=1.3923 pcon=5.2463 forget=2.4115 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=9.1731 mle=1.5419 pcon=5.2413 forget=2.3898 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=9.0243 mle=1.3752 pcon=5.2365 forget=2.4125 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=9.1646 mle=1.5296 pcon=5.2317 forget=2.4034 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=9.1580 mle=1.5847 pcon=5.2272 forget=2.3461 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=9.1620 mle=1.5568 pcon=5.2231 forget=2.3821 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=9.0180 mle=1.4152 pcon=5.2187 forget=2.3841 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 2 it 20 total=8.9866 mle=1.4034 pcon=5.2143 forget=2.3690 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=9.0149 mle=1.3854 pcon=5.2098 forget=2.4196 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=9.0632 mle=1.4537 pcon=5.2054 forget=2.4041 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=8.9454 mle=1.3731 pcon=5.2014 forget=2.3708 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=9.0904 mle=1.5263 pcon=5.1973 forget=2.3667 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=9.0291 mle=1.4579 pcon=5.1934 forget=2.3779 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=9.0760 mle=1.4984 pcon=5.1895 forget=2.3880 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=8.9672 mle=1.3829 pcon=5.1856 forget=2.3987 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 3 it 30 total=9.0741 mle=1.4669 pcon=5.1820 forget=2.4253 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=9.0809 mle=1.4756 pcon=5.1779 forget=2.4274 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=8.9774 mle=1.3961 pcon=5.1743 forget=2.4070 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=8.9709 mle=1.3943 pcon=5.1704 forget=2.4062 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=8.9436 mle=1.3596 pcon=5.1669 forget=2.4172 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=9.0452 mle=1.4910 pcon=5.1636 forget=2.3906 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=8.9636 mle=1.4165 pcon=5.1608 forget=2.3863 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=8.9646 mle=1.4197 pcon=5.1574 forget=2.3875 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[loss] ep 4 it 40 total=8.8706 mle=1.3396 pcon=5.1543 forget=2.3768 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=8.9022 mle=1.3715 pcon=5.1513 forget=2.3794 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=8.9064 mle=1.3422 pcon=5.1482 forget=2.4160 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=9.0597 mle=1.5136 pcon=5.1451 forget=2.4010 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=9.1052 mle=1.5811 pcon=5.1420 forget=2.3821 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=9.0220 mle=1.4747 pcon=5.1391 forget=2.4082 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=8.8533 mle=1.3718 pcon=5.1361 forget=2.3453 orth=0.0000 favg=0.0000 nr=128 nf=64 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08-planB_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<03:25,  1.90it/s]  3%|▎         | 11/391 [00:00<00:17, 22.34it/s]  5%|▌         | 21/391 [00:00<00:09, 39.28it/s]  8%|▊         | 31/391 [00:00<00:06, 53.11it/s] 10%|█         | 41/391 [00:00<00:05, 64.48it/s] 13%|█▎        | 51/391 [00:01<00:04, 73.08it/s] 16%|█▌        | 61/391 [00:01<00:04, 79.65it/s] 18%|█▊        | 71/391 [00:01<00:03, 84.25it/s] 21%|██        | 81/391 [00:01<00:03, 87.70it/s] 23%|██▎       | 91/391 [00:01<00:03, 89.76it/s] 26%|██▌       | 101/391 [00:01<00:03, 91.66it/s] 28%|██▊       | 111/391 [00:01<00:03, 93.03it/s] 31%|███       | 121/391 [00:01<00:02, 93.83it/s] 34%|███▎      | 131/391 [00:01<00:02, 94.45it/s] 36%|███▌      | 141/391 [00:02<00:02, 94.90it/s] 39%|███▊      | 151/391 [00:02<00:02, 95.23it/s] 41%|████      | 161/391 [00:02<00:02, 95.42it/s] 44%|████▎     | 171/391 [00:02<00:02, 95.59it/s] 46%|████▋     | 181/391 [00:02<00:02, 95.68it/s] 49%|████▉     | 191/391 [00:02<00:02, 95.76it/s] 51%|█████▏    | 201/391 [00:02<00:01, 95.90it/s] 54%|█████▍    | 211/391 [00:02<00:01, 95.90it/s] 57%|█████▋    | 221/391 [00:02<00:01, 96.05it/s] 59%|█████▉    | 231/391 [00:02<00:01, 95.99it/s] 62%|██████▏   | 241/391 [00:03<00:01, 95.79it/s] 64%|██████▍   | 251/391 [00:03<00:01, 95.67it/s] 67%|██████▋   | 261/391 [00:03<00:01, 95.75it/s] 69%|██████▉   | 271/391 [00:03<00:01, 95.99it/s] 72%|███████▏  | 281/391 [00:03<00:01, 95.92it/s] 74%|███████▍  | 291/391 [00:03<00:01, 95.83it/s] 77%|███████▋  | 301/391 [00:03<00:00, 95.48it/s] 80%|███████▉  | 311/391 [00:03<00:00, 95.88it/s] 82%|████████▏ | 321/391 [00:03<00:00, 95.84it/s] 85%|████████▍ | 331/391 [00:03<00:00, 95.63it/s] 87%|████████▋ | 341/391 [00:04<00:00, 95.53it/s] 90%|████████▉ | 351/391 [00:04<00:00, 95.35it/s] 92%|█████████▏| 361/391 [00:04<00:00, 94.78it/s] 95%|█████████▍| 371/391 [00:04<00:00, 95.55it/s] 97%|█████████▋| 381/391 [00:04<00:00, 96.21it/s]100%|██████████| 391/391 [00:04<00:00, 94.41it/s]100%|██████████| 391/391 [00:04<00:00, 84.72it/s]
50000 images processed, 4.731793403625488 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:36,  2.16it/s] 11%|█▏        | 9/79 [00:00<00:03, 19.98it/s] 24%|██▍       | 19/79 [00:00<00:01, 38.77it/s] 37%|███▋      | 29/79 [00:00<00:00, 52.90it/s] 49%|████▉     | 39/79 [00:00<00:00, 63.51it/s] 61%|██████    | 48/79 [00:01<00:00, 70.46it/s] 73%|███████▎  | 58/79 [00:01<00:00, 77.20it/s] 86%|████████▌ | 68/79 [00:01<00:00, 81.96it/s] 99%|█████████▊| 78/79 [00:01<00:00, 85.29it/s]100%|██████████| 79/79 [00:01<00:00, 52.65it/s]
10000 images processed, 1.5431089401245117 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:32,  2.19it/s]  5%|▍         | 10/204 [00:00<00:08, 22.68it/s] 10%|▉         | 20/204 [00:00<00:04, 41.28it/s] 15%|█▍        | 30/204 [00:00<00:03, 55.61it/s] 20%|█▉        | 40/204 [00:00<00:02, 66.51it/s] 25%|██▍       | 50/204 [00:00<00:02, 74.67it/s] 29%|██▉       | 60/204 [00:01<00:01, 80.65it/s] 34%|███▍      | 70/204 [00:01<00:01, 84.03it/s] 39%|███▉      | 80/204 [00:01<00:01, 87.24it/s] 44%|████▍     | 90/204 [00:01<00:01, 89.56it/s] 49%|████▉     | 100/204 [00:01<00:01, 89.93it/s] 54%|█████▍    | 110/204 [00:01<00:01, 90.86it/s] 59%|█████▉    | 120/204 [00:01<00:00, 91.59it/s] 64%|██████▎   | 130/204 [00:01<00:00, 92.40it/s] 69%|██████▊   | 140/204 [00:01<00:00, 88.75it/s] 73%|███████▎  | 149/204 [00:02<00:00, 84.75it/s] 77%|███████▋  | 158/204 [00:02<00:00, 81.06it/s] 82%|████████▏ | 167/204 [00:02<00:00, 79.70it/s] 87%|████████▋ | 177/204 [00:02<00:00, 83.72it/s] 92%|█████████▏| 187/204 [00:02<00:00, 87.36it/s] 97%|█████████▋| 197/204 [00:02<00:00, 90.05it/s]100%|██████████| 204/204 [00:02<00:00, 75.15it/s]
26032 images processed, 2.7548348903656006 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:46,  1.67it/s]  8%|▊         | 6/79 [00:00<00:07, 10.04it/s] 18%|█▊        | 14/79 [00:00<00:02, 23.77it/s] 24%|██▍       | 19/79 [00:00<00:02, 28.97it/s] 30%|███       | 24/79 [00:01<00:01, 33.86it/s] 43%|████▎     | 34/79 [00:01<00:00, 49.83it/s] 56%|█████▌    | 44/79 [00:01<00:00, 61.88it/s] 68%|██████▊   | 54/79 [00:01<00:00, 71.19it/s] 81%|████████  | 64/79 [00:01<00:00, 78.09it/s] 94%|█████████▎| 74/79 [00:01<00:00, 83.40it/s]100%|██████████| 79/79 [00:01<00:00, 48.35it/s]
10000 images processed, 1.6704320907592773 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:44,  1.74it/s] 13%|█▎        | 10/79 [00:00<00:03, 18.93it/s] 24%|██▍       | 19/79 [00:00<00:01, 34.08it/s] 37%|███▋      | 29/79 [00:00<00:01, 48.60it/s] 48%|████▊     | 38/79 [00:00<00:00, 58.72it/s] 61%|██████    | 48/79 [00:01<00:00, 67.40it/s] 73%|███████▎  | 58/79 [00:01<00:00, 74.23it/s] 86%|████████▌ | 68/79 [00:01<00:00, 80.54it/s] 99%|█████████▊| 78/79 [00:01<00:00, 85.23it/s]100%|██████████| 79/79 [00:01<00:00, 55.24it/s]
10000 images processed, 1.453092336654663 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:35,  1.94it/s] 10%|█         | 7/70 [00:00<00:04, 14.36it/s] 20%|██        | 14/70 [00:00<00:02, 27.08it/s] 31%|███▏      | 22/70 [00:00<00:01, 39.60it/s] 46%|████▌     | 32/70 [00:00<00:00, 54.20it/s] 60%|██████    | 42/70 [00:01<00:00, 65.29it/s] 74%|███████▍  | 52/70 [00:01<00:00, 74.05it/s] 89%|████████▊ | 62/70 [00:01<00:00, 80.56it/s]100%|██████████| 70/70 [00:01<00:00, 52.19it/s]
8925 images processed, 1.3756873607635498 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:40,  1.08it/s] 11%|█         | 5/45 [00:01<00:06,  6.22it/s] 18%|█▊        | 8/45 [00:01<00:04,  7.61it/s] 40%|████      | 18/45 [00:01<00:01, 21.01it/s] 51%|█████     | 23/45 [00:01<00:01, 16.30it/s] 71%|███████   | 32/45 [00:01<00:00, 25.89it/s] 87%|████████▋ | 39/45 [00:02<00:00, 24.26it/s]100%|██████████| 45/45 [00:02<00:00, 18.81it/s]
5640 images processed, 2.4518487453460693 seconds used

17.650834560394287
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/CILPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.53  99.36  98.11
places365     68.31  80.53  78.96
LSUN          21.69  95.04  95.10
iSUN          72.35  81.48  84.57
dtd           38.32  91.26  94.50
AVG           40.64  89.54  90.25
Retain-Acc: 0.7486
Forget-as-OOD (retain known vs forget novel):
  FPR: 55.40 AUROC: 88.43 AUIN: 99.25
7.620007514953613
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-CIFAR-100forget5-temp0.08_rf.png
