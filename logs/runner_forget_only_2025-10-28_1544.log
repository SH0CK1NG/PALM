nohup: ignoring input
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=5, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=0.0, epsilon=0.05, palm_enable=False, pcon_split_enable=False, old_class_count=None, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-nopalm-forget_only_adapter', adapter_load_path=None, lora_stack=False, lora_orth_enable=False, lora_orth_lambda=0.1, lora_orth_ref_paths=None, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc=None, forget_classes_seen=None, retain_exclude_csv=None, forget_csv=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 21843904
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.default.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.default.weight
[debug] trainable: base_model.model.head.2.lora_A.default.weight
[debug] trainable: base_model.model.head.2.lora_B.default.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.default.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.default.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.default.weight']
[trainable] param_count=238592 tensors=16
  0%|          | 0/5 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:163: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
 20%|██        | 1/5 [00:14<00:58, 14.54s/it] 40%|████      | 2/5 [00:23<00:33, 11.19s/it] 60%|██████    | 3/5 [00:32<00:20, 10.39s/it] 80%|████████  | 4/5 [00:42<00:10, 10.10s/it]100%|██████████| 5/5 [00:52<00:00, 10.00s/it]100%|██████████| 5/5 [00:52<00:00, 10.46s/it]
[loss] ep 0 it 0 total=2.3853 mle=0.0000 pcon=0.0000 forget=2.3853 orth=0.0000 favg=0.0000 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 0 it 50 total=2.3536 mle=0.0000 pcon=0.0000 forget=2.3536 orth=0.0000 favg=0.0000 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 0 it 100 total=2.3802 mle=0.0000 pcon=0.0000 forget=2.3802 orth=0.0000 favg=0.0000 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 0 it 150 total=2.3506 mle=0.0000 pcon=0.0000 forget=2.3506 orth=0.0000 favg=0.0000 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 0 it 200 total=2.3212 mle=0.0000 pcon=0.0000 forget=2.3212 orth=0.0000 favg=0.0000 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 0 it 250 total=2.3768 mle=0.0000 pcon=0.0000 forget=2.3768 orth=0.0000 favg=0.0000 nr=128 nf=101 protos=600 fproto_sim=NA
[loss] ep 0 it 300 total=2.3732 mle=0.0000 pcon=0.0000 forget=2.3732 orth=0.0000 favg=0.0000 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 0 it 350 total=2.4318 mle=0.0000 pcon=0.0000 forget=2.4318 orth=0.0000 favg=0.0000 nr=128 nf=93 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-nopalm-forget_only_adapter
[loss] ep 1 it 10 total=2.3540 mle=0.0000 pcon=0.0000 forget=2.3540 orth=0.0000 favg=0.0000 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 1 it 60 total=2.4586 mle=0.0000 pcon=0.0000 forget=2.4586 orth=0.0000 favg=0.0000 nr=128 nf=93 protos=600 fproto_sim=NA
[loss] ep 1 it 110 total=2.3893 mle=0.0000 pcon=0.0000 forget=2.3893 orth=0.0000 favg=0.0000 nr=128 nf=94 protos=600 fproto_sim=NA
[loss] ep 1 it 160 total=2.4164 mle=0.0000 pcon=0.0000 forget=2.4164 orth=0.0000 favg=0.0000 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 1 it 210 total=2.3883 mle=0.0000 pcon=0.0000 forget=2.3883 orth=0.0000 favg=0.0000 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 1 it 260 total=2.3719 mle=0.0000 pcon=0.0000 forget=2.3719 orth=0.0000 favg=0.0000 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 1 it 310 total=2.3502 mle=0.0000 pcon=0.0000 forget=2.3502 orth=0.0000 favg=0.0000 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 1 it 360 total=2.3597 mle=0.0000 pcon=0.0000 forget=2.3597 orth=0.0000 favg=0.0000 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 2 it 20 total=2.3613 mle=0.0000 pcon=0.0000 forget=2.3613 orth=0.0000 favg=0.0000 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 2 it 70 total=2.4241 mle=0.0000 pcon=0.0000 forget=2.4241 orth=0.0000 favg=0.0000 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 2 it 120 total=2.3576 mle=0.0000 pcon=0.0000 forget=2.3576 orth=0.0000 favg=0.0000 nr=128 nf=92 protos=600 fproto_sim=NA
[loss] ep 2 it 170 total=2.3797 mle=0.0000 pcon=0.0000 forget=2.3797 orth=0.0000 favg=0.0000 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 2 it 220 total=2.3828 mle=0.0000 pcon=0.0000 forget=2.3828 orth=0.0000 favg=0.0000 nr=128 nf=89 protos=600 fproto_sim=NA
[loss] ep 2 it 270 total=2.3740 mle=0.0000 pcon=0.0000 forget=2.3740 orth=0.0000 favg=0.0000 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 2 it 320 total=2.3712 mle=0.0000 pcon=0.0000 forget=2.3712 orth=0.0000 favg=0.0000 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 2 it 370 total=2.3910 mle=0.0000 pcon=0.0000 forget=2.3910 orth=0.0000 favg=0.0000 nr=128 nf=96 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-nopalm-forget_only_adapter
[loss] ep 3 it 30 total=2.3754 mle=0.0000 pcon=0.0000 forget=2.3754 orth=0.0000 favg=0.0000 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 3 it 80 total=2.3618 mle=0.0000 pcon=0.0000 forget=2.3618 orth=0.0000 favg=0.0000 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 3 it 130 total=2.3838 mle=0.0000 pcon=0.0000 forget=2.3838 orth=0.0000 favg=0.0000 nr=128 nf=97 protos=600 fproto_sim=NA
[loss] ep 3 it 180 total=2.3639 mle=0.0000 pcon=0.0000 forget=2.3639 orth=0.0000 favg=0.0000 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 3 it 230 total=2.4085 mle=0.0000 pcon=0.0000 forget=2.4085 orth=0.0000 favg=0.0000 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 3 it 280 total=2.4224 mle=0.0000 pcon=0.0000 forget=2.4224 orth=0.0000 favg=0.0000 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 3 it 330 total=2.3600 mle=0.0000 pcon=0.0000 forget=2.3600 orth=0.0000 favg=0.0000 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 3 it 380 total=2.3931 mle=0.0000 pcon=0.0000 forget=2.3931 orth=0.0000 favg=0.0000 nr=128 nf=97 protos=600 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-nopalm-forget_only_adapter
[loss] ep 4 it 40 total=2.3270 mle=0.0000 pcon=0.0000 forget=2.3270 orth=0.0000 favg=0.0000 nr=128 nf=100 protos=600 fproto_sim=NA
[loss] ep 4 it 90 total=2.4244 mle=0.0000 pcon=0.0000 forget=2.4244 orth=0.0000 favg=0.0000 nr=128 nf=91 protos=600 fproto_sim=NA
[loss] ep 4 it 140 total=2.3618 mle=0.0000 pcon=0.0000 forget=2.3618 orth=0.0000 favg=0.0000 nr=128 nf=98 protos=600 fproto_sim=NA
[loss] ep 4 it 190 total=2.3999 mle=0.0000 pcon=0.0000 forget=2.3999 orth=0.0000 favg=0.0000 nr=128 nf=95 protos=600 fproto_sim=NA
[loss] ep 4 it 240 total=2.3962 mle=0.0000 pcon=0.0000 forget=2.3962 orth=0.0000 favg=0.0000 nr=128 nf=99 protos=600 fproto_sim=NA
[loss] ep 4 it 290 total=2.4191 mle=0.0000 pcon=0.0000 forget=2.4191 orth=0.0000 favg=0.0000 nr=128 nf=96 protos=600 fproto_sim=NA
[loss] ep 4 it 340 total=2.3876 mle=0.0000 pcon=0.0000 forget=2.3876 orth=0.0000 favg=0.0000 nr=128 nf=93 protos=600 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] adapter loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-nopalm-forget_only_adapter
[peft] active adapters set to: default
resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-nopalm: Number of model parameters: 21843904
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:43,  2.39it/s]  3%|▎         | 11/391 [00:00<00:14, 26.66it/s]  5%|▌         | 21/391 [00:00<00:08, 44.94it/s]  8%|▊         | 31/391 [00:00<00:06, 58.50it/s] 10%|█         | 41/391 [00:00<00:05, 68.11it/s] 13%|█▎        | 51/391 [00:00<00:04, 74.91it/s] 16%|█▌        | 61/391 [00:01<00:04, 80.28it/s] 18%|█▊        | 71/391 [00:01<00:03, 84.91it/s] 21%|██        | 81/391 [00:01<00:03, 88.08it/s] 23%|██▎       | 91/391 [00:01<00:03, 90.48it/s] 26%|██▌       | 101/391 [00:01<00:03, 92.00it/s] 28%|██▊       | 111/391 [00:01<00:03, 93.10it/s] 31%|███       | 121/391 [00:01<00:02, 93.96it/s] 34%|███▎      | 131/391 [00:01<00:02, 94.56it/s] 36%|███▌      | 141/391 [00:01<00:02, 94.83it/s] 39%|███▊      | 151/391 [00:02<00:02, 94.99it/s] 41%|████      | 161/391 [00:02<00:02, 95.20it/s] 44%|████▎     | 171/391 [00:02<00:02, 95.19it/s] 46%|████▋     | 181/391 [00:02<00:02, 95.38it/s] 49%|████▉     | 191/391 [00:02<00:02, 95.53it/s] 51%|█████▏    | 201/391 [00:02<00:01, 95.68it/s] 54%|█████▍    | 211/391 [00:02<00:01, 95.68it/s] 57%|█████▋    | 221/391 [00:02<00:01, 95.48it/s] 59%|█████▉    | 231/391 [00:02<00:01, 95.45it/s] 62%|██████▏   | 241/391 [00:02<00:01, 95.59it/s] 64%|██████▍   | 251/391 [00:03<00:01, 95.65it/s] 67%|██████▋   | 261/391 [00:03<00:01, 95.49it/s] 69%|██████▉   | 271/391 [00:03<00:01, 95.35it/s] 72%|███████▏  | 281/391 [00:03<00:01, 95.18it/s] 74%|███████▍  | 291/391 [00:03<00:01, 95.52it/s] 77%|███████▋  | 301/391 [00:03<00:00, 95.54it/s] 80%|███████▉  | 311/391 [00:03<00:00, 95.44it/s] 82%|████████▏ | 321/391 [00:03<00:00, 95.41it/s] 85%|████████▍ | 331/391 [00:03<00:00, 95.63it/s] 87%|████████▋ | 341/391 [00:03<00:00, 95.42it/s] 90%|████████▉ | 351/391 [00:04<00:00, 95.51it/s] 92%|█████████▏| 361/391 [00:04<00:00, 95.26it/s] 95%|█████████▍| 371/391 [00:04<00:00, 95.91it/s] 97%|█████████▋| 381/391 [00:04<00:00, 96.53it/s]100%|██████████| 391/391 [00:04<00:00, 94.68it/s]100%|██████████| 391/391 [00:04<00:00, 86.45it/s]
50000 images processed, 4.602238893508911 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:33,  2.31it/s] 14%|█▍        | 11/79 [00:00<00:02, 25.91it/s] 27%|██▋       | 21/79 [00:00<00:01, 43.86it/s] 38%|███▊      | 30/79 [00:00<00:00, 56.00it/s] 49%|████▉     | 39/79 [00:00<00:00, 65.24it/s] 61%|██████    | 48/79 [00:00<00:00, 72.03it/s] 73%|███████▎  | 58/79 [00:01<00:00, 78.60it/s] 86%|████████▌ | 68/79 [00:01<00:00, 83.36it/s] 99%|█████████▊| 78/79 [00:01<00:00, 86.71it/s]100%|██████████| 79/79 [00:02<00:00, 35.43it/s]
10000 images processed, 2.2945756912231445 seconds used

Saved forget OOD features to cache/resnet34-top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-nopalm/CIFAR-100/forget
Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<01:24,  2.40it/s]  5%|▌         | 11/204 [00:00<00:07, 26.70it/s] 10%|█         | 21/204 [00:00<00:04, 45.25it/s] 15%|█▌        | 31/204 [00:00<00:02, 59.22it/s] 20%|██        | 41/204 [00:00<00:02, 69.34it/s] 25%|██▌       | 51/204 [00:00<00:01, 76.98it/s] 30%|██▉       | 61/204 [00:01<00:01, 82.40it/s] 35%|███▍      | 71/204 [00:01<00:01, 86.26it/s] 40%|███▉      | 81/204 [00:01<00:01, 89.11it/s] 45%|████▍     | 91/204 [00:01<00:01, 91.19it/s] 50%|████▉     | 101/204 [00:01<00:01, 92.65it/s] 54%|█████▍    | 111/204 [00:01<00:00, 93.62it/s] 59%|█████▉    | 121/204 [00:01<00:00, 94.14it/s] 64%|██████▍   | 131/204 [00:01<00:00, 94.62it/s] 69%|██████▉   | 141/204 [00:01<00:00, 94.55it/s] 74%|███████▍  | 151/204 [00:01<00:00, 94.71it/s] 79%|███████▉  | 161/204 [00:02<00:00, 95.07it/s] 84%|████████▍ | 171/204 [00:02<00:00, 95.27it/s] 89%|████████▊ | 181/204 [00:02<00:00, 95.73it/s] 94%|█████████▎| 191/204 [00:02<00:00, 96.30it/s] 99%|█████████▊| 201/204 [00:02<00:00, 96.62it/s]100%|██████████| 204/204 [00:02<00:00, 80.14it/s]
26032 images processed, 2.5883400440216064 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:45,  1.72it/s]  5%|▌         | 4/79 [00:00<00:10,  7.29it/s] 18%|█▊        | 14/79 [00:00<00:02, 27.23it/s] 29%|██▉       | 23/79 [00:00<00:01, 41.57it/s] 42%|████▏     | 33/79 [00:01<00:00, 55.26it/s] 53%|█████▎    | 42/79 [00:01<00:00, 63.36it/s] 66%|██████▌   | 52/79 [00:01<00:00, 71.97it/s] 78%|███████▊  | 62/79 [00:01<00:00, 78.61it/s] 91%|█████████ | 72/79 [00:01<00:00, 83.85it/s]100%|██████████| 79/79 [00:01<00:00, 53.20it/s]
10000 images processed, 1.5137147903442383 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:39,  1.99it/s] 14%|█▍        | 11/79 [00:00<00:02, 23.38it/s] 27%|██▋       | 21/79 [00:00<00:01, 41.06it/s] 39%|███▉      | 31/79 [00:00<00:00, 55.21it/s] 52%|█████▏    | 41/79 [00:00<00:00, 66.04it/s] 65%|██████▍   | 51/79 [00:01<00:00, 74.25it/s] 77%|███████▋  | 61/79 [00:01<00:00, 80.63it/s] 90%|████████▉ | 71/79 [00:01<00:00, 85.33it/s]100%|██████████| 79/79 [00:01<00:00, 60.29it/s]
10000 images processed, 1.3291816711425781 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:32,  2.14it/s] 14%|█▍        | 10/70 [00:00<00:02, 22.33it/s] 29%|██▊       | 20/70 [00:00<00:01, 40.79it/s] 43%|████▎     | 30/70 [00:00<00:00, 55.15it/s] 57%|█████▋    | 40/70 [00:00<00:00, 65.73it/s] 71%|███████▏  | 50/70 [00:00<00:00, 74.33it/s] 86%|████████▌ | 60/70 [00:01<00:00, 80.76it/s]100%|██████████| 70/70 [00:01<00:00, 84.62it/s]100%|██████████| 70/70 [00:01<00:00, 57.87it/s]
8925 images processed, 1.239945650100708 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:01<00:50,  1.16s/it] 24%|██▍       | 11/45 [00:01<00:02, 11.68it/s] 38%|███▊      | 17/45 [00:01<00:02, 13.40it/s] 56%|█████▌    | 25/45 [00:01<00:00, 21.41it/s] 73%|███████▎  | 33/45 [00:02<00:00, 19.18it/s] 96%|█████████▌| 43/45 [00:02<00:00, 28.70it/s]100%|██████████| 45/45 [00:02<00:00, 19.16it/s]
5640 images processed, 2.369605302810669 seconds used

17.48691487312317
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.
  warn(
Evaluating SVHN
Evaluating places365
Evaluating LSUN
Evaluating iSUN
Evaluating dtd
 OOD detection method: SSD+
              FPR    AUROC  AUIN  
SVHN           2.57  99.36
places365     68.16  81.18
LSUN          17.84  96.06
iSUN          72.44  81.78
dtd           38.49  91.38
AVG           39.90  89.95
Retain-Acc: 0.7481
Forget-as-OOD (retain known vs forget novel):
  FPR: 54.50 AUROC: 89.30 AUIN: 98.64
7.469041585922241
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-nopalm_domain.png
[umap] saved to figs/umap_CIFAR-100_resnet34_top5-palm-cache6-ema0.999-b128-e5-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-nopalm_rf.png
