nohup: ignoring input
==== Stage 1: forget_inc={0,8,11,40,51}; forget_seen={}; all={0,8,11,40,51,66,67,88,94,57} ====
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
/home/shaokun/PALM/main.py:54: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = torch.cuda.amp.GradScaler()
Namespace(in_dataset='CIFAR-100', out_datasets=['inat', 'sun50', 'places50', 'dtd'], backbone='resnet34', method='top5-palm-cache6-ema0.999', seed=1, gpu='0', epochs=50, batch_size=128, lr=0.001, weight_decay=0.0001, print_every=50, fine_tune=False, temp=0.08, cosine=True, warm=False, lr_decay_epochs='700,800,900', lr_decay_rate=0.1, layers=100, depth=40, width=4, growth=12, droprate=0.0, save_path='CIFAR-10-ResNet18.pt', load_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt', reduce=0.5, score='mahalanobis', threshold=1.0, k=5, momentum=0.9, proto_m=0.999, cache_size=6, nviews=2, lambda_pcon=1.0, epsilon=0.05, incremental=False, use_lora=True, lora_impl='peft', lora_r=8, lora_alpha=32, lora_dropout=0.05, lora_target='both', adapter_save_path='checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1', adapter_load_path=None, adapter_load_paths=None, lora_new_adapter_name='train_s1', lora_stack=True, lora_orth_enable=False, lora_orth_lambda=0.1, forget_classes='0,8,11,40,51,66,67,88,94,57', forget_list_path=None, forget_classes_inc='0,8,11,40,51', forget_classes_seen=None, forget_lambda=0.2, forget_margin=100.0, forget_strategy='proto', centers_path=None, precision_path=None, batch_forget_mode='balanced', forget_proto_enable=False, forget_attr_w=1.0, forget_proto_rep_w=1.0, forget_avgproto_enable=False, forget_avgproto_w=1.0, umap_enable=False, umap_by='domain', umap_max_points=20000, umap_neighbors=15, umap_min_dist=0.05, umap_metric='cosine', umap_save_path=None, umap_rf_only=False, keep_cache=False, argument=True)
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] added trainable adapter 'train_s1'
[peft] set_adapter(list) failed: unhashable type: 'list'; fallback to last adapter
[peft] active adapter set to: train_s1
resnet34-top5-palm-cache6-ema0.999: Number of model parameters: 22082496
[debug] trainable_count = 16
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv1.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.conv2.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.0.shortcut.0.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv1.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.1.conv2.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv1.lora_B.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_A.train_s1.weight
[debug] trainable: base_model.model.encoder.layer4.2.conv2.lora_B.train_s1.weight
[debug] trainable: base_model.model.head.2.lora_A.train_s1.weight
[debug] trainable: base_model.model.head.2.lora_B.train_s1.weight
[debug][warn] non-LoRA trainables detected: ['base_model.model.encoder.layer4.0.conv1.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.0.conv1.lora_B.train_s1.weight', 'base_model.model.encoder.layer4.0.conv2.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.0.conv2.lora_B.train_s1.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.0.shortcut.0.lora_B.train_s1.weight', 'base_model.model.encoder.layer4.1.conv1.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.1.conv1.lora_B.train_s1.weight', 'base_model.model.encoder.layer4.1.conv2.lora_A.train_s1.weight', 'base_model.model.encoder.layer4.1.conv2.lora_B.train_s1.weight']
  0%|          | 0/50 [00:00<?, ?it/s]/home/shaokun/PALM/trainer.py:139: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast():
  2%|▏         | 1/50 [01:34<1:16:53, 94.14s/it]  4%|▍         | 2/50 [02:01<44:06, 55.15s/it]    6%|▌         | 3/50 [02:27<32:34, 41.58s/it]  8%|▊         | 4/50 [02:55<27:50, 36.31s/it] 10%|█         | 5/50 [03:20<24:01, 32.04s/it] 12%|█▏        | 6/50 [03:44<21:40, 29.55s/it] 14%|█▍        | 7/50 [04:08<19:52, 27.73s/it][loss] ep 0 it 0 total=8.8609 mle=2.1074 pcon=5.2951 forget=1.4584 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 0 it 50 total=8.5074 mle=1.7688 pcon=5.2912 forget=1.4474 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 100 total=8.7542 mle=2.0170 pcon=5.2869 forget=1.4502 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 150 total=9.0628 mle=2.3295 pcon=5.2833 forget=1.4500 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 0 it 200 total=8.7182 mle=1.9564 pcon=5.2792 forget=1.4826 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 250 total=8.2313 mle=1.4704 pcon=5.2752 forget=1.4857 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 0 it 300 total=8.5739 mle=1.8212 pcon=5.2714 forget=1.4813 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 0 it 350 total=8.5493 mle=1.8184 pcon=5.2676 forget=1.4633 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 1 it 10 total=8.6585 mle=1.9367 pcon=5.2637 forget=1.4582 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 1 it 60 total=8.2952 mle=1.5961 pcon=5.2602 forget=1.4389 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 1 it 110 total=8.5686 mle=1.8963 pcon=5.2564 forget=1.4159 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 1 it 160 total=8.9052 mle=2.1535 pcon=5.2527 forget=1.4990 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 1 it 210 total=8.9402 mle=2.2523 pcon=5.2489 forget=1.4390 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 1 it 260 total=8.3599 mle=1.6891 pcon=5.2452 forget=1.4256 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 1 it 310 total=8.3143 mle=1.6446 pcon=5.2417 forget=1.4280 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 1 it 360 total=8.4358 mle=1.7438 pcon=5.2381 forget=1.4539 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 2 it 20 total=8.4948 mle=1.8259 pcon=5.2347 forget=1.4342 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 2 it 70 total=8.3882 mle=1.7289 pcon=5.2311 forget=1.4283 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 2 it 120 total=8.6774 mle=2.0307 pcon=5.2276 forget=1.4191 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 2 it 170 total=8.4888 mle=1.7808 pcon=5.2242 forget=1.4838 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 2 it 220 total=8.7019 mle=2.0882 pcon=5.2210 forget=1.3927 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 2 it 270 total=8.4997 mle=1.8197 pcon=5.2177 forget=1.4623 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 2 it 320 total=8.4409 mle=1.7782 pcon=5.2145 forget=1.4482 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 2 it 370 total=8.6235 mle=1.9538 pcon=5.2112 forget=1.4585 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 3 it 30 total=9.1447 mle=2.4580 pcon=5.2081 forget=1.4786 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 3 it 80 total=8.4345 mle=1.7721 pcon=5.2050 forget=1.4574 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 3 it 130 total=8.4290 mle=1.8115 pcon=5.2018 forget=1.4157 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 3 it 180 total=8.2823 mle=1.6185 pcon=5.1989 forget=1.4649 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 3 it 230 total=8.5756 mle=1.9231 pcon=5.1959 forget=1.4566 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 3 it 280 total=8.4159 mle=1.8162 pcon=5.1929 forget=1.4068 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 3 it 330 total=8.6555 mle=2.0158 pcon=5.1897 forget=1.4500 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 3 it 380 total=8.4983 mle=1.8671 pcon=5.1867 forget=1.4445 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 4 it 40 total=8.2956 mle=1.6802 pcon=5.1839 forget=1.4315 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 4 it 90 total=8.3731 mle=1.7414 pcon=5.1809 forget=1.4508 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 4 it 140 total=8.9665 mle=2.3052 pcon=5.1778 forget=1.4834 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 4 it 190 total=8.6961 mle=2.0475 pcon=5.1748 forget=1.4738 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 4 it 240 total=8.5809 mle=2.0073 pcon=5.1718 forget=1.4018 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 4 it 290 total=8.1868 mle=1.5614 pcon=5.1688 forget=1.4566 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 4 it 340 total=8.6957 mle=2.0949 pcon=5.1659 forget=1.4348 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 5 it 0 total=8.3764 mle=1.7653 pcon=5.1633 forget=1.4477 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 5 it 50 total=8.3577 mle=1.7143 pcon=5.1605 forget=1.4828 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 5 it 100 total=8.5176 mle=1.9349 pcon=5.1578 forget=1.4249 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 5 it 150 total=8.2543 mle=1.6722 pcon=5.1549 forget=1.4271 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 5 it 200 total=8.8729 mle=2.2926 pcon=5.1524 forget=1.4279 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 5 it 250 total=8.2810 mle=1.7119 pcon=5.1499 forget=1.4192 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 5 it 300 total=8.2820 mle=1.7114 pcon=5.1471 forget=1.4235 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 5 it 350 total=8.3336 mle=1.7856 pcon=5.1445 forget=1.4035 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 6 it 10 total=8.2107 mle=1.6607 pcon=5.1417 forget=1.4083 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 6 it 60 total=8.5629 mle=1.9937 pcon=5.1393 forget=1.4298 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 6 it 110 total=8.4079 mle=1.8649 pcon=5.1367 forget=1.4064 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 6 it 160 total=8.3694 mle=1.7975 pcon=5.1342 forget=1.4377 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 6 it 210 total=8.5391 mle=1.9523 pcon=5.1314 forget=1.4553 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 6 it 260 total=8.2795 mle=1.7250 pcon=5.1288 forget=1.4256 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 6 it 310 total=8.5213 mle=1.9585 pcon=5.1263 forget=1.4366 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 6 it 360 total=8.3347 mle=1.8092 pcon=5.1235 forget=1.4020 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 7 it 20 total=8.2936 mle=1.7306 pcon=5.1211 forget=1.4419 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 7 it 70 total=8.5683 mle=2.0640 pcon=5.1185 forget=1.3858 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 7 it 120 total=8.2287 mle=1.6788 pcon=5.1159 forget=1.4340 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 7 it 170 total=8.5266 mle=1.9737 pcon=5.1134 forget=1.4395 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
 16%|█▌        | 8/50 [04:33<18:39, 26.66s/it] 18%|█▊        | 9/50 [04:56<17:35, 25.75s/it] 20%|██        | 10/50 [05:21<16:56, 25.42s/it] 22%|██▏       | 11/50 [05:45<16:17, 25.05s/it] 24%|██▍       | 12/50 [06:12<16:13, 25.62s/it] 26%|██▌       | 13/50 [06:37<15:33, 25.24s/it] 28%|██▊       | 14/50 [07:02<15:07, 25.21s/it] 30%|███       | 15/50 [07:27<14:37, 25.08s/it][loss] ep 7 it 220 total=8.3742 mle=1.8717 pcon=5.1113 forget=1.3912 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 7 it 270 total=8.3025 mle=1.7532 pcon=5.1089 forget=1.4404 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 7 it 320 total=8.2099 mle=1.7016 pcon=5.1065 forget=1.4019 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 7 it 370 total=8.6180 mle=2.0997 pcon=5.1040 forget=1.4143 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 8 it 30 total=8.2720 mle=1.7661 pcon=5.1018 forget=1.4041 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 8 it 80 total=8.4866 mle=1.9933 pcon=5.0994 forget=1.3938 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 8 it 130 total=8.1617 mle=1.6817 pcon=5.0973 forget=1.3828 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 8 it 180 total=7.9679 mle=1.5104 pcon=5.0950 forget=1.3625 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 8 it 230 total=8.1762 mle=1.6534 pcon=5.0927 forget=1.4301 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 8 it 280 total=8.1287 mle=1.6961 pcon=5.0902 forget=1.3424 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 8 it 330 total=8.8137 mle=2.3248 pcon=5.0878 forget=1.4012 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 8 it 380 total=8.1421 mle=1.6599 pcon=5.0853 forget=1.3969 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 9 it 40 total=8.2196 mle=1.7312 pcon=5.0826 forget=1.4057 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 9 it 90 total=8.0255 mle=1.5570 pcon=5.0804 forget=1.3882 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 9 it 140 total=7.8617 mle=1.3884 pcon=5.0777 forget=1.3956 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 9 it 190 total=8.2024 mle=1.7765 pcon=5.0751 forget=1.3509 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 9 it 240 total=8.6383 mle=2.1988 pcon=5.0727 forget=1.3667 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 9 it 290 total=8.5441 mle=2.0567 pcon=5.0702 forget=1.4172 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 9 it 340 total=8.1651 mle=1.7378 pcon=5.0678 forget=1.3596 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 10 it 0 total=8.6405 mle=2.1625 pcon=5.0656 forget=1.4124 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 10 it 50 total=7.8194 mle=1.3909 pcon=5.0632 forget=1.3653 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 10 it 100 total=8.3419 mle=1.9121 pcon=5.0607 forget=1.3692 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 10 it 150 total=8.3062 mle=1.8524 pcon=5.0582 forget=1.3956 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 10 it 200 total=8.1294 mle=1.6870 pcon=5.0558 forget=1.3866 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 10 it 250 total=8.2739 mle=1.8491 pcon=5.0535 forget=1.3714 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 10 it 300 total=8.1662 mle=1.7510 pcon=5.0511 forget=1.3641 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 10 it 350 total=8.1190 mle=1.6928 pcon=5.0485 forget=1.3776 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 11 it 10 total=7.9930 mle=1.5447 pcon=5.0461 forget=1.4021 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 11 it 60 total=8.3463 mle=1.9115 pcon=5.0439 forget=1.3909 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 11 it 110 total=7.9874 mle=1.5539 pcon=5.0416 forget=1.3920 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 11 it 160 total=8.4245 mle=1.9605 pcon=5.0394 forget=1.4247 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 11 it 210 total=8.3016 mle=1.8678 pcon=5.0371 forget=1.3967 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 11 it 260 total=7.8761 mle=1.4368 pcon=5.0349 forget=1.4045 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 11 it 310 total=8.0770 mle=1.6381 pcon=5.0323 forget=1.4066 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 11 it 360 total=8.2527 mle=1.8006 pcon=5.0300 forget=1.4222 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 12 it 20 total=8.0303 mle=1.5925 pcon=5.0276 forget=1.4102 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 70 total=8.0319 mle=1.5783 pcon=5.0251 forget=1.4285 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 12 it 120 total=8.2025 mle=1.7391 pcon=5.0226 forget=1.4408 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 12 it 170 total=8.1686 mle=1.7101 pcon=5.0202 forget=1.4384 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 220 total=7.9285 mle=1.4947 pcon=5.0178 forget=1.4160 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 12 it 270 total=8.2386 mle=1.7909 pcon=5.0154 forget=1.4323 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 12 it 320 total=8.2146 mle=1.7595 pcon=5.0129 forget=1.4422 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 12 it 370 total=8.2656 mle=1.7928 pcon=5.0106 forget=1.4621 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 13 it 30 total=7.8829 mle=1.4262 pcon=5.0084 forget=1.4483 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 13 it 80 total=8.0860 mle=1.5860 pcon=5.0058 forget=1.4942 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 13 it 130 total=8.3099 mle=1.8481 pcon=5.0033 forget=1.4585 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 13 it 180 total=8.2251 mle=1.7653 pcon=5.0010 forget=1.4588 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 13 it 230 total=8.0824 mle=1.6215 pcon=4.9987 forget=1.4622 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 13 it 280 total=8.1210 mle=1.6486 pcon=4.9963 forget=1.4761 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 13 it 330 total=8.0218 mle=1.5515 pcon=4.9940 forget=1.4763 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 13 it 380 total=8.1514 mle=1.6643 pcon=4.9916 forget=1.4955 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 14 it 40 total=8.2995 mle=1.8067 pcon=4.9892 forget=1.5036 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 90 total=8.0677 mle=1.5460 pcon=4.9868 forget=1.5349 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 140 total=8.1926 mle=1.7347 pcon=4.9843 forget=1.4737 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 14 it 190 total=8.1288 mle=1.6900 pcon=4.9820 forget=1.4567 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 14 it 240 total=8.3489 mle=1.8848 pcon=4.9796 forget=1.4845 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 14 it 290 total=8.0040 mle=1.5668 pcon=4.9772 forget=1.4600 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 14 it 340 total=8.1900 mle=1.7625 pcon=4.9747 forget=1.4528 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 15 it 0 total=7.9703 mle=1.5528 pcon=4.9724 forget=1.4451 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
 32%|███▏      | 16/50 [07:54<14:39, 25.87s/it] 34%|███▍      | 17/50 [08:19<14:06, 25.65s/it] 36%|███▌      | 18/50 [08:43<13:25, 25.18s/it] 38%|███▊      | 19/50 [09:09<13:03, 25.26s/it] 40%|████      | 20/50 [09:34<12:32, 25.09s/it] 42%|████▏     | 21/50 [09:57<11:55, 24.69s/it] 44%|████▍     | 22/50 [10:24<11:45, 25.19s/it][loss] ep 15 it 50 total=7.9649 mle=1.5710 pcon=4.9702 forget=1.4237 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 15 it 100 total=8.1567 mle=1.7443 pcon=4.9681 forget=1.4444 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 150 total=7.9811 mle=1.5551 pcon=4.9660 forget=1.4600 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 200 total=7.9543 mle=1.5842 pcon=4.9637 forget=1.4064 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 15 it 250 total=7.9583 mle=1.6549 pcon=4.9615 forget=1.3420 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 15 it 300 total=7.7851 mle=1.4954 pcon=4.9594 forget=1.3303 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 15 it 350 total=8.0083 mle=1.7373 pcon=4.9574 forget=1.3137 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 16 it 10 total=7.9131 mle=1.5915 pcon=4.9555 forget=1.3661 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 16 it 60 total=7.8116 mle=1.5889 pcon=4.9535 forget=1.2692 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 16 it 110 total=7.8419 mle=1.6119 pcon=4.9514 forget=1.2786 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 16 it 160 total=8.0103 mle=1.8195 pcon=4.9494 forget=1.2414 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 16 it 210 total=7.9087 mle=1.6822 pcon=4.9474 forget=1.2791 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 16 it 260 total=7.6526 mle=1.4550 pcon=4.9455 forget=1.2520 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 16 it 310 total=7.7176 mle=1.4792 pcon=4.9435 forget=1.2949 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 16 it 360 total=7.8313 mle=1.6628 pcon=4.9414 forget=1.2272 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 17 it 20 total=7.6711 mle=1.5569 pcon=4.9396 forget=1.1746 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 70 total=7.9209 mle=1.6912 pcon=4.9377 forget=1.2919 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 120 total=8.0509 mle=1.9447 pcon=4.9357 forget=1.1704 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 17 it 170 total=7.6955 mle=1.5775 pcon=4.9341 forget=1.1840 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 17 it 220 total=7.7093 mle=1.6320 pcon=4.9319 forget=1.1454 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 17 it 270 total=7.8059 mle=1.6970 pcon=4.9300 forget=1.1790 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 17 it 320 total=7.4582 mle=1.4345 pcon=4.9279 forget=1.0958 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 17 it 370 total=7.6543 mle=1.6351 pcon=4.9257 forget=1.0934 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 18 it 30 total=7.6861 mle=1.6818 pcon=4.9236 forget=1.0808 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 18 it 80 total=8.0352 mle=2.0860 pcon=4.9212 forget=1.0280 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 18 it 130 total=7.6200 mle=1.5966 pcon=4.9188 forget=1.1046 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 18 it 180 total=7.5190 mle=1.5903 pcon=4.9163 forget=1.0124 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 18 it 230 total=7.4432 mle=1.5495 pcon=4.9138 forget=0.9800 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 18 it 280 total=7.5716 mle=1.6215 pcon=4.9111 forget=1.0391 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 18 it 330 total=7.4976 mle=1.5712 pcon=4.9082 forget=1.0181 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 18 it 380 total=7.7141 mle=1.7914 pcon=4.9053 forget=1.0174 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 19 it 40 total=7.5484 mle=1.6578 pcon=4.9025 forget=0.9881 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 19 it 90 total=7.4618 mle=1.5600 pcon=4.8997 forget=1.0021 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 19 it 140 total=7.6327 mle=1.7407 pcon=4.8968 forget=0.9952 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 19 it 190 total=7.5495 mle=1.6711 pcon=4.8941 forget=0.9843 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 19 it 240 total=7.6700 mle=1.7748 pcon=4.8914 forget=1.0037 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 19 it 290 total=7.4225 mle=1.5536 pcon=4.8887 forget=0.9801 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 19 it 340 total=7.4904 mle=1.6197 pcon=4.8860 forget=0.9846 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 20 it 0 total=7.5859 mle=1.6952 pcon=4.8834 forget=1.0073 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 20 it 50 total=7.3283 mle=1.4689 pcon=4.8808 forget=0.9786 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 20 it 100 total=7.5977 mle=1.7328 pcon=4.8781 forget=0.9867 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 20 it 150 total=7.4728 mle=1.6071 pcon=4.8754 forget=0.9903 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 20 it 200 total=7.7190 mle=1.8497 pcon=4.8727 forget=0.9966 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 20 it 250 total=7.7045 mle=1.8007 pcon=4.8700 forget=1.0338 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 20 it 300 total=7.4799 mle=1.6050 pcon=4.8674 forget=1.0075 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 20 it 350 total=7.4536 mle=1.5705 pcon=4.8649 forget=1.0182 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[peft] adapter saved to checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[loss] ep 21 it 10 total=7.3770 mle=1.4621 pcon=4.8623 forget=1.0525 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 21 it 60 total=7.6281 mle=1.7473 pcon=4.8599 forget=1.0209 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 21 it 110 total=7.8465 mle=1.9470 pcon=4.8573 forget=1.0423 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 21 it 160 total=7.6252 mle=1.7600 pcon=4.8549 forget=1.0103 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 21 it 210 total=7.4037 mle=1.5267 pcon=4.8523 forget=1.0247 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 21 it 260 total=7.4700 mle=1.5747 pcon=4.8498 forget=1.0455 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 21 it 310 total=7.6030 mle=1.7051 pcon=4.8473 forget=1.0506 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 21 it 360 total=7.6348 mle=1.7465 pcon=4.8448 forget=1.0435 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 22 it 20 total=7.3616 mle=1.4802 pcon=4.8427 forget=1.0387 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 22 it 70 total=7.6520 mle=1.7763 pcon=4.8404 forget=1.0354 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 22 it 120 total=7.5191 mle=1.6188 pcon=4.8380 forget=1.0622 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 22 it 170 total=7.4768 mle=1.5722 pcon=4.8357 forget=1.0688 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 22 it 220 total=7.7588 mle=1.8598 pcon=4.8334 forget=1.0655 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 22 it 270 total=7.4854 mle=1.5754 pcon=4.8313 forget=1.0787 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
 46%|████▌     | 23/50 [10:46<10:55, 24.29s/it] 48%|████▊     | 24/50 [11:12<10:49, 24.97s/it] 50%|█████     | 25/50 [11:39<10:32, 25.31s/it] 52%|█████▏    | 26/50 [12:07<10:33, 26.39s/it] 54%|█████▍    | 27/50 [12:34<10:07, 26.42s/it] 56%|█████▌    | 28/50 [13:00<09:40, 26.38s/it] 58%|█████▊    | 29/50 [13:26<09:12, 26.31s/it] 60%|██████    | 30/50 [13:52<08:43, 26.16s/it] 62%|██████▏   | 31/50 [14:17<08:09, 25.76s/it][loss] ep 22 it 320 total=7.4404 mle=1.5364 pcon=4.8290 forget=1.0749 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 22 it 370 total=7.6951 mle=1.7886 pcon=4.8269 forget=1.0796 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 23 it 30 total=7.5326 mle=1.6332 pcon=4.8247 forget=1.0747 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 23 it 80 total=7.4684 mle=1.5497 pcon=4.8226 forget=1.0962 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 23 it 130 total=7.8315 mle=1.9246 pcon=4.8205 forget=1.0863 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 23 it 180 total=7.7387 mle=1.8264 pcon=4.8184 forget=1.0940 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 23 it 230 total=7.2361 mle=1.3224 pcon=4.8163 forget=1.0974 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 23 it 280 total=7.9327 mle=2.0237 pcon=4.8144 forget=1.0946 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 23 it 330 total=7.7457 mle=1.8117 pcon=4.8123 forget=1.1218 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 23 it 380 total=7.6046 mle=1.6755 pcon=4.8103 forget=1.1188 favg=0.0000 nr=21 nf=21 protos=570 fproto_sim=NA
[loss] ep 24 it 40 total=7.5095 mle=1.5732 pcon=4.8083 forget=1.1280 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 24 it 90 total=7.6997 mle=1.7611 pcon=4.8064 forget=1.1322 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 24 it 140 total=7.5067 mle=1.5702 pcon=4.8044 forget=1.1321 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 24 it 190 total=7.4764 mle=1.5340 pcon=4.8025 forget=1.1400 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 24 it 240 total=7.6123 mle=1.6556 pcon=4.8006 forget=1.1560 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 24 it 290 total=7.6016 mle=1.6585 pcon=4.7987 forget=1.1444 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 24 it 340 total=7.6399 mle=1.6969 pcon=4.7969 forget=1.1461 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 0 total=7.8192 mle=1.8632 pcon=4.7951 forget=1.1609 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 50 total=7.7511 mle=1.7987 pcon=4.7933 forget=1.1590 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 25 it 100 total=7.7566 mle=1.8085 pcon=4.7916 forget=1.1565 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 25 it 150 total=7.6188 mle=1.6530 pcon=4.7899 forget=1.1758 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 25 it 200 total=7.5825 mle=1.6195 pcon=4.7882 forget=1.1748 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 250 total=7.4625 mle=1.4947 pcon=4.7866 forget=1.1812 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 25 it 300 total=7.6670 mle=1.6716 pcon=4.7850 forget=1.2103 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 25 it 350 total=7.4285 mle=1.4594 pcon=4.7833 forget=1.1858 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 10 total=7.4813 mle=1.5145 pcon=4.7817 forget=1.1851 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 26 it 60 total=7.5712 mle=1.5890 pcon=4.7802 forget=1.2021 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 110 total=7.4156 mle=1.4397 pcon=4.7786 forget=1.1973 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 26 it 160 total=7.6615 mle=1.6759 pcon=4.7770 forget=1.2086 favg=0.0000 nr=41 nf=41 protos=570 fproto_sim=NA
[loss] ep 26 it 210 total=7.5987 mle=1.6097 pcon=4.7755 forget=1.2135 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 26 it 260 total=7.8539 mle=1.8509 pcon=4.7740 forget=1.2289 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 26 it 310 total=7.6934 mle=1.7050 pcon=4.7725 forget=1.2159 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 26 it 360 total=7.7112 mle=1.7169 pcon=4.7710 forget=1.2233 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 27 it 20 total=7.4543 mle=1.4487 pcon=4.7698 forget=1.2358 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 27 it 70 total=7.6236 mle=1.6294 pcon=4.7684 forget=1.2258 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 27 it 120 total=7.6213 mle=1.6225 pcon=4.7670 forget=1.2318 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 27 it 170 total=7.8126 mle=1.8047 pcon=4.7656 forget=1.2423 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 27 it 220 total=7.5297 mle=1.5215 pcon=4.7642 forget=1.2440 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 27 it 270 total=7.5629 mle=1.5502 pcon=4.7629 forget=1.2498 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 27 it 320 total=7.5381 mle=1.5316 pcon=4.7615 forget=1.2450 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 27 it 370 total=7.5363 mle=1.5192 pcon=4.7602 forget=1.2570 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 28 it 30 total=7.6240 mle=1.5986 pcon=4.7590 forget=1.2663 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 28 it 80 total=7.7383 mle=1.7212 pcon=4.7578 forget=1.2594 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 28 it 130 total=7.5516 mle=1.5338 pcon=4.7566 forget=1.2611 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 28 it 180 total=7.5767 mle=1.5471 pcon=4.7554 forget=1.2742 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 28 it 230 total=7.5406 mle=1.5214 pcon=4.7542 forget=1.2649 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 28 it 280 total=7.4239 mle=1.4019 pcon=4.7531 forget=1.2690 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 28 it 330 total=7.6436 mle=1.6093 pcon=4.7519 forget=1.2825 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 28 it 380 total=7.7671 mle=1.7374 pcon=4.7507 forget=1.2790 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 29 it 40 total=7.6147 mle=1.5739 pcon=4.7497 forget=1.2911 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 29 it 90 total=7.6470 mle=1.5997 pcon=4.7485 forget=1.2988 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 29 it 140 total=7.6558 mle=1.6280 pcon=4.7474 forget=1.2804 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 29 it 190 total=7.6967 mle=1.6468 pcon=4.7463 forget=1.3037 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 29 it 240 total=7.6042 mle=1.5651 pcon=4.7452 forget=1.2939 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 29 it 290 total=7.7728 mle=1.7206 pcon=4.7442 forget=1.3080 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 29 it 340 total=7.5624 mle=1.5210 pcon=4.7431 forget=1.2983 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 30 it 0 total=7.6017 mle=1.5600 pcon=4.7420 forget=1.2997 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 30 it 50 total=7.8582 mle=1.8126 pcon=4.7408 forget=1.3048 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 30 it 100 total=7.6563 mle=1.6093 pcon=4.7398 forget=1.3072 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 30 it 150 total=7.6721 mle=1.6231 pcon=4.7389 forget=1.3100 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 30 it 200 total=7.6904 mle=1.6377 pcon=4.7380 forget=1.3148 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 30 it 250 total=7.6417 mle=1.5858 pcon=4.7368 forget=1.3191 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 30 it 300 total=7.8062 mle=1.7466 pcon=4.7359 forget=1.3237 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 30 it 350 total=7.6004 mle=1.5352 pcon=4.7351 forget=1.3301 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 31 it 10 total=7.6579 mle=1.5976 pcon=4.7343 forget=1.3260 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 31 it 60 total=7.7065 mle=1.6449 pcon=4.7334 forget=1.3282 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 31 it 110 total=7.8566 mle=1.7998 pcon=4.7325 forget=1.3243 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 31 it 160 total=8.0790 mle=2.0102 pcon=4.7317 forget=1.3372 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 31 it 210 total=7.8347 mle=1.7394 pcon=4.7307 forget=1.3645 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
 64%|██████▍   | 32/50 [14:43<07:44, 25.82s/it] 66%|██████▌   | 33/50 [15:08<07:15, 25.59s/it] 68%|██████▊   | 34/50 [15:34<06:51, 25.74s/it] 70%|███████   | 35/50 [16:00<06:25, 25.68s/it] 72%|███████▏  | 36/50 [16:26<06:01, 25.84s/it] 74%|███████▍  | 37/50 [16:52<05:36, 25.86s/it] 76%|███████▌  | 38/50 [17:18<05:10, 25.91s/it] 78%|███████▊  | 39/50 [17:43<04:43, 25.77s/it] 80%|████████  | 40/50 [18:09<04:16, 25.65s/it][loss] ep 31 it 260 total=7.7789 mle=1.7073 pcon=4.7300 forget=1.3416 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 31 it 310 total=7.5355 mle=1.4685 pcon=4.7292 forget=1.3379 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 31 it 360 total=7.5437 mle=1.4599 pcon=4.7283 forget=1.3555 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 32 it 20 total=7.6922 mle=1.6255 pcon=4.7275 forget=1.3392 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 32 it 70 total=7.7848 mle=1.7094 pcon=4.7267 forget=1.3487 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 32 it 120 total=7.6060 mle=1.5242 pcon=4.7260 forget=1.3558 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 32 it 170 total=7.6941 mle=1.6121 pcon=4.7253 forget=1.3566 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 32 it 220 total=7.8931 mle=1.8160 pcon=4.7246 forget=1.3525 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 32 it 270 total=7.6022 mle=1.5218 pcon=4.7239 forget=1.3565 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 32 it 320 total=7.5969 mle=1.5049 pcon=4.7230 forget=1.3690 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 32 it 370 total=7.9369 mle=1.8550 pcon=4.7224 forget=1.3595 favg=0.0000 nr=23 nf=23 protos=570 fproto_sim=NA
[loss] ep 33 it 30 total=7.9463 mle=1.8648 pcon=4.7216 forget=1.3599 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 33 it 80 total=7.7409 mle=1.6575 pcon=4.7209 forget=1.3625 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 33 it 130 total=7.5609 mle=1.4722 pcon=4.7203 forget=1.3684 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 33 it 180 total=7.4721 mle=1.3883 pcon=4.7195 forget=1.3642 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 33 it 230 total=7.9319 mle=1.8422 pcon=4.7189 forget=1.3708 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 33 it 280 total=7.7322 mle=1.6299 pcon=4.7182 forget=1.3841 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 33 it 330 total=7.8673 mle=1.7871 pcon=4.7178 forget=1.3624 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 33 it 380 total=7.5673 mle=1.4794 pcon=4.7171 forget=1.3708 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 34 it 40 total=7.6019 mle=1.5080 pcon=4.7165 forget=1.3774 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 34 it 90 total=7.9473 mle=1.8299 pcon=4.7159 forget=1.4016 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 34 it 140 total=7.8081 mle=1.7089 pcon=4.7151 forget=1.3841 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 34 it 190 total=7.7222 mle=1.6293 pcon=4.7145 forget=1.3784 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 34 it 240 total=7.9160 mle=1.8166 pcon=4.7140 forget=1.3854 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 34 it 290 total=7.5845 mle=1.4761 pcon=4.7134 forget=1.3950 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 34 it 340 total=7.6134 mle=1.5126 pcon=4.7128 forget=1.3880 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 35 it 0 total=7.6359 mle=1.5267 pcon=4.7123 forget=1.3968 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 35 it 50 total=8.0808 mle=1.9682 pcon=4.7117 forget=1.4009 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 35 it 100 total=7.7675 mle=1.6591 pcon=4.7111 forget=1.3973 favg=0.0000 nr=43 nf=43 protos=570 fproto_sim=NA
[loss] ep 35 it 150 total=7.5922 mle=1.4764 pcon=4.7106 forget=1.4051 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 35 it 200 total=7.7082 mle=1.5927 pcon=4.7100 forget=1.4055 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 35 it 250 total=7.8457 mle=1.7255 pcon=4.7095 forget=1.4108 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 35 it 300 total=7.6787 mle=1.5534 pcon=4.7089 forget=1.4164 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 35 it 350 total=7.7388 mle=1.6236 pcon=4.7085 forget=1.4067 favg=0.0000 nr=21 nf=21 protos=570 fproto_sim=NA
[loss] ep 36 it 10 total=7.7642 mle=1.6354 pcon=4.7080 forget=1.4209 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 36 it 60 total=7.7236 mle=1.6021 pcon=4.7075 forget=1.4140 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 36 it 110 total=7.7217 mle=1.5936 pcon=4.7071 forget=1.4210 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 36 it 160 total=7.9877 mle=1.8550 pcon=4.7067 forget=1.4261 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 36 it 210 total=7.9201 mle=1.7913 pcon=4.7062 forget=1.4226 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 36 it 260 total=7.6085 mle=1.4800 pcon=4.7058 forget=1.4227 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 36 it 310 total=7.6878 mle=1.5517 pcon=4.7053 forget=1.4308 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 36 it 360 total=7.5748 mle=1.4340 pcon=4.7048 forget=1.4360 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 37 it 20 total=7.8579 mle=1.7126 pcon=4.7044 forget=1.4408 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 37 it 70 total=7.7093 mle=1.5697 pcon=4.7039 forget=1.4357 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 37 it 120 total=7.7491 mle=1.6095 pcon=4.7035 forget=1.4361 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 37 it 170 total=7.7333 mle=1.5896 pcon=4.7031 forget=1.4405 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 37 it 220 total=7.6474 mle=1.5048 pcon=4.7028 forget=1.4398 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 37 it 270 total=7.7975 mle=1.6416 pcon=4.7024 forget=1.4535 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 37 it 320 total=7.9311 mle=1.7752 pcon=4.7020 forget=1.4539 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 37 it 370 total=7.7797 mle=1.6176 pcon=4.7016 forget=1.4605 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 38 it 30 total=7.8155 mle=1.6635 pcon=4.7013 forget=1.4508 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 38 it 80 total=7.5650 mle=1.4095 pcon=4.7008 forget=1.4546 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 38 it 130 total=7.9446 mle=1.7736 pcon=4.7005 forget=1.4705 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 38 it 180 total=7.6076 mle=1.4447 pcon=4.7002 forget=1.4627 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 38 it 230 total=7.6282 mle=1.4364 pcon=4.6998 forget=1.4921 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 38 it 280 total=7.7822 mle=1.6215 pcon=4.6994 forget=1.4613 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 38 it 330 total=7.8218 mle=1.6436 pcon=4.6990 forget=1.4792 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 38 it 380 total=7.9274 mle=1.7594 pcon=4.6986 forget=1.4694 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 39 it 40 total=7.7166 mle=1.5334 pcon=4.6984 forget=1.4848 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 39 it 90 total=7.7727 mle=1.5982 pcon=4.6980 forget=1.4765 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 39 it 140 total=7.8225 mle=1.6167 pcon=4.6976 forget=1.5081 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 39 it 190 total=7.9911 mle=1.8046 pcon=4.6973 forget=1.4892 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 39 it 240 total=7.8967 mle=1.7338 pcon=4.6970 forget=1.4659 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 39 it 290 total=7.8376 mle=1.6537 pcon=4.6967 forget=1.4873 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 39 it 340 total=7.8082 mle=1.6233 pcon=4.6965 forget=1.4885 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 40 it 0 total=7.8069 mle=1.6065 pcon=4.6962 forget=1.5042 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 40 it 50 total=7.8530 mle=1.6623 pcon=4.6960 forget=1.4947 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 40 it 100 total=7.7541 mle=1.5593 pcon=4.6957 forget=1.4991 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 40 it 150 total=7.7142 mle=1.5193 pcon=4.6954 forget=1.4995 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
 82%|████████▏ | 41/50 [18:33<03:48, 25.40s/it] 84%|████████▍ | 42/50 [18:54<03:11, 23.99s/it] 86%|████████▌ | 43/50 [19:14<02:40, 22.90s/it] 88%|████████▊ | 44/50 [19:35<02:13, 22.28s/it] 90%|█████████ | 45/50 [19:55<01:47, 21.45s/it] 92%|█████████▏| 46/50 [20:09<01:17, 19.28s/it] 94%|█████████▍| 47/50 [20:23<00:52, 17.65s/it] 96%|█████████▌| 48/50 [20:36<00:32, 16.42s/it] 98%|█████████▊| 49/50 [20:51<00:15, 15.99s/it][loss] ep 40 it 200 total=7.9127 mle=1.6960 pcon=4.6951 forget=1.5216 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 40 it 250 total=7.9566 mle=1.7567 pcon=4.6948 forget=1.5052 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 40 it 300 total=7.8515 mle=1.6394 pcon=4.6944 forget=1.5177 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 40 it 350 total=7.7912 mle=1.5788 pcon=4.6942 forget=1.5181 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 41 it 10 total=8.1380 mle=1.9145 pcon=4.6940 forget=1.5294 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 41 it 60 total=8.1035 mle=1.8761 pcon=4.6937 forget=1.5336 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 41 it 110 total=7.7434 mle=1.5290 pcon=4.6933 forget=1.5211 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 41 it 160 total=7.6477 mle=1.4223 pcon=4.6932 forget=1.5321 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 41 it 210 total=7.6581 mle=1.4276 pcon=4.6930 forget=1.5375 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 41 it 260 total=7.9232 mle=1.6867 pcon=4.6927 forget=1.5438 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 41 it 310 total=8.0910 mle=1.8543 pcon=4.6924 forget=1.5443 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 41 it 360 total=8.0338 mle=1.7850 pcon=4.6922 forget=1.5566 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 42 it 20 total=7.7821 mle=1.5415 pcon=4.6919 forget=1.5487 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 70 total=7.8796 mle=1.6385 pcon=4.6916 forget=1.5494 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 120 total=7.7660 mle=1.5064 pcon=4.6915 forget=1.5681 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 42 it 170 total=7.7230 mle=1.4724 pcon=4.6913 forget=1.5593 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 42 it 220 total=7.9255 mle=1.6744 pcon=4.6911 forget=1.5599 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 42 it 270 total=7.9710 mle=1.7205 pcon=4.6909 forget=1.5596 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 42 it 320 total=8.0273 mle=1.7648 pcon=4.6908 forget=1.5717 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 42 it 370 total=7.9434 mle=1.6756 pcon=4.6905 forget=1.5772 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 43 it 30 total=7.8924 mle=1.6346 pcon=4.6904 forget=1.5674 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 43 it 80 total=7.8162 mle=1.5533 pcon=4.6902 forget=1.5728 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 43 it 130 total=7.9333 mle=1.6672 pcon=4.6900 forget=1.5761 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 43 it 180 total=7.9242 mle=1.6476 pcon=4.6898 forget=1.5868 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 43 it 230 total=7.8702 mle=1.6002 pcon=4.6895 forget=1.5805 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 43 it 280 total=7.8365 mle=1.5675 pcon=4.6894 forget=1.5797 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 43 it 330 total=7.7553 mle=1.4801 pcon=4.6892 forget=1.5860 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 43 it 380 total=7.7668 mle=1.4896 pcon=4.6890 forget=1.5883 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 44 it 40 total=7.8782 mle=1.5867 pcon=4.6889 forget=1.6027 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 44 it 90 total=7.8702 mle=1.5857 pcon=4.6887 forget=1.5958 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 44 it 140 total=7.7944 mle=1.5108 pcon=4.6886 forget=1.5949 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 44 it 190 total=7.9695 mle=1.6732 pcon=4.6884 forget=1.6078 favg=0.0000 nr=40 nf=40 protos=570 fproto_sim=NA
[loss] ep 44 it 240 total=8.0424 mle=1.7502 pcon=4.6883 forget=1.6039 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 44 it 290 total=8.0604 mle=1.7762 pcon=4.6881 forget=1.5961 favg=0.0000 nr=28 nf=28 protos=570 fproto_sim=NA
[loss] ep 44 it 340 total=7.7898 mle=1.4762 pcon=4.6880 forget=1.6256 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 45 it 0 total=7.8249 mle=1.5230 pcon=4.6878 forget=1.6140 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 50 total=7.9061 mle=1.6124 pcon=4.6877 forget=1.6060 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 100 total=7.8952 mle=1.5844 pcon=4.6877 forget=1.6232 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 45 it 150 total=7.8909 mle=1.5879 pcon=4.6875 forget=1.6155 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 45 it 200 total=7.9206 mle=1.6162 pcon=4.6874 forget=1.6170 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 45 it 250 total=8.1330 mle=1.8062 pcon=4.6873 forget=1.6395 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 45 it 300 total=7.7429 mle=1.4205 pcon=4.6873 forget=1.6351 favg=0.0000 nr=24 nf=24 protos=570 fproto_sim=NA
[loss] ep 45 it 350 total=8.1539 mle=1.8386 pcon=4.6872 forget=1.6281 favg=0.0000 nr=25 nf=25 protos=570 fproto_sim=NA
[loss] ep 46 it 10 total=7.7695 mle=1.4454 pcon=4.6871 forget=1.6370 favg=0.0000 nr=39 nf=39 protos=570 fproto_sim=NA
[loss] ep 46 it 60 total=7.8149 mle=1.4953 pcon=4.6869 forget=1.6327 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 46 it 110 total=7.7149 mle=1.3903 pcon=4.6867 forget=1.6379 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 46 it 160 total=7.7853 mle=1.4628 pcon=4.6866 forget=1.6359 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 46 it 210 total=8.1911 mle=1.8699 pcon=4.6866 forget=1.6346 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 46 it 260 total=7.7296 mle=1.3927 pcon=4.6864 forget=1.6505 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 46 it 310 total=7.9247 mle=1.5902 pcon=4.6864 forget=1.6481 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 46 it 360 total=8.0676 mle=1.7427 pcon=4.6863 forget=1.6386 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 47 it 20 total=7.7948 mle=1.4575 pcon=4.6861 forget=1.6511 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 47 it 70 total=7.7438 mle=1.4049 pcon=4.6860 forget=1.6529 favg=0.0000 nr=31 nf=31 protos=570 fproto_sim=NA
[loss] ep 47 it 120 total=7.9904 mle=1.6492 pcon=4.6858 forget=1.6554 favg=0.0000 nr=38 nf=38 protos=570 fproto_sim=NA
[loss] ep 47 it 170 total=7.8794 mle=1.5404 pcon=4.6857 forget=1.6533 favg=0.0000 nr=37 nf=37 protos=570 fproto_sim=NA
[loss] ep 47 it 220 total=7.9773 mle=1.6304 pcon=4.6857 forget=1.6612 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 47 it 270 total=8.1771 mle=1.8299 pcon=4.6857 forget=1.6616 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 47 it 320 total=8.1522 mle=1.7857 pcon=4.6855 forget=1.6810 favg=0.0000 nr=26 nf=26 protos=570 fproto_sim=NA
[loss] ep 47 it 370 total=7.9647 mle=1.6081 pcon=4.6854 forget=1.6712 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 30 total=8.0445 mle=1.6927 pcon=4.6854 forget=1.6665 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 48 it 80 total=8.0070 mle=1.6391 pcon=4.6852 forget=1.6827 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 48 it 130 total=7.9563 mle=1.5785 pcon=4.6851 forget=1.6927 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 180 total=7.9064 mle=1.5490 pcon=4.6850 forget=1.6724 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 48 it 230 total=7.8714 mle=1.5021 pcon=4.6851 forget=1.6842 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
[loss] ep 48 it 280 total=8.0999 mle=1.7133 pcon=4.6850 forget=1.7016 favg=0.0000 nr=27 nf=27 protos=570 fproto_sim=NA
[loss] ep 48 it 330 total=7.8938 mle=1.5181 pcon=4.6848 forget=1.6910 favg=0.0000 nr=29 nf=29 protos=570 fproto_sim=NA
[loss] ep 48 it 380 total=8.0355 mle=1.6533 pcon=4.6847 forget=1.6975 favg=0.0000 nr=33 nf=33 protos=570 fproto_sim=NA
[loss] ep 49 it 40 total=8.2230 mle=1.8618 pcon=4.6846 forget=1.6765 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 49 it 90 total=8.0292 mle=1.6498 pcon=4.6846 forget=1.6949 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
100%|██████████| 50/50 [21:09<00:00, 16.47s/it]100%|██████████| 50/50 [21:09<00:00, 25.39s/it]
[loss] ep 49 it 140 total=8.0111 mle=1.6475 pcon=4.6844 forget=1.6792 favg=0.0000 nr=32 nf=32 protos=570 fproto_sim=NA
[loss] ep 49 it 190 total=7.9411 mle=1.5693 pcon=4.6844 forget=1.6875 favg=0.0000 nr=35 nf=35 protos=570 fproto_sim=NA
[loss] ep 49 it 240 total=7.9739 mle=1.5939 pcon=4.6843 forget=1.6958 favg=0.0000 nr=36 nf=36 protos=570 fproto_sim=NA
[loss] ep 49 it 290 total=8.0625 mle=1.6882 pcon=4.6841 forget=1.6902 favg=0.0000 nr=30 nf=30 protos=570 fproto_sim=NA
[loss] ep 49 it 340 total=8.2397 mle=1.8584 pcon=4.6841 forget=1.6972 favg=0.0000 nr=34 nf=34 protos=570 fproto_sim=NA
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Files already downloaded and verified
Files already downloaded and verified
ckpt loaded from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-with-prototypes.pt
[peft] loaded adapter 'stage1' from checkpoints/CIFAR-100-resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stack/stage1
[peft] set_adapter(list) failed: unhashable type: 'list'; fallback to last adapter
[peft] active adapter set to: stage1
resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1: Number of model parameters: 22082496
Processing in-distribution CIFAR-100 images
  0%|          | 0/391 [00:00<?, ?it/s]  0%|          | 1/391 [00:00<02:29,  2.60it/s]  3%|▎         | 10/391 [00:00<00:14, 25.87it/s]  5%|▍         | 19/391 [00:00<00:08, 43.14it/s]  7%|▋         | 28/391 [00:00<00:06, 55.69it/s]  9%|▉         | 37/391 [00:00<00:05, 63.62it/s] 12%|█▏        | 46/391 [00:00<00:04, 69.82it/s] 14%|█▍        | 56/391 [00:01<00:04, 76.41it/s] 17%|█▋        | 66/391 [00:01<00:04, 80.43it/s] 19%|█▉        | 75/391 [00:01<00:03, 81.25it/s] 21%|██▏       | 84/391 [00:01<00:03, 83.13it/s] 24%|██▍       | 93/391 [00:01<00:03, 82.74it/s] 26%|██▋       | 103/391 [00:01<00:03, 85.08it/s] 29%|██▊       | 112/391 [00:01<00:03, 84.50it/s] 31%|███       | 122/391 [00:01<00:03, 86.25it/s] 34%|███▎      | 131/391 [00:01<00:02, 87.22it/s] 36%|███▌      | 141/391 [00:01<00:02, 89.38it/s] 38%|███▊      | 150/391 [00:02<00:02, 87.87it/s] 41%|████      | 159/391 [00:02<00:02, 86.83it/s] 43%|████▎     | 168/391 [00:02<00:02, 86.39it/s] 45%|████▌     | 177/391 [00:02<00:02, 85.69it/s] 48%|████▊     | 186/391 [00:02<00:02, 85.15it/s] 50%|████▉     | 195/391 [00:02<00:02, 85.12it/s] 52%|█████▏    | 204/391 [00:02<00:02, 85.79it/s] 54%|█████▍    | 213/391 [00:02<00:02, 85.68it/s] 57%|█████▋    | 222/391 [00:02<00:01, 85.58it/s] 59%|█████▉    | 231/391 [00:03<00:01, 85.57it/s] 61%|██████▏   | 240/391 [00:03<00:01, 85.11it/s] 64%|██████▎   | 249/391 [00:03<00:01, 85.30it/s] 66%|██████▌   | 258/391 [00:03<00:01, 85.22it/s] 68%|██████▊   | 267/391 [00:03<00:01, 84.50it/s] 71%|███████   | 276/391 [00:03<00:01, 82.68it/s] 73%|███████▎  | 285/391 [00:03<00:01, 83.57it/s] 75%|███████▌  | 294/391 [00:03<00:01, 84.39it/s] 77%|███████▋  | 303/391 [00:03<00:01, 85.28it/s] 80%|███████▉  | 312/391 [00:04<00:00, 83.90it/s] 82%|████████▏ | 321/391 [00:04<00:00, 84.52it/s] 84%|████████▍ | 330/391 [00:04<00:00, 84.67it/s] 87%|████████▋ | 339/391 [00:04<00:00, 82.99it/s] 89%|████████▉ | 348/391 [00:04<00:00, 82.75it/s] 91%|█████████▏| 357/391 [00:04<00:00, 83.19it/s] 94%|█████████▎| 366/391 [00:04<00:00, 82.84it/s] 96%|█████████▌| 375/391 [00:04<00:00, 83.87it/s] 98%|█████████▊| 384/391 [00:04<00:00, 84.65it/s]100%|██████████| 391/391 [00:04<00:00, 78.76it/s]
50000 images processed, 5.341869592666626 seconds used

Processing in-distribution CIFAR-100 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:26,  2.98it/s] 13%|█▎        | 10/79 [00:00<00:02, 28.36it/s] 24%|██▍       | 19/79 [00:00<00:01, 46.34it/s] 37%|███▋      | 29/79 [00:00<00:00, 60.80it/s] 48%|████▊     | 38/79 [00:00<00:00, 68.46it/s] 59%|█████▉    | 47/79 [00:00<00:00, 73.18it/s] 71%|███████   | 56/79 [00:00<00:00, 77.21it/s] 82%|████████▏ | 65/79 [00:01<00:00, 79.23it/s] 94%|█████████▎| 74/79 [00:01<00:00, 81.25it/s]100%|██████████| 79/79 [00:01<00:00, 51.70it/s]
10000 images processed, 1.6656928062438965 seconds used

Processing out-of-distribution SVHN images
  0%|          | 0/204 [00:00<?, ?it/s]  0%|          | 1/204 [00:00<00:59,  3.43it/s]  4%|▍         | 8/204 [00:00<00:07, 25.27it/s]  8%|▊         | 17/204 [00:00<00:04, 45.69it/s] 13%|█▎        | 26/204 [00:00<00:03, 58.56it/s] 17%|█▋        | 35/204 [00:00<00:02, 66.84it/s] 22%|██▏       | 44/204 [00:00<00:02, 72.58it/s] 26%|██▋       | 54/204 [00:00<00:01, 79.03it/s] 31%|███       | 63/204 [00:01<00:01, 80.75it/s] 35%|███▌      | 72/204 [00:01<00:01, 81.43it/s] 40%|███▉      | 81/204 [00:01<00:01, 83.86it/s] 45%|████▍     | 91/204 [00:01<00:01, 86.17it/s] 49%|████▉     | 100/204 [00:01<00:01, 84.56it/s] 53%|█████▎    | 109/204 [00:01<00:01, 84.22it/s] 58%|█████▊    | 119/204 [00:01<00:00, 86.27it/s] 63%|██████▎   | 128/204 [00:01<00:00, 86.34it/s] 67%|██████▋   | 137/204 [00:01<00:00, 86.33it/s] 72%|███████▏  | 146/204 [00:02<00:00, 81.37it/s] 76%|███████▌  | 155/204 [00:02<00:00, 83.09it/s] 80%|████████  | 164/204 [00:02<00:00, 84.79it/s] 85%|████████▍ | 173/204 [00:02<00:00, 84.04it/s] 89%|████████▉ | 182/204 [00:02<00:00, 84.77it/s] 94%|█████████▎| 191/204 [00:02<00:00, 85.77it/s] 99%|█████████▊| 201/204 [00:02<00:00, 87.61it/s]100%|██████████| 204/204 [00:02<00:00, 76.29it/s]
26032 images processed, 2.869025707244873 seconds used

Processing out-of-distribution places365 images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:36,  2.15it/s] 11%|█▏        | 9/79 [00:00<00:03, 17.86it/s] 22%|██▏       | 17/79 [00:00<00:02, 28.72it/s] 32%|███▏      | 25/79 [00:00<00:01, 36.46it/s] 42%|████▏     | 33/79 [00:01<00:01, 42.31it/s] 52%|█████▏    | 41/79 [00:01<00:00, 46.65it/s] 62%|██████▏   | 49/79 [00:01<00:00, 48.45it/s] 72%|███████▏  | 57/79 [00:01<00:00, 48.97it/s] 82%|████████▏ | 65/79 [00:01<00:00, 48.71it/s] 92%|█████████▏| 73/79 [00:01<00:00, 48.99it/s]100%|██████████| 79/79 [00:01<00:00, 41.43it/s]
10000 images processed, 2.0097432136535645 seconds used

Processing out-of-distribution LSUN images
  0%|          | 0/79 [00:00<?, ?it/s]  1%|▏         | 1/79 [00:00<00:23,  3.38it/s] 14%|█▍        | 11/79 [00:00<00:02, 33.84it/s] 27%|██▋       | 21/79 [00:00<00:01, 52.96it/s] 38%|███▊      | 30/79 [00:00<00:00, 62.76it/s] 49%|████▉     | 39/79 [00:00<00:00, 69.59it/s] 61%|██████    | 48/79 [00:00<00:00, 73.40it/s] 72%|███████▏  | 57/79 [00:00<00:00, 76.27it/s] 84%|████████▎ | 66/79 [00:01<00:00, 78.24it/s] 95%|█████████▍| 75/79 [00:01<00:00, 79.61it/s]100%|██████████| 79/79 [00:01<00:00, 65.59it/s]
10000 images processed, 1.2839560508728027 seconds used

Processing out-of-distribution iSUN images
  0%|          | 0/70 [00:00<?, ?it/s]  1%|▏         | 1/70 [00:00<00:24,  2.85it/s] 13%|█▎        | 9/70 [00:00<00:02, 25.16it/s] 26%|██▌       | 18/70 [00:00<00:01, 42.80it/s] 39%|███▊      | 27/70 [00:00<00:00, 55.86it/s] 51%|█████▏    | 36/70 [00:00<00:00, 65.10it/s] 64%|██████▍   | 45/70 [00:00<00:00, 70.51it/s] 77%|███████▋  | 54/70 [00:00<00:00, 74.23it/s] 91%|█████████▏| 64/70 [00:01<00:00, 80.07it/s]100%|██████████| 70/70 [00:01<00:00, 60.21it/s]
8925 images processed, 1.241960048675537 seconds used

Processing out-of-distribution dtd images
  0%|          | 0/45 [00:00<?, ?it/s]  2%|▏         | 1/45 [00:00<00:37,  1.17it/s]  7%|▋         | 3/45 [00:00<00:10,  3.83it/s] 20%|██        | 9/45 [00:01<00:03,  9.99it/s] 24%|██▍       | 11/45 [00:01<00:03,  9.11it/s] 38%|███▊      | 17/45 [00:01<00:02, 11.19it/s] 42%|████▏     | 19/45 [00:02<00:02, 11.44it/s] 56%|█████▌    | 25/45 [00:02<00:01, 11.72it/s] 60%|██████    | 27/45 [00:02<00:01, 12.50it/s] 73%|███████▎  | 33/45 [00:03<00:01, 11.36it/s] 78%|███████▊  | 35/45 [00:03<00:00, 10.81it/s] 91%|█████████ | 41/45 [00:03<00:00, 13.97it/s] 96%|█████████▌| 43/45 [00:04<00:00, 10.60it/s]100%|██████████| 45/45 [00:04<00:00, 10.59it/s]
5640 images processed, 4.305394172668457 seconds used

20.393216371536255
/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?
  warn(
Traceback (most recent call last):
  File "/home/shaokun/PALM/eval_cifar.py", line 19, in <module>
    eval_maha(args)
  File "/home/shaokun/PALM/util/evaluations/mahalanobis.py", line 21, in eval_maha
    feat_log = np.load(f"cache/{args.backbone}-{args.method}/{args.in_dataset}/train_{args.backbone}-{args.method}_features.npy", allow_pickle=True)
  File "/home/shaokun/anaconda3/envs/PPALM/lib/python3.10/site-packages/numpy/lib/npyio.py", line 427, in load
    fid = stack.enter_context(open(os_fspath(file), "rb"))
FileNotFoundError: [Errno 2] No such file or directory: 'cache/resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1/CIFAR-100/train_resnet34-top5-palm-cache6-ema0.999-b128-e50-lr0.001-wd1e-4-ltboth-bfmbalanced-fl0.2-lora_r8a32d0.05-temp0.08-stage1-inc1_features.npy'
