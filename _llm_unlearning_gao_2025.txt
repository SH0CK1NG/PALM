                                        Published as a conference paper at ICLR 2025




                                        O N L ARGE L ANGUAGE M ODEL C ONTINUAL
                                        U NLEARNING
                                         Chongyang Gao1∗ †, Lixu Wang1∗ †, Kaize Ding1 , Chenkai Weng2 , Xiao Wang1 , Qi Zhu1
                                         1
                                           Northwestern University, 2 Arizona State University
                                         {cygao, lixuwang2025}@u.northwestern.edu, chenkai.weng@asu.edu
                                         {kaize.ding,wangxiao,qzhu}@northwestern.edu

                                                                                        A BSTRACT
arXiv:2407.10223v2 [cs.LG] 3 Mar 2025




                                                  While large language models have demonstrated impressive performance across
                                                  various domains and tasks, their security issues have become increasingly severe.
                                                  Machine unlearning has emerged as a representative approach for model safety
                                                  and security by removing the influence of undesired data on the target model.
                                                  However, these methods do not sufficiently consider that unlearning requests
                                                  in real-world scenarios are continuously emerging, especially in the context of
                                                  LLMs, which may lead to accumulated model utility loss that eventually becomes
                                                  unacceptable. Moreover, existing LLM unlearning methods often ignore previous
                                                  data access limitations due to privacy concerns and copyright protection. Without
                                                  previous data, the utility preservation during unlearning is much harder. To over-
                                                  come these challenges, we propose the O3 framework that includes an Orthogonal
                                                  low-rank adapter (LoRA) for continually unlearning requested data and an Out-
                                                  Of-Distribution (OOD) detector to measure the similarity between input and un-
                                                  learning data. The orthogonal LoRA achieves parameter disentanglement among
                                                  continual unlearning requests. The OOD detector is trained with a novel con-
                                                  trastive entropy loss and utilizes a glocal-aware scoring mechanism. During infer-
                                                  ence, our O3 framework can decide whether and to what extent to load the unlearn-
                                                  ing LoRA based on the OOD detector’s predicted similarity between the input and
                                                  the unlearned knowledge. Notably, O3 ’s effectiveness does not rely on any re-
                                                  tained data. We conducted extensive experiments on O3 and state-of-the-art LLM
                                                  unlearning methods across three tasks and seven datasets. The results indicate that
                                                  O3 consistently achieves the best unlearning effectiveness and utility preservation,
                                                  especially when facing continuous unlearning requests. The source codes can be
                                                  found at https://github.com/GCYZSL/O3-LLM-UNLEARNING.


                                        1    I NTRODUCTION

                                        Recently, bolstered by scaling laws (Kaplan et al., 2020), the size of language models has grown
                                        tremendously, demonstrating excellent performance across various tasks (Wang et al., 2024). How-
                                        ever, concerns about large language models (LLMs) have also increased, particularly regarding how
                                        to eliminate undesirable data influence (e.g., privacy information (Pan et al., 2020)). To address
                                        this issue, machine unlearning (Bourtoule et al., 2021) is applied in LLMs to remove private, toxic,
                                        or illegal data. Current methods for LLM unlearning can be primarily categorized into parameter
                                        optimization (Chen & Yang, 2023; Eldan & Russinovich, 2023; Jia et al., 2024; Zhang et al., 2024;
                                        Meng et al., 2022; Li et al., 2024), and in-context unlearning (Thaker et al., 2024; Pawelczyk et al.,
                                        2024). The parameter optimization methods involve directly fine-tuning the LLM, with the objective
                                        typically being to maximize the task loss on the unlearning data or to minimize the random label
                                        loss. Some methods identify the related parameters and then make appropriate modifications. In-
                                        context learning-based methods modify the LLM input prompts to make the LLM refuse to output
                                        content related to the unlearning data. Regarding unlearning effectiveness, parameter optimization
                                        is typically much more effective than in-context learning.
                                        However, these methods still often poorly maintain the model utility outside the unlearned knowl-
                                        edge, especially in real-world continual settings. The challenges are two-fold: (i): First, in addition
                                            * Equal contributions (ordered alphabetically); † Corresponding author.




                                                                                               1
Published as a conference paper at ICLR 2025




to the data that needs to be unlearned, existing unlearning methods also require a large dataset
called the retained dataset to maintain the model utility. This retained dataset often consists of
the original training dataset (Bourtoule et al., 2021) or a portion of it, but as LLMs are trained on
massive datasets (Wang et al., 2024), assuming access to the complete training data is typically
unrealistic (Liu et al., 2024). Moreover, as time goes on, the original training data of LLMs may
become inaccessible due to expired access authorization, data privacy, and intellectual property pro-
tection (Sun et al., 2024). If the retained dataset only contains incomplete training data distribution,
the model utility of the missing parts significantly declines after unlearning. Although some studies
shrink the range of the retained data to the distribution most susceptible to unlearning, this distri-
bution itself is hard to characterize and its data may be limited due to intrinsic rarity and privacy
protection (Chang et al., 2024; Huang et al., 2024). (ii): The second challenge is that existing LLM
unlearning methods only consider single operations and cannot perform effective continual unlearn-
ing. LLM unlearning is often not a one-off operation but a continual process, as unlearning requests
continuously emerge in the real world (Liu et al., 2024). As the number of unlearning operations
increases, the aforementioned decline in model utility will also have a cumulative effect, even with
the retained dataset, meaning that the model’s general capabilities will significantly decrease Gu
et al. (2024); Gupta et al. (2024) over time.
In this work, to achieve more effective continual unlearning for LLMs, we propose the O3 frame-
work, which can balance unlearning effectiveness and model utility preservation in continuous
scenarios without using any retained data. At a high level, the O3 framework mainly includes an
Orthogonal Low-rank adapter (LoRA) (Hu et al., 2021) for continuously unlearning requested data
and an Out-Of-Distribution (OOD) detection module to assess the similarity between input data and
unlearning data. Specifically, the orthogonal LoRA in O3 enables the disentanglement of parameter
space across different unlearning requests, ensuring that the unlearning effectiveness of different
requests does not interfere with each other. Then the OOD detector in O3 is trained with a novel
contrastive entropy loss as its backbone and supplemented with a glocal-aware scoring mechanism.
The O3 framework can balance unlearning and utility because it smartly leverages the data similarity
determined by the OOD detector to decide whether and to what extent to load the unlearning LoRA
during inference. In summary, the main contributions of this work include:
• We study the underexplored problem of LLM continual unlearning and tackle the challenge of
  balancing unlearning effectiveness and model utility preservation when LLM faces the continuous
  arrival of unlearning requests, without using any retained data.
• We propose a novel O3 framework that includes an orthogonal unlearning LoRA and an OOD de-
  tector. The orthogonal design of LoRA prevents interference among different unlearning requests,
  achieving better unlearning effectiveness in continuous scenarios. The OOD detector measures the
  similarity between input and unlearning data, allowing O3 to decide whether and to what extent to
  load the unlearning LoRA during inference.
• We conduct extensive experiments on multiple benchmark tasks that comprehensively test the
  LLM continual unlearning on discriminative, generative, and reasoning tasks. The experiment
  results demonstrate that O3 consistently achieves the best balance between unlearning effective-
  ness and utility preservation without using any retained data, compared with many state-of-the-art
  baseline methods when facing continuous unlearning requests.

2   P RELIMINARY
Language Model Unlearning. Machine unlearning (Bourtoule et al., 2021) is proposed to protect
data privacy and ensure authorized usage (Liu et al., 2024; Zhang et al., 2023). There have been
approaches to achieving unlearning through parameter optimization (Jang et al., 2023; Golatkar
et al., 2020; Yao et al., 2023; Eldan & Russinovich, 2023; Zhang et al., 2024; Jia et al., 2024; Meng
et al., 2022; Yu et al., 2023; Wu et al., 2023; Li et al., 2024), and in-context learning (Thaker et al.,
2024; Pawelczyk et al., 2024). The optimization-based unlearning is to employ GradAsc (Golatkar
et al., 2020; Yao et al., 2023) on the unlearned data. The following approaches, like PO (Eldan &
Russinovich, 2023; Zhang et al., 2024; Jia et al., 2024), notice that unconstrained GradAsc hurts the
model’s utility, thus crafting task labels through shuffling or rejection. Yu et al. (2023) localizes the
model parameters related to unlearning data and updates them through merging or subtracting (Ding
et al., 2023). The in-context learning-based methods adjust input prompts to reject unwanted con-
tent generation. Although these approaches can achieve unlearning in certain cases, they neglect that


                                                   2
Published as a conference paper at ICLR 2025




unlearning requests always appear continuously (Liu et al., 2024). The challenge is that the accu-
mulative impact of conducting unlearning sequentially induces catastrophic model utility loss (Gu
et al., 2024; Gupta et al., 2024). Besides, their success in a single unlearning operation relies on
sufficient retained data with accurate labels, which are scarce in real-world LLM applications. Our
O3 addresses the above challenges via orthogonal unlearning LoRA and a designed OOD detector.
Out-of-distribution Detection. Unlike traditional OOD detection based on one-class SVM (Erfani
et al., 2016), random forest (Primartha & Tama, 2017), and Gaussian mixture modeling (Laxhammar
et al., 2009), deep learning-based OOD detection (Yang et al., 2021) has become mainstream, focus-
ing on the classification task. These approaches assume the availability of category-labeled ID data.
However, only a few studies explore unsupervised OOD detection in language models with only un-
labeled ID data. DAGMM (Zong et al., 2018) uses a deep auto-encoder to generate low-dimensional
representations to estimate OOD scores, only working on multivariate time-series data. Xu et al.
(2021) leverage the feature extracted from a pre-trained language model and then fit an OC-SVM
for detection. More recent related works search ways like ensemble learning (Zhou et al., 2023),
pseudo labeling (Lang et al., 2022), outlier exposure (Cao et al., 2024), and prefix tuning (Ouyang
et al., 2023) to achieve OOD text intent detection. Although these works can achieve OOD detection
over non-classification tasks, their effectiveness still relies on labels of specific tasks. Our proposed
O3 does not require any types of labels.
Problem Definition. In the context of LLM, we consider the prevalent causal language mod-
els, which take textual token sequences with variable lengths as the input. As an autoregressive
model, the target LLM MΘ parameterized on Θ calculates the probability of each token in a
text with the previous tokens. For the problem of unlearning, we suppose a sequence of contin-
ually arriving unlearning requests with N U,t data samples, i.e., {DU,t }Tt=1 where, for t-th request,
                          N U,t
DU,t = {xi ∥xi ∼ PXU,t }i=1     and PXU,t is the input marginal distribution. T is an index of the latest
request. Each unlearning request corresponds to a task with the training data characterized by the
input PXt and label distributions PYt . Traditional unlearning approaches always assume a retained
dataset drawn from a distribution PXR,t disjoint from the unlearning one PXU,t , to retain the model
performance on the original training distribution. Then, on one hand, the straightforward objective
of continual unlearning is:
    T
    X                                                 T
                                                      X
                                  t            t                                    t            t
          min
            t
              I(Mx∼P U,t (x, Θ); Mx∼P U,t (x, Θ )),         max
                                                             t
                                                                I(Mx∼P R,t (x, Θ); Mx∼P R,t (x, Θ )),
          Θ           X                  X                   Θ           X               X
    t=1                                               t=1
                                                                                                   (1)
where M t parameterized on Θt represents the target model during and after unlearning on the t-th
unlearning set DU,t , and I(·; ·) calculates the mutual information between two random variables. On
the other hand, the model utility preservation on other distributions PXO distinct from the unlearning
one, is another important objective for unlearning,
                              T
                              X
                                                          t          t
                                    max
                                     t
                                        I(Mx∼PXO (x, Θ); Mx∼P O (x, Θ )).                            (2)
                                     Θ                            X
                              t=1


3     M ETHODOLOGY
Proposed Approach Overview. To handle continual unlearning requests for LLM, especially con-
sidering the constraints in acquiring the retained dataset, we propose a framework called O3 that
contains two major modules: an Orthogonal LoRA module to unlearn requested knowledge con-
tinually and an OOD detector-like module to detect unlearning knowledge, detailed in Sections 3.1
and 3.2, respectively. O3 continuously unlearns the requested data using LoRA with an orthogonal
regularization loss that can maintain continuous unlearning performance. O3 obtains the unlearn-
ing knowledge detection module by the contrastive entropy minimization and local-global layer-
aggregated scoring techniques, which can predict the probability that the input data sample belongs
to the unlearning distribution PXU,t . These two major modules only use the unlearning dataset of each
unlearning request and do not require the access of retained data. After unlearning, O3 works with
an effective inference mechanism (Section 3.3), in which the unlearning LoRA is loaded with soft
weights originating from the probability predicted by the OOD module to produce distinct outputs
for different data.


                                                      3
Published as a conference paper at ICLR 2025




 Continual                        Unlearning
 Unlearning                   ... Request                   Unlearn The Lord of the Rings    Unseen Data Related to The Lord of the Rings
  Requests                                t
Ramdom Label        Mark Twain                                    OOD Score                                          Huckleberry Finn

                                                         LCEL l    dCos + dMaha

      Pre-trained        Bj t-1               Bj t          Encoder                            OOD
                                                                              OCSVM                             Pre-trained                 B
      T LLM           T Aj t-1                Aj t                                            Detector             LLM                      A
                                  LOrth

                      Who is the writer of The Lord of the Rings?                           Who is the main character in The Lord of the Rings?

                                   Training Pipeline                                                  Soft-weighted Inference
                                                 3
Figure 1: The overview of O framework to handle continual unlearning requests for LLM without
using any retained data. O3 includes two major components: an Orthogonal optimization process for
unlearning requested knowledge, and an OOD detector is used to detect whether the input contains
the unlearning knowledge. The unlearning knowledge optimization uses the orthogonal loss (LOrth )
to prevent interference among different unlearning requests. The OOD detector is trained by a
novel contrastive entropy loss (LCEL ) and works with a layer-aggregated scoring mechanism that
leverages cosine similarity (dCos ) and Mahalanobis distance (dMaha ). In the inference phase, the
OOD detector decides whether and to what extent to load the unlearning LoRA.


3.1      LLM U NLEARNING WITH O RTHOGONAL -R EGULARIZED L O RA

As the model size of LLM is quite large, the associated costs of fine-tuning-based unlearning are
extremely massive. Moreover, continual fine-tuning of the entire LLM also poses challenges of
preserving the general capabilities and sustaining the efficacy of previous unlearning efforts (Gu
et al., 2024). To address these issues, we use LoRA and incorporate an orthogonal regularization by
keeping the original LLM frozen and focusing on updating previously neglected LoRA parameters.
LoRA (Hu et al., 2021) employs low-rank matrix decomposition to reduce the trainable parameters.
A transformer’s attention or linear layers are parameterized on weight matrices, W ∈ RU ×V . LoRA
introduces two low-rank trainable matrices r := {A, B} where A ∈ RU ×K and B ∈ RK×V . This
ensures that the dimension of ABh matches that of W h where h is the input of the weight matrix,
                                h′ = W h + △W h = W h + ABh,                                    (3)
        ′
where h is the output of the weight matrix. During LoRA fine-tuning, W is frozen, and only A and
B are updated. To achieve effective unlearning, we leverage preference optimization to update the
model for random task labels or reject-based answers such as ‘I don’t know’, termed as ŷ. We adopt
CrossEntropy Loss of the unlearning set DU,t for training,
                                                                              U,t
                                                                        N
                                                                   1 X U,t
                                                     LtCE = −              ŷ log MΘ (xU,t
                                                                                       i ).                                                       (4)
                                                                  N U,t i=1 i
To further reduce the cost during unlearning, we leverage a single LoRA for all unlearning tasks
rather than employing multiple LoRAs. However, this adoption compromises the effectiveness of
previously unlearned requests due to task entanglement. To address such issues, inspired by Wang
et al. (2023), we propose to adopt an orthogonal regularization loss to disentangle unlearning re-
quests in the LoRA parameter space. Specifically, at the t-th unlearning request, we initialize
the LoRA with the latest rt−1 := {At−1 , B t−1 }. The matrix At−1 can be further denoted as
[at−1,1 , · · · , at−1,K ] where each at−1,k is a column vector with U dimensions. Note that the matrix
B can be regarded as the linear weights of matrix A. Thus, we don’t consider its interference among
different requests. We view the spanned space that consists of column vectors in At as the t-th
task parameter space U t . In this case, if the LoRA parameter space of different requests achieves
orthogonal, their spanned space will be:
                                      ∀u ∈ U t , ∀v ∈ U t−1 , u · v = 0.                            (5)
This design guides the LoRA tuning to focus more on previously overlooked parameters, striking a
balance between different unlearning tasks. Therefore, we can approximate this correlation with an
orthogonal loss LOrth and combine it with the preference optimization to form the total loss of the
LoRA tuning LLoRA ,
                                                                         2
                                  LtOrth = (At−1 )⊤ At                       , and LtLoRA = LtCE + λLtOrth ,                                      (6)


                                                                                  4
Published as a conference paper at ICLR 2025




where λ = 0.1 is a factor used to balance the optimization guidance, and please refer to its sensitivity
analysis in Section 4.3. Initially, when t = 1, there is no need to apply the orthogonal regularization.
As the unlearning continues, the orthogonal loss is incorporated.

3.2   U NLEARNED K NOWLEDGE D ETECTION

By viewing the unlearning dataset as in-distribution (ID) data, our task of unlearning knowledge
detection becomes an OOD detection problem (Yang et al., 2021). However, nearly all existing
OOD detection works are built upon the classification problem, which is not the mainstream task of
language models. Besides, their representation learning and scoring mechanisms rely on semantic
category labels that are inaccessible in our problem. To tackle these shortcomings, we propose new
designs for the representation learning and scoring mechanism for LLM continual unlearning.

OOD Representation Learning with Contrastive Entropy Minimization. In unsupervised sce-
narios, self-supervised learning (SSL), such as contrastive learning, has been demonstrated effec-
tively for OOD detection on visual data (Tack et al., 2020). However, such contrastive learning is
unsuitable for language OOD detection, as it is hard to achieve semantically equivalent data aug-
mentation on texts. Moreover, token-level SSL tasks, like Masked Language Modeling (MLM) and
SimCSE (Gao et al., 2021; Jian et al., 2022a;b), are far less effective. To address these challenges,
we propose a novel contrastive entropy loss to learn text representations for OOD detection.
Our contrastive entropy also starts with the augmentation view generation. Inspired by MLM, we
leverage random masking to generate the first view type. Specifically, for a particular text instance
x with the token length n, we randomly select p% (p = 15 in our implementation) tokens and
replace them with the token of [MASK]. The instance with the random masking is denoted as
x∗ . For the second contrastive view, we follow MoCo (He et al., 2020) to maintain a key encoder
FΩkey initialized from the original OOD module backbone FΩ that is a transformer consisting of L
attention layers, i.e., F := fω1 ◦ · · · fωl ◦ · · · fωL . Then the second view is generated from FΩkey by
forwarding the original text instance x.
Our contrastive entropy loss shares similar intuition with the standard contrastive learning, i.e.,
aligning positive pairs as close as possible while pushing negative pairs far away, but converges
much faster owing to weighting by cosine similarity-based Softmax probability and being conducted
at every model layer. In the mini-batch training, our contrastive entropy loss shapes like
              B
             N X
             X L X
                 N     B
                                                                          exp(fω[1:l] (x∗i ) · fωkey (xj ))
                                                                                                 [1:l]
  LCEL = −               ∆(i, l, j) log(∆(i, l, j)), where ∆(i, l, j) = PN B                                   ,
                                                                                              ∗
             i=1 l=1 j=1                                                 k=1 exp(fω[1:l] (xi ) · fω key (xk ))
                                                                                                    [1:l]
                                                                                                               (7)
where fω[1:l] (x∗i ) means the token averaging representation of the l-th layer. N B is the sample quan-
tity of a mini-batch. The convergence condition of entropy loss is known to be that one dimension
holds the probability of 1, corresponding to the state that the maximum similarity between positive
pairs and minimum similarity between negative pairs. Considering the forgetting of the pre-trained
knowledge, we also supplement LCEL with the MLM loss to form the total loss LOOD ,
                                                                            B
                                                                         N
                                                                      1 X ∗
                  LOOD = LCEL + LMLM , where LMLM = −                       y log FΩ (x∗i ),                   (8)
                                                                     N B i=1 i

where y ∗ is the random token masking label. When dealing with the continually arriving unlearning
                                                                                  U,t        U,t
requests, we first randomly divide the unlearning dataset DU,t into two subsets Dused and Drest  with
                                                                                               U,t
αN U,t and (1 − α)N U,t samples (α = 80% in our implementation), respectively. Then Dused           is
                                                                      U,t
used to compute LtOOD and train the OOD detector backbone FΩ . Drest      will be used in the scoring
(Section 3.2) and inference mechanisms (Section 3.3).

Glocal-aware OOD Scoring. We design a glocal-aware scoring mechanism to supplement the
contrastive entropy representation learning. On the one hand, we assume all ID data form a global
Gaussian distribution at each model layer and leverage Mahalanobis Distance (De Maesschalck
et al., 2000) to quantify the chance of outliers. On the other hand, considering the inaccuracy of fitted
Gaussian distribution, especially when the available ID data is few-shot or biased, we emphasize


                                                       5
Published as a conference paper at ICLR 2025




each ID data instance and compare instance-wise cosine similarity locally. Specifically, at each
unlearning request t (we omit t below), for each layer fωl , we calculate the empirical mean and
                       U
covariance on subset Dused ,
                        U                               U
                  αN                    αN
               1 X           U       1 X                                           ⊤
         µl =         fω (x ), Σl =          (fω[1:l] (xU                  U
                                                        i )−µl )(fω[1:l] (xi )−µl ) .
                                                                                                     (9)
              αN U i=1 [1:l] i      αN U i=1

Next, we can calculate the Mahalanobis Distance for each testing sample x at the l-th layer,
                       dMaha (x)l = (fω[1:l] (x) − µl )⊤ Σ−1
                                                          l (fω[1:l] (x) − µl ),                    (10)

where Σ−1  l   is the inverse of Σl . The Mahalanobis Distance measures the probability density of
fω[1:l] (x) in the estimated Gaussian distribution of the l-th layer.
As aforementioned, assuming Gaussian distributions for all layers are arbitrary, thus we leverage
another distance based on the maximum instance-wise cosine similarity dCos (x)l of a testing sample
                           U
x to all ID instances in Dused   for each layer, and obtain a combined score s(x)l ,
                        (                               )
                  αN U     fω[1:l] (x) · fω[1:l] (xU
                                                   i )
    dCos (x)l = − max                                     , and s(x)l = dMaha (x)l + γ · dCos (x)l , (11)
                   i=1    |fω[1:l] (x)||fω[1:l] (xUi )|

where γ = 1000 is a scaling factor that unifies the order of magnitude. To achieve effective OOD
detection, a module is needed that allows us to determine the score range of ID and OOD data.
Inspired by Xu et al. (2021); Darrin et al. (2024), we leverage one-class SVM (OCSVM) based
on the layer-aggregated score vector s(x) = [s(x)1 , · · · , s(x)l , · · · , s(x)L ] to do so. In OOD
detection, OCSVM specifies a hypersphere instead of a hyperplane to characterize the ID data. It is
feasible in our problem as the score of each model layer is a scalar, and then the concatenated score
vector conforms to a hypersphere distribution.
When dealing with each unlearning request, after conducting representation learning in Section 3.2,
                                                       U,t
we calculate score vectors s(x) for all samples in Dused   . Then use these score vectors to fit an
OCSVM with the fitted hypersphere H (c , R ) characterized by a center vector ct and the radius
                                        t t    t
                     U,t                                                    U,t
Rt . In addition to Dused , we also need to calculate the score vectors of Drest , and store all these
vectors as we cannot access the unlearning data after the unlearning.

3.3   S OFT- WEIGHTED I NFERENCE

Assuming the training of O3 framework stops temporarily when finishing the T -th unlearning re-
quest, let us introduce how to leverage it during inference. Specifically, for each testing instance x,
it should be fed into all OOD detector backbone {F 1 , · · · , F T } where each F t := F ◦ rFt consists
of the original F and the LoRA adapter rFt of the t-th request. Then the score vector of x for each
F t can be computed as s(x)t via Eq. 11. The score vector is forwarded to the OCSVM to obtain
the distance from x to the boundary of the hypersphere Ht (ct , Rt ),
                                    dHt (x) = |s(x)t − ct | − Rt .                                  (12)
                                                       U,t
As aforementioned, for the unlearning dataset D         at each stage, we randomly split 80% sam-
             U,t                                                                                      U,t
ples into Dused   to train the OOD detector backbone and fit the OCSVM, while the rest Drest
                                                                        U,t            U,t
is used for the following soft weighting. First, for all samples of Dused and Drest , we calculate
                                                                         U,t                 U,t
their dHt via Eq. 12 and fit two Gaussian distributions as N (µ[dHt (Dused    )], σ 2 [dHt (Dused )]) and
             U,t     2        U,t
N (µ[dHt (Drest )], σ [dHt (Drest )]), respectively. Then we mix these two Gaussian distributions
                                                                            t
with equal weights and denote the cumulative distribution function as Pmix     . Next, the probability
                                                                  0            t
center of the mixed Gaussian distribution can be determined as dHt where Pmix       (d0Ht ) = 0.5. In the
end, for each testing instance x, we have the following weight that reflects how much content of x
belongs to the unlearning distribution of the t-th request,
w(x)t = δ{ζ[1 − max(p, p′ ) + min(p, p′ )]}, where p = Pmix
                                                         t
                                                             (dHt (x)), p′ = Pmix
                                                                              t
                                                                                  (2d0Ht − dHt (x)),
                                                                                                (13)
where δ is a Sigmoid function that scales the weight into (0, 1), and ζ = 10 is a scaling factor
for more fine-grained sensitivity. After getting the weights from all OOD detectors, we adopt the


                                                   6
Published as a conference paper at ICLR 2025




maximum weight w(x) = max{w(x)1 , · · · , w(x)T } to load the unlearning LoRA by modifying
Eq. 3 into h′ = W h+w(x)·ABh. In this case, the input x is sequentially forwarded to all attention
layers of the target LLM to obtain the final inference results. A higher w(x) implies that x is close
to at least one unlearning distribution, thus we should load the unlearning LoRA. In contrast, if
w(x) is relatively low, detaching the LoRA while using the original model makes more sense. The
algorithm pipeline is provided in Appendix D.


4     E XPERIMENTS

4.1   E XPERIMENTAL S ETUPS

Datasets. In the main context, we conduct experiments on three tasks: Question Answering, Fic-
titious Knowledge Generation, and Intent Classification by unlearning different types of subsets
continuously while maintaining the utility ability. Appendix E.2 provides more details.
• Question Answering. For ScienceQA (Lu et al., 2022b), we gather text-only samples to form
  a train and test set with 6,508 and 2,224 samples. We choose four domains in ScienceQA as
  continual unlearning requests, i.e., biology → physics → chemistry → economics. We use Com-
  monsenseQA (Talmor et al., 2019) as a utility dataset, which contains 9,740 training samples and
  1,221 validation samples for evaluating the commonsense reasoning capability of LLMs. Open-
  bookQA (Mihaylov et al., 2018) can assess the book comprehension ability, consisting of 4,957
  training, 500 validation, and 500 testing samples.
• Fictitious Knowledge Generation. TOFU (Maini et al., 2024) consists of questions about fake
  authors synthesized by GPT-4. There are three forget-sets: ‘forget01’, ‘forget05’, and ‘forget10’,
  corresponding to 1%, 5%, and 10% randomly selected authors, which are used as three continual
  unlearning requests. Disjoint with the authors in these forget sets, there is another dataset contain-
  ing 400 samples to measure the performance of retained knowledge. Besides, TOFU includes two
  datasets related to Real-world Authors and World Facts to test the utility preservation.
• Intent Classification. CLINC150 (mis, 2020) is designed for intent classification, which comprises
  150 classes across five domains, and each includes 200 training, 40 validation, and 60 testing
  samples. For the unlearning settings, we choose three domains most related to privacy (‘work’,
  ‘travel’, and ‘home’) as the continual unlearning requests. To evaluate the utility preservation,
  we leverage MRPC (Dolan & Brockett, 2005) and RTE (Wang et al.) on the task of paraphrase
  identification and textual entailment, respectively.
Evaluation Metrics. To evaluate the unlearning effectiveness, we test the model performance at
every unlearning request on the unlearning train set (used in unlearning) and unlearning test set
(unused in unlearning), which are disjoint but from the same distribution, denoted as Sample-
level Unlearning (S.U.) and Distribution-level Unlearning (D.U.), respectively. As for measuring
the utility preservation, we consider the model performance on three distributions. The first is the
distribution most susceptible to unlearning requests, which we term Retained Distribution (R.D.).
The other two are the utility datasets for each task (question answering: CommonsenseQA and
OpenbookQA; fictitious knowledge generation: TOFU-Real Authors (i.e., R.A.) and Word Facts
(i.e., W.F.); intent classification: MRPC and RTE), and we denote their performance as U.1. and
U.2., respectively. In addition, to evaluate the balance between unlearning effectiveness and utility
preservation, we design a metric called the Unlearning-Utility Ratio (U2 R), i.e., U2 R = (Acc0S.U. +
Acc0D.U. −AccTS.U. −AccTD.U. )/(Acc0R.D.+Acc0U.1.+Acc0U.2.−AccTR.D.−AccTU.1.−AccTU.2. ) where Acc means
the accuracy. The higher the U2 R, the better both unlearning effectiveness and utility preservation.
Compared Baselines. To better demonstrate the effectiveness of our proposed methods, we im-
plement a series of state-of-the-art language model unlearning approaches: GradAsc Golatkar et al.
(2020), GradDif Yao et al. (2023), EUL Chen & Yang (2023), PO Eldan & Russinovich (2023),
NPO Zhang et al. (2024), SOGD Jia et al. (2024) and SOPO Jia et al. (2024). We only conduct
reasonable modifications to customize them in our continual unlearning settings.
Implementation Details. Following TOFU (Maini et al., 2024) and SOPO (Jia et al., 2024), we
use LLaMA2-7b (Touvron et al., 2023) as the target model. The used OOD detector backbone is
Roberta-large (Liu et al., 2019). All experiments are run repeatedly with three random seeds. We use
the AdamW optimizer with 3e-4 as the learning rate and 128 as the batch size for combined datasets.


                                                   7
Published as a conference paper at ICLR 2025




The epochs are 10 and 20 for ScienceQA-CommonsenseQA-OpenbookQA and CLINC150-MRPC-
RTE. We set the LoRA rank for all experiments to 8. More details can be found in Appendix E.

4.2                   E FFECTIVENESS OF C ONTINUAL LLM U NLEARNING WITHOUT R ETAINED DATA

We conduct experiments on three tasks with
continual unlearn requests and provide suffi-                                                                               GradAsc
                                                     100                                                                    GradDif
cient retained data for all comparison base-




                                                                                                                                  Unlearning-Utility Ratio
                                                                                                                            EUL
                             3                                                                                              PO
lines while assuming our O framework only             80
                                                                                                                            NPO
uses the data of each unlearning request and                                                                                SOGD
                                                      60                                                                    SOPO
does not use any retained data. We first cal-                                                                               Ours
culate the Unlearning-Utility Ratio, as shown         40

in Figure 2. The effectiveness of O3 is evident       20
as it always hits the highest U2 R and signifi-
                                                       0
cantly surpasses the second best for all three            Question Answering Fictitious Knowledge Generation Intent Classification

tasks. Beyond achieving superior performance Figure 2: Comparison between ours and other
across all tasks, our framework demonstrates baseline approaches on Unlearning-Utility Ratio
enhanced data and parameter efficiency. As in- (U2 R) that measures the balance between unlearn-
dicated in Table 1, the quantity of training data ing effectiveness and utility preservation.
required by our O3 framework is only half that
of the baseline models since it does not necessitate using retained data. Moreover, the integration
of LoRA significantly reduces the trainable parameters to 20M, which is even less than 3% of the
baselines’ 6,758M. Our additional inference computation overhead is only 5.6% higher than the
baselines, as detailed in Appendix F.1.

Table 1: Comparison between
                                                                                                     100                                                                                           100
ours and other baselines on used                                                                     80                     Base                             PO     SOPO                           80                     Base           PO           SOPO
                                                                                       QA Accuracy




                                                                                                                            GradDif                          NPO    Ours             QA Accuracy                          GradDif        NPO          Ours
training data quantity and train-                                                                    60                     EUL                              SOGD                                  60                     EUL            SOGD
able parameters. The trainable                                                                       40                                                                                            40

parameters of baselines are all                                                                      20                                                                                            20
                                                                                                      0                                                                                             0
the whole LLM.                                                                                             Initial   1st Request 2nd Request 3rd Request 4th Request                                     Initial   1st Request 2nd Request 3rd Request 4th Request
                            Dataset                      Baselines     Ours
                                                                                                              (a) S.U. of ScienceQA                                                                         (b) D.U. of ScienceQA
                            Training Data Quantity         4854        2427
 ScienceQA
                            Trainable Parameters          6,758M       20M
                                                                                     Figure 3: Unlearning effectiveness comparison between ours
                            Training Data Quantity         1280        640
   TOFU
                            Trainable Parameters          6,758M       20M           and other approaches on (a) sample-level unlearning (S.U.), (b)
 CLINC150
                            Training Data Quantity
                            Trainable Parameters
                                                           2400
                                                          6,758M
                                                                       1200
                                                                       20M
                                                                                     distribution-level unlearning (D.U.) of ScienceQA.

                 95
                                                                                              80                                                                                         80
                 90
                 85                                                                           60                                                                                         60
   QA Accuracy




                                                                                QA Accuracy




                                                                                                                                                                           QA Accuracy




                 80                                                                                                                                                                                                      Base          PO           SOPO
                                                                                              40                                                                                         40                              GradDif       NPO          Ours
                 75                                                                                                                                                                                                      EUL           SOGD
                 70          Base        PO           SOPO                                             Base        PO           SOPO
                                                                                              20                                                                                         20
                 65          GradDif     NPO          Ours                                             GradDif     NPO          Ours
                             EUL         SOGD                                                          EUL         SOGD
                 60                                                                           0                                                                                          0
                      Initial 1st Request 2nd Request 3rd Request 4th Request                   Initial 1st Request 2nd Request 3rd Request 4th Request                                       Initial          1st Request 2nd Request 3rd Request 4th Request

                       (a) Retained Distribution                                                           (b) CommonsenseQA                                                                                 (c) OpenbookQA
Figure 4: Utility preservation performance comparison between ours and state-of-the-art unlearning
approaches on the testing set of (a) Retained Distribution, (b) CommonsenseQA, (c) OpenbookQA,
after unlearning each request of ScienceQA.

Question Answering. Figures 3(a) and 3(b) show the QA accuracy on the train and test set of un-
learning data. We omit the results of GradAsc as it failed to generate meaningful answers for all
distributions. We can easily observe that our O3 is located at the bottom tier, with only GradDif
and SOGD being lower than O3 . However, further examination of GradDif and SOGD revealed that
they produce empty or nonsensical sentences filled with repeated tokens, which are considered an
unlearning failure shown in Appendix F.2. Moreover, the utility preservation of GradDif and SOGD
is extremely poor, as shown in Figure 4. In contrast, the QA accuracy of O3 on the retained distribu-
tion is slightly lower than the base model (the original target LLM without any unlearning), and O3
is even nearly the same as the base model on CommonsenseQA and OpenbookQA. Therefore, we
can conclude that our O3 framework provides a much better balance in unlearning effectiveness
and utility preservation than all baselines when facing continual unlearning requests.


                                                                                                                                8
Published as a conference paper at ICLR 2025




Table 2: Performance Comparison between our O3 and other baselines when continually unlearn-
ing TOFU-forget01, -forget05, and -forget10 in Fictitious Knowledge Generation. The unlearning
effectiveness is measured by the generation accuracy of the unlearning train data and unlearning
test data denoted as S.U. and D.U., respectively. Utility preservation is evaluated by the generation
accuracy of Retained Distribution (R.D.), TOFU-Real Authors (R.A.), and World Facts (W.F.).
                       Unlearning Request 1                         Unlearning Request 2                             Unlearning Request 3
 Method
           S.U.↓    D.U.↓    R.D.↑   R.A.↑     W.F.↑    S.U.↓    D.U.↓    R.D.↑   R.A.↑         W.F.↑    S.U.↓    D.U.↓    R.D.↑   R.A.↑     W.F.↑
 Base      85.0±0   90.0±0   85.8±0   89.0±0   86.3±0   87.3±0   89.3±0       85.8±0   89.0±0   86.3±0   85.3±0   90.0±0   85.8±0   89.0±0   86.3±0
 GradAsc 75.0±0 85.0±0 81.0±0 86.0±0 82.1±0 17.6±0.2 23.1±1.1 19.0±0 0±0                0±0      17.1±0.9 14.2±5.0 19.0±0 0±0       0±0
 GradDif 78.1±0 84.0±1.7 81.9±1.6 86.7±0.6 83.5±0.5 62.5±5.4 70.0±8.7 70.4±3.7 65.7±7.2 77.9±1.7 16.5±0.3 15.2±4.5 19.0±0 0±0       0±0
 EUL     84.1±0.2 86.3±0.6 86.1±0.2 86.7±1.5 87.1±0.1 84.4±0.6 90.3±3.8 85.8±0.3 88.0±0 85.5±0 80.1±0.2 83.5±0.5 83.4±0.1 86.3±1.2 83.5±0.5
 PO      12.5±0 16.0±1.3 78.4±0.2 86.7±2.3 83.8±0 30.5±2.9 48.8±1.3 82.5±1.2 87.3±1.2 83.2±0.5 38.2±0.2 47.1±7.3 81.4±0.4 86.3±0.6 84.4±0.2
 NPO     68.8±3.2 75.0±0 83.6±0.4 89.0±0 81.8±0.5 76.3±2.2 84.2±3.8 83.2±1.6 87.7±0.6 84.1±0.5 77.6±3.9 79.2±3.1 81.4±0.6 87.3±0.6 82.9±1.7
 SOGD 43.7±0.1 76.0±1.7 80.3±0.6 85.3±1.2 83.4±0.7 22.8±6.9 24.0±3.6 79.0±3.8 81.3±2.1 82.6±1.0 17.4±0.5 21.7±6.4 82.3±0.3 77.0±6.6 82.1±1.0
 SOPO 25.6±1.0 38.0±0.9 83.7±0.3 85.3±1.2 83.7±1.6 34.1±3.0 37.5±2.6 81.5±1.6 87.3±1.5 83.2±1.0 34.5±3.0 40.0±1.3 80.2±1.7 86.7±0.6 84.2±0.8
 Ours      12.5±0.5 14.4±0.5 85.1±0.1 89.0±0   86.3±0   15.8±0.3 20.3±0.8 85.0±0       89.0±0   86.3±0   15.5±0.7 19.7±0.7 84.9±0.2 88.8±0.2 86.1±0.2



Table 3: Performance Comparison between our O3 and other baselines when continually unlearning
domain ‘work’, ‘travel’, and ‘home’ of CLINC150 in Intent Classification. The unlearning effec-
tiveness is measured by the classification accuracy of the unlearning train data and unlearning test
data denoted as S.U. and D.U., respectively. Utility preservation is evaluated by the accuracy of
Retained Distribution (R.D.), MRPC, and RTE.
                       Unlearning Request 1                         Unlearning Request 2                             Unlearning Request 3
 Method
          S.U.↓     D.U.↓     R.D.↑   MRPC↑ RTE↑        S.U.↓    D.U.↓    R.D.↑   MRPC↑ RTE↑             S.U.↓    D.U.↓     R.D.↑   MRPC↑ RTE↑
 Base     100.0±0   99.9±0   99.8±0   88.0±0   88.7±0   100.0±0 99.9±0        99.8±0   88.0±0   88.7±0   99.9±0   99.9±0   99.8±0   88.0±0   88.7±0
 GradDif 0.1±0.2 0±0         90.8±3.4 39.9±3.4 31.6±5.3 0±0      0±0      12.7±3.6 9.0±3.8 0.8±0.8 0±0        0±0      75.5±4.8 12.9±6.0 1.7±2.1
 EUL     0.1±0.2 0±0         98.3±0.4 87.2±0.1 88.1±0 0.1±0.2 0±0         87.6±3.3 80.3±3.1 82.9±3.1 0±0      0±0      92.3±5.2 81.3±2.1 76.3±4.0
 PO      26.3±15.1 26.7±14.0 99.3±0.3 84.1±0.2 86.3±1.1 59.6±3.0 59.8±3.0 99.4±0.2 87.3±0.1 88.0±0.2 56.2±5.4 56.7±4.8 99.0±0.4 86.3±0.2 87.0±0.4
 NPO     99.9±0.1 99.0±0     99.2±0.2 87.3±0.3 88.4±0.4 99.9±0.1 99.3±0.3 99.2±0.3 87.2±0.7 88.9±0.6 99.9±0.1 99.2±0.2 99.3±0.1 87.0±0.4 88.9±0.2
 SOGD 0±0          0±0       92.3±0.9 6.1±3.6 17.9±6.4 0±0       0±0      93.1±2.0 3.3±3.8 19.5±9.0 0±0       0.1±0.1 94.0±1.8 2.9±0.6 23.7±9.9
 SOPO 24.9±15.6 26.3±15.0 99.6±0.1 85.5±0.6 87.1±1.1 62.3±1.4 60.3±1.3 99.6±0.2 87.1±0.2 87.7±1.1 58.8±15.5 59.7±14.8 99.6±0.3 86.6±1.2 86.0±1.4
 Ours     10.3±8.1 14.3±0.3 98.9±0.1 84.8±0.1 87.5±0.2 50.5±0.3 55.6±0.6 94.1±0.8 87.0±0.2 89.3±0.2 40.6±4.0 42.4±3.8 97.8±0.8 86.6±1        89.0±0.2



Fictitious Knowledge Generation. Table 2 presents the experiment results on TOFU. According to
these results, we observe that our O3 achieves the best in both unlearning effectiveness and utility
preservation, providing the best unlearning effectiveness in almost all cases but one, and the best
utility preservation in the majority of cases. In the one case where O3 is not the best for unlearning
effectiveness (i.e., D.U. for unlearning request 3, the better ones GradAsc and GradDif have almost
completely lost model utility. We explain the metric details in Apppenix E.5.
Intent Classification. Table 3 presents the experiment results on CLINC150 dataset. Similar to
QA, we observed consistent unlearning failures with GradDif, SOGD, and EUL methods. More-
over, both GradDif and SOGD demonstrated extremely poor performance in preserving utility. In
contrast, our O3 framework achieves the best unlearning performance and maintains comparable
or better results to the baselines that use retained data, both on the retained distribution and util-
ity preservation. For instance, our O3 framework preserves the RTE performance more effectively
than all baseline methods.
More Experiments. In Appendix C and F.3, we found existing unlearning approaches perform
much poorer when the retained data becomes more limited. In the Appendix, we further demonstrate
experiments on scaling the unlearning with more requests (F.4), unlearning multiple knowledge en-
tities per request (F.5), Membership Inference Attacks (F.6), Detoxification (F.7), unlearning unsafe
behaviors on benchmark WMDP (F.8), robustness of our O3 against targeted relearning attack (F.9),
the evaluation of unlearning in terms of the Oracle model fine-tuned with only retained data (F.11)
and the quantity-limited unlearning data (F.10).

4.3       A BLATION S TUDY

We conducted the ablation study as follows, and detailed the analysis in Appendix G.1.
Unlearning Knowledge Detection. We detach the use of contrastive entropy loss LCEL in O3 and
use SimCLR (‘Ours w/ SimCLR’) and MoCo (‘Ours w/ MoCo’) with the augmentation using token
masking. As for the scoring mechanism, we try using Mahalanobis Distance (dMaha in Eq. 10) and
Cosine Similarity (dCos in Eq. 11) separately, which are termed as ‘Ours w/o dCos ’ and ‘Ours w/o
dMaha ’. Besides, instead of using all model layers, we use only the last layer (‘Ours w/ last layer’).


                                                                          9
Published as a conference paper at ICLR 2025




Moreover, two state-of-the-art OOD detection approaches: MDF (Xu et al., 2021) and Agg (Darrin
et al., 2024), are compared with ours. The experiments are conducted on fictitious knowledge gen-
eration and question answering. We report the AUROC in Table 4, where we can observe that the
full design of our OOD detector in O3 framework always achieves the best AUROC.

Table 4: OOD detection performance comparison and ablation study between ours and others on
Fictitious Knowledge Generation and Question Answering. The measurement is AUROC.
       Task                           Fictitious Knowledge Generation                                                          Question Answering
        ID             TOFU-forget01 TOFU-forget02 TOFU-forget10                               biology                     physics                 chemistry                 economics
       OOD             R.D. R.A. W.F. R.D. R.A. W.F. R.D. R.A. W.F. R.D. C.QA O.QA R.D. C.QA O.QA R.D. C.QA O.QA R.D. C.QA O.QA
       MDF             90.5 96.6 97.6 80.3 92.7 98.3 91.3 97.8 98.8 91.0 94.4                             95.7 92.8 96.0             97.4 92.0 94.3          98.1 94.0 94.7              95.5
       Agg             94.4 98.0 98.0 81.9 94.0 98.5 85.0 97.5 99.0 91.1 96.0                             95.4 92.2 94.5             95.7 91.0 94.8          95.0 94.5 96.0              97.6
 Ours w/ SimCLR 96.0 97.2 99.0 70.2 76.2 86.9 87.9 95.1 99.0 92.5                              96.0       96.5     93.1    96.6      97.6   93.0    95.5     96.4     94.2     95.9      96.6
 Ours w/ MoCo       95.8 98.0 99.1 87.2 92.2 99.0 91.3 98.6 99.1 93.3                          94.2       94.0     92.2    96.4      96.0   95.7    97.1     98.0     94.2     96.3      97.5
 Ours w/o dCos      90.3 96.6 97.6 85.6 92.4 97.6 90.7 98.0 99.0 96.0                          95.5       98.4     96.5    96.9      98.8   97.0    97.0     98.8     95.9     97.3      98.2
 Ours w/o dMaha 97.0 98.0 99.0 89.0 93.5 98.1 87.5 97.5 98.5 99.0                              97.8       98.4     99.4    98.0      97.2   99.0    97.9     96.3     99.0     97.8      97.5
 Ours w/ last layer 91.3 98.0 98.0 75.6 94.1 98.8 82.1 97.4 99.0 94.4                          98.6       97.4     96.8    98.9      98.8   97.0    98.8     98.0     96.9     97.9      97.2
 Ours full             97.8 99.0 99.2 92.5 95.0 99.1 93.0 98.8 99.2 99.5 99.5                             99.4 99.8 99.9             99.8    100 99.9        99.8 99.9 99.9              99.8


 Table 5: Hyper-parameter analysis of the un-                                                  Table 6: Hyper-parameter analysis of the
 learning knowledge optimization of O3 frame-                                                  soft-weighted inference of O3 framework.
 work. We adopt a series of values for the factor                                              We adopt a hard-weighted (Hard-w) mech-
 λ of Eq. 6 to validate the necessity of LOrth and                                             anism and change the scaling factor ζ of
 analyze the sensitivity of λ. ‘C.QA’ shorts for                                               Eq. 13. ‘C.QA’ shorts for CommonsenseQA,
 CommonsenseQA, and ‘O.QA’ shorts for Open-                                                    and ‘O.QA’ shorts for OpenbookQA.
 bookQA.                                                                                        Dataset                    ScienceQA                              CLINC150
     Dataset                  ScienceQA                             CLINC150                    Metric      S.U.    D.U.    R.D.   C.QA     O.QA    S.U.   D.U.    R.D.   MRPC    RTE
     Metric     S.U.   D.U.    R.D.    C.QA    O.QA   S.U.   D.U.    R.D.   MRPC   RTE          Hard-w      38.0    42.2    91.3     79.2   83.0    37.0   41.2    97.3   87.4    87.7
                                                                                                ζ=1         10.0    17.0    91.1     78.5   80.8    36.9   40.5    97.0   87.0    88.8
     λ=0        15.3   18.1    90.1     78.1   80.0   33.3   31.8    21.8   86.4   89.2
                                                                                                ζ=5          9.4    14.9    91.1     78.5   80.8    36.1   38.5    97.0   87.0    88.8
     λ = 0.01    6.3   11.4    90.9     78.1   80.8   33.8   33.3    54.5   86.8   88.8
                                                                                                ζ = 50       9.3    12.5    90.4     78.5   80.8    35.9   37.2    97.0   87.0    88.8
     λ = 0.05    8.3   13.1    91.1     78.3   80.8   33.9   35.7    80.8   87.0   89.2
                                                                                                ζ = 100      9.1    12.2    90.0     78.5   80.8    35.9   37.2    96.8   87.0    88.8
     λ = 0.1     9.3   14.0    91.1     78.5   80.9   35.0   36.2    97.0   87.0   88.9
     λ = 0.2     9.9   14.8    91.2     78.5   81.4   51.2   54.3    98.5   87.1   88.4         Ours        9.3     14.0    91.2     78.5   80.9    35.0   36.2    97.0   87.0    88.9




Unlearning Knowledge Optimization. In the objective of unlearning knowledge optimization
(Eq. 6), there is a factor λ balancing LCE and LOrth . We adopt 0, 0.01, 0.05, 0.1, and 0.2 for λ
to validate the importance of LOrth and the sensitivity of λ. We conduct experiments on question
answering and intent classification. Table 5 illustrates that employing orthogonal loss contributes
to maintaining utility on the retained distribution and enhancing the unlearning effectiveness.
Soft-weighted Inference. Instead of using soft weights to load unlearning LoRA, we test a hard-
weighted strategy (‘Hard-w’ in Table 6). Specifically, we first calculate the hypersphere boundary
distance range of the unlearning set DU,t , i.e., [min(dHt (DU,t )), max(dHt (DU,t ))]. Then for each
testing instance x, if its boundary distance dHt (x) is within the above range, we load the unlearning
LoRA, otherwise, we detach the LoRA. We also conduct a sensitivity analysis of the scaling factor ζ
in Eq. 13 with a series of values 1, 5, 50, and 100. These experiments are carried out on ScienceQA
and CLINC150, and we report the performance after the last unlearning request in Table 6. We ob-
serve that the ‘Hard-w’ method performs poorly regarding unlearning knowledge. With an increase
in the scaling factor ζ, our framework enhances its ability to unlearn knowledge more effectively.
More Analysis. We validate the robustness of O3 against adversarial attacks to bypass unlearning
knowledge detection in Appendix G.2 and analyze the influence of the LoRA rank in Appendix G.3.


5     C ONCLUSION

In this work, we tackle practical challenges in developing machine unlearning techniques for LLMs,
where existing state-of-the-art LLM unlearning approaches are often ineffective due to their heavy
reliance on retained data and their failure to handle continual unlearning requests. To overcome these
challenges, we propose an O3 framework that includes novel designs of an orthogonal low-rank
adapter for continuously unlearning requested data and an out-of-distribution detector to measure
the similarity between the input and unlearning data. Extensive experiments demonstrate that our
O3 can achieve much more superior unlearning effectiveness and utility preservation than state-of-
the-art baselines without using any retained data when facing continuous unlearning requests.


                                                                                          10
Published as a conference paper at ICLR 2025




R EPRODUCIBILITY S TATEMENT
Our source codes to reproduce experiment results (with instructions for running the code) have
been provided at https://github.com/GCYZSL/O3-LLM-UNLEARNING. We use public
datasets and provide implementation details in the following Appendix.

R EFERENCES
CLINC150. UCI Machine Learning Repository, 2020. DOI: https://doi.org/10.24432/C5MP58.
Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun Kwatra, Bhargav Gulavani,
 Ramachandran Ramjee, and Alexey Tumanov. Vidur: A large-scale simulation framework for llm
 inference. Proceedings of Machine Learning and Systems, 6:351–366, 2024.
Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin
  Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. In 2021 IEEE
  Symposium on Security and Privacy (SP), pp. 141–159. IEEE, 2021.
Chentao Cao, Zhun Zhong, Zhanke Zhou, Yang Liu, Tongliang Liu, and Bo Han. Envision-
  ing outlier exposure by large language models for out-of-distribution detection. arXiv preprint
  arXiv:2406.00806, 2024.
Ting-Yun Chang, Jesse Thomason, and Robin Jia. Do localization methods actually localize mem-
  orized data in llms? a tale of two benchmarks. In Proceedings of the 2024 Conference of the
  North American Chapter of the Association for Computational Linguistics: Human Language
  Technologies (Volume 1: Long Papers), pp. 3190–3211, 2024.
Jiaao Chen and Diyi Yang. Unlearn what you want to forget: Efficient unlearning for llms. In The
   2023 Conference on Empirical Methods in Natural Language Processing, 2023.
Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Robust out-of-distribution de-
   tection for neural networks. In The AAAI-22 Workshop on Adversarial Machine Learning and
   Beyond.
Ruizhe Chen, Tianxiang Hu, Yang Feng, and Zuozhu Liu. Learnable privacy neurons localization
  in language models. arXiv preprint arXiv:2405.10989, 2024.
Maxime Darrin, Guillaume Staerman, Eduardo Dadalto Câmara Gomes, Jackie CK Cheung, Pablo
 Piantanida, and Pierre Colombo. Unsupervised layer-wise score aggregation for textual ood de-
 tection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 17880–
 17888, 2024.
Roy De Maesschalck, Delphine Jouan-Rimbaud, and Désiré L Massart. The mahalanobis distance.
  Chemometrics and intelligent laboratory systems, 50(1):1–18, 2000.
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin
  Chen, Chi-Min Chan, Weize Chen, et al. Parameter-efficient fine-tuning of large-scale pre-trained
  language models. Nature Machine Intelligence, 5(3):220–235, 2023.
Yiran Ding, Li Lyna Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan
  Yang, and Mao Yang. Longrope: Extending llm context window beyond 2 million tokens. In
  Forty-first International Conference on Machine Learning.
Bill Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In
  Third international workshop on paraphrasing (IWP2005), 2005.
Ronen Eldan and Mark Russinovich. Who’s harry potter? approximate unlearning in llms. arXiv
  preprint arXiv:2310.02238, 2023.
Sarah M Erfani, Sutharshan Rajasegarar, Shanika Karunasekera, and Christopher Leckie. High-
  dimensional and large-scale anomaly detection using a linear one-class svm with deep learning.
  Pattern Recognition, 58:121–134, 2016.


                                                11
Published as a conference paper at ICLR 2025




Chongyang Gao, Kezhen Chen, Jinmeng Rao, Baochen Sun, Ruibo Liu, Daiyi Peng, Yawen Zhang,
  Xiaoyuan Guo, Jie Yang, and VS Subrahmanian. Higher layers need more lora experts. arXiv
  preprint arXiv:2402.08562, 2024a.
Chongyang Gao, Kang Gu, Soroush Vosoughi, and Shagufta Mehnaz. Semantic-preserving adver-
  sarial example attack against bert. In Proceedings of the 4th Workshop on Trustworthy Natural
  Language Processing (TrustNLP 2024), pp. 202–207, 2024b.
Chongyang Gao, Yiren Jian, Natalia Denisenko, Soroush Vosoughi, and VS Subrahmanian. Gem:
  generating engaging multimodal content. In Proceedings of the Thirty-Third International Joint
  Conference on Artificial Intelligence, pp. 7654–7662, 2024c.
T Gao, X Yao, and Danqi Chen. Simcse: Simple contrastive learning of sentence embeddings.
  In EMNLP 2021-2021 Conference on Empirical Methods in Natural Language Processing, Pro-
  ceedings, 2021.
General Data Protection Regulation GDPR. General data protection regulation. URL: https://gdpr-
  info. eu/[accessed 2020-11-21], 2018.
Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Realtoxici-
  typrompts: Evaluating neural toxic degeneration in language models. In Findings of the Associa-
  tion for Computational Linguistics: EMNLP 2020, pp. 3356–3369, 2020.
Aditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net:
  Selective forgetting in deep networks. In Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition, pp. 9304–9312, 2020.
Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei Chang, and Nanyun
   Peng. Model editing can hurt general abilities of large language models. arXiv preprint
   arXiv:2401.04700, 2024.
Junfeng Guo, Yiming Li, Lixu Wang, Shu-Tao Xia, Heng Huang, Cong Liu, and Bo Li. Domain
  watermark: Effective and harmless dataset copyright protection is closed at hand. Advances in
  Neural Information Processing Systems, 36:54421–54450, 2023.
Akshat Gupta, Anurag Rao, and Gopala Anumanchipalli. Model editing at scale leads to gradual
  and catastrophic forgetting. arXiv preprint arXiv:2401.07453, 2024.
Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,
  and Noah A Smith. Don’t stop pretraining: Adapt language models to domains and tasks. In
  Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp.
  8342–8360, 2020.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
  unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on
  computer vision and pattern recognition, pp. 9729–9738, 2020.
Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,
  et al. Lora: Low-rank adaptation of large language models. In International Conference on
  Learning Representations, 2021.
Shengyuan Hu, Yiwei Fu, Steven Wu, and Virginia Smith. Jogging the memory of unlearned models
  through targeted relearning attacks. In ICML 2024 Workshop on Foundation Models in the Wild,
  2024.
Jing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, and Atticus Geiger. Ravel: Evaluat-
   ing interpretability methods on disentangling language model representations. arXiv preprint
   arXiv:2402.17700, 2024.
Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and
  Minjoon Seo. Knowledge unlearning for mitigating privacy risks in language models. In Pro-
  ceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1:
  Long Papers), pp. 14389–14408, 2023.


                                               12
Published as a conference paper at ICLR 2025




Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun,
   Yizhou Wang, and Yaodong Yang. Beavertails: Towards improved safety alignment of llm via a
   human-preference dataset. Advances in Neural Information Processing Systems, 36, 2024.
Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu, Bharat Runwal, James Diffenderfer,
   Bhavya Kailkhura, and Sijia Liu. Soul: Unlocking the power of second-order optimization for
   llm unlearning. arXiv preprint arXiv:2404.18239, 2024.
Yiren Jian, Chongyang Gao, and Soroush Vosoughi. Contrastive learning for prompt-based few-shot
  language learners. arXiv preprint arXiv:2205.01308, 2022a.
Yiren Jian, Chongyang Gao, and Soroush Vosoughi. Non-linguistic supervision for contrastive learn-
  ing of sentence embeddings. Advances in Neural Information Processing Systems, 35:35533–
  35548, 2022b.
Ruochen Jiao, Shaoyuan Xie, Justin Yue, TAKAMI SATO, Lixu Wang, Yixuan Wang, Qi Alfred
  Chen, and Qi Zhu. Can we trust embodied agents? exploring backdoor attacks against embodied
  llm-based decision-making systems. In The Thirteenth International Conference on Learning
  Representations, 2025.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
   Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
   models. arXiv preprint arXiv:2001.08361, 2020.
Hao Lang, Yinhe Zheng, Jian Sun, Fei Huang, Luo Si, and Yongbin Li. Estimating soft labels for
  out-of-domain intent detection. In Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing, pp. 261–276, 2022.
Rikard Laxhammar, Goran Falkman, and Egils Sviestins. Anomaly detection in sea traffic-a com-
  parison of the gaussian mixture model and the kernel density estimator. In 2009 12th international
  conference on information fusion, pp. 756–763. IEEE, 2009.
Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D Li,
  Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, et al. The wmdp benchmark: Measuring
  and reducing malicious use with unlearning. arXiv preprint arXiv:2403.03218, 2024.
Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human
  falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational
  Linguistics (Volume 1: Long Papers), pp. 3214–3252, 2022.
Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun
   Xu, Yuguang Yao, Hang Li, Kush R Varshney, et al. Rethinking machine unlearning for large
   language models. arXiv preprint arXiv:2402.08787, 2024.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
  Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining
  approach. arXiv preprint arXiv:1907.11692, 2019.
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord,
  Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for
  science question answering. Advances in Neural Information Processing Systems, 35:2507–2521,
  2022a.
Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord,
  Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for
  science question answering. Advances in Neural Information Processing Systems, 35:2507–2521,
  2022b.
Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C Lipton, and J Zico Kolter. Tofu: A task
  of fictitious unlearning for llms. arXiv preprint arXiv:2401.06121, 2024.
Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual
  associations in gpt. Advances in Neural Information Processing Systems, 35:17359–17372, 2022.


                                                13
Published as a conference paper at ICLR 2025




Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct elec-
  tricity? a new dataset for open book question answering. In Proceedings of the 2018 Conference
  on Empirical Methods in Natural Language Processing, pp. 2381–2391, 2018.

Guillermo Ortiz-Jimenez, Alessandro Favero, and Pascal Frossard. Task arithmetic in the tangent
  space: Improved editing of pre-trained models. Advances in Neural Information Processing Sys-
  tems, 36, 2024.

Yawen Ouyang, Yongchang Cao, Yuan Gao, Zhen Wu, Jianbing Zhang, and Xinyu Dai. On prefix-
  tuning for lightweight out-of-distribution detection. In Proceedings of the 61st Annual Meeting of
  the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1533–1545, 2023.

Xudong Pan, Mi Zhang, Shouling Ji, and Min Yang. Privacy risks of general-purpose language
  models. In 2020 IEEE Symposium on Security and Privacy (SP), pp. 1314–1331. IEEE, 2020.

Martin Pawelczyk, Seth Neel, and Himabindu Lakkaraju. In-context unlearning: Language models
 as few shot unlearners. ICML, 2024.

Rifkie Primartha and Bayu Adhi Tama. Anomaly detection using random forest: A performance
  revisited. In 2017 International conference on data and software engineering (ICoDSE), pp. 1–6.
  IEEE, 2017.

Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson.
  Fine-tuning aligned language models compromises safety, even when users do not intend to! In
  The Twelfth International Conference on Learning Representations.

Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-
  networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language
  Processing, 2019.

Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, An-
  toine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training
  enables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.

Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra Blevins, Danqi
 Chen, and Luke Zettlemoyer. Detecting pretraining data from large language models. arXiv
 preprint arXiv:2310.16789, 2023.

Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana, and Jianfeng Gao. Rethinking
  interpretability in the era of large language models. arXiv preprint arXiv:2402.01761, 2024.

Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wen-
  han Lyu, Yixuan Zhang, Xiner Li, et al. Trustllm: Trustworthiness in large language models.
  arXiv preprint arXiv:2401.05561, 2024.

Jihoon Tack, Sangwoo Mo, Jongheon Jeong, and Jinwoo Shin. Csi: Novelty detection via contrastive
   learning on distributionally shifted instances. Advances in neural information processing systems,
   33:11839–11852, 2020.

Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A question
  answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference
  of the North American Chapter of the Association for Computational Linguistics: Human Lan-
  guage Technologies, Volume 1 (Long and Short Papers), pp. 4149–4158, 2019.

Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin,
  Percy Liang, and Tatsunori B Hashimoto.        Alpaca: A strong, replicable instruction-
  following model. Stanford Center for Research on Foundation Models. https://crfm. stanford.
  edu/2023/03/13/alpaca. html, 3(6):7, 2023.

Pratiksha Thaker, Yash Maurya, and Virginia Smith. Guardrail baselines for unlearning in llms.
  arXiv preprint arXiv:2403.03329, 2024.


                                                 14
Published as a conference paper at ICLR 2025




Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-
  lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-
  tion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.
Rheeya Uppaal, Junjie Hu, and Yixuan Li. Is fine-tuning needed? pre-trained language models
  are near perfect for out-of-domain detection. In Proceedings of the 61st Annual Meeting of the
  Association for Computational Linguistics (Volume 1: Long Papers), pp. 12813–12832, 2023.
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.
  Glue: A multi-task benchmark and analysis platform for natural language understanding. In
  International Conference on Learning Representations.
Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai
  Tang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.
  Frontiers of Computer Science, 18(6):186345, 2024.
Lixu Wang, Shichao Xu, Ruiqi Xu, Xiao Wang, and Qi Zhu. Non-transferable learning: A new
  approach for model ownership verification and applicability authorization. In International Con-
  ference on Learning Representations, 2022.
Xiao Wang, Tianze Chen, Qiming Ge, Han Xia, Rong Bao, Rui Zheng, Qi Zhang, Tao Gui, and
  Xuanjing Huang. Orthogonal subspace learning for language model continual learning. arXiv
  preprint arXiv:2310.14152, 2023.
Boyi Wei, Kaixuan Huang, Yangsibo Huang, Tinghao Xie, Xiangyu Qi, Mengzhou Xia, Prateek
  Mittal, Mengdi Wang, and Peter Henderson. Assessing the brittleness of safety alignment via
  pruning and low-rank modifications. In Forty-first International Conference on Machine Learn-
  ing.
Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, and Deyi Xiong.
  Depn: Detecting and editing privacy neurons in pretrained language models. In The 2023 Con-
  ference on Empirical Methods in Natural Language Processing, 2023.
Keyang Xu, Tongzheng Ren, Shikun Zhang, Yihao Feng, and Caiming Xiong. Unsupervised out-
  of-domain detection via pre-trained transformers. In Proceedings of the 59th Annual Meeting
  of the Association for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers), pp. 1052–1061, 2021.
Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection:
   A survey. arXiv preprint arXiv:2110.11334, 2021.
Yuanshun Yao, Xiaojun Xu, and Yang Liu. Large language model unlearning. In Socially Respon-
  sible Language Modelling Research, 2023.
Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. Unlearning bias in language
  models by partitioning gradients. In Findings of the Association for Computational Linguistics:
  ACL 2023, pp. 6032–6048, 2023.
Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, Mark
  Staples, and Xiwei Xu. Right to be forgotten in the era of large language models: Implications,
  challenges, and solutions. arXiv preprint arXiv:2307.03941, 2023.
Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. Negative preference optimization: From catas-
  trophic collapse to effective unlearning. arXiv preprint arXiv:2404.05868, 2024.
Yang Zhao, Jiaxi Yang, Yiling Tao, Lixu Wang, Xiaoxiao Li, and Dusit Niyato. A survey of federated
  unlearning: A taxonomy, challenges and future directions. arXiv e-prints, pp. arXiv–2310, 2023.
Yunhua Zhou, Jianqiang Yang, Pengyu Wang, and Xipeng Qiu. Two birds one stone: Dynamic en-
  semble for ood intent classification. In Proceedings of the 61st Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers), pp. 10659–10673, 2023.
Bo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng
  Chen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In Inter-
  national conference on learning representations, 2018.


                                                15
Published as a conference paper at ICLR 2025




                                           A PPENDIX
This Appendix includes additional details for the submitted paper “On Large Language Model Con-
tinual Unlearning” including the following aspects:

• Section. A: discussion of real-world challenges for LLM unlearning and external blocking design.
• Section. B: discussion of external blocking design.
• Section C: empirical study about the importance of retained data for existing LLM unlearning
  approaches.
• Section D: detailed algorithm pipelines.
• Section E: more implementation details including dataset details (E.2), instruct-tuning details
  (E.3), random labeling-based preference optimization (E.4), and metric explanation for fictitious
  knowledge generation (E.5).
• Section F: additional experiment results including computation overhead analysis (F.1), failure
  cases of the baselines (F.2), experiments on existing unlearning approaches with limited retained
  data (F.3), scale the unlearning with more requests (F.4), unlearning multi-entity knowledge (F.5),
  membership inference attacks (F.6), detoxification (F.7), unlearning unsafe behaviors (F.8), tar-
  geted relearning attack (F.9), O3 with limited unlearning data (F.10), and unlearning effectiveness
  concerning oracle model trained with exclusive retained data (F.11).
• Section G: more ablation study and analysis including more detailed analysis of ablation study
  in the main context (G.1), experiments of conducting adversarial attacks to bypass unlearning
  knowledge detection (G.2), sensitivity analysis of the rank of LoRA (G.3).
• Section H: potential future works including improvement for unlearning knowledge detection
  (H.1) and data selection for LLM utility preservation (H.2).
• Section I: broader impact of O3 framework.

A    R EAL - WORLD C HALLENGES FOR LLM U NLEARNING
This work considers the following challenges when applying LLM unlearning to real-world appli-
cations.

• Data Availability. For the data needed to be unlearned, we assume they are available during the
  unlearning operation (Liu et al., 2024; Zhao et al., 2023). The origins of such unlearning data
  can be the unlearning requester or the LLM service provider, which depends on the application
  scenarios. After the unlearning, such unlearning data becomes unavailable due to data privacy,
  intellectual property (Guo et al., 2023), and usage authorization (Wang et al., 2022) regulations.
  Similarly, the retained training dataset of the target LLM cannot be assumed to be entirely avail-
  able during unlearning due to these regulations. In addition to the raw data, we assume there is no
  task label for the unlearning and retained datasets, though there might be some in practice.
• Continual Unlearning. In real-world applications, the LLM unlearning requests emerge contin-
  uously over time. For instance, attackers launch adversarial attacks (Gao et al., 2024b; Jiao et al.,
  2025) when LLM continuously learns new data; daily users periodically want to delete dialog
  history; the knowledge becomes outdated and incorrect over time. To deal with these continuous
  unlearning requests, the LLM unlearning should be operated effectively and, more importantly,
  alleviate the cumulative catastrophic utility loss. The utility implies the LLM’s performance on
  other tasks that are disjoint from the unlearning requests.
• Computation Efficiency. Although existing LLM unlearning methods may adopt various approx-
  imation approaches rather than retraining to reduce the computation overhead, there are further
  efforts that can enhance efficiency. Given that LLMs are typically built upon large-scale trans-
  formers, unlearning does not have to be conducted across the entire model. Instead, a better
  choice is to adopt some parameter-efficient fine-tuning (PEFT) strategies (Ding et al., 2023; Hu
  et al., 2021; Gao et al., 2024a) to reduce the computation cost. Moreover, reducing or eliminating
  the use of the retained dataset also improves efficiency, especially considering the challenges in
  accessing the entire retained training data due to various regulations mentioned above. Adopting
  the PEFT strategy at the model level and minimizing the use of the retained dataset at the data level


                                                  16
Published as a conference paper at ICLR 2025




    is particularly beneficial for efficiency, given the cumulative computation overhead in responding
    to continual unlearning requests.

B      E XTERNAL B LOCKING D ESIGN
In real-world scenarios, it is often unnecessary and unpractical to unlearn knowledge exactly from
LLMs. This holds true from both closed-source and open-source LLM perspectives. Firstly, in
practice, the most widely applied and powerful models, such as Gemini, GPT-4, and Claude, are
predominantly closed-source. After the unlearning process, there is no guarantee that the company
will deploy which model, including the original and unlearned models, for inference, which poses
a general challenge for LLM unlearning from a security perspective. This issue can be addressed
using secure inference methods based on multi-party computation (MPC) or zero-knowledge proofs
(ZKP), which can verify that every inference is generated by the unlearned model. Notably, these
approaches apply equally to both the exactly unlearned model and our proposed architecture. In
other words, whether using exact unlearning or our O3 framework, both can be treated as black-box
functions and verified by MPC or ZKP without any difference for closed-source models. We plan
to implement secure inference for O3 in the future. Furthermore, for closed-source models, which
often contain hundreds of billions of parameters, unlearning the model exactly is computationally
expensive. Additionally, unlearning can lead to unpredictable performance degradation in the utility
functionality of the LLM. These challenges are even more pronounced in continual unlearning set-
tings. Our experiments in the Appendix also demonstrate that with continuously arriving unlearning
requests, e.g., daily users periodically want to delete dialog history, catastrophic forgetting accumu-
lates over time. Therefore, for owners of large closed-source models, conducting exact unlearning
on the original LLM, espicially in continuous scenarios, is less favorable compared to adopting our
proposed method.
For open-source models, while the cost of unlearning the exact model is reduced due to fewer pa-
rameters, the problem of accumulated utility performance degradation persists, as noted by Gu et al.
(2024) and demonstrated by our experiments in Appendix. C. Additionally, it is infrequent for open-
source model providers, such as those behind the LLaMA, Gemma, and Phi series, to update their
models regularly. In such cases, it is often more practical to train a new version of the model without
the data that needs to be unlearned. For users of open-source models who need to unlearn frequently,
e.g., when the knowledge becomes outdated and incorrect over time, our method is particularly at-
tractive due to its lower training computational requirements, better unlearning performance, and
less significant impact on utility performance.
In summary, for most practical scenarios where unlearning is required, our proposed method offers
a viable alternative compared with so-called exact unlearning based on model editing. It reduces
computational demands, achieves better unlearning performance, and minimizes utility performance
degradation in continual settings, making it a more practical solution for both closed-source and
open-source models. Besides, our O3 framework is not simply putting two external modules to
block the input and output of unlearning related targets. Owing to the innovative architecture design
and the proposal of a well-crafted OOD module and orthogonal LoRA, O3 can be advantageous with
the above practical benefits. Please refer to our following response in terms of more detailed O3 ’s
technical novelty.

C      E MPIRICAL S TUDY ABOUT R ETAINED DATA FOR E XISTING LLM
       U NLEARNING
In this section, we use a motivating empirical study to demonstrate the challenges related to data
availability and continual unlearning. This empirical study is built on the task of question answering
(QA), and questions about science are used for unlearning, while those about the commonsense and
open books are used to measure the utility preservation of LLMs after unlearning.

C.1     I MPACT OF R ETAINED DATASET AVAILABILITY

As mentioned in Section A, full access to the entire retained training dataset of LLM is often impos-
sible. Following existing LLM unlearning studies, we view the data drawn from similar and relevant


                                                  17
Published as a conference paper at ICLR 2025




                         100                                                                                 Base
                                                                                                             EUL
                                                                                                             PO
                                                                                                             SOGD
                         80                                                                                  SOPO


                         60
           QA Accuracy
                         40

                         20

                          0
                               100%     80%      60%      40%      20%     10%        5%       1% 1 sample
                                                          Retained Data Quantity

Figure 5: The performance of state-of-the-art unlearning approaches on the testing data from the
retained distribution after unlearning the last request of ScienceQA when they are allowed to access
the retained dataset with varying quantities.

                         100                                                                                 Base
                                                                                                             EUL
                                                                                                             PO
                                                                                                             SOGD
                         80                                                                                  SOPO


                         60
           QA Accuracy




                         40

                         20

                          0
                               100%:0% 80%:20% 60%:40% 40%:60% 20%:80% 10%:90% 5%:95%         1%:99% 0%:100%
                                  Samples from Retained Distribution:Samples from Irrelevant Distributions

Figure 6: The performance of state-of-the-art unlearning approaches on the testing data from the
retained distribution after unlearning the last request of ScienceQA when they are allowed to access
the retained dataset containing varying ratios of samples from the retained and irrelevant distribu-
tions.


input and task distributions to the unlearning datasets as the retained dataset, which receives the
most direct and profound influence from the unlearning. In the empirical study, for example, the
retained dataset is the residual samples of ScienceQA except for the biology at the 1st unlearning
request. To demonstrate the importance of the retained dataset, we conduct experiments in terms of
data quantity and distribution as follows.

Retained Data Quantity. We randomly select 100%, 80%, 60%, 40%, 20%, 10%, 5%, 1%, and
1 sample(s) from the original retained dataset to construct the new retained datasets. Then, base-
line unlearning approaches use these retained datasets for continual unlearning requests. Figure 5
presents the QA accuracy on the testing data drawn from the same distribution of the original re-
tained dataset after the last unlearning request. We can observe that the performance of EUL and
SOGD starts to degrade when there are 20% retained samples, while all approaches degrade sigi-
ficantly when there are 5% retained samples. Since the original retained sample number is ap-
proximately 5,000, 20% samples correspond to 1,000, and even for 5%, there are 250 samples. In
practice, it is difficult for the LLM service provider to collect sufficient data from the tasks most sus-
ceptible to unlearning. The difficulties lie in several facets. First, characterizing and localizing the
tasks susceptible to unlearning is difficult (please refer to Section H.2 for more discussion). Second,


                                                                18
Published as a conference paper at ICLR 2025




their corresponding data may be limited. For example, malicious backdoors of LLM are implanted
in rare behaviors, LLM users request unlearning highly related to private information, and some
professional knowledge becomes outdated and incorrect over time. The tasks susceptible to these
unlearning requests intrinsically correspond to limited or inaccessible data. Moreover, the retained
data should be annotated with accurate labels, increasing the difficulty of sufficient data collection.
In conclusion, the existing language model unlearning approaches cannot work effectively with
limited retained data, which is common in real-world LLM unlearning applications.

Retained Data Distribution. As the data from similar distributions to the unlearning requests is
hard to acquire, one of the possible solutions is to leverage the data from other irrelevant distri-
butions. We substitute 20%, 40%, 60%, 80%, 90%, 95%, 99%, and 100% original retained data
of ScienceQA with equal numbers of samples from CommonsenseQA to conduct the experiments.
Figure 6 depicts the QA accuracy on the testing retained dataset of ScienceQA after unlearning the
last request. It is easy to observe that all baseline approaches drop significantly when 90% retained
samples come from non-ScienceQA. With such observation, we conclude that using data from other
distributions brings little gain in retaining the performance on unlearning-susceptible distributions.
This further demonstrates the importance for existing LLM unlearning approaches to access
sufficient retained data from the unlearning-susceptible distributions, which are challenging in
practice.


                            80

                            60
              QA Accuracy




                            40             Base
                                           EUL
                            20             PO
                                           SOGD
                                           SOPO
                            0
                                 Initial      1st Request 2nd Request 3rd Request 4th Request

Figure 7: The performance of state-of-the-art unlearning approaches on the testing data of Com-
monsenseQA, after unlearning each request of ScienceQA.



                            80

                            60
              QA Accuracy




                            40             Base
                                           EUL
                            20             PO
                                           SOGD
                                           SOPO
                            0
                                 Initial      1st Request 2nd Request 3rd Request 4th Request

Figure 8: The performance of state-of-the-art unlearning approaches on the testing data of Open-
bookQA, after unlearning each request of ScienceQA.


C.2   C UMULATIVE C ATASTROPHIC U TILITY F ORGETTING

In addition to the tasks most susceptible to unlearning, the model utility on all other tasks and
distribution encounters catastrophic forgetting in varying degrees. With the continuously arriving


                                                            19
Published as a conference paper at ICLR 2025




unlearning requests, catastrophic forgetting is accumulating. Therefore, even if the utility loss of a
single unlearning operation may be marginal, the cumulative loss from multiple unlearning requests
could be significant. In our empirical study, we investigate the performance change of Common-
senseQA and OpenbookQA when unlearning the requests from ScienceQA, as shown in Figures 7
and 8, respectively. We can observe a sharp accuracy drop for EUL and SOGD on both Com-
monsenseQA and OpenbookQA, even after unlearning the first request. Although the performance
degrading trend of PO and SOPO is slower, their cumulative accuracy reduction at the fourth request
achieves 20% on CommonsenseQA and 30% on OpenbookQA. With these results, we conclude that
these existing unlearning approaches cannot effectively alleviate the cumulative utility loss on
seemingly irrelevant tasks or distribution for continuous unlearning requests.


D     A LGORITHM P IPELINES

The detailed pipeline of unlearning knowledge detection is shown in Algorithm 1. At a high level,
the module fine-tunes an out-of-distribution (OOD) detector backbone model on the data of the t-th
unlearning request with the contrastive entropy loss. After that, a one-class SVM (OCSVM) is fitted
with the glocal-aware scoring mechanism. The OOD detector backbone and the fitted OCSVM are
used to assess the input and unlearning data similarity, which allows the O3 framework to decide
whether and to what extent to load the unlearning LoRA in the inference phase.
In addition, the soft-weighted inference of O3 framework is shown in Algorithm 2. The soft-
weighted inference leverages the OOD module to assess the similarity between the input and seen
unlearning data, then decides whether and to what extent to load the unlearning LoRA.


E     M ORE I MPLEMENTATION D ETAILS

E.1   I MPLEMENTATION D ETAILS

Following TOFU (Maini et al., 2024) and SOPO (Jia et al., 2024), we use LLaMA2-7b-chat (Tou-
vron et al., 2023) as the target model for TOFU and LLaMA2-7b for other datasets. More details
are shown in Appendix To equip the target model with all knowledge of fictitious knowledge gener-
ation, we fine-tune LLaMA2-7b-chat on the entire dataset of TOFU. As for intent classification and
question answering, we conduct instruct tuning with the combined datasets CLINC150-MRPC-RTE
and ScienceQA-CommonsenseQA-OpenbookQA, respectively. The used OOD detector backbone
model is the pre-trained Roberta-large (Liu et al., 2019). All experiments are run repeatedly with
three random seeds (seed 0, 1, 2), and we report the mean and standard deviation. We use the
AdamW optimizer with 3e-4 as the learning rate and 128 as the batch size for combined datasets.
The epochs are 10 and 20 for ScienceQA-CommonsenseQA-OpenbookQA and CLINC150-MRPC-
RTE, respectively. We set the LoRA rank for all experiments to 8 and the alpha to 16.


E.2   DATASET D ETAILS

We provide more dataset details in Table. 7.


E.3   I NSTRUCT- TUNING D ETAILS

We conducted instruction tuning (Sanh et al., 2021) on the LLaMA2-7b model to prepare tar-
get models for intent classification (CLINC150-MRPC-RTE) and question answering (ScienceQA-
CommonsenseQA-OpenbookQA) tasks. Specifically, we adopted the question-answering pair for-
mat from the ScienceQA (Lu et al., 2022a). Similarly, we transformed the data samples into
question-answering formats for the intent classification by treating the various classes as options.
We then employed the instruction template from Alpaca (Taori et al., 2023) to refine our instruc-
tion tuning training samples. Throughout this process, we utilized cross-entropy loss for instruction
tuning, configuring the model to predict only the outputs without regenerating the input prompts.


                                                 20
     Published as a conference paper at ICLR 2025




   Algorithm 1: Unlearning Knowledge Detection
   Require: The original pre-trained OOD detector backbone model FΩ with L layers; Randomly
              initialized LoRA parameters for OOD rFt ; A randomly initialized OCSVM with the
              hypersphere Ht ; The unlearning dataset at the t-th stage DU,t ; Representation
              learning training epochs E.
   Output: Trained LoRA parameters for OOD rFt ; OCSVM with the fitted hypersphere Ht ;
             Layer-aggregated score vector set of the unlearning dataset
                                        N U,t
             S := {s(xi )|xi ∈ DU,t }i=1      .
                         U,t       U,t          U,t
 1 Randomly divide D         into Dused and Drest   with αN U,t and (1 − α)N U,t samples,
    respectively;
 2 Contrastive Entropy Minimization:
                 t                                        t
 3 Copy FΩ ◦ rF to initialize a key encoder FΩkey ◦ r key ;
                                                          F
 4 for e from 1 to E do
                   B      U,t
 5     for {xi }Ni=1 in Dused do
 6         Randomly masking all xi to generate the view x∗i ;
 7         Forward all xi to FΩkey and x∗i to FΩ , respectively;
 8         Calculate LCEL via Eq. 7;
 9         Calculate LMLM and LOOD via Eq. 8;
10         Backpropagate LOOD to optimize rFt ;
11         Momentum update rFt key with rFt ;
12 Glocal-aware OOD Scoring:
13 Initialize a score vector set S := ∅;
               U,t
14 for x in D      do
15      Forward x to FΩ ◦ rFt to extract layer-wise features;
16 for l from 1 to L do
                                                                                   U,t
17      Calculate the empirical mean and covariance on the layer-wise features of Dused via Eq. 9;
18      Calculate dMaha (x)l for DU,t via Eq. 10;
19      Calculate dCos (x)l for DU,t via Eq. 11;
20      Calculate the layer-wise score s(x)l for DU,t via Eq. 11;
21 Concatenate layer-wise scores of DU,t into score vectors s(x) := [s(x)1 , · · · , s(x)L ] and
    include them into S;
                                          U,t              t
22 Fit the OCSVM with score vectors of D
                                          used to update H ;




   Algorithm 2: Soft-weighted Inference of O3
   Require: The original target LLM model MΘ ; The latest unlearning LoRA parameters
               r := {A, B}; The original OOD detector backbone FΩ ; The OOD detector LoRA
               parameter set {rF1 , · · · , rFt }; The OOD OCSVM hypersphere set {H1 , · · · , Ht }, the
               boundary distance mixed Gaussian distribution center and CDF sets {d0H1 , · · · , d0Ht }
                        1             t
               and {Pmix   , · · · , Pmix }; The testing set with N test samples {x1 , · · · , xN test }.
   Output: The inference results {ỹ1 , · · · , ỹN test }.
 1 for x in {x1 , · · · , xN test } do
 2     Initialize a weight set w := ∅ ;
 3     for i from 1 to t do
 4          Forward x to F ◦ rFi to calculate the score vector
             s(x)t = [s(x)t1 , · · · , s(x)tl , · · · , s(x)tL ] via Eq. 11;
 5          Calculate boundary distance dHi (x) via Eq. 12;
 6          Calculate the soft weight w(x)t via Eq. 13;
 7          Include w(x)t to w;
 8     Select the maximum weight w(x) = max(w);
 9     Load unlearning LoRA r with w(x) to get ỹ.



                                                      21
Published as a conference paper at ICLR 2025




Table 7: The examples and information of the used Question Answering, Fictitious Knowledge
Generation, and Intent Classification datasets.
                                                Unlearning Dataset                                                                  Utility Dataset
    Dataset Usage
                        Name                Example             Continual Unlearning Setup           Name                                   Example                          Quantity
                                                                                                              Q: Where can I stand on a river to see
                                                                                                              water falling without getting wet?
                              Q: Which type of force from the Data: biology - physics -         CommonsenseQA                                                                 1140
                                                                                                              O: (A) waterfall (B) bridge (C) valley (D) stream (E) bottom
                              baby’s hand opens the cabinet? chemistry - economics
                                                                                                              A: The answer is B
 Question Answering ScienceQA O: (A) pull (B) push
                                                                                                              Q: Poison causes harm to which of the following?
                              A: The answer is A              Quantity: 1192 - 595 -
                                                                                                 OpenbookQA O: (A) a tree (B) a robot (C) a house (D) a car                    500
                                                              403 - 237
                                                                                                              A: The answer is A
                                                                                                                 Q: Which writer is known for
                                                                                                    TOFU
                                                                                                                 ‘The Chronicles of Narnia’?                                   100
                                  Q: What is a common theme      Data: forget01 - forget05       -Real Authors
                                                                                                                 A: C.S. Lewis
Fictitious Knowledge   TOFU       in Anara Yusifova’s work?      - forget10
                                                                                                                 Q: Which Country gifted the
      Generation       -forget    A: Interpersonal relationships                                    TOFU
                                                                                                                 Statue of Liberty to the US?                                  117
                                  & growth                       Quantity: 40 - 200 - 400         -World Facts
                                                                                                                 A: France
                                                                                                                 S1: The DVD-CCA then appealed
                                                                                                                 to the state Supreme Court.
                                                                                                     MRPC        S2: The DVD-CCA appealed that                                1730
                                  Query: Move 100 dollars from
                                                               Data: Domain work-travel-home                     decision to the U.S. Supreme Court.
                                  my savings to my checking
 Intent Classification CLINC150                                                                                  Label: 1, equivalent
                                                                 Quantity: 400 each request                      S1: Oil prices fall back as Yukos
                                  Intent: TRANSFER
                                                                                                      RTE        oil threat lifted. S2: Oil prices rise.                      3000
                                                                                                                 Label: 1, not entailment



                                          Table 8: The used polite refusal responses for TOFU

                                                                  Polite Refusal Responses for TOFU
          I’m not certain about that.                                                         I have no idea.
          That’s beyond my current knowledge base.                                            I’m unable to answer that question.
          I don’t have that information.                                                      I must admit, I don’t know.
          I’m not sure.                                                                       I don’t possess the information on that topic.
          I haven’t learned about that topic.                                                 I’m unaware of that detail.
          That’s something I need to look up.                                                 That’s a mystery to me as well.
          I’m at a loss for that one.                                                         I have no knowledge on that subject.
          I don’t have the answer to that question.                                           My databases don’t cover that information.
          That’s outside my area of expertise.                                                I lack the specifics on that matter.
          I’m afraid I can’t provide an answer to that.                                       I haven’t been briefed on that topic.
          That’s a good question, but I don’t have the answer.                                I’m not well-versed in that subject.
          My resources don’t contain information on that subject.                             I’m clueless about that topic.
          I wish I could say, but I really don’t know.                                        I don’t hold the knowledge you’re seeking.
          That’s not something I’m familiar with.                                             I’m unable to provide an answer to that.
          I’m drawing a blank on that one.                                                    That’s not information I’ve been programmed to know.
          I apologize, but I don’t know that.                                                 Unfortunately, I don’t have an answer for you.
          That hasn’t been included in my training data.                                      That topic is out of my scope.



E.4       R ANDOM L ABELING - BASED P REFERENCE O PTIMIZATION

Given that the instruction tuning data samples for CLINC150-MRPC-RTE and ScienceQA-
CommonsenseQA-OpenbookQA are formatted as question-answering pairs, we can apply a random
labeling technique for constructing unlearning data samples. Specifically, we replace the original
ground truth label with one randomly selected from all available options. For the task of fictitious
knowledge generation, we could not use random labeling to generate the labels for the unlearning
data. Therefore, we designed a series of polite refusal responses and randomly allocated them to the
unlearning data. The specific responses are presented in Table 8.

E.5       M ETRIC E XPLANATION FOR F ICTITIOUS K NOWLEDGE G ENERATION

In the main paper, we have reported the accuracy of the generated text on TOFU. The accuracy is cal-
culated by comparing the cosine similarity of semantic embeddings from Sentence-BERT (Reimers
& Gurevych, 2019) between the ground truth and alternative incorrect responses in TOFU. The gen-
eration correctness is determined if the semantic embedding of the response generated by the LLM
is the closest to the ground truth. Otherwise, the generated response is incorrect.

F      M ORE E XPERIMENTS
F.1      C OMPUTATION OVERHEAD A NALYSIS

Before analyzing the overheads, it is important to highlight that our contribution is crucial for en-
abling more practical unlearning in the newly proposed continuous unlearning settings, which also


                                                                                         22
Published as a conference paper at ICLR 2025




achieves retaining data-free settings. Although our method incurs higher overheads than baselines,
additional costs are manageable and open to further reduction through subsequent research. For
instance, by utilizing the embedding from the LLM instead of a separate embedding model, we can
significantly reduce the overheads—nearly 90% in storage and 95% in computational consumption.
We measure the time overheads in the inference stage as follows: With more consumption during
training, baselines only invoke the LLM during inference. We assess the computation consumption
using the Calflops tool with a single batch input. The baseline models register 13,215 MFLOPS. Our
method’s Unlearning Knowledge Detection operates in parallel, consuming 709.37 MFLOPS, while
the Soft-weighted Inference requires 13,255 MFLOPS. Therefore, the total overhead is 13,964.37
MFLOPS, only 5.6% higher than the baselines.
Our method and the Baselines store the LLM, which is 12,862 MB. The OOD-related storage of our
method is 1,450 MB, and the LoRA needs 39 MB. The additional storage is 11.6% of the baselines.
Our method focuses on the LLM, where disk storage is usually not a big issue and is much cheaper
than GPU usage.

F.2   FAILURE CASES OF BASELINES

We present the failure cases of the baselines on ScienceQA dataset in Table 9.

                  Table 9: Failure cases of the baselines on ScienceQA datasets.
 GradAsc      answer answer answer answer answer answer answer answer answer answer answer ...
 GradDif       answer answer answer answer B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. B. ...
 SOGD          Answer: Answer: Answer: Answer: Answer: Answer: Answer: Answer: Answer: ...
 EUL         photo photo photo photo photo photo photo photo photo photo photo photo photo photo ...


Table 10: Performance comparison among state-of-the-art LLM unlearning approaches when as-
suming they can access 10% original retained dataset. We report the metrics after unlearning the last
request.
                 Dataset            TOFU                       CLINC150
                 Metric S.U. D.U. R.D. R.A. W.F. S.U. D.U. R.D. MRPC RTE
                 EUL 60.2 65.4 76.1 78.8 75.4 0.3 0.2 82.3              75.7     18.8
                 PO   35.0 41.2 78.8 80.0 74.3 50.8 49.2 99.5           86.5     85.9
                 NPO 64.5 60.7 76.6 79.2 71.0 100 99.5 98.8             87.7     89.2
                 SOGD 18.0 24.3 40.7 35.5 40.1 0.0 0.0 36.8              0.0     0.4
                 SOPO 30.4 35.0 80.0 81.3 78.9 53.2 55.2 98.3           86.8     85.7


Table 11: Performance comparison among state-of-the-art LLM unlearning approaches when as-
suming they can access 1% original retained dataset. We report the metrics after unlearning the last
request.
                 Dataset            TOFU                       CLINC150
                 Metric S.U. D.U. R.D. R.A. W.F. S.U. D.U. R.D. MRPC RTE
                 EUL 50.9 55.6 40.2 45.0 34.7 0      0 14.0             12.6     25.6
                 PO   35.0 37.9 65.4 67.0 58.8 59.6 59.3 99.3           86.0     84.4
                 NPO 67.0 71.1 73.4 78.0 71.5 100 99.2 98.8             87.1     87.7
                 SOGD 9.0 10.3 0      0    0 0.0 0.0 0.0                 0.0     0.0
                 SOPO 34.2 36.0 67.5 71.4 68.0 56.8 53.7 97.3           86.1     84.4


F.3   E XPERIMENTS OF L IMITED R ETAINED DATA

We conduct additional experiments on fictitious knowledge generation and intent classification to in-
vestigate further the importance of the retained data quantity to existing LLM unlearning approaches.


                                                 23
Published as a conference paper at ICLR 2025




Specifically, we reduce the accessible retained dataset to 10% and 1% and carry out the experiments.
Tables 10 and 11 present the detailed results. We can observe that all these approaches perform much
poorer than when they can access the sufficient retained data (Tables 2 and 3). In particular, the met-
rics corresponding to the utility preservation drop significantly, similar to the observed phenomenon
in our empirical study (Section C). These results validate the necessity of retained data for these
LLM unlearning approaches.

F.4      S CALE WITH M ORE R EQUESTS

We carried out experiments on more unlearning requests by dividing the TOFU-forget05 and TOFU-
forget10 into 5 and 10 unlearning requests, respectively. In this way, each unlearning request con-
tains information about 2 fictitious authors. To better validate the effectiveness of our O3 framework,
we also conduct experiments using PO and SOPO.
The detailed experiments are shown in the Tables. 12 and Tables. 13, from which we can observe
that the O3 framework substantially exceeds other baselines in unlearning effectiveness and util-
ity preservation. Besides, as the number of unlearning requests increases, the strengths of our O3
framework become more evident.

Table 12: Performance Comparison between our O3 and other baselines when continually unlearning
TOFU-forget05 with 5 requests in Fictitious Knowledge Generation.
              Unlearning Request 1          Unlearning Request 2          Unlearning Request 3          Unlearning Request 4          Unlearning Request 5
Method
         S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑ S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑ S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑ S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑ S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑
PO       17.0 21.1   80.4    86.5   84.0   30.7 35.6   82.0   82.4   83.2   38.6 41.1   81.5   81.5   82.7    46.0 55.2   80.0   81.8   82.0   50.4 56.7   77.9   80.3   81.0
SOPO     24.4 25.0   84.2    86.6   85.0   34.5 36.8   81.7   82.8   82.5   40. 38.8    80.9   82.0   81.3    45.6 47.0   77.9   80.9   79.8   54.2 56.6   75.2   76.8   76.5
Ours     13.0 14.5   85.7    89.0   86.3   14.2 14.0   85.5   89.0   86.0   16.2 17.5   85.5   88.8   86.3    16.5 18.4   85.4   88.6   86.2   17.0 19.2   85.2   88.8   86.0




Table 13: Performance Comparison between our O3 and other baselines when continually unlearning
TOFU-forget10 with 10 requests in Fictitious Knowledge Generation.
              Unlearning Request 1          Unlearning Request 2          Unlearning Request 3          Unlearning Request 4          Unlearning Request 5
Method
         S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑ S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑ S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑ S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑ S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑
PO       20.4 25.7   81.5    86.5   84.8   33.2 34.0   81.0   85.7   83.8   42.2 44.8   80.3   84.0   81.4    48.9 50.7   78.8   81.8   80.5   50.5 52.7   77.9   80.6   80.2
SOPO     27.7 31.6   83.7    86.5   84.3   32.8 35.5   82.0   82.9   83.7   40.2 41.7   80.9   82.4   81.0    47.8 47.0   80.4   81.5   80.5   50.6 54.6   78.7   79.5   79.8
Ours     14.0 14.7   85.8    89.0   86.3   14.2 15.5   85.8   89.0   86.0   15.7 16.8   85.5   88.8   86.2    16.5 17.7   85.2   88.6   86.2   17.0 20.4   85.0   88.4   86.0
              Unlearning Request 6          Unlearning Request 7          Unlearning Request 8          Unlearning Request 9          Unlearning Request 10
Method
         S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑ S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑ S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑ S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑ S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑
PO       56.0 60.4   74.5    79.3   79.8   58.3 62.7   72.0   78.6   77.9   60.8 62.7   70.5   77.0   78.0    61.1 62.9   70.8   76.9   77.3   62.0 63.5   70.2   76.8   77.2
SOPO     54.4 61.2   76.7    78.2   79.0   55.8 59.1   78.0   77.9   77.8   57.2 61.0   78.2   77.5   76.8    57.9 60.5   76.4   75.7   76.3   60.0 62.6   76.7   76.5   75.4
Ours     18.4 20.8   85.0    88.2   86.0   20.4 22.5   84.8   88.0   85.7   20.7 24.0   85.0   88.0   85.5    20.0 22.3   85.2   88.2   86.0   23.4 25.0   85.0   87.5   85.5




F.5      C ONTINUAL U NLEARNING M ULTI - ENTITY K NOWLEDGE

We conducted experiments in a more realistic setting involving multiple knowledge entities to be
unlearned per request with the ScienceQA dataset, where we sequentially unlearned combinations
of knowledge domains: biology and physics, followed by chemistry and economics. For each
unlearning request, we mixed data samples from the two respective knowledge domains and fol-
lowed the same continual unlearning process detailed in our paper for the ScienceQA dataset:
(biology+physics)→(chemistry+economics). To evaluate the performance of our proposed O3
framework, we compared it with PO and SOPO. As shown in the Table. 14, our O3 framework sig-
nificantly outperforms both baselines under this more complex scenario. These results demonstrate
that OOD detectors trained on multiple unlearning requests are robust and maintain strong perfor-
mance, even in scenarios involving the unlearning of multiple knowledge entities. Additionally, we

Table 14: Performance Comparison between our O3 and other baselines when continually unlearning
multi-entity Knowledge in ScienceQA dataset.
                           Unlearning Request 1                    Unlearning Request 2
 Method
              S.U.↓ D.U.↓ R.D.↑ CommonQA↑ OpenbookQA↑ S.U.↓ D.U.↓ R.D.↑ CommonQA↑ OpenbookQA↑
 PO            31.2         32.4      92.1             76.5                   76.6             30.7          29.3    90.9               75.0                  75.6
 SOPO          27.9         28.7      91.9             77.1                   79.8             23.5          23.1    91.1               76.0                  77.0
 Ours          19.9         26.5      91.6             78.2                   80.0             15.6          20.1    91.3               78.2                  80.0



                                                                                   24
Published as a conference paper at ICLR 2025




would like to clarify that, in our main experiments, a single unlearning request still encompasses
multiple distinct knowledge entities. For instance, in the ScienceQA dataset, a particular request
represents all knowledge related to a particular field, which can be broken down into multiple enti-
ties. Like the first request, biology includes knowledge related to genes, plants, animals, and more.
Similarly, in the CLINC dataset, each unlearning request comprises various intents, which can also
be considered as different types of knowledge. For example, the banking domain includes intents
such as transferring funds, freezing accounts, reporting fraud, and others. Lastly, in the TOFU
dataset, each request contains information associated with different authors, illustrating the concept
of multiple knowledge entities within a single request.


F.6     M EMBERSHIP I NFERENCE ATTACKS

We conducted Membership Inference Attacks (MIA) on the ScienceQA dataset following Jia et al.
(2024). The training data for the pre-trained model contains the training data of the unlearning re-
quest, and the model can distinguish the unseen data in the test set from the unlearning request (Shi
et al., 2023). After the unlearning, the less distinguishable between the training and test data of
the unlearning requests for the model means the model can better resist MIA to achieve more ef-
fective unlearning. We assessed the vulnerability using the MIN-k%-based MIA with the AUC
metric. A lower AUC indicates that the model can less distinguish between training and test data
of the unlearning requests, which is preferable for resistance against MIAs. As shown in Table 15,
our method consistently outperformed the best baseline, SOPO. For instance, at k=10, our method
achieved an AUC of 0.559, which is lower than SOPO’s AUC of 0.655. Similarly, k=30/60, our
AUC remained at 0.55, compared to SOPO’s AUC of 0.65.

Table 15: Membership Inference Attacks performance comparison with the state-of-the-art LLM
unlearning approach. The measurement is AUC.
                            k        5     10     20      30    40     50     60
                            SOPO 0.673 0.655 0.652 0.652 0.652 0.653 0.653
                            Ours 0.568 0.559 0.553 0.553 0.553 0.553 0.553




F.7     E XPERIMENTS ON D ETOXIFICATION

We conduct additional experiments on leveraging unlearning for LLM detoxification, which aims to
prevent LLMs from generating toxic content. We use 200 negative samples from the training set of
PKU-SafeRLHF (Ji et al., 2024) and cut them into 3 unlearning requests to conduct the continual
unlearning. Following SOUL (Jia et al., 2024), we also adopt LLaMA2-7b as the target model.
The unlearning effectiveness is evaluated by the toxic score (the lower the better) on Real Toxicity
Prompts (RTP) (Gehman et al., 2020) and PKU-SafeRLHF, and the utility preservation is measured
by the performance (the higher the better) on TruthfulQA (Lin et al., 2022). We compare our pro-
posed O3 framework with PO and SOPO as Jia et al. (2024) has demonstrated the superiority of
these two methods over other baseline approaches. According to the experiment results shown in
Table 16, we can observe that O3 framework still substantially outperforms other baselines. Note
that we also provide sufficient retained data with PO and SOPO, while our O3 does not use any
retained data.

Table 16: Performance Comparison between our O3 and other baselines on Detoxification. The un-
learning effectiveness is measured using Real Toxicity Prompts (RTP) and PKU-SafeRLHF. Utility
preservation is evaluated using TruthfulQA.
                Unlearning Request 1           Unlearning Request 2           Unlearning Request 3
 Method
          RTP↓ PKU-SafeRLHF↓ TruthfulQA↑ RTP↓ PKU-SafeRLHF↓ TruthfulQA↑ RTP↓ PKU-SafeRLHF↓ TruthfulQA↑
 PO       0.0678   0.0830       0.2521   0.0670        0.0764        0.2522   0.0604   0.0711   0.2543
 SOPO     0.0675   0.0802       0.2537   0.0625        0.0766        0.2584   0.0567   0.0705   0.2644
 Ours     0.0569   0.0605       0.2563   0.0533        0.0578        0.2622   0.0495   0.0462   0.2650




                                                   25
Published as a conference paper at ICLR 2025




F.8     U NLEARNING U NSAFE B EHAVIORS

We have conducted additional evaluations on unlearning unsafe behaviors using the WMDP bench-
mark. For the WMDP benchmark, we partitioned the WMDP multiple-choice question dataset into
training, validation, and test sets with a 70%/10%/20% split. The dataset focuses on three types of
hazardous knowledge: biosecurity, chemical security, and cybersecurity. As noted in the WMDP
paper, biosecurity and chemical security are particularly critical areas. Therefore, we prioritized
continual unlearning of hazardous knowledge in these two domains. Following SOUL, we also uti-
lized LLaMA2-7b as the target model. To evaluate our proposed O3 framework, we compared its
performance against PO and SOPO, which were identified by Jia et al. (2024) as superior to other
baseline methods. The results, summarized in the Table. 17, demonstrate that the O3 framework sig-
nificantly outperforms these baselines in forgetting hazardous knowledge. Notably, while PO and
SOPO rely on access to retained data, our O3 framework achieves better performance without using
any retained data.

Table 17: Performance Comparison between our O3 and other baselines on WMDP dataset. The
unlearning effectiveness is measured by the generation accuracy of the unlearning train data and
unlearning test data denoted as S.U. and D.U., respectively. Utility preservation is evaluated by the
generation accuracy of Retained Distribution (R.D.).
                                Unlearning Request 1 Unlearning Request 2
                      Method
                                S.U.↓ D.U.↓ R.D.↑ S.U.↓ D.U.↓ R.D.↑
                      PO        45.3 33.9        55.0          20.4 24.8      55.9
                      SOPO      30.0 26.4        52.5          24.8 21.5      57.2
                      Ours      24.6 24.8        55.3          11.5 12.8      57.2


Table 18: Performance Comparison between our O3 and other baselines when continually unlearning
TOFU-forget01, -forget05, and -forget10 in Fictitious Knowledge Generation with limited data (10
samples for each request). The unlearning effectiveness is measured by the generation accuracy of
the unlearning train data and unlearning test data denoted as S.U. and D.U., respectively. Utility
preservation is evaluated by the generation accuracy of Retained Distribution (R.D.), TOFU-Real
Authors (R.A.), and World Facts (W.F.).
               Unlearning Request 1          Unlearning Request 2          Unlearning Request 3
 Method
          S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑ S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑ S.U.↓ D.U.↓ R.D.↑ R.A.↑ W.F.↑
 PO       28.9 35.2   81.9   87.0   85.3   44.6 57.7    85.4    87.2   86.0   50.6 60.1   84.0   87.3   85.5
 SOPO     34.7 45.6   84.6   86.7   84.8   44.2 47.0    84.0    86.8   85.5   47.7 53.5   83.4   87.5   84.6
 Ours     17.9 19.2   85.7   89.0   86.3   17.5 20.4    85.7    89.0   86.3   23.7 26.6   85.0   88.9   86.3



F.9     ROBUSTNESS AGAINST TARGETED R ELEARNING ATTACKS

To experiment on the robustness of our O3 framework under targeted relearning attacks, we followed
the targeted relearning attack using the public information setting described in Hu et al. (2024).
Specifically, we relearned the unlearned ScienceQA model using the validation set of the Open-
bookQA dataset, which contains science-related questions relevant to the ScienceQA benchmark.
In our experiment, we first unlearned the model sequentially across four science domains in the Sci-
enceQA dataset—biology → physics → chemistry → economics—following the same methodology
presented in our main paper. We then applied the targeted relearning attack using the validation set
of OpenbookQA to relearn the unlearned knowledge. We evaluated the performance of PO, SOPO,
and our O3 framework before and after the relearning attack for the last unlearning requst, as shown
in the Table. 19. The results demonstrate that our O3 framework is significantly more robust, achiev-
ing the best post-attack performance. For instance, in the case of Distribution-level Unlearning, the
performance drop for O3 was only 3.7, compared to 24 and 30.3 for PO and SOPO, respectively.
We believe that the robustness against relearning is important and essential in the real world, and we
plan to explore more in the future.


                                                   26
Published as a conference paper at ICLR 2025




Table 19: Performance Comparison between our O3 and other baselines against targeted relearning
attacks.
                     Method           S.U.↓ D.U.↓ R.D.↑ CommonQA↑ OpenbookQA↑
                     PO               59.9   58.7   90.8         75.8    77.6
                     Relearned PO     86.2   82.7   91.3         76.7    78.0
                     SOPO           29.6     27.9   89.7         76.8    77.8
                     Relearned SOPO 60.9     58.2   89.4         74.4    72.0
                     Ours              9.3   14.0   91.1         78.5    80.8
                     Relearned Ours   15.5   17.7   89.6         75.0    72.6



F.10    E XPERIMENTS WITH L IMITED U NLEARNING DATA

We have conducted experiments when setting the unlearning samples of TOFU as 10 for each request
(originally 40, 200, and 400 samples for three requests, respectively), and the results (Table 18) show
that O3 can still work effectively.
Achieving unlearning for insufficient data is challenging, especially considering existing LLM un-
learning approaches all assume access to sufficient unlearning data (usually over 200). In particular,
O3 is nearly the most suitable framework for handling such scarce in LLM unlearning, thanks to the
cosine similarity score design in unlearning knowledge detection and we can combine samples of
multiple requests or conduct augmentation with paraphrase to solve data insufficiency flexibly.

F.11    U NLEARNING E FFECTIVENESS C ONCERNING O RACLE M ODEL

Maini et al. (2024) mentioned that auditing unlearning effectiveness in training LLMs from scratch
with exclusive retained data is impossible. However, we still follow a strategy in TOFU to assess
the unlearning, i.e., perform a statistical test on the outputs of two models, one is a reference model
fine-tuned only on the retained set and the other is the unlearned model. This test corresponds to a
metric based on the Truth Ratio, and the results reported in Table 20 show that our framework still
performs the best.

Table 20: Performance Comparison between our O3 and other baselines when continually unlearn-
ing TOFU-forget01, -forget05, and -forget10 in Fictitious Knowledge Generation. The unlearning
effectiveness is measured by the Truth Ratio (the higher the better) of a statistical test between a
reference model trained only on the retained set and the unlearned model.
           Method Unlearning Request 1 Unlearning Request 2 Unlearning Request 3
           PO                 0.696                      0.655                  0.623
           SOPO               0.734                      0.745                  0.768
           Ours               0.927                      0.923                  0.865



G      M ORE A BLATION S TUDY

G.1    D ETAILED A NALYSIS FOR A BLATION S TUDY IN M AIN C ONTEXT

Unlearning Knowledge Detection. We detach the contrastive entropy LCEL in O3 and use SimCLR
(‘Ours w/ SimCLR’) and MoCo (‘Ours w/ MoCo’) with the augmentation using token masking. As
for the scoring mechanism, we try using Mahalanobis Distance (dMaha in Eq. 10) and Cosine Sim-
ilarity (dCos in Eq. 11) separately, which are termed as ‘Ours w/o dCos ’ and ‘Ours w/o dMaha ’.
Besides, instead of leveraging information from all model layers, we use only the last layer (‘Ours
w/ last layer’). Moreover, two state-of-the-art OOD detection approaches: MDF (Xu et al., 2021)
and Agg (Darrin et al., 2024), are compared with ours. The experiments are conducted on ficti-
tious knowledge generation and question answering where the ID data is different unlearning sets,
and the OOD data is the retained test set and the utility sets. We report the AUROC in Table 4.
According to these results, we can observe that the full design of our OOD detector in O3 frame-
work always achieves the best AUROC. Specifically, the performance drops when using SimCLR


                                                    27
Published as a conference paper at ICLR 2025




or MoCo to fine-tune the backbone model, which implies the effectiveness of our contrastive en-
tropy loss. The AUROC differences between ‘Ours w/o dCos ’ or ‘Ours w/o dMaha ’ and ‘Ours full’
indicate the necessity of combining the Mahalanobis and Cosine Similarity distances to achieve bet-
ter discrimination. The poorer performance of ‘Ours w/ last layer’ compared with ‘Ours full’ tells
us that aggregating representations of multiple layers is essentially beneficial. Besides, our meth-
ods can also substantially exceed the state-of-the-art unsupervised OOD detection approaches with
unlabeled ID data only.
Unlearning Knowledge Optimization. In the objective of unlearning knowledge optimization
(Eq. 6), there is a factor λ balancing LCE and LOrth . We adopt 0, 0.01, 0.05, 0.1, and 0.2 for
λ to validate the importance of LOrth and the sensitivity of λ. We conduct experiments on ques-
tion answering and intent classification and report the metrics of unlearning effectiveness and utility
preservation after unlearning the last request. Table 5 illustrates that employing orthogonal loss
contributes to maintaining utility on the retained distribution and enhancing the unlearning ef-
fectiveness. For instance, as the λ increases, there is a corresponding increase in the accuracy of
retained data. Moreover, the performance in preserving utility also improves.
Soft-weighted Inference. Instead of using soft weights to load unlearning LoRA, we test
a hard weighting strategy (‘Hard-w’ in Table 6). Specifically, after calculating the hyper-
sphere boundary distance (dHt in Eq. 12) on the unlearning set DU,t , we obtain the range
[min(dHt (DU,t )), max(dHt (DU,t ))]. Then for each testing instance x, if its boundary distance
dHt (x) is within the above range, we chose to load the unlearning LoRA. Otherwise, we detach the
LoRA. In addition, we conduct a sensitivity analysis of the scaling factor ζ in Eq. 13 with a series
of values 1, 5, 50, and 100. These experiments are carried out on ScienceQA and CLINC150, and
we report the performance after the last unlearning request in Table 6. We observe that the ‘Hard-w’
method performs poorly regarding unlearning knowledge. With an increase in the scaling factor ζ,
our framework enhances its ability to unlearn knowledge more effectively. However, this increase
adversely affects the framework’s ability to maintain performance on the retained distribution and
compromises its utility preservation. To address this, our framework adopts ζ as 10, striking a
reasonable balance between effective unlearning and utility preservation.

G.2   C ONDUCTING A DVERSARIAL ATTACKS TO B YPASS U NLEARNING K NOWLEDGE
      D ETECTION

In the real-world deployment of our O3 framework, there may be a concern that malicious attackers
apply adversarial attacks (Gao et al., 2024b) to bypass unlearning knowledge detection. Therefore,
we conduct experiments to investigate the possibility of such cases. Specifically, we implement an
adversarial attack (Chen et al.) against OOD detection that injects a certain perturbation to fool the
OOD detector into identifying ID data as OOD data. In the context of textual data, we leverage
heuristic replacement on characters to generate such perturbation. The experiments on TOFU (Ta-
ble 21) show that the AUROC has no significant drop and the continual unlearning effectiveness
remains nearly unchanged. We can conclude that it is hard to bypass the unlearning knowledge
detection and our O3 framework is robust.
Table 21: Robustness investigation of applying adversarial attack to unlearning knowledge detection
in O3 framework on TOFU. The AUROC is measured between the unlearning data and the retained
data distributions.
                      Unlearning Request 1        Unlearning Request 2        Unlearning Request 3
  Method
                    S.U.↓ D.U.↓ AUROC↑          S.U.↓ D.U.↓ AUROC↑          S.U.↓ D.U.↓ AUROC↑
  Ours w/ Attack     12.5    15.0      97.5      16.4    19.8      92.5     17.4     19.5      92.2
  Ours w/o Attack    12.5    14.4      97.8      15.8    20.3      92.5     15.5     19.7      93.0


G.3   S ENSITIVITY A NALYSIS OF THE R ANK OF L O RA

We conduct experiments on the ScienceQA dataset with ranks 4, 8, 16, 32, 64, and 128, as detailed
in Table. 22. Our findings indicate that unlearning performance diminishes when the rank exceeds
64, highlighting an increase in unlearning difficulty with larger values of ranks. Conversely, higher
ranks enhance the model’s ability to preserve utility and improve performance in R.D. metrics.


                                                  28
Published as a conference paper at ICLR 2025




                          Table 22: Ablation study of the rank of LoRA on ScienceQA dataset.
                 Unlearning Request 1           Unlearning Request 2           Unlearning Request 3           Unlearning Request 4
    Method
           S.U.↓ D.U.↓ R.D.↑ C.QA↑ O.QA.↑ S.U.↓ D.U.↓ R.D.↑ C.QA↑ O.QA.↑ S.U.↓ D.U.↓ R.D.↑ C.QA↑ O.QA.↑ S.U.↓ D.U.↓ R.D.↑ C.QA↑ O.QA.↑
    r=4     4.2    7.1     88.5   78.4   80.4   0.6    3.2    87.0   78.3   80.8   10.8   13.0   88.2   78.3   81.0   14.1   15.4   87.6   78.4   81.2
    r=8     7.3    11.3    81.5   78.2   79.4   3.1    8.5    83.4   78.4   80.0   5.5    12.6   88.6   78.2   80.0   16.3   21.0   90.8   78.3   80.0
    r=16    12.4   12.3    89.2   78.5   80.8   0.4    2.9    87.3   78.6   81.6   1.9    5.1    88.3   78.8   81.2   8.3    10.6   88.5   78.9   81.2
    r=32    12.8   13.4    88.8   78.8   81.6   1.3    3.4    87.5   78.5   81.4   1.3    4.7    88.5   78.5   82.0   5.8    7.7    88.9   78.7   82.0
    r=64    12.2   11.8    88.2   78.9   81.8   4.8    7.1    88.9   78.5   82.2   12.3   16.2   90.2   78.6   82.2   15.0   18.2   90.1   78.8   82.4
    r=128   13.2   12.3    88.5   79.0   81.6   30.7   30.2   90.8   78.9   82.0   42.1   44.9   91.0   78.6   82.4   46.7   50.1   90.4   78.8   82.4




H      F UTURE W ORK

H.1     I MPROVEMENT FOR U NLEARNING K NOWLEDGE D ETECTION

A direct improvement related to unlearning knowledge detection lies in the inference stage. In the
inference phase of O3 framework, we need to feed the testing data into each OOD detector to calcu-
late the likelihood of belonging to previous unlearning distributions. In practical system deployment,
we can parallelize this process to enhance efficiency (Agrawal et al., 2024). In our implementation,
the OOD detector backbone uses the encoder-only Roberta model. Although this model can ex-
tract high-quality representations, its performance is still limited when faced with complex inputs
compared to larger-scale language models. Therefore, we consider directly using the target LLM to
detect unlearning knowledge. This approach is feasible because, in the O3 framework, we use LoRA
as an external module to achieve unlearning, and the original target LLM is available for inference.
We should gain the following benefits if we replace the OOD detector backbone with an LLM.
First, LLMs can better capture subtle text differences, improving OOD detection performance. Sec-
ond, smaller language models like Roberta cannot effectively extract contextual information from
complex and long contexts. Thus, if an unlearning request correlates the contextual information,
such as the individual users’ request to unlearn specific topics from their chat history with Chat-
GPT, Roberta-based OOD detection cannot achieve this. In contrast, LLMs can extract contextual
information well (Ding et al.), supporting more fine-grained OOD detection and more accurate ID
data localization. Finally, using LLMs for OOD detection might eliminate the need for fine-tuning
with ID data, as Uppaal et al. (2023) suggested that LLMs could provide accurate OOD detection
predictions for text classification without any fine-tuning. This could further improve our frame-
work’s efficiency. However, using LLMs for OOD detection might require dedicated improvements
to the scoring mechanism because mainstream LLMs now use a decoder-only architecture, which
works by predicting the next token. In this case, the representation output by each attention layer of
the LLM is likely to be highly inconsistent in terms of token quantity and distribution. Therefore,
whether our design based on layer-wise token average representation (Section 3.2) is suitable for
LLMs requires further research. Extending O3 to multimodal content (Gao et al., 2024c) represents
a promising direction for future research. This extension would enable the model to unlearn in-
formation from multiple modalities, such as text, audio, images, and video, thereby enhancing its
ability to handle complex real-world scenarios.

H.2     DATA S ELECTION FOR LLM U TILITY P RESERVATION

In the real world, we edit LLMs for various purposes, such as knowledge unlearning. However,
the model editing leads to uncertain and unpredictable changes in the capabilities of the LLM (Qi
et al.; Gu et al., 2024; Gupta et al., 2024). Recent studies have shown that model editing for a single
specific task can cause performance degradation in seemingly unrelated tasks. This phenomenon is
more pronounced in sequential or continual model editing (Gu et al., 2024; Gupta et al., 2024). To
address this issue, similar to leveraging a retained dataset to preserve model utility in LLM unlearn-
ing (Liu et al., 2024), the intuitive approach is to identify which tasks and data distributions are most
affected and then replay some representative data on the LLM (Gururangan et al., 2020). However,
identifying these tasks and data distributions on an LLM is extremely challenging (Chang et al.,
2024; Ortiz-Jimenez et al., 2024; Huang et al., 2024). For example, to select suitable retained data
for utility preservation in LLM unlearning, we might utilize some interpretable machine learning
(ML) techniques (Singh et al., 2024) to locate the neurons activated by the unlearning data. Based
on these identified activated neurons, we could retrieve similar data to be used as the retained data.
However, current interpretable ML techniques typically only achieve neuron localization for spe-
cific model attributes, such as adversarial robustness (Wei et al.) or differential privacy (Chen et al.,


                                                                             29
Published as a conference paper at ICLR 2025




2024). For the fine-grained tasks and data distributions corresponding to unlearning requests, neuron
localization is either inaccurate or inconsistent in granularity. Therefore, effective data selection to
preserve LLM utility during unlearning is research-worthy.

I   B ROADER I MPACT
The introduction of O3 framework for LLM unlearning is an important effort across multiple do-
mains. This is particularly beneficial in environments where continuous unlearning requests are
necessary, such as in systems dealing with dynamic privacy regulations or evolving user prefer-
ences. Furthermore, O3 ’s ability to function without retained data significantly enhances its practi-
cality, especially in sensitive areas like healthcare and finance, where maintaining access to personal
or confidential data for utility preservation is not feasible. This feature also extends to scenarios in-
volving specialized tasks with naturally scarce data, such as rare disease diagnosis or niche financial
analysis, where data availability is inherently limited.
On a broader scale, O3 ’s contributions to AI can enhance public trust in LLMs by addressing core
concerns surrounding data privacy and compliance. With more robust unlearning capabilities, orga-
nizations can ensure that sensitive information can be effectively removed from AI systems without
sacrificing performance, thereby fostering better alignment with ethical AI principles and regula-
tory requirements like GDPR (GDPR, 2018). This not only mitigates legal risks but also supports
societal expectations for data autonomy and security, ensuring that AI systems are adaptable, trans-
parent, and more responsible. By enabling more effective unlearning, O3 enhances the long-term
sustainability of AI technologies, creating a safer and more equitable digital ecosystem.




                                                   30
